{"article_id": "1111.2228", "article_text": ["in recent years , mapreduce has emerged as a computational paradigm for processing large - scale data sets in a series of rounds executed on conglomerates of commodity servers @xcite , and has been widely adopted by a number of large web companies ( e.g. , google , yahoo ! , amazon ) and in several other applications ( e.g. , gpu and multicore processing ) .", "( see @xcite and references therein . )", "informally , a mapreduce computation transforms an input set of key - value pairs into an output set of key - value pairs in a number of _ rounds _ , where in each round each pair is first individually transformed into a ( possibly empty ) set of new pairs ( _ map step _ ) and then all values associated with the same key are processed , separately for each key , by an instance of the same reduce function ( simply called _ reducer _ in the rest of the paper ) thus producing the next new set of key - value pairs ( _ reduce step _ ) .", "in fact , as already noticed in @xcite , a reduce step can clearly embed the subsequent map step so that a mapreduce computation can be simply seen as a sequence of rounds of ( augmented ) reduce steps .", "the mapreduce paradigm has a functional flavor , in that it merely requires that the algorithm designer decomposes the computation into rounds and , within each round , into independent tasks through the use of keys .", "this enables parallelism without forcing an algorithm to cater for the explicit allocation of processing resources .", "nevertheless , the paradigm implicitly posits the existence of an underlying unstructured and possibly heterogeneous parallel infrastructure , where the computation is eventually run . while mostly ignoring the details of such an underlying infrastructure , existing formalizations of the mapreduce paradigm constrain the computations to abide with some local and aggregate memory limitations .    in this paper , we look at both modeling and algorithmic issues related to the mapreduce paradigm .", "we first provide a formal specification of the model , aimed at overcoming some limitations of the previous modeling efforts , and then derive interesting tradeoffs between memory constraints and round complexity for the fundamental problem of matrix multiplication and some of its applications .", "the mapreduce paradigm has been introduced in @xcite without a fully - specified formal computational model for algorithm design and analysis .", "triggered by the popularity quickly gained by the paradigm , a number of subsequent works have dealt more rigorously with modeling and algorithmic issues  @xcite .    in @xcite , a mapreduce algorithm specifies a sequence of rounds as described in the previous section .", "somewhat arbitrarily , the authors impose that in each round the memory needed by any reducer to store and transform its input pairs has size @xmath0 , and that the aggregate memory used by all reducers has size @xmath1 , where @xmath2 denotes the input size and @xmath3 is a fixed constant in @xmath4 .", "the cost of local computation , that is , the work performed by the individual reducers , is not explicitly accounted for , but it is required to be polynomial in @xmath2 .", "the authors also postulate , again somewhat arbitrarily , that the underlying parallel infrastructure consists of @xmath5 processing elements with @xmath5 local memory each , and hint at a possible way of supporting the computational model on such infrastructure , where the reduce instances are scheduled among the available machines so to distribute the aggregate memory in a balanced fashion .", "it has to be remarked that such a distribution may hide non negligible costs for very fine - grained computations ( due to the need of allocating multiple reducer with different memory requirements to a fixed number of machines ) when , in fact , the algorithmic techniques of @xcite do not fully explore the larger power of the mapreduce model with respect to a model with fixed parallelism . in @xcite the same model of @xcite is adopted but when evaluating an algorithm the authors also consider the total work and introduce the notion of work - efficiency typical of the literature on parallel algorithms .    an alternative computational model for mapreduce is proposed in @xcite , featuring two parameters which describe bandwidth and latency characteristics of the underlying communication infrastructure , and an additional parameter that limits the amount of i / o performed by each reducer .", "also , a bsp - like cost function is provided which combines the internal work of the reducers with the communication costs incurred by the shuffling of the data needed at each round . unlike the model of @xcite , no limits are posed to the aggregate memory size .", "this implies that in principle there is no limit to the allowable parallelism while , however , the bandwidth / latency parameters must somewhat reflect the topology and , ultimately , the number of processing elements .", "thus , the model mixes the functional flavor of mapreduce with the more descriptive nature of bandwidth - latency models such as bsp @xcite .", "a model which tries to merge the spirit of mapreduce with the features of data - streaming is the mud model of @xcite , where the reducers receive their input key - value pairs as a stream to be processed in one pass using small working memory , namely polylogarithmic in the input size .", "a similar model has been adopted in @xcite .", "mapreduce algorithms for a variety of problems have been developed on the aforementioned mapreduce variants including , among others , primitives such as prefix sums , sorting , random indexing @xcite , and graph problems such as triangle counting @xcite minimum spanning tree , @xmath6-@xmath7 connectivity , @xcite , maximal and approximate maximum matching , edge cover , minimum cut @xcite , and max cover @xcite .", "moreover simulations of the pram and bsp in mapreduce have been presented in @xcite . in particular , it is shown that a @xmath8-step erew pram algorithm can be simulated by an @xmath9-round mapreduce algorithm , where each reducer uses constant - size memory and the aggregate memory is proportional to the amount of shared memory required by the pram algorithm @xcite .", "the simulation of crew or crcw pram algorithms incurs a further @xmath10 slowdown , where @xmath11 denotes the local memory size available for each reducer and @xmath12 the aggregate memory size @xcite .", "all of the aforementioned algorithmic efforts have been aimed at achieving the minimum number of rounds , possibly constant , provided that enough local memory for the reducer ( typically , sublinear yet polynomial in the input size ) and enough aggregate memory is available .", "however , so far , to the best of our knowledge , there has been no attempt to fully explore the tradeoffs that can be exhibited for specific computational problems between the local and aggregate memory sizes , on one side , and the number of rounds , on the other , under reasonable constraints of the amount of total work performed by the algorithm .", "our results contribute to filling this gap .", "matrix multiplication is a building block for many problems , including matching  @xcite , matrix inversion  @xcite , all - pairs shortest path  @xcite , graph contraction  @xcite , cycle detection  @xcite , and parsing context free languages  @xcite .", "parallel algorithms for matrix multiplication of dense matrices have been widely studied : among others , we remind @xcite which provide upper and lower bounds exposing a tradeoff between communication complexity and processor memory . for sparse matrices ,", "interesting results are given in @xcite for some network topologies like hypercubes , in @xcite for pram , and in @xcite for a bsp - like model .", "in particular , techniques in @xcite are used in the following sections for deriving efficient mapreduce algorithms . in the sequential settings ,", "some interesting works providing upper and lower bounds are @xcite for dense matrix multiplication , and @xcite for sparse matrix multiplication .", "the contribution of this paper is twofold , since it targets both modeling and algorithmic issues .", "we first formally specify a computational model for mapreduce which captures the functional flavor of the paradigm by allowing a flexible use of parallelism .", "more specifically , our model generalizes the one proposed in @xcite by letting the local and aggregate memory sizes be two independent parameters , @xmath11 and @xmath12 , respectively .", "moreover our model makes no assumption on the underlying execution infrastructure , for instance it does not impose a bound on the number of available machines , thus fully decoupling the degree of parallelism exposed by a computation from the one of the machine where the computation will be eventually executed .", "this decoupling greatly simplifies algorithm design , which has been one of the original objectives of the mapreduce paradigm .", "( in section  [ sec : preliminary ] , we quantify the cost of implementing a round of our model on a system with fixed parallelism . )", "our algorithmic contributions concern the study of attainable tradeoffs in mapreduce for several variants of the fundamental primitive of matrix multiplication .", "in particular , building on the well - established three - dimensional algorithmic strategy for matrix multiplication @xcite , we develop upper and lower bounds for dense - dense matrix multiplication and provide similar bounds for deterministic and/or randomized algorithms for sparse - sparse and sparse - dense matrix multiplication .", "the algorithms are parametric in the local and aggregate memory constraints and achieve optimal or quasi - optimal round complexity in the entire range of variability of such parameters .", "finally , building on the matrix multiplication results , we derive similar space - round tradeoffs for matrix inversion and matching , which are important by - products of matrix multiplication .", "the rest of the paper is structured as follows . in section  [ sec : preliminary ] we introduce our computational model for mapreduce and describe important algorithmic primitives ( sorting and prefix sums ) that we use in our algorithms .", "section  [ sec : intromatrix ] deals with matrix multiplication in our model , presenting theoretical bounds to the complexity of algorithms to solve this problem .", "we apply these results in section  [ sec : applications ] to derive algorithms for matrix inversion and for matching in graphs .", "our model is defined in terms of two integral parameters @xmath12 and @xmath11 , whose meaning will be explained below , and is named .", "algorithms specified in this model will be referred to as _ mr - algorithms_. an mr - algorithm specifies a sequence of _ rounds _ : the @xmath13-th round , with @xmath14 transforms a multiset @xmath15 of key - value pairs into two multisets @xmath16 and @xmath17 of key - value pairs , where @xmath16 is the input of the next round ( empty , if @xmath13 is the last round ) , and @xmath17 is a ( possibly empty ) subset of the final output .", "the input of the algorithm is represented by @xmath18 while the output is represented by @xmath19 , with @xmath20 denoting the union of multisets .", "the universes of keys and values may vary at each round , and we let @xmath21 denote the universe of keys of @xmath15 . the computation performed by round @xmath13", "is defined by a _", "reducer _ function @xmath22 which is applied independently to each multiset @xmath23 consisting of all entries in @xmath15 with key @xmath24 .", "let @xmath2 be the input size .", "the two parameters @xmath12 and @xmath11 specify the memory requirements that each round of an mr - algorithm must satisfy . in particular ,", "let @xmath25 denote the space needed to compute @xmath26 on a ram with @xmath27-bit words , including the space taken by the input ( i.e. , latexmath:[$m_{r , k } \\geq    output , which contributes either to @xmath17 ( i.e. , the final output ) or to @xmath16 .", "the model imposes that @xmath29 , for every @xmath14 and @xmath24 , that @xmath30 , for every @xmath14 , and that @xmath31 . the complexity of an mr - algorithm is the number of rounds that it executes in the worst case , and it is expressed as a function of the input size @xmath2 and of parameters @xmath11 and @xmath12 . the dependency on the parameters @xmath11 and @xmath12 allows for a finer analysis of the cost of an mr - algorithm .", "as in @xcite , we require that each reducer function runs in time polynomial in @xmath2 .", "in fact , it can be easily seen that the model defined in @xcite is equivalent to the  model with @xmath32 and @xmath33 , for some fixed constant @xmath34 , except that we eliminate the additional restrictions that the number of rounds of an algorithm be polylogarithmic in @xmath2 and that the number of physical machines on which algorithms are executed are @xmath35 , which in our opinion should not be posed at the model level .", "compared to the model in @xcite , our  model introduces the parameter @xmath12 which limits the size of the aggregate memory required at each round , whereas in @xcite this size is virtually unbounded .", "moreover , the complexity analysis in  focuses on the tradeoffs between @xmath11 and @xmath12 , on one side , and the number of rounds on the other side , while in @xcite a more complex cost function is defined which accounts for the overall message complexity of each round , the time complexity of each reducer computation , and the latency and bandwidth characteristics of the executing platform .", "sorting and prefix sum primitives are used in the algorithms presented in this paper .", "the input to both primitives consists of a set of @xmath2 key - value pairs @xmath36 with @xmath37 and @xmath38 , where @xmath39 denotes a suitable set . for sorting ,", "a total order is defined over @xmath39 and the output is a set of @xmath2 key - value pairs @xmath40 , where the @xmath41 s form a permutation of the @xmath42 s and @xmath43 for each @xmath44 . for prefix sums , a binary associative operation @xmath45 is defined over @xmath39 and the output consists of a collection of @xmath2 pairs @xmath40 where @xmath46 , for @xmath47 .", "by straightforwardly adapting the results in @xcite to our model we have :    [ prefixsorting ] the sorting and prefix sum primitives for inputs of size @xmath2 can be implemented in  with round complexity @xmath48 for @xmath49 .", "we remark that the each reducer in the implementation of the sorting and prefix primitives makes use of @xmath50 memory words .", "hence , the same round complexity can be achieved in a more restrictive scenario with fixed parallelism . in fact", ", our  model can be simulated on a platform with @xmath51 processing elements , each with internal memory of size @xmath50 , at the additional cost of one prefix computation per round .", "therefore , @xmath52 can be regarded as an upper bound on the relative power of our model with respect to one with fixed parallelism .", "goodrich  @xcite claims that the round complexities stated in theorem  [ prefixsorting ] are optimal for any @xmath53 as a consequence of the lower bound for computing the or of @xmath2 bits on the bsp model  @xcite .", "it can be shown that the optimality carries through to our model where the output of a reducer is not bounded by @xmath11 .", "let @xmath54 and @xmath55 be two @xmath56 matrices and let @xmath57 .", "we use @xmath58 and @xmath59 , with @xmath60 , to denote the entries of @xmath61 and @xmath62 , respectively . in this section", "we present upper and lower bounds for computing the product @xmath62 in .", "the algorithms we present envision the matrices as conceptually divided into submatrices of size @xmath63 , and we denote these matrices with @xmath64 , @xmath65 and @xmath66 , respectively , for @xmath67 . clearly , @xmath68 .", "all our algorithms exploit the following partition of the @xmath69 products between submatrices ( e.g. , @xmath70 ) into @xmath71 _ groups _ : group @xmath72 , with @xmath73 , consists of products @xmath74 , for every @xmath67 and for @xmath75 .", "observe that each submatrix of @xmath54 and @xmath55 occurs exactly once in each group @xmath72 .", "we focus our attention on matrices whose entries belong to a semiring @xmath76 such that for any @xmath77 we have @xmath78 , where @xmath79 is the identity for @xmath45 . in this", "setting , efficient matrix multiplication techniques such as strassen s can not be employed .", "moreover , we assume that the inner products of any row of @xmath54 and of any column of @xmath55 with overlapping nonzero entries never cancel to zero , which is a reasonable assumption when computing over natural numbers or over real numbers with a finite numerical precision .    in our algorithms , any input matrix @xmath80 ( @xmath81 ) is provided as a set of key - value pairs @xmath82 for all elements @xmath83 .", "key @xmath84 represents a progressive index , e.g. , the number of nonzero entries preceding @xmath85 in the row - major scan of @xmath80 .", "we call a @xmath86 matrix _ dense _ if the number of its nonzero entries is @xmath87 , and we call it _ sparse _ otherwise . we suppose that @xmath12 is sufficiently large to contain the input and output matrices .", "in what follows , we present different algorithms tailored for the multiplication of dense - dense ( section  [ sec : ddmult ] ) , sparse - sparse ( section  [ sec : ssmult ] ) , and sparse - dense matrices ( section  [ sec : sdmult ] ) .", "we also derive lower bounds which demonstrate that our algorithms are either optimal or close to optimal ( section  [ sec : lb ] ) , and an algorithm for estimating the number of nonzero entries in the product of two sparse matrices ( section  [ sec : evaluation ] ) .      in this section", "we provide a simple , deterministic algorithm for multiplying two dense matrices , which will be proved optimal in subsection  [ sec : lb ] .", "the algorithm is a straightforward adaptation of the well - established three - dimensional algorithmic strategy for matrix multiplication of @xcite , however we describe a few details of its implementation in  since the strategy is also at the base of algorithms for sparse matrices .", "we may assume that @xmath88 , since otherwise matrix multiplication can be executed by a trivial sequential algorithm .", "we consider matrices @xmath54 and @xmath55 as decomposed into @xmath89 submatrices and subdivide the products between submatrices into groups as described above .    in each round", ", the algorithm computes all products within @xmath90 consecutive groups , namely , at round @xmath91 , all multiplications in @xmath72 are computed , with @xmath92 .", "the idea is that in a round all submatrices of @xmath54 and @xmath55 can be replicated @xmath93 times and paired in such a way that each reducer performs a distinct multiplication in @xmath94 .", "then , each reducer sums the newly computed product to a partial sum which accumulates all of the products contributing to the same submatrix of @xmath62 belonging to groups with the same index modulo @xmath93 dealt with in previous rounds . at the end of the @xmath95-th round", ", all submatrix products have been computed .", "the final matrix @xmath62 is then obtained by adding together the @xmath93 partial sums contributing to each entry of @xmath62 through a prefix computation .", "we have the following result .", "[ th : upddmult ] the above -algorithm multiplies two @xmath96 dense matrices in @xmath97 rounds .", "the algorithm clearly complies with the memory constraints of  since each reducer multiplies two @xmath98 submatrices and the degree of replication is such that the algorithm never exceeds the aggregate memory bound of @xmath12 . also , the @xmath69 products are computed in @xmath99 rounds , while the final prefix computation requires @xmath100 rounds    we remark that the multiplication of two @xmath96 dense matrices can be performed in a constant number of rounds whenever @xmath101 , for constant @xmath102 , and @xmath103 .", "consider two @xmath86 sparse matrices @xmath54 and @xmath55 and denote with @xmath104 the maximum number of nonzero entries in any of the two matrices , and with @xmath105 the number of nonzero entries in the product @xmath106 .", "below , we present two deterministic mr - algorithms ( d1 and d2 ) and a randomized one ( r1 ) , each of which turns out to be more efficient than the others for suitable ranges of parameters .", "we consider only the case @xmath107 , since otherwise matrix multiplication can be executed by a trivial one - round mr - algorithm using only one reducer .", "we also assume that the value @xmath108 is provided in input .", "( if this were not the case , such a value could be computed with a simple prefix computation in @xmath52 rounds , which does not affect the asymptotic complexity of our algorithms . )", "however , we do not assume that @xmath105 is known in advance since , unlike @xmath108 , this value can not be easily computed .", "in fact , the only source of randomization in algorithm r1 stems from the need to estimate @xmath105 .", "this algorithm is based on the following strategy adapted from @xcite . for @xmath109 ,", "let @xmath42 ( resp . ,", "@xmath41 ) be the number of nonzero entries in the @xmath110th column of @xmath54 ( resp .", ", @xmath110th row of @xmath55 ) , and let @xmath111 be the set containing all nonzero entries in the @xmath110th column of @xmath54 and in the @xmath110th row of @xmath55 .", "it is easily seen that all of the @xmath112 products between entries in @xmath111 ( one from @xmath54 and one from @xmath55 ) must be computed .", "the algorithm performs a sequence of _ phases _ as follows .", "suppose that at the beginning of phase  @xmath7 , with @xmath113 , all products between entries in @xmath111 , for each @xmath114 and for a suitable value @xmath13 ( initially , @xmath115 ) , have been computed and added to the appropriate entries of @xmath62 . through a prefix computation ,", "phase  @xmath7 computes the largest @xmath116 such that @xmath117 .", "then , all products between entries in @xmath118 , for every @xmath119 , are computed using one reducer with constant memory for each such product .", "the products are then added to the appropriate entries of @xmath62 using again a prefix computation .", "algorithm d1 multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries each in @xmath120 rounds , on an .", "the correctness is trivial and the memory constraints imposed by the model are satisfied since in each phase at most @xmath12 elementary products are performed .", "the theorem follows by observing that the maximum number of elementary products is @xmath121 and that two consecutive phases compute at least @xmath12 elementary products in @xmath122 rounds .", "the algorithm exploits the same three - dimensional algorithmic strategy used in the dense - dense case and consists of a sequence of phases . in phase @xmath7 , @xmath113 ,", "all @xmath63-size products within @xmath116 consecutive groups are performed in parallel , where @xmath116 is a phase - specific value . observe that the computation of all products within a group @xmath72 requires space @xmath123 $ ] , since each submatrix of @xmath54 and @xmath55 occurs only once in @xmath72 and each submatrix product contributes to a distinct submatrix of @xmath62 .", "however , the value @xmath124 can be determined in @xmath125 space and @xmath52 rounds by `` simulating '' the execution of the products in @xmath72 ( without producing the output values ) and adding up the numbers of nonzero entries contributed by each product to the output matrix .", "the value @xmath116 is determined as follows .", "suppose that , at the beginning of phase @xmath7 , groups @xmath72 have been processed , for each @xmath126 and for a suitable value @xmath13 ( initially , @xmath115 ) .", "the algorithm replicates the input matrices @xmath127 times .", "subsequently , through sorting and prefix computations the algorithm computes @xmath128 for each @xmath129 and determines the largest @xmath130 such that @xmath131 .", "then , the actual products in @xmath132 , for each @xmath133 are executed and accumulated ( again using a prefix computation ) in the output matrix @xmath62 .", "we have the following theorem .", "algorithm @xmath134 multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries each in @xmath135 rounds on an , where @xmath105 denotes the maximum number of nonzero entries in the output matrix .", "the correctness of the algorithm is trivial .", "phase @xmath7 requires a constant number of sorting and prefix computations to determine @xmath116 and to add the partial contributions to the output matrix @xmath62 .", "since each value @xmath124 is @xmath136 and the groups are @xmath71 , clearly , @xmath137 , and the theorem follows .", "we remark that the value @xmath105 appearing in the stated round complexity needs not be explicitly provided in input to the algorithm .", "we also observe that with respect to algorithm d1 , algorithm d2 features a better exploitation of the local memories available to the individual reducers , which compute @xmath89-size products rather than working at the granularity of the single entries .", "by suitably combining algorithms d1 and d2 , we can get the following result .", "[ d12round ] there is a deterministic algorithm which multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries each in @xmath138 rounds on an , where @xmath105 denotes the maximum number of nonzero entries in the output matrix .", "algorithm d2 requires @xmath122 rounds in each phase @xmath7 for computing the number @xmath116 of groups to be processed .", "however , if @xmath105 were known , we could avoid the computation of @xmath116 and resort to the fixed-@xmath93 strategy adopted in the dense - dense case , by processing @xmath139 consecutive groups per round .", "this would yield an overall @xmath140 round complexity , where the @xmath141 additive term accounts for the complexity of summing up , at the end , the @xmath93 contributions to each entry of @xmath62 .", "however , @xmath105 may not be known a priori . in this case , using the strategy described in section  [ sec : evaluation ] we can compute a value @xmath142 which is a 1/2-approximation to @xmath105 with probability at least @xmath143 .", "( we say that @xmath142 @xmath144-approximates @xmath105 if @xmath145 . ) hence , in the algorithm we can plug in @xmath146 as an upper bound to @xmath105 . by using the result of theorem  [ otilde ] with @xmath147 and @xmath148", ", we have :    [ r1round ] let @xmath149 .", "algorithm r1 multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries in @xmath150 rounds on an , with probability at least @xmath143 .    by comparing the rounds complexities stated in corollary  [ d12round ] and theorem  [ r1round ] ,", "it is easily seen that the randomized algorithm r1 outperforms the deterministic strategies when @xmath151 , for any constant @xmath3 , @xmath152 , and @xmath153 . for a concrete example , r1 exhibits better performance when @xmath154 , @xmath155 , and @xmath11 is polylogarithmic in @xmath12 .", "moreover , both the deterministic and randomized strategies can achieve a constant round complexity for suitable values of the memory parameters .", "observe that a @xmath156-approximation to @xmath105 derives from the following simple argument .", "let @xmath42 and @xmath41 be the number of nonzero entries in the @xmath110th column of @xmath54 and in the @xmath110th row of @xmath55 respectively , for each @xmath157 . then , @xmath158 .", "evaluating the sum requires @xmath159 sorting and prefix computations , hence a @xmath156-approximation of @xmath105 can be computed in @xmath160 rounds .", "however , such an approximation is too weak for our purposes and we show below how to achieve a tighter approximation by adapting a strategy born in the realm of streaming algorithms .", "let @xmath161 and @xmath162 be two arbitrary values .", "an @xmath144-approximation to @xmath105 can be derived by adapting the algorithm of  @xcite for counting distinct elements in a stream @xmath163 , whose entries are in the domain @xmath164=\\{0,\\ldots , n-1\\}$ ] .", "the algorithm of  @xcite makes use of a very compact data structure , customarily called _ sketch _ in the literature , which consists of @xmath165 lists , @xmath166 .", "for @xmath167 , @xmath168 contains the @xmath169 distinct smallest values of the set @xmath170 , where @xmath171\\rightarrow [ n^3]$ ] is a hash function picked from a pairwise independent family .", "it is shown in @xcite that the median of the values @xmath172 , where @xmath173 denotes the @xmath7th smallest value in @xmath168 , is an @xmath144-approximation to the number of distinct elements in the stream , with probability at least @xmath174 . in order to compute an @xmath144-approximation of @xmath105 for a product @xmath175 of @xmath176 matrices , we can modify the algorithm as follows .", "consider the stream of values in @xmath164 $ ] where each element of the stream corresponds to a distinct product @xmath177 and consists of the value @xmath178 .", "clearly , the number of distinct elements in this stream is exactly @xmath105 .", "( a similar approach has been used in @xcite in the realm of sparse boolean matrix products . )", "we now show how to implement this idea on an .", "the mr - algorithm is based on the crucial observation that if the stream of values defined above is partitioned into segments , the sketch for the entire stream can be obtained by combining the sketches computed for the individual segments . specifically , two sketches are combined by merging each pair of lists with the same index and selecting the @xmath7 smallest values in the merged list .", "the -algorithm consists of a number of phases , where each phase , except for the last one , produces set of @xmath179 sketches , while the last phase combines the last batch of @xmath179 sketches into the final sketch , and outputs the approximation to @xmath105 .", "we refer to the partition of the matrices into @xmath180 submatrices and group the products of submatrices as done before . in phase", "@xmath7 , with @xmath181 , the algorithm processes the products in @xmath182 consecutive groups , assigning each pair of submatrices in one of the @xmath93 groups to a distinct reducer .", "a reducer receiving @xmath183 and @xmath184 , each with at least a nonzero entry , either computes a sketch for the stream segment of the nonzero products between entries of @xmath183 and @xmath184 , if the total number of nonzero entries of @xmath183 and @xmath184 exceeds the size of the sketch , namely @xmath185 words , or otherwise leaves the two submatrices untouched ( observe that in neither case the actual product of the two submatrices is computed ) . in this latter case , we refer to the pair of ( very sparse ) submatrices as a _", "pseudosketch_. at this point , the sketches produced by the previous phase ( if @xmath186 ) , together with the sketches and pseudosketches produced in the current phase are randomly assigned to @xmath179 reducers .", "each of these reducers can now produce a single sketch from its assigned pseudosketches ( if any ) and merge it with all other sketches that were assigned to it . in the last phase ( @xmath187 ) the @xmath179 sketches are combined into the final one through a prefix computation , and the approximation to @xmath105", "is computed .", "[ otilde ] let @xmath188 and let @xmath189 and @xmath190 be arbitrary values .", "then , with probability at least @xmath191 , the above algorithm computes an @xmath144-approximation to @xmath105 in @xmath192 rounds , on an    the correctness of the algorithm follows from the results of @xcite and the above discussion .", "recall that the value computed by the algorithm is an @xmath144-approximation to @xmath105 with probability @xmath174 . as for the rounds complexity", "we observe that each phase , except for the last one , requires a constant number of rounds , while the last one involves a prefix computation thus requiring @xmath122 rounds .", "we only have to make sure that in each phase the memory constraints are satisfied ( with high probability ) .", "note also that a sketch of size @xmath193 is generated either in the presence of a pair of submatrices @xmath183 , @xmath184 containing at least @xmath194 entries , or within one of the @xmath179 reducers . by the choice of @xmath93 , it is easy to see that in any case , the overall memory occupied by the sketches is @xmath195 . as for the constraint on local memories , a simple modification of the standard balls - into - bins argument @xcite and the union bound suffices to show that with probability @xmath174 , in every phase when sketches and pseudosketches are assigned to @xmath179 reducers , each reducer receives in @xmath196 words .", "the theorem follows .", "( more details will be provided in the full version of the paper . )", "let @xmath54 be a sparse @xmath56 matrix with at most @xmath108 nonzero entries and let @xmath55 be a dense @xmath56 matrix ( the symmetric case , where @xmath54 is dense and @xmath55 sparse , is equivalent ) .", "the algorithm for dense - dense matrix multiplication does not exploit the sparsity of @xmath54 and requires @xmath197 rounds .", "also , if we simply plug @xmath198 in the complexities of the three algorithms for the sparse - sparse case ( where @xmath108 represented the maximum number of nonzero entries of @xmath54 or @xmath55 ) we do not achieve a better round complexity .", "however , a careful analysis of algorithm d1 in the sparse - dense case reveals that its round complexity is @xmath199 .", "therefore , by interleaving algorithm d1 and the dense - dense algorithm we have the following corollary .", "the multiplication on  of a sparse @xmath56 matrix with at most @xmath108 nonzero entries and of a dense @xmath56 matrix requires a number of rounds which is the minimum between @xmath200 and @xmath201 .", "observe that the above sparse - dense strategy outperforms all previous algorithms for instance when @xmath202 .", "in this section we provide lower bounds for dense - dense and sparse - sparse matrix multiplication .", "we restrict our attention to algorithms which perform all nonzero elementary products , that is , on _ conventional _ matrix multiplication @xcite .", "although this assumption limits the class of algorithms , ruling out strassen - like techniques , an elaboration of a result in  @xcite shows that computing all nonzero elementary products is necessary when entries of the input matrices are from the semirings @xmath203 and @xmath204 .", "semiring , where @xmath205 is the identity of the @xmath206 operation , is usually adopted while computing the shortest path matrix of a graph given its connection matrix .", "] indeed , we have the following lemma which provides a lower bound on the number of products required by an algorithm multiplying any two matrices of size @xmath56 , containing @xmath207 and @xmath208 nonzero entries and where zero entries have fixed positions ( a similar lemma holds for @xmath209 ) . as a consequence of the lemma , an algorithm that multiplies any two arbitrary matrices in the semiring", "@xmath203 must perform all nonzero products .", "consider an algorithm @xmath210 which multiples two @xmath56 matrices @xmath54 and @xmath55 with @xmath207 and @xmath208 nonzero entries , respectively , from the semiring @xmath211 and where the positions of zero entries are fixed .", "then , algorithm @xmath210 must perform all the nonzero elementary products .", "@xcite shows that each @xmath59 can be computed only by summing all terms @xmath212 , with @xmath213 , if the algorithm uses only semiring operations .", "the proof relies on the analysis of the output for some suitable input matrices , and makes some assumptions that force the algorithm to compute even zero products .", "however , the result still holds if we allow all the zero products to be ignored , but some adjustments are required . in particular , the input matrices used in @xcite do not work in our scenario because may contain less than @xmath207 and @xmath208 nonzero entries , however it is easy to find inputs with the same properties working in our case .", "more details will be provided in the full version .", "the following theorem exhibits a tradeoff in the lower bound between the amount of local and aggregate memory and the round complexity of an algorithm performing conventional matrix multiplication .", "the proof is similar to the one proposed in  @xcite for lower bounding the communication complexity of dense - dense matrix multiplication in a bsp - like model : however , differences arise since we focus on round complexity and our model does not assume the outdegree of a reducer to be bounded . in the proof of the theorem we use the following lemma which was proved using the red - blue pebbling game in  @xcite and then restated in  @xcite as follows .", "[ lem : nummult ] consider the conventional matrix multiplication @xmath214 , where @xmath54 and @xmath55 are two arbitrary matrices .", "a processor that uses @xmath215 elements of @xmath54 , @xmath216 elements of @xmath55 , and contributes to @xmath217 elements of @xmath62 can compute at most @xmath218 multiplication terms .", "[ th : lbddmult ] consider an -algorithm @xmath210 for multiplying two @xmath56 matrices containing at most @xmath207 and @xmath208 nonzero entries , using conventional matrix multiplications .", "let @xmath219 and @xmath105 denote the number of nonzero elementary products and the number of nonzero entries in the output matrix , respectively .", "then , the round complexity of @xmath210 is @xmath220    let @xmath210 be an @xmath221-round -algorithm computing @xmath214 .", "we prove that @xmath222 .", "consider the @xmath13-th round , with @xmath223 , and let @xmath224 be an arbitrary key in @xmath21 and @xmath225 .", "we denote with @xmath226 the space taken by the output of @xmath26 which contributes either to @xmath17 or to @xmath16 , and with @xmath25 the space needed to compute @xmath26 including the input and working space but excluding the output . clearly , @xmath227 , @xmath228 , and @xmath229 .", "suppose @xmath230 . by lemma  [ lem : nummult ]", ", the reducer @xmath22 with input @xmath231 can compute at most @xmath232 elementary products since @xmath233 and @xmath234 , where @xmath215 and @xmath216 denote the entries of @xmath54 and @xmath55 used in @xmath26 and @xmath217 the entries of @xmath62 for which contributions are computed by @xmath26 .", "then , the number of terms computed in the @xmath13-th round is at most @xmath235 since @xmath236 and the summation is maximized when @xmath237 for each @xmath238 .", "suppose now that @xmath239 .", "partition the keys in @xmath21 into @xmath240 sets @xmath241 such that @xmath242 for each @xmath243 ( the lower bound may be not satisfied for @xmath244 ) . clearly , @xmath245 . by lemma  [ lem : nummult ] ,", "the number of elementary products computed by all the reducers @xmath26 with keys in a set @xmath246 is at most @xmath247 . since @xmath248 for each non negative assignment of the @xmath249 variables and since @xmath250", ", it follows that at most @xmath251 elementary products can be computed using keys in @xmath246 , where @xmath252 .", "therefore , the number of elementary products computed in the @xmath13-th round is at most @xmath253 since @xmath254 and the sum is maximized when @xmath255 for each @xmath243 .", "therefore , in each round @xmath256 nonzero elementary products can be computed , and then @xmath257 . the second term of the lower bound follows since there is at least one entry of @xmath62 given by the sum of @xmath258 nonzero elementary products .", "we now specialize the above lower bound for algorithms for generic dense - dense and sparse - sparse matrix multiplication .", "an -algorithm for multiplying any two dense @xmath86 matrices , using conventional matrix multiplication , requires @xmath259 rounds . on the other hand ,", "an -algorithm for multiplying any two sparse matrices with at most @xmath108 nonzero entries requires @xmath260 rounds .    in the dense - dense case", "the lower bound follows by the above theorem  [ th : lbddmult ] since we have @xmath261 and @xmath262 when @xmath263 . in the sparse - sparse case ,", "we set @xmath264 and we observe that there exist assignments of the input matrices for which @xmath265 , and others where @xmath266    the deterministic algorithms for matrix multiplication provided in this section perform conventional matrix multiplication , and hence the above corollary applies .", "thus , the algorithm for dense - dense matrix multiplication described in section  [ sec : ddmult ] is optimal for any value of the parameters . on the other hand , the deterministic algorithm d2 for sparse - sparse matrix multiplication given in section  [ sec : dssmult ] is optimal as soon as @xmath267 , @xmath268 and @xmath11 is polynomial in @xmath12 .", "our matrix multiplications results can be used to derive efficient algorithms for inverting a square matrix and for solving several variants of the matching problem in a graph .", "the algorithms in this section make use of division and exponentiation . to avoid the intricacies of dealing with limited precision , we assume each memory word is able to store any value that occurs in the computation .", "a similar assumption is made in the presentation of algorithms for the same problems on other parallel models ( see e.g. @xcite ) .      in this section", "we study the problem of inverting a lower triangular matrix @xmath54 of size @xmath269 .", "we adopt the simple recursive algorithm which leverages on the easy formula for inverting a @xmath270 lower triangular matrix  ( * ? ? ?", "@xmath271^{-1 } = \\left [ \\begin{array}{cc } a^{-1 } & 0 \\\\ -c^{-1}ba^{-1 }   & c^{-1 } \\end{array } \\right].\\ ] ] for @xmath272 and @xmath273 , let @xmath274 be the @xmath275 submatrix resulting from the splitting of @xmath54 into submatrices of size @xmath276 . since equation   holds even when @xmath277 are matrices , we have that @xmath278 can be expressed as in equation   in figure  [ fig : prodmatrinv ] .", "note that @xmath279 .", "@xmath280 , 0\\leq i\\leq 2^k-1.\\ ] ]    the -algorithm for computing the inverse of @xmath54 works in @xmath281 phases .", "let @xmath282 for @xmath283 .", "in the first part of phase @xmath79 , the inverses of all the lower triangular submatrices @xmath284 , with @xmath285 , are computed in parallel .", "since each submatrix has size @xmath98 , each inverse can be computed sequentially within a single reducer . in the second part of phase @xmath79 , each product @xmath286 for @xmath287 ,", "is computed within a reducer .    in phase @xmath13 , with @xmath288 , each term @xmath289 for @xmath290 , is computed in parallel by performing two matrix multiplications using @xmath291 aggregate memory and local size @xmath11 .", "therefore , at the end of phase @xmath292 we have all the components of @xmath293 , i.e. , of @xmath294 .", "[ thm : trianmatrinv ] the above algorithm computes the inverse of a nonsingular lower triangular @xmath269 matrix @xmath54 in @xmath295 rounds on an .    the correctness of the algorithm follows from the correctness of   which in turns easily follows from the correctness of the formula to invert a lower triangular @xmath296 matrix . from the above discussion", "it easy to see that the memory requirements are all satisfied .", "we now analyze the round complexity of the algorithm . at phase @xmath13", "we have to compute @xmath297 products between matrices of size @xmath298 each product is computed in parallel by using @xmath299 aggregate memory and thus each phase @xmath13 requires @xmath300 rounds by using the algorithm described in section  [ sec : ddmult ] .", "the cost of the lower triangular matrix inversion algorithm is then @xmath301 which gives the bound stated in the theorem .", "if @xmath302 is @xmath303 and @xmath101 for some constant @xmath3 , the complexity reduces to @xmath27 rounds , which is a logarithmic factor better than what could be obtained by simulating the pram algorithm .", "it is also possible to compute @xmath294 using the closed formula derived by unrolling a blocked forward substitution . in general", ", the closed formula contains an exponential number of terms .", "there are nonetheless special cases of matrices for which a large number of terms in the sum are zero and only a polynomial number of terms is left .", "this is , for instance , the case for triangular band matrices .", "( note that the inverse of a triangular band matrix is triangular but not necessarily a triangular band matrix . )", "if the width of the band is @xmath304 , then we have a polynomial number of terms in the formula . in this case", "we can do matrix inversion in constant rounds for sufficiently large values of @xmath11 and @xmath12 .", "a complete discussion of this method will be presented in the full version of the paper .", "building on the inversion algorithm for triangular matrices presented in the previous subsection , and on the dense - dense matrix multiplication algorithm , in this section we develop an -algorithm to invert a general @xmath269 matrix @xmath54 .", "let the trace @xmath305 of @xmath54 be defined as @xmath306 , where @xmath307 denotes the entry of @xmath54 on the @xmath110-th row and @xmath110-th column .", "the algorithm is based on the following known strategy ( see e.g. , ( * ? ? ?", "8.8 ) ) .    1 .", "compute the powers @xmath308.[step : inv1 ] 2 .", "compute the traces @xmath309 , for @xmath310.[step : inv2 ] 3 .", "compute the coefficients @xmath311 of the characteristic polynomial of @xmath54 by solving a lower triangular system of @xmath156 linear equations involving the traces @xmath312 ( the system is shown below).[step : inv3 ] 4 .", "compute @xmath313[step : inv4 ]    we now provide more details on the mr implementation of above strategy .", "the algorithm requires @xmath314 , which ensures that enough aggregate memory is available to store all the @xmath156 powers of @xmath54 . in step  [ step : inv1 ]", ", the algorithm computes naively the powers in the form @xmath315 , @xmath316 , by performing a sequence of @xmath317 matrix multiplications using the algorithm in section  [ sec : ddmult ] .", "then , each one of the remaining powers is computed using @xmath318 aggregate memory and by performing a sequence of at most @xmath319 multiplications of the matrices @xmath315 obtained earlier . in step  [", "step : inv2 ] , the @xmath156 traces @xmath312 are computed in parallel using a prefix like computation , while the coefficients @xmath311 of the characteristic polynomial are computed in step  [ step : inv3 ] by solving the following lower triangular system : @xmath320 \\left [ \\begin{array}{c }    c_{n-1 } \\\\", "c_{n-1 } \\\\", "c_{n-3 } \\\\    \\vdots", "\\\\    c_0 \\end{array } \\right ] = - \\left [ \\begin{array}{c }    s_1 \\\\    s_2 \\\\    s_3 \\\\", "\\vdots \\\\    s_n \\end{array } \\right].\\ ] ] if we denote with @xmath321 the matrix on the left hand side , with @xmath62 the vector of unknowns , and with @xmath39 the vector of the traces on the right hand side , we have @xmath322 . in order to compute the coefficients in @xmath62 the algorithm inverts the @xmath269 lower triangular matrix @xmath321 as described in section  [ sec : lowertrianinv ] , and computes the product between @xmath323 and @xmath39 , to obtain @xmath62 . finally , step  [ step : inv4 ] requires a prefix like computation .", "we have the following theorem .", "[ thm : genmatrinv ] the above algorithm computes the inverse of any nonsingular @xmath269 matrix @xmath54 in @xmath324 rounds on , with @xmath314 .    for the correctness of the algorithm see ( * ? ? ?", "it is easy to check that the memory requirements of the  model are satisfied .", "we focus here on analyzing the round complexity .", "computing the powers in the form @xmath315 , @xmath316 requires @xmath325 rounds , since the algorithm performs a sequence of @xmath317 products .", "the remaining powers are computed in @xmath326 rounds since each power is computed by performing at most @xmath317 product using @xmath327 aggregate memory . the prefix like computation for finding the @xmath156 traces @xmath312 requires @xmath52 rounds , while the linear system takes @xmath328 rounds .", "the final step takes @xmath52 rounds using a prefix like computation .", "the round complexity in the statement follows .", "if @xmath302 is @xmath329 and @xmath101 for some constant @xmath3 , the complexity reduces to @xmath27 rounds , which is a quadratic logarithmic factor better than what could be obtained by simulating the pram algorithm .", "the above algorithm for computing the inverse of any nonegative matrix requires @xmath314 . in this section", "we provide an -algorithm providing a strong approximation of @xmath294 assuming @xmath53 .", "a matrix @xmath55 is a", "_ strong approximation _ of the inverse of an @xmath269 matrix @xmath54 if @xmath330 , for some constant @xmath331 .", "the norm @xmath332 of a matrix @xmath54 is defined as @xmath333 where @xmath334 denotes the euclidean norm of a vector .", "the condition number @xmath335 of a matrix @xmath54 is defined as @xmath336 .", "an iterative method to compute a strong approximation of the inverse of a @xmath269 matrix @xmath54 is proposed in  ( * ? ? ?", "the method works as follows .", "let @xmath337 be a @xmath269 matrix satisfying the condition @xmath338 for some @xmath339 and where @xmath340 is the @xmath269 identity matrix .", "for a @xmath269 matrix @xmath62 let @xmath341 .", "we define @xmath342 , for @xmath343 .", "we have @xmath344 by setting @xmath345 where @xmath346 , we have @xmath347  @xcite . then ,", "if @xmath348 for some constant @xmath349 , @xmath350 provides a strong approximation when @xmath351 . from the above discussion , it is easy to derive an efficient -algorithm to compute a strong approximation of the inverse of a matrix using the algorithm for dense matrix multiplication in section  [ sec : ddmult ] .", "the above algorithm provides a strong approximation of the inverse of any nonegative @xmath352 matrix @xmath54 in @xmath353 rounds on an  when @xmath348 for some constant @xmath349 .", "the correctness of the algorithm derives from  @xcite .", "once again we only focus on the round complexity of the algorithm .", "computing @xmath354 requires a a constant number of prefix like computations , and hence takes @xmath52 rounds . to compute @xmath350 , @xmath343 from @xmath355 ,", "we need the value @xmath356 which involves a multiplication between two @xmath269 matrices and a subtraction between two matrices .", "hence , each phase requires @xmath357 rounds .", "since the algorithm terminates when @xmath358 , the theorem follows .", "a strategy for computing , with probability at least 1/2 , a perfect matching of a general graph using matrix inversion is presented in  @xcite .", "the strategy is the following :    1 .", "let the input of the algorithm be the adjacency matrix @xmath54 of a graph @xmath359 with @xmath156 vertices and @xmath224 edges .", "[ step : match1 ] 2 .", "let @xmath55 be the matrix obtained from @xmath54 by substituting the entries @xmath360 corresponding to edges in the graph with the integers @xmath361 and @xmath362 respectively , for @xmath363 , where @xmath364 is an integer chosen independently and uniformly at random from @xmath365 $ ] .", "we denote the entry on the @xmath110th row and @xmath366th column of @xmath55 as @xmath367.[step : match2 ] 3 .", "compute the determinant @xmath368 of @xmath55 and the greatest integer @xmath369 such that @xmath370 divides @xmath368.[step : match3 ] 4 .", "compute @xmath371 , the adjugate matrix of @xmath55 , and denote the entry on the @xmath110th row and @xmath366th column as @xmath372.[step : match4 ] 5 .   for each edge", "@xmath373 , compute @xmath374 if @xmath375 is odd , then add the edge @xmath376 to the matching.[step : match5 ]    an -algorithm for perfect matching easily follows by the above strategy .", "we now provide more details on the mr implementation which assumes @xmath314 .    in step  [ step : match2 ] ,", "@xmath55 is obtained as follows .", "the algorithm partitions @xmath54 into square @xmath98 submatrices @xmath377 , @xmath378 , and then assigns each pair of submatrices @xmath379 to a different reducer .", "this assignment ensures that each pair of entries @xmath380 of @xmath54 is sent to the same reducer .", "consider now the reducer receiving the pair of submatrices @xmath381 and consider the set of pairs @xmath380 of @xmath54 such that @xmath360 , where @xmath382 , @xmath383 , and @xmath384 .", "for each of these pairs the reducer chooses a @xmath364 independently and uniformly at random from @xmath365 $ ] , and sets @xmath367 to @xmath361 and @xmath385 to @xmath362 .", "for all the other entries @xmath386 , the reducer sets @xmath387 .", "let @xmath388 , @xmath389 be the coefficients of the characteristic polynomial of @xmath55 , which can be computed as described in section  [ sec : genmatrinv ] .", "steps  [ step : match3 ] and  [ step : match4 ] can be easily implemented since the determinant of @xmath55 is @xmath390 and @xmath391    finally , in step  [ step : match5 ] , matrices @xmath55 and @xmath371 are partitioned in square submatrices of size @xmath98 , and corresponding submatrices assigned to the same reducer , which computes the values @xmath375 for the entries in its submatrices and outputs the edges belonging to the matching .", "the above algorithm computes , with probability at least 1/2 , a perfect matching of the vertices of a graph @xmath392 , in @xmath324 rounds on , where @xmath314 .", "the correctness of the algorithm follows from the correctness of  @xcite and it is easy to see that the memory requirements of the  model are satisfied .", "we focus here on the round complexity . from the above description , it is easy to see that the computation of @xmath55 and the @xmath364 s in step  [ step : match2 ] only takes one round .", "steps  [ step : match3 ] and  [ step : match4 ] require the computation of the coefficients of the characteristic polynomial of @xmath55 , and so takes a number of rounds equal to the algorithm for matrix inversion described in section  [ sec : genmatrinv ] , i.e. , @xmath393 .", "step  [ step : match5 ] takes one round . since the round complexity is dominated by the number of rounds needed to compute the coefficients of the characteristic polynomial of @xmath55 , the theorem follows .", "we note that matching is as easy as matrix inversion in the  model", ". the above result can be extend to minimum weight perfect matching , to maximum matching , and to other variants of matching in the same way as in  ( * ? ? ?", "* sect .  5 ) .", "in this paper , we provided a formal computational model for the mapreduce paradigm which is parametric in the local and aggregate memory sizes and retains the functional flavor originally intended for the paradigm , since it does not require algorithms to explicitly specify a processor allocation for the reduce instances .", "performance in the model is represented by the round complexity , which is consistent with the idea that when processing large data sets the dominant cost is the reshuffling of the data .", "the two memory parameters featured by the model allow the algorithm designer to explore a wide spectrum of tradeoffs between round complexity and memory availability . in the paper , we covered interesting such tradeoffs for the fundamental problem of matrix multiplication and some of its applications .", "the study of similar tradeoffs for other important applications ( e.g. , graph problems ) constitutes an interesting open problem .", "the work of pietracaprina , pucci and silvestri was supported , in part , by miur of italy under project algodeep , and by the university of padova under the strategic project stpd08ja32 and project cpda099949/09 .", "the work of riondato and upfal was supported , in part , by nsf award iis-0905553 and by the university of padova through the visiting scientist 2010/2011 grant ."], "abstract_text": ["<S> this work explores fundamental modeling and algorithmic issues arising in the well - established mapreduce framework . </S>", "<S> first , we formally specify a computational model for mapreduce which captures the functional flavor of the paradigm by allowing for a flexible use of parallelism . </S>", "<S> indeed , the model diverges from a traditional processor - centric view by featuring parameters which embody only global and local memory constraints , thus favoring a more data - centric view . </S>", "<S> second , we apply the model to the fundamental computation task of matrix multiplication presenting upper and lower bounds for both dense and sparse matrix multiplication , which highlight interesting tradeoffs between space and round complexity . finally , building on the matrix multiplication results , we derive further space - round tradeoffs on matrix inversion and matching .    algorithms for distributed computing ; algorithms for high performance computing ; parallel algorithms ; parallel complexity theory . </S>"], "labels": null, "section_names": ["introduction", "model definition and basic primitives", "matrix multiplication", "applications", "conclusions", "acknowledgments"], "sections": [["in recent years , mapreduce has emerged as a computational paradigm for processing large - scale data sets in a series of rounds executed on conglomerates of commodity servers @xcite , and has been widely adopted by a number of large web companies ( e.g. , google , yahoo ! , amazon ) and in several other applications ( e.g. , gpu and multicore processing ) .", "( see @xcite and references therein . )", "informally , a mapreduce computation transforms an input set of key - value pairs into an output set of key - value pairs in a number of _ rounds _ , where in each round each pair is first individually transformed into a ( possibly empty ) set of new pairs ( _ map step _ ) and then all values associated with the same key are processed , separately for each key , by an instance of the same reduce function ( simply called _ reducer _ in the rest of the paper ) thus producing the next new set of key - value pairs ( _ reduce step _ ) .", "in fact , as already noticed in @xcite , a reduce step can clearly embed the subsequent map step so that a mapreduce computation can be simply seen as a sequence of rounds of ( augmented ) reduce steps .", "the mapreduce paradigm has a functional flavor , in that it merely requires that the algorithm designer decomposes the computation into rounds and , within each round , into independent tasks through the use of keys .", "this enables parallelism without forcing an algorithm to cater for the explicit allocation of processing resources .", "nevertheless , the paradigm implicitly posits the existence of an underlying unstructured and possibly heterogeneous parallel infrastructure , where the computation is eventually run . while mostly ignoring the details of such an underlying infrastructure , existing formalizations of the mapreduce paradigm constrain the computations to abide with some local and aggregate memory limitations .    in this paper , we look at both modeling and algorithmic issues related to the mapreduce paradigm .", "we first provide a formal specification of the model , aimed at overcoming some limitations of the previous modeling efforts , and then derive interesting tradeoffs between memory constraints and round complexity for the fundamental problem of matrix multiplication and some of its applications .", "the mapreduce paradigm has been introduced in @xcite without a fully - specified formal computational model for algorithm design and analysis .", "triggered by the popularity quickly gained by the paradigm , a number of subsequent works have dealt more rigorously with modeling and algorithmic issues  @xcite .    in @xcite , a mapreduce algorithm specifies a sequence of rounds as described in the previous section .", "somewhat arbitrarily , the authors impose that in each round the memory needed by any reducer to store and transform its input pairs has size @xmath0 , and that the aggregate memory used by all reducers has size @xmath1 , where @xmath2 denotes the input size and @xmath3 is a fixed constant in @xmath4 .", "the cost of local computation , that is , the work performed by the individual reducers , is not explicitly accounted for , but it is required to be polynomial in @xmath2 .", "the authors also postulate , again somewhat arbitrarily , that the underlying parallel infrastructure consists of @xmath5 processing elements with @xmath5 local memory each , and hint at a possible way of supporting the computational model on such infrastructure , where the reduce instances are scheduled among the available machines so to distribute the aggregate memory in a balanced fashion .", "it has to be remarked that such a distribution may hide non negligible costs for very fine - grained computations ( due to the need of allocating multiple reducer with different memory requirements to a fixed number of machines ) when , in fact , the algorithmic techniques of @xcite do not fully explore the larger power of the mapreduce model with respect to a model with fixed parallelism . in @xcite the same model of @xcite is adopted but when evaluating an algorithm the authors also consider the total work and introduce the notion of work - efficiency typical of the literature on parallel algorithms .    an alternative computational model for mapreduce is proposed in @xcite , featuring two parameters which describe bandwidth and latency characteristics of the underlying communication infrastructure , and an additional parameter that limits the amount of i / o performed by each reducer .", "also , a bsp - like cost function is provided which combines the internal work of the reducers with the communication costs incurred by the shuffling of the data needed at each round . unlike the model of @xcite , no limits are posed to the aggregate memory size .", "this implies that in principle there is no limit to the allowable parallelism while , however , the bandwidth / latency parameters must somewhat reflect the topology and , ultimately , the number of processing elements .", "thus , the model mixes the functional flavor of mapreduce with the more descriptive nature of bandwidth - latency models such as bsp @xcite .", "a model which tries to merge the spirit of mapreduce with the features of data - streaming is the mud model of @xcite , where the reducers receive their input key - value pairs as a stream to be processed in one pass using small working memory , namely polylogarithmic in the input size .", "a similar model has been adopted in @xcite .", "mapreduce algorithms for a variety of problems have been developed on the aforementioned mapreduce variants including , among others , primitives such as prefix sums , sorting , random indexing @xcite , and graph problems such as triangle counting @xcite minimum spanning tree , @xmath6-@xmath7 connectivity , @xcite , maximal and approximate maximum matching , edge cover , minimum cut @xcite , and max cover @xcite .", "moreover simulations of the pram and bsp in mapreduce have been presented in @xcite . in particular , it is shown that a @xmath8-step erew pram algorithm can be simulated by an @xmath9-round mapreduce algorithm , where each reducer uses constant - size memory and the aggregate memory is proportional to the amount of shared memory required by the pram algorithm @xcite .", "the simulation of crew or crcw pram algorithms incurs a further @xmath10 slowdown , where @xmath11 denotes the local memory size available for each reducer and @xmath12 the aggregate memory size @xcite .", "all of the aforementioned algorithmic efforts have been aimed at achieving the minimum number of rounds , possibly constant , provided that enough local memory for the reducer ( typically , sublinear yet polynomial in the input size ) and enough aggregate memory is available .", "however , so far , to the best of our knowledge , there has been no attempt to fully explore the tradeoffs that can be exhibited for specific computational problems between the local and aggregate memory sizes , on one side , and the number of rounds , on the other , under reasonable constraints of the amount of total work performed by the algorithm .", "our results contribute to filling this gap .", "matrix multiplication is a building block for many problems , including matching  @xcite , matrix inversion  @xcite , all - pairs shortest path  @xcite , graph contraction  @xcite , cycle detection  @xcite , and parsing context free languages  @xcite .", "parallel algorithms for matrix multiplication of dense matrices have been widely studied : among others , we remind @xcite which provide upper and lower bounds exposing a tradeoff between communication complexity and processor memory . for sparse matrices ,", "interesting results are given in @xcite for some network topologies like hypercubes , in @xcite for pram , and in @xcite for a bsp - like model .", "in particular , techniques in @xcite are used in the following sections for deriving efficient mapreduce algorithms . in the sequential settings ,", "some interesting works providing upper and lower bounds are @xcite for dense matrix multiplication , and @xcite for sparse matrix multiplication .", "the contribution of this paper is twofold , since it targets both modeling and algorithmic issues .", "we first formally specify a computational model for mapreduce which captures the functional flavor of the paradigm by allowing a flexible use of parallelism .", "more specifically , our model generalizes the one proposed in @xcite by letting the local and aggregate memory sizes be two independent parameters , @xmath11 and @xmath12 , respectively .", "moreover our model makes no assumption on the underlying execution infrastructure , for instance it does not impose a bound on the number of available machines , thus fully decoupling the degree of parallelism exposed by a computation from the one of the machine where the computation will be eventually executed .", "this decoupling greatly simplifies algorithm design , which has been one of the original objectives of the mapreduce paradigm .", "( in section  [ sec : preliminary ] , we quantify the cost of implementing a round of our model on a system with fixed parallelism . )", "our algorithmic contributions concern the study of attainable tradeoffs in mapreduce for several variants of the fundamental primitive of matrix multiplication .", "in particular , building on the well - established three - dimensional algorithmic strategy for matrix multiplication @xcite , we develop upper and lower bounds for dense - dense matrix multiplication and provide similar bounds for deterministic and/or randomized algorithms for sparse - sparse and sparse - dense matrix multiplication .", "the algorithms are parametric in the local and aggregate memory constraints and achieve optimal or quasi - optimal round complexity in the entire range of variability of such parameters .", "finally , building on the matrix multiplication results , we derive similar space - round tradeoffs for matrix inversion and matching , which are important by - products of matrix multiplication .", "the rest of the paper is structured as follows . in section  [ sec : preliminary ] we introduce our computational model for mapreduce and describe important algorithmic primitives ( sorting and prefix sums ) that we use in our algorithms .", "section  [ sec : intromatrix ] deals with matrix multiplication in our model , presenting theoretical bounds to the complexity of algorithms to solve this problem .", "we apply these results in section  [ sec : applications ] to derive algorithms for matrix inversion and for matching in graphs ."], ["our model is defined in terms of two integral parameters @xmath12 and @xmath11 , whose meaning will be explained below , and is named .", "algorithms specified in this model will be referred to as _ mr - algorithms_. an mr - algorithm specifies a sequence of _ rounds _ : the @xmath13-th round , with @xmath14 transforms a multiset @xmath15 of key - value pairs into two multisets @xmath16 and @xmath17 of key - value pairs , where @xmath16 is the input of the next round ( empty , if @xmath13 is the last round ) , and @xmath17 is a ( possibly empty ) subset of the final output .", "the input of the algorithm is represented by @xmath18 while the output is represented by @xmath19 , with @xmath20 denoting the union of multisets .", "the universes of keys and values may vary at each round , and we let @xmath21 denote the universe of keys of @xmath15 . the computation performed by round @xmath13", "is defined by a _", "reducer _ function @xmath22 which is applied independently to each multiset @xmath23 consisting of all entries in @xmath15 with key @xmath24 .", "let @xmath2 be the input size .", "the two parameters @xmath12 and @xmath11 specify the memory requirements that each round of an mr - algorithm must satisfy . in particular ,", "let @xmath25 denote the space needed to compute @xmath26 on a ram with @xmath27-bit words , including the space taken by the input ( i.e. , latexmath:[$m_{r , k } \\geq    output , which contributes either to @xmath17 ( i.e. , the final output ) or to @xmath16 .", "the model imposes that @xmath29 , for every @xmath14 and @xmath24 , that @xmath30 , for every @xmath14 , and that @xmath31 . the complexity of an mr - algorithm is the number of rounds that it executes in the worst case , and it is expressed as a function of the input size @xmath2 and of parameters @xmath11 and @xmath12 . the dependency on the parameters @xmath11 and @xmath12 allows for a finer analysis of the cost of an mr - algorithm .", "as in @xcite , we require that each reducer function runs in time polynomial in @xmath2 .", "in fact , it can be easily seen that the model defined in @xcite is equivalent to the  model with @xmath32 and @xmath33 , for some fixed constant @xmath34 , except that we eliminate the additional restrictions that the number of rounds of an algorithm be polylogarithmic in @xmath2 and that the number of physical machines on which algorithms are executed are @xmath35 , which in our opinion should not be posed at the model level .", "compared to the model in @xcite , our  model introduces the parameter @xmath12 which limits the size of the aggregate memory required at each round , whereas in @xcite this size is virtually unbounded .", "moreover , the complexity analysis in  focuses on the tradeoffs between @xmath11 and @xmath12 , on one side , and the number of rounds on the other side , while in @xcite a more complex cost function is defined which accounts for the overall message complexity of each round , the time complexity of each reducer computation , and the latency and bandwidth characteristics of the executing platform .", "sorting and prefix sum primitives are used in the algorithms presented in this paper .", "the input to both primitives consists of a set of @xmath2 key - value pairs @xmath36 with @xmath37 and @xmath38 , where @xmath39 denotes a suitable set . for sorting ,", "a total order is defined over @xmath39 and the output is a set of @xmath2 key - value pairs @xmath40 , where the @xmath41 s form a permutation of the @xmath42 s and @xmath43 for each @xmath44 . for prefix sums , a binary associative operation @xmath45 is defined over @xmath39 and the output consists of a collection of @xmath2 pairs @xmath40 where @xmath46 , for @xmath47 .", "by straightforwardly adapting the results in @xcite to our model we have :    [ prefixsorting ] the sorting and prefix sum primitives for inputs of size @xmath2 can be implemented in  with round complexity @xmath48 for @xmath49 .", "we remark that the each reducer in the implementation of the sorting and prefix primitives makes use of @xmath50 memory words .", "hence , the same round complexity can be achieved in a more restrictive scenario with fixed parallelism . in fact", ", our  model can be simulated on a platform with @xmath51 processing elements , each with internal memory of size @xmath50 , at the additional cost of one prefix computation per round .", "therefore , @xmath52 can be regarded as an upper bound on the relative power of our model with respect to one with fixed parallelism .", "goodrich  @xcite claims that the round complexities stated in theorem  [ prefixsorting ] are optimal for any @xmath53 as a consequence of the lower bound for computing the or of @xmath2 bits on the bsp model  @xcite .", "it can be shown that the optimality carries through to our model where the output of a reducer is not bounded by @xmath11 ."], ["let @xmath54 and @xmath55 be two @xmath56 matrices and let @xmath57 .", "we use @xmath58 and @xmath59 , with @xmath60 , to denote the entries of @xmath61 and @xmath62 , respectively . in this section", "we present upper and lower bounds for computing the product @xmath62 in .", "the algorithms we present envision the matrices as conceptually divided into submatrices of size @xmath63 , and we denote these matrices with @xmath64 , @xmath65 and @xmath66 , respectively , for @xmath67 . clearly , @xmath68 .", "all our algorithms exploit the following partition of the @xmath69 products between submatrices ( e.g. , @xmath70 ) into @xmath71 _ groups _ : group @xmath72 , with @xmath73 , consists of products @xmath74 , for every @xmath67 and for @xmath75 .", "observe that each submatrix of @xmath54 and @xmath55 occurs exactly once in each group @xmath72 .", "we focus our attention on matrices whose entries belong to a semiring @xmath76 such that for any @xmath77 we have @xmath78 , where @xmath79 is the identity for @xmath45 . in this", "setting , efficient matrix multiplication techniques such as strassen s can not be employed .", "moreover , we assume that the inner products of any row of @xmath54 and of any column of @xmath55 with overlapping nonzero entries never cancel to zero , which is a reasonable assumption when computing over natural numbers or over real numbers with a finite numerical precision .    in our algorithms , any input matrix @xmath80 ( @xmath81 ) is provided as a set of key - value pairs @xmath82 for all elements @xmath83 .", "key @xmath84 represents a progressive index , e.g. , the number of nonzero entries preceding @xmath85 in the row - major scan of @xmath80 .", "we call a @xmath86 matrix _ dense _ if the number of its nonzero entries is @xmath87 , and we call it _ sparse _ otherwise . we suppose that @xmath12 is sufficiently large to contain the input and output matrices .", "in what follows , we present different algorithms tailored for the multiplication of dense - dense ( section  [ sec : ddmult ] ) , sparse - sparse ( section  [ sec : ssmult ] ) , and sparse - dense matrices ( section  [ sec : sdmult ] ) .", "we also derive lower bounds which demonstrate that our algorithms are either optimal or close to optimal ( section  [ sec : lb ] ) , and an algorithm for estimating the number of nonzero entries in the product of two sparse matrices ( section  [ sec : evaluation ] ) .      in this section", "we provide a simple , deterministic algorithm for multiplying two dense matrices , which will be proved optimal in subsection  [ sec : lb ] .", "the algorithm is a straightforward adaptation of the well - established three - dimensional algorithmic strategy for matrix multiplication of @xcite , however we describe a few details of its implementation in  since the strategy is also at the base of algorithms for sparse matrices .", "we may assume that @xmath88 , since otherwise matrix multiplication can be executed by a trivial sequential algorithm .", "we consider matrices @xmath54 and @xmath55 as decomposed into @xmath89 submatrices and subdivide the products between submatrices into groups as described above .    in each round", ", the algorithm computes all products within @xmath90 consecutive groups , namely , at round @xmath91 , all multiplications in @xmath72 are computed , with @xmath92 .", "the idea is that in a round all submatrices of @xmath54 and @xmath55 can be replicated @xmath93 times and paired in such a way that each reducer performs a distinct multiplication in @xmath94 .", "then , each reducer sums the newly computed product to a partial sum which accumulates all of the products contributing to the same submatrix of @xmath62 belonging to groups with the same index modulo @xmath93 dealt with in previous rounds . at the end of the @xmath95-th round", ", all submatrix products have been computed .", "the final matrix @xmath62 is then obtained by adding together the @xmath93 partial sums contributing to each entry of @xmath62 through a prefix computation .", "we have the following result .", "[ th : upddmult ] the above -algorithm multiplies two @xmath96 dense matrices in @xmath97 rounds .", "the algorithm clearly complies with the memory constraints of  since each reducer multiplies two @xmath98 submatrices and the degree of replication is such that the algorithm never exceeds the aggregate memory bound of @xmath12 . also , the @xmath69 products are computed in @xmath99 rounds , while the final prefix computation requires @xmath100 rounds    we remark that the multiplication of two @xmath96 dense matrices can be performed in a constant number of rounds whenever @xmath101 , for constant @xmath102 , and @xmath103 .", "consider two @xmath86 sparse matrices @xmath54 and @xmath55 and denote with @xmath104 the maximum number of nonzero entries in any of the two matrices , and with @xmath105 the number of nonzero entries in the product @xmath106 .", "below , we present two deterministic mr - algorithms ( d1 and d2 ) and a randomized one ( r1 ) , each of which turns out to be more efficient than the others for suitable ranges of parameters .", "we consider only the case @xmath107 , since otherwise matrix multiplication can be executed by a trivial one - round mr - algorithm using only one reducer .", "we also assume that the value @xmath108 is provided in input .", "( if this were not the case , such a value could be computed with a simple prefix computation in @xmath52 rounds , which does not affect the asymptotic complexity of our algorithms . )", "however , we do not assume that @xmath105 is known in advance since , unlike @xmath108 , this value can not be easily computed .", "in fact , the only source of randomization in algorithm r1 stems from the need to estimate @xmath105 .", "this algorithm is based on the following strategy adapted from @xcite . for @xmath109 ,", "let @xmath42 ( resp . ,", "@xmath41 ) be the number of nonzero entries in the @xmath110th column of @xmath54 ( resp .", ", @xmath110th row of @xmath55 ) , and let @xmath111 be the set containing all nonzero entries in the @xmath110th column of @xmath54 and in the @xmath110th row of @xmath55 .", "it is easily seen that all of the @xmath112 products between entries in @xmath111 ( one from @xmath54 and one from @xmath55 ) must be computed .", "the algorithm performs a sequence of _ phases _ as follows .", "suppose that at the beginning of phase  @xmath7 , with @xmath113 , all products between entries in @xmath111 , for each @xmath114 and for a suitable value @xmath13 ( initially , @xmath115 ) , have been computed and added to the appropriate entries of @xmath62 . through a prefix computation ,", "phase  @xmath7 computes the largest @xmath116 such that @xmath117 .", "then , all products between entries in @xmath118 , for every @xmath119 , are computed using one reducer with constant memory for each such product .", "the products are then added to the appropriate entries of @xmath62 using again a prefix computation .", "algorithm d1 multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries each in @xmath120 rounds , on an .", "the correctness is trivial and the memory constraints imposed by the model are satisfied since in each phase at most @xmath12 elementary products are performed .", "the theorem follows by observing that the maximum number of elementary products is @xmath121 and that two consecutive phases compute at least @xmath12 elementary products in @xmath122 rounds .", "the algorithm exploits the same three - dimensional algorithmic strategy used in the dense - dense case and consists of a sequence of phases . in phase @xmath7 , @xmath113 ,", "all @xmath63-size products within @xmath116 consecutive groups are performed in parallel , where @xmath116 is a phase - specific value . observe that the computation of all products within a group @xmath72 requires space @xmath123 $ ] , since each submatrix of @xmath54 and @xmath55 occurs only once in @xmath72 and each submatrix product contributes to a distinct submatrix of @xmath62 .", "however , the value @xmath124 can be determined in @xmath125 space and @xmath52 rounds by `` simulating '' the execution of the products in @xmath72 ( without producing the output values ) and adding up the numbers of nonzero entries contributed by each product to the output matrix .", "the value @xmath116 is determined as follows .", "suppose that , at the beginning of phase @xmath7 , groups @xmath72 have been processed , for each @xmath126 and for a suitable value @xmath13 ( initially , @xmath115 ) .", "the algorithm replicates the input matrices @xmath127 times .", "subsequently , through sorting and prefix computations the algorithm computes @xmath128 for each @xmath129 and determines the largest @xmath130 such that @xmath131 .", "then , the actual products in @xmath132 , for each @xmath133 are executed and accumulated ( again using a prefix computation ) in the output matrix @xmath62 .", "we have the following theorem .", "algorithm @xmath134 multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries each in @xmath135 rounds on an , where @xmath105 denotes the maximum number of nonzero entries in the output matrix .", "the correctness of the algorithm is trivial .", "phase @xmath7 requires a constant number of sorting and prefix computations to determine @xmath116 and to add the partial contributions to the output matrix @xmath62 .", "since each value @xmath124 is @xmath136 and the groups are @xmath71 , clearly , @xmath137 , and the theorem follows .", "we remark that the value @xmath105 appearing in the stated round complexity needs not be explicitly provided in input to the algorithm .", "we also observe that with respect to algorithm d1 , algorithm d2 features a better exploitation of the local memories available to the individual reducers , which compute @xmath89-size products rather than working at the granularity of the single entries .", "by suitably combining algorithms d1 and d2 , we can get the following result .", "[ d12round ] there is a deterministic algorithm which multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries each in @xmath138 rounds on an , where @xmath105 denotes the maximum number of nonzero entries in the output matrix .", "algorithm d2 requires @xmath122 rounds in each phase @xmath7 for computing the number @xmath116 of groups to be processed .", "however , if @xmath105 were known , we could avoid the computation of @xmath116 and resort to the fixed-@xmath93 strategy adopted in the dense - dense case , by processing @xmath139 consecutive groups per round .", "this would yield an overall @xmath140 round complexity , where the @xmath141 additive term accounts for the complexity of summing up , at the end , the @xmath93 contributions to each entry of @xmath62 .", "however , @xmath105 may not be known a priori . in this case , using the strategy described in section  [ sec : evaluation ] we can compute a value @xmath142 which is a 1/2-approximation to @xmath105 with probability at least @xmath143 .", "( we say that @xmath142 @xmath144-approximates @xmath105 if @xmath145 . ) hence , in the algorithm we can plug in @xmath146 as an upper bound to @xmath105 . by using the result of theorem  [ otilde ] with @xmath147 and @xmath148", ", we have :    [ r1round ] let @xmath149 .", "algorithm r1 multiplies two sparse @xmath96 matrices with at most @xmath108 nonzero entries in @xmath150 rounds on an , with probability at least @xmath143 .    by comparing the rounds complexities stated in corollary  [ d12round ] and theorem  [ r1round ] ,", "it is easily seen that the randomized algorithm r1 outperforms the deterministic strategies when @xmath151 , for any constant @xmath3 , @xmath152 , and @xmath153 . for a concrete example , r1 exhibits better performance when @xmath154 , @xmath155 , and @xmath11 is polylogarithmic in @xmath12 .", "moreover , both the deterministic and randomized strategies can achieve a constant round complexity for suitable values of the memory parameters .", "observe that a @xmath156-approximation to @xmath105 derives from the following simple argument .", "let @xmath42 and @xmath41 be the number of nonzero entries in the @xmath110th column of @xmath54 and in the @xmath110th row of @xmath55 respectively , for each @xmath157 . then , @xmath158 .", "evaluating the sum requires @xmath159 sorting and prefix computations , hence a @xmath156-approximation of @xmath105 can be computed in @xmath160 rounds .", "however , such an approximation is too weak for our purposes and we show below how to achieve a tighter approximation by adapting a strategy born in the realm of streaming algorithms .", "let @xmath161 and @xmath162 be two arbitrary values .", "an @xmath144-approximation to @xmath105 can be derived by adapting the algorithm of  @xcite for counting distinct elements in a stream @xmath163 , whose entries are in the domain @xmath164=\\{0,\\ldots , n-1\\}$ ] .", "the algorithm of  @xcite makes use of a very compact data structure , customarily called _ sketch _ in the literature , which consists of @xmath165 lists , @xmath166 .", "for @xmath167 , @xmath168 contains the @xmath169 distinct smallest values of the set @xmath170 , where @xmath171\\rightarrow [ n^3]$ ] is a hash function picked from a pairwise independent family .", "it is shown in @xcite that the median of the values @xmath172 , where @xmath173 denotes the @xmath7th smallest value in @xmath168 , is an @xmath144-approximation to the number of distinct elements in the stream , with probability at least @xmath174 . in order to compute an @xmath144-approximation of @xmath105 for a product @xmath175 of @xmath176 matrices , we can modify the algorithm as follows .", "consider the stream of values in @xmath164 $ ] where each element of the stream corresponds to a distinct product @xmath177 and consists of the value @xmath178 .", "clearly , the number of distinct elements in this stream is exactly @xmath105 .", "( a similar approach has been used in @xcite in the realm of sparse boolean matrix products . )", "we now show how to implement this idea on an .", "the mr - algorithm is based on the crucial observation that if the stream of values defined above is partitioned into segments , the sketch for the entire stream can be obtained by combining the sketches computed for the individual segments . specifically , two sketches are combined by merging each pair of lists with the same index and selecting the @xmath7 smallest values in the merged list .", "the -algorithm consists of a number of phases , where each phase , except for the last one , produces set of @xmath179 sketches , while the last phase combines the last batch of @xmath179 sketches into the final sketch , and outputs the approximation to @xmath105 .", "we refer to the partition of the matrices into @xmath180 submatrices and group the products of submatrices as done before . in phase", "@xmath7 , with @xmath181 , the algorithm processes the products in @xmath182 consecutive groups , assigning each pair of submatrices in one of the @xmath93 groups to a distinct reducer .", "a reducer receiving @xmath183 and @xmath184 , each with at least a nonzero entry , either computes a sketch for the stream segment of the nonzero products between entries of @xmath183 and @xmath184 , if the total number of nonzero entries of @xmath183 and @xmath184 exceeds the size of the sketch , namely @xmath185 words , or otherwise leaves the two submatrices untouched ( observe that in neither case the actual product of the two submatrices is computed ) . in this latter case , we refer to the pair of ( very sparse ) submatrices as a _", "pseudosketch_. at this point , the sketches produced by the previous phase ( if @xmath186 ) , together with the sketches and pseudosketches produced in the current phase are randomly assigned to @xmath179 reducers .", "each of these reducers can now produce a single sketch from its assigned pseudosketches ( if any ) and merge it with all other sketches that were assigned to it . in the last phase ( @xmath187 ) the @xmath179 sketches are combined into the final one through a prefix computation , and the approximation to @xmath105", "is computed .", "[ otilde ] let @xmath188 and let @xmath189 and @xmath190 be arbitrary values .", "then , with probability at least @xmath191 , the above algorithm computes an @xmath144-approximation to @xmath105 in @xmath192 rounds , on an    the correctness of the algorithm follows from the results of @xcite and the above discussion .", "recall that the value computed by the algorithm is an @xmath144-approximation to @xmath105 with probability @xmath174 . as for the rounds complexity", "we observe that each phase , except for the last one , requires a constant number of rounds , while the last one involves a prefix computation thus requiring @xmath122 rounds .", "we only have to make sure that in each phase the memory constraints are satisfied ( with high probability ) .", "note also that a sketch of size @xmath193 is generated either in the presence of a pair of submatrices @xmath183 , @xmath184 containing at least @xmath194 entries , or within one of the @xmath179 reducers . by the choice of @xmath93 , it is easy to see that in any case , the overall memory occupied by the sketches is @xmath195 . as for the constraint on local memories , a simple modification of the standard balls - into - bins argument @xcite and the union bound suffices to show that with probability @xmath174 , in every phase when sketches and pseudosketches are assigned to @xmath179 reducers , each reducer receives in @xmath196 words .", "the theorem follows .", "( more details will be provided in the full version of the paper . )", "let @xmath54 be a sparse @xmath56 matrix with at most @xmath108 nonzero entries and let @xmath55 be a dense @xmath56 matrix ( the symmetric case , where @xmath54 is dense and @xmath55 sparse , is equivalent ) .", "the algorithm for dense - dense matrix multiplication does not exploit the sparsity of @xmath54 and requires @xmath197 rounds .", "also , if we simply plug @xmath198 in the complexities of the three algorithms for the sparse - sparse case ( where @xmath108 represented the maximum number of nonzero entries of @xmath54 or @xmath55 ) we do not achieve a better round complexity .", "however , a careful analysis of algorithm d1 in the sparse - dense case reveals that its round complexity is @xmath199 .", "therefore , by interleaving algorithm d1 and the dense - dense algorithm we have the following corollary .", "the multiplication on  of a sparse @xmath56 matrix with at most @xmath108 nonzero entries and of a dense @xmath56 matrix requires a number of rounds which is the minimum between @xmath200 and @xmath201 .", "observe that the above sparse - dense strategy outperforms all previous algorithms for instance when @xmath202 .", "in this section we provide lower bounds for dense - dense and sparse - sparse matrix multiplication .", "we restrict our attention to algorithms which perform all nonzero elementary products , that is , on _ conventional _ matrix multiplication @xcite .", "although this assumption limits the class of algorithms , ruling out strassen - like techniques , an elaboration of a result in  @xcite shows that computing all nonzero elementary products is necessary when entries of the input matrices are from the semirings @xmath203 and @xmath204 .", "semiring , where @xmath205 is the identity of the @xmath206 operation , is usually adopted while computing the shortest path matrix of a graph given its connection matrix .", "] indeed , we have the following lemma which provides a lower bound on the number of products required by an algorithm multiplying any two matrices of size @xmath56 , containing @xmath207 and @xmath208 nonzero entries and where zero entries have fixed positions ( a similar lemma holds for @xmath209 ) . as a consequence of the lemma , an algorithm that multiplies any two arbitrary matrices in the semiring", "@xmath203 must perform all nonzero products .", "consider an algorithm @xmath210 which multiples two @xmath56 matrices @xmath54 and @xmath55 with @xmath207 and @xmath208 nonzero entries , respectively , from the semiring @xmath211 and where the positions of zero entries are fixed .", "then , algorithm @xmath210 must perform all the nonzero elementary products .", "@xcite shows that each @xmath59 can be computed only by summing all terms @xmath212 , with @xmath213 , if the algorithm uses only semiring operations .", "the proof relies on the analysis of the output for some suitable input matrices , and makes some assumptions that force the algorithm to compute even zero products .", "however , the result still holds if we allow all the zero products to be ignored , but some adjustments are required . in particular , the input matrices used in @xcite do not work in our scenario because may contain less than @xmath207 and @xmath208 nonzero entries , however it is easy to find inputs with the same properties working in our case .", "more details will be provided in the full version .", "the following theorem exhibits a tradeoff in the lower bound between the amount of local and aggregate memory and the round complexity of an algorithm performing conventional matrix multiplication .", "the proof is similar to the one proposed in  @xcite for lower bounding the communication complexity of dense - dense matrix multiplication in a bsp - like model : however , differences arise since we focus on round complexity and our model does not assume the outdegree of a reducer to be bounded . in the proof of the theorem we use the following lemma which was proved using the red - blue pebbling game in  @xcite and then restated in  @xcite as follows .", "[ lem : nummult ] consider the conventional matrix multiplication @xmath214 , where @xmath54 and @xmath55 are two arbitrary matrices .", "a processor that uses @xmath215 elements of @xmath54 , @xmath216 elements of @xmath55 , and contributes to @xmath217 elements of @xmath62 can compute at most @xmath218 multiplication terms .", "[ th : lbddmult ] consider an -algorithm @xmath210 for multiplying two @xmath56 matrices containing at most @xmath207 and @xmath208 nonzero entries , using conventional matrix multiplications .", "let @xmath219 and @xmath105 denote the number of nonzero elementary products and the number of nonzero entries in the output matrix , respectively .", "then , the round complexity of @xmath210 is @xmath220    let @xmath210 be an @xmath221-round -algorithm computing @xmath214 .", "we prove that @xmath222 .", "consider the @xmath13-th round , with @xmath223 , and let @xmath224 be an arbitrary key in @xmath21 and @xmath225 .", "we denote with @xmath226 the space taken by the output of @xmath26 which contributes either to @xmath17 or to @xmath16 , and with @xmath25 the space needed to compute @xmath26 including the input and working space but excluding the output . clearly , @xmath227 , @xmath228 , and @xmath229 .", "suppose @xmath230 . by lemma  [ lem : nummult ]", ", the reducer @xmath22 with input @xmath231 can compute at most @xmath232 elementary products since @xmath233 and @xmath234 , where @xmath215 and @xmath216 denote the entries of @xmath54 and @xmath55 used in @xmath26 and @xmath217 the entries of @xmath62 for which contributions are computed by @xmath26 .", "then , the number of terms computed in the @xmath13-th round is at most @xmath235 since @xmath236 and the summation is maximized when @xmath237 for each @xmath238 .", "suppose now that @xmath239 .", "partition the keys in @xmath21 into @xmath240 sets @xmath241 such that @xmath242 for each @xmath243 ( the lower bound may be not satisfied for @xmath244 ) . clearly , @xmath245 . by lemma  [ lem : nummult ] ,", "the number of elementary products computed by all the reducers @xmath26 with keys in a set @xmath246 is at most @xmath247 . since @xmath248 for each non negative assignment of the @xmath249 variables and since @xmath250", ", it follows that at most @xmath251 elementary products can be computed using keys in @xmath246 , where @xmath252 .", "therefore , the number of elementary products computed in the @xmath13-th round is at most @xmath253 since @xmath254 and the sum is maximized when @xmath255 for each @xmath243 .", "therefore , in each round @xmath256 nonzero elementary products can be computed , and then @xmath257 . the second term of the lower bound follows since there is at least one entry of @xmath62 given by the sum of @xmath258 nonzero elementary products .", "we now specialize the above lower bound for algorithms for generic dense - dense and sparse - sparse matrix multiplication .", "an -algorithm for multiplying any two dense @xmath86 matrices , using conventional matrix multiplication , requires @xmath259 rounds . on the other hand ,", "an -algorithm for multiplying any two sparse matrices with at most @xmath108 nonzero entries requires @xmath260 rounds .    in the dense - dense case", "the lower bound follows by the above theorem  [ th : lbddmult ] since we have @xmath261 and @xmath262 when @xmath263 . in the sparse - sparse case ,", "we set @xmath264 and we observe that there exist assignments of the input matrices for which @xmath265 , and others where @xmath266    the deterministic algorithms for matrix multiplication provided in this section perform conventional matrix multiplication , and hence the above corollary applies .", "thus , the algorithm for dense - dense matrix multiplication described in section  [ sec : ddmult ] is optimal for any value of the parameters . on the other hand , the deterministic algorithm d2 for sparse - sparse matrix multiplication given in section  [ sec : dssmult ] is optimal as soon as @xmath267 , @xmath268 and @xmath11 is polynomial in @xmath12 ."], ["our matrix multiplications results can be used to derive efficient algorithms for inverting a square matrix and for solving several variants of the matching problem in a graph .", "the algorithms in this section make use of division and exponentiation . to avoid the intricacies of dealing with limited precision , we assume each memory word is able to store any value that occurs in the computation .", "a similar assumption is made in the presentation of algorithms for the same problems on other parallel models ( see e.g. @xcite ) .      in this section", "we study the problem of inverting a lower triangular matrix @xmath54 of size @xmath269 .", "we adopt the simple recursive algorithm which leverages on the easy formula for inverting a @xmath270 lower triangular matrix  ( * ? ? ?", "@xmath271^{-1 } = \\left [ \\begin{array}{cc } a^{-1 } & 0 \\\\ -c^{-1}ba^{-1 }   & c^{-1 } \\end{array } \\right].\\ ] ] for @xmath272 and @xmath273 , let @xmath274 be the @xmath275 submatrix resulting from the splitting of @xmath54 into submatrices of size @xmath276 . since equation   holds even when @xmath277 are matrices , we have that @xmath278 can be expressed as in equation   in figure  [ fig : prodmatrinv ] .", "note that @xmath279 .", "@xmath280 , 0\\leq i\\leq 2^k-1.\\ ] ]    the -algorithm for computing the inverse of @xmath54 works in @xmath281 phases .", "let @xmath282 for @xmath283 .", "in the first part of phase @xmath79 , the inverses of all the lower triangular submatrices @xmath284 , with @xmath285 , are computed in parallel .", "since each submatrix has size @xmath98 , each inverse can be computed sequentially within a single reducer . in the second part of phase @xmath79 , each product @xmath286 for @xmath287 ,", "is computed within a reducer .    in phase @xmath13 , with @xmath288 , each term @xmath289 for @xmath290 , is computed in parallel by performing two matrix multiplications using @xmath291 aggregate memory and local size @xmath11 .", "therefore , at the end of phase @xmath292 we have all the components of @xmath293 , i.e. , of @xmath294 .", "[ thm : trianmatrinv ] the above algorithm computes the inverse of a nonsingular lower triangular @xmath269 matrix @xmath54 in @xmath295 rounds on an .    the correctness of the algorithm follows from the correctness of   which in turns easily follows from the correctness of the formula to invert a lower triangular @xmath296 matrix . from the above discussion", "it easy to see that the memory requirements are all satisfied .", "we now analyze the round complexity of the algorithm . at phase @xmath13", "we have to compute @xmath297 products between matrices of size @xmath298 each product is computed in parallel by using @xmath299 aggregate memory and thus each phase @xmath13 requires @xmath300 rounds by using the algorithm described in section  [ sec : ddmult ] .", "the cost of the lower triangular matrix inversion algorithm is then @xmath301 which gives the bound stated in the theorem .", "if @xmath302 is @xmath303 and @xmath101 for some constant @xmath3 , the complexity reduces to @xmath27 rounds , which is a logarithmic factor better than what could be obtained by simulating the pram algorithm .", "it is also possible to compute @xmath294 using the closed formula derived by unrolling a blocked forward substitution . in general", ", the closed formula contains an exponential number of terms .", "there are nonetheless special cases of matrices for which a large number of terms in the sum are zero and only a polynomial number of terms is left .", "this is , for instance , the case for triangular band matrices .", "( note that the inverse of a triangular band matrix is triangular but not necessarily a triangular band matrix . )", "if the width of the band is @xmath304 , then we have a polynomial number of terms in the formula . in this case", "we can do matrix inversion in constant rounds for sufficiently large values of @xmath11 and @xmath12 .", "a complete discussion of this method will be presented in the full version of the paper .", "building on the inversion algorithm for triangular matrices presented in the previous subsection , and on the dense - dense matrix multiplication algorithm , in this section we develop an -algorithm to invert a general @xmath269 matrix @xmath54 .", "let the trace @xmath305 of @xmath54 be defined as @xmath306 , where @xmath307 denotes the entry of @xmath54 on the @xmath110-th row and @xmath110-th column .", "the algorithm is based on the following known strategy ( see e.g. , ( * ? ? ?", "8.8 ) ) .    1 .", "compute the powers @xmath308.[step : inv1 ] 2 .", "compute the traces @xmath309 , for @xmath310.[step : inv2 ] 3 .", "compute the coefficients @xmath311 of the characteristic polynomial of @xmath54 by solving a lower triangular system of @xmath156 linear equations involving the traces @xmath312 ( the system is shown below).[step : inv3 ] 4 .", "compute @xmath313[step : inv4 ]    we now provide more details on the mr implementation of above strategy .", "the algorithm requires @xmath314 , which ensures that enough aggregate memory is available to store all the @xmath156 powers of @xmath54 . in step  [ step : inv1 ]", ", the algorithm computes naively the powers in the form @xmath315 , @xmath316 , by performing a sequence of @xmath317 matrix multiplications using the algorithm in section  [ sec : ddmult ] .", "then , each one of the remaining powers is computed using @xmath318 aggregate memory and by performing a sequence of at most @xmath319 multiplications of the matrices @xmath315 obtained earlier . in step  [", "step : inv2 ] , the @xmath156 traces @xmath312 are computed in parallel using a prefix like computation , while the coefficients @xmath311 of the characteristic polynomial are computed in step  [ step : inv3 ] by solving the following lower triangular system : @xmath320 \\left [ \\begin{array}{c }    c_{n-1 } \\\\", "c_{n-1 } \\\\", "c_{n-3 } \\\\    \\vdots", "\\\\    c_0 \\end{array } \\right ] = - \\left [ \\begin{array}{c }    s_1 \\\\    s_2 \\\\    s_3 \\\\", "\\vdots \\\\    s_n \\end{array } \\right].\\ ] ] if we denote with @xmath321 the matrix on the left hand side , with @xmath62 the vector of unknowns , and with @xmath39 the vector of the traces on the right hand side , we have @xmath322 . in order to compute the coefficients in @xmath62 the algorithm inverts the @xmath269 lower triangular matrix @xmath321 as described in section  [ sec : lowertrianinv ] , and computes the product between @xmath323 and @xmath39 , to obtain @xmath62 . finally , step  [ step : inv4 ] requires a prefix like computation .", "we have the following theorem .", "[ thm : genmatrinv ] the above algorithm computes the inverse of any nonsingular @xmath269 matrix @xmath54 in @xmath324 rounds on , with @xmath314 .    for the correctness of the algorithm see ( * ? ? ?", "it is easy to check that the memory requirements of the  model are satisfied .", "we focus here on analyzing the round complexity .", "computing the powers in the form @xmath315 , @xmath316 requires @xmath325 rounds , since the algorithm performs a sequence of @xmath317 products .", "the remaining powers are computed in @xmath326 rounds since each power is computed by performing at most @xmath317 product using @xmath327 aggregate memory . the prefix like computation for finding the @xmath156 traces @xmath312 requires @xmath52 rounds , while the linear system takes @xmath328 rounds .", "the final step takes @xmath52 rounds using a prefix like computation .", "the round complexity in the statement follows .", "if @xmath302 is @xmath329 and @xmath101 for some constant @xmath3 , the complexity reduces to @xmath27 rounds , which is a quadratic logarithmic factor better than what could be obtained by simulating the pram algorithm .", "the above algorithm for computing the inverse of any nonegative matrix requires @xmath314 . in this section", "we provide an -algorithm providing a strong approximation of @xmath294 assuming @xmath53 .", "a matrix @xmath55 is a", "_ strong approximation _ of the inverse of an @xmath269 matrix @xmath54 if @xmath330 , for some constant @xmath331 .", "the norm @xmath332 of a matrix @xmath54 is defined as @xmath333 where @xmath334 denotes the euclidean norm of a vector .", "the condition number @xmath335 of a matrix @xmath54 is defined as @xmath336 .", "an iterative method to compute a strong approximation of the inverse of a @xmath269 matrix @xmath54 is proposed in  ( * ? ? ?", "the method works as follows .", "let @xmath337 be a @xmath269 matrix satisfying the condition @xmath338 for some @xmath339 and where @xmath340 is the @xmath269 identity matrix .", "for a @xmath269 matrix @xmath62 let @xmath341 .", "we define @xmath342 , for @xmath343 .", "we have @xmath344 by setting @xmath345 where @xmath346 , we have @xmath347  @xcite . then ,", "if @xmath348 for some constant @xmath349 , @xmath350 provides a strong approximation when @xmath351 . from the above discussion , it is easy to derive an efficient -algorithm to compute a strong approximation of the inverse of a matrix using the algorithm for dense matrix multiplication in section  [ sec : ddmult ] .", "the above algorithm provides a strong approximation of the inverse of any nonegative @xmath352 matrix @xmath54 in @xmath353 rounds on an  when @xmath348 for some constant @xmath349 .", "the correctness of the algorithm derives from  @xcite .", "once again we only focus on the round complexity of the algorithm .", "computing @xmath354 requires a a constant number of prefix like computations , and hence takes @xmath52 rounds . to compute @xmath350 , @xmath343 from @xmath355 ,", "we need the value @xmath356 which involves a multiplication between two @xmath269 matrices and a subtraction between two matrices .", "hence , each phase requires @xmath357 rounds .", "since the algorithm terminates when @xmath358 , the theorem follows .", "a strategy for computing , with probability at least 1/2 , a perfect matching of a general graph using matrix inversion is presented in  @xcite .", "the strategy is the following :    1 .", "let the input of the algorithm be the adjacency matrix @xmath54 of a graph @xmath359 with @xmath156 vertices and @xmath224 edges .", "[ step : match1 ] 2 .", "let @xmath55 be the matrix obtained from @xmath54 by substituting the entries @xmath360 corresponding to edges in the graph with the integers @xmath361 and @xmath362 respectively , for @xmath363 , where @xmath364 is an integer chosen independently and uniformly at random from @xmath365 $ ] .", "we denote the entry on the @xmath110th row and @xmath366th column of @xmath55 as @xmath367.[step : match2 ] 3 .", "compute the determinant @xmath368 of @xmath55 and the greatest integer @xmath369 such that @xmath370 divides @xmath368.[step : match3 ] 4 .", "compute @xmath371 , the adjugate matrix of @xmath55 , and denote the entry on the @xmath110th row and @xmath366th column as @xmath372.[step : match4 ] 5 .   for each edge", "@xmath373 , compute @xmath374 if @xmath375 is odd , then add the edge @xmath376 to the matching.[step : match5 ]    an -algorithm for perfect matching easily follows by the above strategy .", "we now provide more details on the mr implementation which assumes @xmath314 .    in step  [ step : match2 ] ,", "@xmath55 is obtained as follows .", "the algorithm partitions @xmath54 into square @xmath98 submatrices @xmath377 , @xmath378 , and then assigns each pair of submatrices @xmath379 to a different reducer .", "this assignment ensures that each pair of entries @xmath380 of @xmath54 is sent to the same reducer .", "consider now the reducer receiving the pair of submatrices @xmath381 and consider the set of pairs @xmath380 of @xmath54 such that @xmath360 , where @xmath382 , @xmath383 , and @xmath384 .", "for each of these pairs the reducer chooses a @xmath364 independently and uniformly at random from @xmath365 $ ] , and sets @xmath367 to @xmath361 and @xmath385 to @xmath362 .", "for all the other entries @xmath386 , the reducer sets @xmath387 .", "let @xmath388 , @xmath389 be the coefficients of the characteristic polynomial of @xmath55 , which can be computed as described in section  [ sec : genmatrinv ] .", "steps  [ step : match3 ] and  [ step : match4 ] can be easily implemented since the determinant of @xmath55 is @xmath390 and @xmath391    finally , in step  [ step : match5 ] , matrices @xmath55 and @xmath371 are partitioned in square submatrices of size @xmath98 , and corresponding submatrices assigned to the same reducer , which computes the values @xmath375 for the entries in its submatrices and outputs the edges belonging to the matching .", "the above algorithm computes , with probability at least 1/2 , a perfect matching of the vertices of a graph @xmath392 , in @xmath324 rounds on , where @xmath314 .", "the correctness of the algorithm follows from the correctness of  @xcite and it is easy to see that the memory requirements of the  model are satisfied .", "we focus here on the round complexity . from the above description , it is easy to see that the computation of @xmath55 and the @xmath364 s in step  [ step : match2 ] only takes one round .", "steps  [ step : match3 ] and  [ step : match4 ] require the computation of the coefficients of the characteristic polynomial of @xmath55 , and so takes a number of rounds equal to the algorithm for matrix inversion described in section  [ sec : genmatrinv ] , i.e. , @xmath393 .", "step  [ step : match5 ] takes one round . since the round complexity is dominated by the number of rounds needed to compute the coefficients of the characteristic polynomial of @xmath55 , the theorem follows .", "we note that matching is as easy as matrix inversion in the  model", ". the above result can be extend to minimum weight perfect matching , to maximum matching , and to other variants of matching in the same way as in  ( * ? ? ?", "* sect .  5 ) ."], ["in this paper , we provided a formal computational model for the mapreduce paradigm which is parametric in the local and aggregate memory sizes and retains the functional flavor originally intended for the paradigm , since it does not require algorithms to explicitly specify a processor allocation for the reduce instances .", "performance in the model is represented by the round complexity , which is consistent with the idea that when processing large data sets the dominant cost is the reshuffling of the data .", "the two memory parameters featured by the model allow the algorithm designer to explore a wide spectrum of tradeoffs between round complexity and memory availability . in the paper , we covered interesting such tradeoffs for the fundamental problem of matrix multiplication and some of its applications .", "the study of similar tradeoffs for other important applications ( e.g. , graph problems ) constitutes an interesting open problem ."], ["the work of pietracaprina , pucci and silvestri was supported , in part , by miur of italy under project algodeep , and by the university of padova under the strategic project stpd08ja32 and project cpda099949/09 .", "the work of riondato and upfal was supported , in part , by nsf award iis-0905553 and by the university of padova through the visiting scientist 2010/2011 grant ."]]}
{"article_id": "1207.1936", "article_text": ["in the scenario of _ secure network coding _ introduced by cai et al .", "@xcite , a source node transmits @xmath0 packets from @xmath0 outgoing links to sink nodes through a network that implements network coding @xcite , and each sink node receives @xmath0 packets from @xmath0 incoming links . in the network", ", there is a wiretapper who observes @xmath1 links .", "the problem is how to encode a secret message into @xmath0 transmitted packets at the source node , in such a way that the wiretapper obtain no information about the message in the sense of information theoretic security .", "as shown in @xcite , secure network coding can be seen as a generalization of the wiretap channel ii @xcite or secret sharing schemes based on linear codes @xcite for network coding .", "hence , in secure network coding , the secrecy is realized by introducing the randomness into @xmath0 transmitted packets as follows .", "suppose the message is represented by @xmath2 packets @xmath3 @xmath4 .", "then , the source node encodes @xmath5 together with @xmath6 random packets by linear codes , and generates @xmath0 transmitted packets @xcite .", "silva et al .", "@xcite proposed the _", "universal secure network coding _ that is based on maximum rank distance ( mrd ) codes @xcite .", "their scheme was universal in the sense that their scheme guarantees that over _ any _ underlying network code , no information about @xmath7 leaks out even if any @xmath6 links are observed by a wiretapper .", "as shown in @xcite , their scheme with mrd codes is optimal in terms of security and communication rate .", "however , there exists some restrictions in universal secure network coding with mrd codes . in their scheme ,", "the network must transport packets of size @xmath8 .", "the mrd code used in the scheme is defined over an @xmath9 , where @xmath10 is an @xmath11-degree field extension of a field @xmath12 with order @xmath13 .", "thus , the size of the field @xmath10 increases exponentially with @xmath11 , and the restriction of mrd codes with @xmath8 invokes the large computational cost for encoding and decoding of mrd codes if @xmath0 is large . it is undesirable especially in resource constraint environments .    considering secure network coding without such a restriction ,", "ngai et al .", "@xcite , and later zhang et al .", "@xcite , investigated the security performance of secure network coding based on general linear codes .", "they introduced a new parameter of linear codes , called the _ relative network generalized hamming weight _ ( rnghw ) , and revealed that the security performance is expressed in terms of the rnghw .", "the rnghw depends on the set of coding vectors of the underlying network code . hence , the rnghw is not universal .", "the aim of this paper is to investigate the security performance of universal secure network coding based on general linear codes , which is always guaranteed over _ any _ underlying network code , even over random network code .", "this paper defines the universal security performance by the following two criteria .", "one is called the _ universal equivocation _", "@xmath14 that is the minimum uncertainty of the message under observation of @xmath1 links , guaranteed independently of the underlying network code .", "the other is called the _ universal @xmath15-strong security _ , where @xmath15 is a performance measure such that no part of the secret message is deterministically revealed even if at most @xmath15 links are observed .", "the paper @xcite proposed a specific construction of the secure network coding that attains the universal @xmath16-strong security , and such a scheme is called universal strongly secure network coding @xcite .", "namely , the definition of universal @xmath15-strong security given in this paper is a generalization of universal strongly secure network coding considered in @xcite for the number of tapped links .    in order to express @xmath14 and @xmath15 in terms of code parameters ,", "this paper introduces two parameters of linear codes , called the _ relative dimension / intersection profile _ ( rdip ) and the _ relative generalized rank weight _ ( rgrw ) .", "the rgrw is a generalization of the minimum rank distance @xcite of a code .", "we reveal that @xmath14 and @xmath15 can be expressed in terms of the rdip and the rgrw of the codes .", "duursma et al .", "@xcite first observed that the _ relative generalized hamming weight _", "@xcite exactly expresses the security performance and the error correction capability of secret sharing .", "our definitions of rgrw and rdip are motivated by their result @xcite .", "assume that the attacker is able not only to eavesdrop but also to inject erroneous packets anywhere in the network .", "also assume that the network may suffer from the rank deficiency of the transfer matrix at a sink node .", "silva et al.s scheme based on mrd codes @xcite enables to correct such errors and rank deficiency at each sink node , where its error correction capability is guaranteed over any underlying network code , universal .", "this paper also generalizes their result and reveals that the universal error correction capability of secure network coding based on arbitrary linear codes can be expressed in terms of the rgrw of the codes .", "the remainder of this paper is organized as follows .", "presents basic notations , and introduces linear network coding . defines the universal security performance and universal error correction capability of secure network coding over wiretap network . defines the rdip and rgrw of linear codes , and introduces their basic properties . in", ", the universal security performance is expressed in terms of the rdip and rgrw .", "the security of existing schemes @xcite is also analyzed as applications of the rdip and rgrw in examples [ ex1 ] and [ ex2 ] .", "gives the expression of the universal error correction capability in terms of the rgrw , and also analyze the error correction of @xcite by the rgrw in example [ ex3 ] .", "let @xmath17 be the shannon entropy for a random variable @xmath18 , @xmath19 be the conditional entropy of @xmath18 given @xmath20 , and @xmath21 be the mutual information between @xmath18 and @xmath20 @xcite .", "we write @xmath22 as the cardinality of a set @xmath23 .", "the entropy and the mutual information are always computed by using @xmath24 .", "let @xmath12 stand for a finite field containing @xmath13 elements and @xmath10 be an @xmath11-degree field extension of @xmath25 ( @xmath26 ) .", "let @xmath27 denote an @xmath0-dimensional row vector space over @xmath12 .", "similarly , @xmath9 stands for an @xmath0-dimensional row vector space over @xmath10 . unless otherwise stated , we consider subspaces , ranks , dimensions , etc , over the field extension @xmath10 instead of the base field @xmath12 .", "an @xmath28 $ ] linear code @xmath29 over @xmath9 is a @xmath30-dimensional subspace of @xmath9 .", "let @xmath31 denote a _ dual code _ of a code @xmath29 .", "a subspace of a code is called a _ subcode _", "for @xmath32 , we denote by @xmath33 a _ subfield subcode _ of @xmath29 over @xmath12 @xcite .", "observe that @xmath34 means the dimension of @xmath29 as a vector space over @xmath10 whereas @xmath35 is the dimension of @xmath36 over @xmath12 .    for a vector @xmath37\\in\\f_{q^m}^n$ ] and a subspace @xmath38 ,", "we denote @xmath39 $ ] and @xmath40 .", "define a family of subspaces @xmath41 satisfying @xmath42 by @xmath43 . also define @xmath44 . for a subspace @xmath45 , the followings are equivalent : 1 ) @xmath46 ; 2 ) @xmath47 ( * ? ? ?", "* lemma 1 ) .      as in @xcite", ", we consider a multicast communication network represented by a directed multigraph with unit capacity links , a single source node , and multiple sink nodes .", "we assume that _ linear network coding _", "@xcite is employed over the network .", "elements of a column vector space @xmath48 are called _", "packets_. assume that each link in the network can carry a single @xmath12-symbol per one time slot , and that each link transports a single packet over @xmath11 time slots without delays , erasures , or errors .", "the source node produces @xmath0 packets @xmath49 ,  , @xmath50 and transmits @xmath49 , ", ", @xmath51 on @xmath0 outgoing links over @xmath11 consecutive time slots .", "define the @xmath52 matrix @xmath53 $ ] .", "the data flow on any link can be represented as an @xmath12-linear combination of packets @xmath54 .", "namely , the information transmitted on a link @xmath55 can be denoted as @xmath56 , where @xmath57 is called a _ global coding vector _ ( gcv ) of @xmath55 .", "suppose that a sink node has @xmath58 incoming links .", "then , the information received at a sink node can be represented as an @xmath59 matrix @xmath60 , where @xmath61 is the transfer matrix constructed by gathering the gcv s of @xmath58 incoming links .", "the network code is called _ feasible _ if every transfer matrix to a sink node has rank @xmath0 over @xmath12 .", "the system is called _ coherent _ if @xmath62 is known to each sink node ; otherwise , called _", "this section introduces the wiretap network model with packet errors and the nested coset coding scheme in secure network coding @xcite .", "then , we define the universal security performance in terms of the _ universal equivocation _ and the _ universal @xmath15-strong security _ on the wiretap network model .", "we also define the universal error correction capability of secure network coding . from now on ,", "only one sink node is assumed without loss of generality .", "in addition , we focus on the fundamental case of coherent systems in this paper due to the space constraint . but , as in @xcite , all analysis in this paper can be easily adapted to the case of noncoherent systems .", "following @xcite , assume that in the setup of , there is a wiretapper who has access to packets transmitted on any @xmath63 links .", "let @xmath64 be the set of @xmath65 links observed by the wiretapper .", "then the packets observed by the wiretapper are given by @xmath66 , where rows of @xmath67 are the gcv s associated with the links in @xmath64 .    in the scenario @xcite", ", the source node first regards an @xmath11-dimensional column vector space @xmath48 as @xmath10 , and fix @xmath2 for @xmath68 .", "let @xmath69 \\!\\in\\ !", "\\f_{q^m}^l$ ] be the secret message , and assume that @xmath3 are uniformly distributed over @xmath70 and mutually independent . under the wiretapper s observation , the source node wants to transmit @xmath7 without information leakage to the wiretapper . to protect @xmath7 from the wiretapper , the source node encodes @xmath7 to a transmitted vector @xmath71\\!\\in\\!\\f_{q^m}^n$ ] of @xmath0 packets by applying the _ nested coset coding scheme", "@xcite on @xmath7 . in @xcite , its special case", "is called a _ secret sharing scheme based on linear codes_.    [ def : nestedcoding ] let @xmath72 be a linear code over @xmath10 ( @xmath26 ) , and @xmath73 be its subcode with dimension @xmath74 over @xmath10 .", "let @xmath75 be an arbitrary isomorphism .", "for a secret message @xmath76 , we choose @xmath18 from a coset @xmath77 uniformly at random and independently of @xmath7 .", "then , the source node finally transmit @xmath18 over the network coded network .", "includes the ozarow - wyner coset coding scheme @xcite as a special case with @xmath78 .", "hence , when we set @xmath78 , this is the secure network coding based on ozarow - wyner coset coding scheme @xcite .", "corresponding to @xmath18 transmitted from the source node , the sink node receives a vector of @xmath58 packets @xmath79 . here", "we extend the basic network model described in to incorporate packet errors and rank deficiency of the transfer matrix @xmath80 of the sink node .", "suppose that at most @xmath81 errors can occur in any of links , causing the corresponding packets to become corrupted .", "then , as @xcite , @xmath20 can be expressed by @xmath82 where @xmath83 is the @xmath81 error packets , and @xmath84 is the transfer matrix of @xmath85 .", "we define @xmath86 as the rank deficiency of @xmath62 . in this setup , we want to decode @xmath7 correctly from @xmath20 .", "if the network is free of errors and the network code used is feasible , @xmath18 can be always reconstructed from @xmath87 as described in .", "then , the coset @xmath88 , and hence @xmath7 , is uniquely determined from @xmath18 from .      the security performance of secure network coding in the above model was measured by the following criterion @xcite .    [ def : nonuniversalperformance ] the minimum uncertainty @xmath89 of @xmath7 given @xmath90 for all possible @xmath64 s ( @xmath65 ) in the network is called _ equivocation _ , defined as @xmath91 .    as defined in", ", @xmath89 depends on the underlying network code . in @xcite , @xmath89 for @xmath92", "was expressed in terms of the relative network generalized hamming weight ( rnghw ) of @xmath93 and @xmath94 .", "the rnghw is the value determined according to gcv s of all links in the network .", "hence , the rnghw can not determine the equivocation over random linear network code @xcite .", "here , we extend by requiring the independence of the underlying network code , as follows .", "[ def : universalperformance ] the _ universal equivocation _", "@xmath14 is the minimum uncertainty of @xmath7 given @xmath95 for all @xmath96 , defined as @xmath97    as defined in , @xmath14 does not depend on the set of @xmath64 s in the network .", "silva et al.s universal secure network coding scheme based on mrd codes @xcite achieves @xmath98 in provided @xmath8 .    defines the security for the whole components of a message @xmath99 $ ] . here", "we focus on the security for every part of @xmath7 , and give the following definition .", "[ def : universalalpha ] let @xmath100 be a tuple for a subset @xmath101 .", "we say that a secure network coding scheme attains the _", "universal @xmath15-strong security _ if we have @xmath102    as @xcite , a scheme with universal @xmath15-strong security does not leak any @xmath103 components of @xmath7 even if at most @xmath104 links are observed by the wiretapper .", "moreover , this guarantee holds over any underlying network code as @xmath14 .", "we note that if a scheme achieves the @xmath15-strong security , the universal equivocation @xmath14 for @xmath105 must be @xmath106 as shown in .", "however , the converse does not always hold .", "the scheme in @xcite achieves @xmath107 provided @xmath108 by nested coset coding with mrd codes .", "the universal strongly security in @xcite is a special case of with @xmath109 .      in the model described in", ", the error correction capability of secure network coding , guaranteed over any underlying network code , is defined as follows .", "a secure network coding scheme is called _ universally @xmath81-error-@xmath110-erasure - correcting _ ,", "if @xmath111 @xmath7 can be uniquely determined from @xmath20 against @xmath81 errors over any underlying network code with at most @xmath110 rank deficiency .", "silva et al.s scheme ( * ? ? ?", "* section vi ) is universally @xmath81-error-@xmath110-erasure - correcting when the minimum rank distance @xcite of @xmath93 is greater than @xmath112 .", "this section introduce the _ relative dimension / intersection profile _ ( rdip ) and the _ relative generalized rank weight _ ( rgrw ) of linear codes . in the following sections ,", "these parameters are used to characterize the universal security performance and the universal error correction capability of secure network coding .", "we first define the _ relative dimension / intersection profile _ ( rdip ) of linear codes as follows .", "[ def : rdip ] let @xmath72 be a linear code and @xmath113 be its subcode .", "then , the @xmath114-th relative dimension / intersection profile ( rdip ) of @xmath93 and @xmath94 is the greatest difference between dimensions over @xmath10 of intersections , defined as @xmath115 for @xmath116 .", "next , we define the _ relative generalized rank weight _ ( rgrw ) of linear codes as follows .", "[ def : rgrw ] let @xmath72 be a linear code and @xmath113 be its subcode .", "then , the @xmath114-th relative generalized rank weight ( rgrw ) of @xmath93 and @xmath94 is defined by @xmath117 for @xmath118 .    the relative dimension / length profile and the relative generalized hamming weight introduced in @xcite", "are equivalent to and ( [ eq : defrgrw ] ) with @xmath119 and @xmath120 replaced by suitable smaller sets , respectively .", "this subsection introduces some basic properties of the rdip and the rgrw , and also shows the relation between the rgrw and the rank distance @xcite .", "these will be used for expressions of the universal security performance and the universal error correction capability of secure network coding .", "first , we introduce the following theorem and lemma about the rdip and the rgrw .", "[ thm : monotonerdip ] let @xmath72 be a linear code and @xmath73 be its subcode .", "then , the @xmath114-th rdip @xmath121 is nondecreasing with @xmath114 from @xmath122 to @xmath123 , and @xmath124 holds .", "@xmath122 and @xmath123 , are obvious from .", "recall that @xmath125 for @xmath126 from ( * ? ? ?", "* lemma 1 ) .", "this implies that for any subspace @xmath127 , there always exist some @xmath128 s satisfying @xmath129 and @xmath130 .", "this yields @xmath131 .", "next we show that the increment at each step is at most @xmath132 .", "consider arbitrary subspaces @xmath133 such that @xmath134 and @xmath135 .", "let @xmath136 ; @xmath137 .", "since @xmath138 and @xmath73 , we have @xmath139 and hence @xmath140 .", "[ lma : rgrw ] let @xmath72 be a linear code and @xmath73 be its subcode . then , the @xmath114-th rgrw @xmath141 is strictly increasing with @xmath114 .", "moreover , @xmath142 and @xmath143 where @xmath144 .", "first we have @xmath145 from , we have @xmath146 .", "we thus have @xmath147 therefore the rgrw is strictly increasing with @xmath114 and thus @xmath148 is established .", "next , we show the relation between the rank distance @xcite and the rgrw .", "let @xmath149 be an @xmath12-linear isomorphism that expands an element of @xmath10 as a column vector over @xmath12 with respect to some fixed basis for @xmath10 over @xmath12 .", "then , we define the _ rank over @xmath12 _ of a vector @xmath150 \\in \\f_{q^m}^n$ ] , denoted by @xmath151 , as the rank of @xmath52 matrix @xmath152 $ ] over @xmath12 .", "the rank distance @xcite between two vectors @xmath153 is given by @xmath154 .", "the minimum rank distance @xcite of a code @xmath29 is given as @xmath155 .", "for a subspace @xmath41 , we define by @xmath156 the sum of subspaces @xmath157 .", "[ lma : xxxx ] for a subspace @xmath38 with @xmath158 , we have @xmath159 .", "let @xmath160 \\!\\in\\ !", "v$ ] be a nonzero vector , which implies @xmath161 .", "let @xmath162_{i , j=1}^{m , n } \\!\\in\\ ! \\f_{q^m}^{m \\times n}$ ] , @xmath163 . each vector in @xmath164 is represented by an @xmath10-linear combination of @xmath165 , and hence @xmath166 .    for @xmath167 , @xmath168 , we have @xmath169 .", "this implies that there always exists some @xmath170 with @xmath171 satisfying @xmath172 \\!\\in\\ !", "\\f_{q^m}^n , g_j   \\!\\neq\\ ! 0 , \\label{eq : transform}\\end{aligned}\\ ] ] where @xmath173 are linearly independent over @xmath12 , and note that @xmath174 represents the elementary column operation on @xmath175 $ ] . also for @xmath176 , @xmath177 , we have @xmath178 ( @xmath179 ) .", "hence , for @xmath170 satisfying , we also have @xmath180 \\!\\in\\ !", "\\f_{q^m}^n$ ] for all @xmath181 .", "thus , by the elementary column operation on @xmath182 over @xmath12 , represented by @xmath174 , we get @xmath183 . by eliminating zero columns from @xmath183", ", we obtain a matrix @xmath184_{i , j=1}^{m , d_r(v)}$ ] , @xmath185 , where @xmath186 .", "let @xmath187 @xmath188 be the submatrix consisting of the first @xmath30 rows of @xmath189 . since @xmath190 and @xmath173 are linearly independent , @xmath191 is the generator matrix of @xmath192 $ ] gabidulin code and @xmath193 @xcite .", "thus , @xmath194 is nonsingular , and hence we have @xmath195 .", "therefore , @xmath196 .", "[ lma : rankdistance ] for a code @xmath72 and its subcode @xmath73 , the first rgrw can be represented as @xmath197 .", "@xmath198 can be represented as @xmath199 for any subspace @xmath38 with @xmath200 , there always exists some @xmath201 satisfying @xmath202 , because we have @xmath203 and @xmath204 . also , for subspaces @xmath205 and @xmath206 with @xmath200 , if @xmath205 is the smallest space in @xmath120 including @xmath207 , then @xmath208 @xcite .", "thus can be rewritten as @xmath209 where the last equality of is obtained by @xmath210 , and @xmath211 from @xmath212 .", "for subspaces @xmath207 and @xmath213 , we have @xmath214 .", "therefore , can be rewritten as follows .", "@xmath215 +    immediately yields the following corollary .", "[ coro : rankdistance ] for a linear code @xmath29 , @xmath216 holds .", "this shows that @xmath217 is a generalization of @xmath218 .", "now we present the following proposition that generalizes the singleton - type bound of the rank distance @xcite .", "[ prop : generalizedsingleton ] let @xmath72 be a linear code and @xmath73 be its subcode .", "then , the rgrw of @xmath93 and @xmath94 is upper bounded by @xmath219 for @xmath220 .", "we can consider that @xmath94 is a systematic code without loss of generality .", "that is , the first @xmath221 coordinates of each basis of @xmath94 is one of canonical bases of @xmath222 .", "let @xmath223 be a linear code such that @xmath93 is a direct sum of @xmath94 and @xmath224 . then , after suitable permutation of coordinates , a basis of @xmath224 can be chosen such that its first @xmath221 coordinates are zero .", "then , the effective length @xcite of a code @xmath224 is less than or equal to @xmath225 .", "hence we have @xmath226 from the singleton - type bound for rank metric @xcite .    here", "we write @xmath227 for the sake of simplicity . recall that @xmath228 from , and @xmath229 holds from .", "we shall use the mathematical induction on @xmath81 .", "we see that is true for @xmath230 .", "assume that for some @xmath231 , @xmath232 is true .", "then , by the monotonicity shown in , @xmath233 holds .", "thus , it is proved by mathematical induction that holds for @xmath234 .", "lastly , we prove by the above discussion about the rgrw of @xmath224 and @xmath235 . for an arbitrary fixed subspace @xmath38 , we have @xmath236 , because @xmath93 is a direct sum of @xmath224 and @xmath94 .", "hence , @xmath237 holds , and we have @xmath238 for @xmath239 from .", "therefore , from the foregoing proof , we have @xmath240 for @xmath239 , and the proposition is proved .", "immediately yields the following corollary .", "[ coro : mrdrgrw ] for a linear code @xmath32 , @xmath241 for @xmath242 .", "the equality holds for all @xmath114 if and only if @xmath29 is an mrd code .", "in this section , we express @xmath14 and @xmath15 given in in terms of the rdip and rgrw . from now on , we use the following definition .    for @xmath243", ", we define @xmath244 .    recall that if an @xmath10-linear space @xmath38 admits a basis in @xmath245 then @xmath246 @xcite , which implies @xmath247    first , we give the following theorem for the universal equivocation @xmath14 given in    [ thm : equivocation ] consider the nested coset coding in . then , the universal equivocation @xmath14 of @xmath248 is given by @xmath249    let @xmath250 be an arbitrary matrix . by the chain rule @xcite", ", we have the following equation for the conditional entropy of @xmath7 given @xmath95 : @xmath251 then , from ( * ? ? ?", "* proof of lemma 4.2 ) , we have @xmath252 by substituting these equations into , we have @xmath253 by we have @xmath254 thus , by and , the universal equivocation @xmath14 is given as follows .", "@xmath255 +    [ ex1 ] the existing schemes @xcite used mrd codes as @xmath256 and @xmath257 , where @xmath8 . by , we have @xmath258 for any @xmath259 .", "this implies @xmath260 for @xmath261 .    on the other hand , @xmath262 by .", "since @xmath263 for any @xmath264 by , we have @xmath265 . by , @xmath266 , @xmath267 for @xmath268 .    by", ", we see that @xmath269 for @xmath270 in the schemes @xcite .", "we then have the following corollary by the rgrw .", "shows that the wiretapper obtain no information of @xmath7 from any @xmath271 links .", "[ prop : perfectsecrecy ] consider the nested coset coding in .", "then , the wiretapper must observe at least @xmath272 links to obtain the mutual information @xmath273 ( @xmath274 ) between @xmath7 and observed packets .    from", ", the smallest number @xmath63 of tapped links satisfying @xmath275 @xmath276 is @xmath277 from ( * ? ?", "* lemma 1 ) and , this equation can be rewritten as follows .", "@xmath278 +    although the message @xmath7 has been assumed to be uniformly distributed over @xmath70 in , the following proposition reveals that the wiretapper still obtain no information of @xmath7 from any @xmath271 links even if @xmath7 is arbitrarily distributed .", "[ coro : distribution ] fix the transfer matrix @xmath279 to the wiretapper .", "suppose that the wiretapper obtain no information of @xmath7 from @xmath95 when @xmath7 is uniformly distributed over @xmath70 as described in .", "then , even if @xmath7 is chosen according to an arbitrary distribution over @xmath70 , the wiretapper still obtain no information of @xmath7 from @xmath95 , that is , @xmath280 .", "when we assume that @xmath7 is arbitrarily distributed over @xmath70 , @xmath281 is upper bounded as follows from ( * ? ? ?", "* proof of lemma 6 ) and ( * ? ? ? * proof of lemma 4.2 ) .", "@xmath282 also , since @xmath18 is uniformly distributed over a coset @xmath283 for fixed @xmath7 , we have @xmath284 . for the dimension of a subspace @xmath285", ", we have @xmath286 where @xmath287 is a generator matrix of @xmath93 .", "hence we have @xmath288 .", "we thus have @xmath289 for any distribution of @xmath7 .", "by @xmath290 and we can see that the equality holds if @xmath7 is uniformly distributed .", "therefore , for fixed @xmath279 , if @xmath280 holds for uniformly distributed @xmath7 , then the right hand side of is zero , which implies that @xmath280 also holds for arbitrarily distributed @xmath7 from the nonnegativity of mutual information @xcite .", "lastly , we express @xmath15 in in terms of the rgrw . for a subset @xmath291 and a vector @xmath292\\in\\f_{q^m}^n$ ] , let @xmath293 be a vector of length @xmath294 over @xmath10 , obtained by removing the @xmath81-th components @xmath295 for @xmath296 .", "for example for @xmath297 and @xmath298 $ ] ( @xmath299 ) , we have @xmath300 $ ] .", "the _ punctured code _", "@xmath301 of a code @xmath302 is given by @xmath303 .", "the _ shortened code _", "@xmath304 of a code @xmath305 is defined by @xmath306 \\in \\c , c_i = 0 \\text { for } i \\notin \\mathcal{j } \\right\\}$ ] .", "for example for @xmath307,[1,1,0],[1,0,1],[0,1,1]\\}$ ] ( @xmath308 ) and @xmath309 , we have @xmath310,[1,1]\\}$ ] .", "we then have the following theorem for the universal @xmath15-strong security defined in .", "[ thm : universalalpha ] let @xmath311 .", "fix @xmath93 , @xmath94 and @xmath312 in and consider the corresponding nested coset coding scheme in . by using @xmath93 , @xmath94 and @xmath312 , define @xmath313 : s \\in \\f_{q^m}^l\\text {   and } x\\in\\psi(s)\\right\\ } \\subseteq \\f_{q^m}^{l+n}.\\end{aligned}\\ ] ] for each index @xmath314 , we define a punctured code @xmath315 of @xmath316 as @xmath317 , and a shortened code @xmath318 of @xmath316 as @xmath319 .", "then , the value @xmath15 in is given by @xmath320    define @xmath321 : \\vec{c}_2 \\in \\c_2\\}\\subseteq\\f_{q^m}^{l+n}$ ] . since @xmath73 , @xmath322 is also a subcode of @xmath316 .", "thus , in terms of @xmath316 and @xmath322 , we can see that the vector @xmath323\\in\\f_{q^m}^{l+n}$ ] is generated by a nested coset coding scheme of @xmath316 and @xmath322 from @xmath7 .", "then , from the definition of @xmath316 and @xmath322 , we can see that @xmath318 is a subcode of @xmath315 with dimension @xmath324 over @xmath10 for each @xmath325 .", "let @xmath326 and @xmath327 $ ] for each @xmath328 .", "for @xmath329 define a coset @xmath330 :   s_{\\mathcal{l}\\backslash\\{i\\ } } \\in\\f_{q^m}^{l-1 }   \\text { and }   x \\in \\psi(s)\\right\\ } \\in\\d_{1,i}/\\d_{2,i}.\\end{aligned}\\ ] ] here we define @xmath331)= [ s_{\\mathcal{l}\\backslash\\{i\\}},x ] \\in \\d_{1,i}$ ] .", "recall that @xmath3 are mutually independent and uniformly distributed over @xmath10 .", "thus , considering a nested coset coding scheme that generates @xmath332 from a secret message @xmath333 with @xmath334 , we can see that @xmath335 is chosen uniformly at random from @xmath336 .", "therefore , we have @xmath337 for any @xmath338 whenever @xmath339 from .    for an arbitrary subset @xmath340 ,", "define a matrix @xmath341 that consists of @xmath342 rows of an @xmath343 identity matrix , satisfying @xmath344^{\\rm t } \\!=\\ ! f_\\mathcal{r", "} s_{\\mathcal{l}\\backslash\\{i\\}}^{\\rm t}$ ] . for an arbitrary matrix @xmath345 ( @xmath346 ) , set @xmath347 $ ] .", "then , from the foregoing proof , we have @xmath348 whenever @xmath349 . since @xmath350 is equivalent to from ( * ? ? ?", "* prop.5 ) , we have by selecting the minimum value of @xmath351 for @xmath352 .", "[ ex2 ] the scheme proposed in @xcite used a systematic mrd code as @xmath316 ( not @xmath93 ) , where @xmath108 .", "we proved @xmath353 in ( * ? ? ? * proof of theorem 4 ) . by , we see that the scheme @xcite attains the universal @xmath16-strong security in the sense of , while @xcite proved it by adapting the proof argument in @xcite .    as shown in , no information of @xmath7 is leaked from less than @xmath354 tapped links even if @xmath7 is arbitrarily distributed .", "in contrast , @xmath7 must be uniformly distributed over @xmath70 to establish .", "this is because elements of @xmath7 need to be treated as extra random packets , as in strongly secure network coding schemes @xcite .", "this section derives the universal error correction capability by the approach of ( * ? ? ?", "* section iii ) . recall that the received packets @xmath20 is given by @xmath355 in the setup of , and that @xmath18 is chosen from the coset @xmath283 corresponding to @xmath7 by the nested coset coding in . from now on ,", "we write @xmath356 for the sake of simplicity .", "first , we define the _ discrepancy _ @xcite between @xmath23 and @xmath20 by @xmath357 where the second equality is derived from ( * ? ? ?", "* lemma 4 ) .", "this definition of @xmath358 represents the minimum number @xmath359 of error packets @xmath85 required to be injected in order to transform at least one element of @xmath23 into @xmath20 , as ( * ? ? ?", "* eq.(9 ) ) .", "let @xmath363 and let @xmath374 .", "then , @xmath375 from .", "let @xmath376 and @xmath377 be vectors satisfying @xmath378 . from the proof of (", "* theorem 6 ) , we can always find two vectors @xmath379 such that @xmath380 , @xmath381 and @xmath382 .", "taking @xmath383 , we have @xmath384 and @xmath385 .", "we thus obtain @xmath386 and @xmath387 from . on the other hand , since @xmath388 , we have @xmath389 for any @xmath390 from from", ". therefore , @xmath391 and @xmath392 hold .", "[ prop : deltacorrection ] a nested coset coding scheme with @xmath248 is guaranteed to determine the unique coset @xmath23 against any @xmath81 packet errors for any fixed @xmath62 if and only if @xmath395 .            [ thm : errorcorrectioncap ] consider the nested coset coding in .", "then , the scheme is a universally ( simultaneously for all @xmath80 with rank deficiency at most @xmath110 ) @xmath81-error-@xmath110-erasure - correcting secure network coding if and only if @xmath398 .    for the rank deficiency @xmath399 , we have @xmath400 , and there always exists @xmath401 depending on @xmath402 such that the equality holds .", "thus , from , we have @xmath403 & \\!=\\ ! \\min \\left\\ { d_r(x,\\vec{0 } ) : x \\in \\c_1 , x \\notin \\c_2 \\right\\}-\\rho\\\\ & \\!=\\ !", "m_{r,1}(\\c_1,\\c_2 ) -\\rho .", "\\quad \\text{(by \\lma{lma : rankdistance})}\\end{aligned}\\ ] ] therefore , we have @xmath404 for @xmath405 , and hence we obtain @xmath406 @xmath407 .", "[ ex3 ] the existing scheme @xcite used mrd codes as @xmath248 , where @xmath8 .", "then , by , we have @xmath408 . since @xmath409 for any @xmath410 by and @xmath411 , we have @xmath412 .", "thus , by and , the scheme is universally @xmath81-error-@xmath110-erasure - correcting when @xmath413 , as shown in ( * ? ? ?", "* theorem 11 ) .", "h.  chen , r.  cramer , s.  goldwasser , r.  de  haan , and v.  vaikuntanathan , `` secure computation from random error correcting codes , '' in _ proc .", "eurocrypt 2007 _ , ser .", "lecture notes in computer science , vol .", "4515.1em plus 0.5em minus 0.4emspringer - verlag , 2007 , pp . 291310 .", "t.  ho , m.  mdard , r.  koetter , d.  r. karger , m.  effros , j.  shi , and b.  leong , `` a random linear network coding approach to multicast , '' _ ieee trans .", "inf . theory _ ,", "52 , no .", "10 , pp . 44134430 , oct ."], "abstract_text": ["<S> the universal secure network coding presented by silva et al . realizes secure and reliable transmission of a secret message over any underlying network code , by using maximum rank distance codes . </S>", "<S> inspired by their result , this paper considers the secure network coding based on arbitrary linear codes , and investigates its security performance and error correction capability that are guaranteed independently of the underlying network code . </S>", "<S> the security performance and error correction capability are said to be _ universal _ when they are independent of underlying network codes . </S>", "<S> this paper introduces new code parameters , the relative dimension / intersection profile ( rdip ) and the relative generalized rank weight ( rgrw ) of linear codes . </S>", "<S> we reveal that the universal security performance and universal error correction capability of secure network coding are expressed in terms of the rdip and rgrw of linear codes . the security and error correction of existing schemes </S>", "<S> are also analyzed as applications of the rdip and rgrw . </S>"], "labels": null, "section_names": ["introduction", "preliminary", "universal security performance and universal error correction capability of secure network coding", "new parameters of linear codes and their properties", "universal security performance on wiretap networks", "universal error correction capability of secure network coding"], "sections": [["in the scenario of _ secure network coding _ introduced by cai et al .", "@xcite , a source node transmits @xmath0 packets from @xmath0 outgoing links to sink nodes through a network that implements network coding @xcite , and each sink node receives @xmath0 packets from @xmath0 incoming links . in the network", ", there is a wiretapper who observes @xmath1 links .", "the problem is how to encode a secret message into @xmath0 transmitted packets at the source node , in such a way that the wiretapper obtain no information about the message in the sense of information theoretic security .", "as shown in @xcite , secure network coding can be seen as a generalization of the wiretap channel ii @xcite or secret sharing schemes based on linear codes @xcite for network coding .", "hence , in secure network coding , the secrecy is realized by introducing the randomness into @xmath0 transmitted packets as follows .", "suppose the message is represented by @xmath2 packets @xmath3 @xmath4 .", "then , the source node encodes @xmath5 together with @xmath6 random packets by linear codes , and generates @xmath0 transmitted packets @xcite .", "silva et al .", "@xcite proposed the _", "universal secure network coding _ that is based on maximum rank distance ( mrd ) codes @xcite .", "their scheme was universal in the sense that their scheme guarantees that over _ any _ underlying network code , no information about @xmath7 leaks out even if any @xmath6 links are observed by a wiretapper .", "as shown in @xcite , their scheme with mrd codes is optimal in terms of security and communication rate .", "however , there exists some restrictions in universal secure network coding with mrd codes . in their scheme ,", "the network must transport packets of size @xmath8 .", "the mrd code used in the scheme is defined over an @xmath9 , where @xmath10 is an @xmath11-degree field extension of a field @xmath12 with order @xmath13 .", "thus , the size of the field @xmath10 increases exponentially with @xmath11 , and the restriction of mrd codes with @xmath8 invokes the large computational cost for encoding and decoding of mrd codes if @xmath0 is large . it is undesirable especially in resource constraint environments .    considering secure network coding without such a restriction ,", "ngai et al .", "@xcite , and later zhang et al .", "@xcite , investigated the security performance of secure network coding based on general linear codes .", "they introduced a new parameter of linear codes , called the _ relative network generalized hamming weight _ ( rnghw ) , and revealed that the security performance is expressed in terms of the rnghw .", "the rnghw depends on the set of coding vectors of the underlying network code . hence , the rnghw is not universal .", "the aim of this paper is to investigate the security performance of universal secure network coding based on general linear codes , which is always guaranteed over _ any _ underlying network code , even over random network code .", "this paper defines the universal security performance by the following two criteria .", "one is called the _ universal equivocation _", "@xmath14 that is the minimum uncertainty of the message under observation of @xmath1 links , guaranteed independently of the underlying network code .", "the other is called the _ universal @xmath15-strong security _ , where @xmath15 is a performance measure such that no part of the secret message is deterministically revealed even if at most @xmath15 links are observed .", "the paper @xcite proposed a specific construction of the secure network coding that attains the universal @xmath16-strong security , and such a scheme is called universal strongly secure network coding @xcite .", "namely , the definition of universal @xmath15-strong security given in this paper is a generalization of universal strongly secure network coding considered in @xcite for the number of tapped links .    in order to express @xmath14 and @xmath15 in terms of code parameters ,", "this paper introduces two parameters of linear codes , called the _ relative dimension / intersection profile _ ( rdip ) and the _ relative generalized rank weight _ ( rgrw ) .", "the rgrw is a generalization of the minimum rank distance @xcite of a code .", "we reveal that @xmath14 and @xmath15 can be expressed in terms of the rdip and the rgrw of the codes .", "duursma et al .", "@xcite first observed that the _ relative generalized hamming weight _", "@xcite exactly expresses the security performance and the error correction capability of secret sharing .", "our definitions of rgrw and rdip are motivated by their result @xcite .", "assume that the attacker is able not only to eavesdrop but also to inject erroneous packets anywhere in the network .", "also assume that the network may suffer from the rank deficiency of the transfer matrix at a sink node .", "silva et al.s scheme based on mrd codes @xcite enables to correct such errors and rank deficiency at each sink node , where its error correction capability is guaranteed over any underlying network code , universal .", "this paper also generalizes their result and reveals that the universal error correction capability of secure network coding based on arbitrary linear codes can be expressed in terms of the rgrw of the codes .", "the remainder of this paper is organized as follows .", "presents basic notations , and introduces linear network coding . defines the universal security performance and universal error correction capability of secure network coding over wiretap network . defines the rdip and rgrw of linear codes , and introduces their basic properties . in", ", the universal security performance is expressed in terms of the rdip and rgrw .", "the security of existing schemes @xcite is also analyzed as applications of the rdip and rgrw in examples [ ex1 ] and [ ex2 ] .", "gives the expression of the universal error correction capability in terms of the rgrw , and also analyze the error correction of @xcite by the rgrw in example [ ex3 ] ."], ["let @xmath17 be the shannon entropy for a random variable @xmath18 , @xmath19 be the conditional entropy of @xmath18 given @xmath20 , and @xmath21 be the mutual information between @xmath18 and @xmath20 @xcite .", "we write @xmath22 as the cardinality of a set @xmath23 .", "the entropy and the mutual information are always computed by using @xmath24 .", "let @xmath12 stand for a finite field containing @xmath13 elements and @xmath10 be an @xmath11-degree field extension of @xmath25 ( @xmath26 ) .", "let @xmath27 denote an @xmath0-dimensional row vector space over @xmath12 .", "similarly , @xmath9 stands for an @xmath0-dimensional row vector space over @xmath10 . unless otherwise stated , we consider subspaces , ranks , dimensions , etc , over the field extension @xmath10 instead of the base field @xmath12 .", "an @xmath28 $ ] linear code @xmath29 over @xmath9 is a @xmath30-dimensional subspace of @xmath9 .", "let @xmath31 denote a _ dual code _ of a code @xmath29 .", "a subspace of a code is called a _ subcode _", "for @xmath32 , we denote by @xmath33 a _ subfield subcode _ of @xmath29 over @xmath12 @xcite .", "observe that @xmath34 means the dimension of @xmath29 as a vector space over @xmath10 whereas @xmath35 is the dimension of @xmath36 over @xmath12 .    for a vector @xmath37\\in\\f_{q^m}^n$ ] and a subspace @xmath38 ,", "we denote @xmath39 $ ] and @xmath40 .", "define a family of subspaces @xmath41 satisfying @xmath42 by @xmath43 . also define @xmath44 . for a subspace @xmath45 , the followings are equivalent : 1 ) @xmath46 ; 2 ) @xmath47 ( * ? ? ?", "* lemma 1 ) .      as in @xcite", ", we consider a multicast communication network represented by a directed multigraph with unit capacity links , a single source node , and multiple sink nodes .", "we assume that _ linear network coding _", "@xcite is employed over the network .", "elements of a column vector space @xmath48 are called _", "packets_. assume that each link in the network can carry a single @xmath12-symbol per one time slot , and that each link transports a single packet over @xmath11 time slots without delays , erasures , or errors .", "the source node produces @xmath0 packets @xmath49 ,  , @xmath50 and transmits @xmath49 , ", ", @xmath51 on @xmath0 outgoing links over @xmath11 consecutive time slots .", "define the @xmath52 matrix @xmath53 $ ] .", "the data flow on any link can be represented as an @xmath12-linear combination of packets @xmath54 .", "namely , the information transmitted on a link @xmath55 can be denoted as @xmath56 , where @xmath57 is called a _ global coding vector _ ( gcv ) of @xmath55 .", "suppose that a sink node has @xmath58 incoming links .", "then , the information received at a sink node can be represented as an @xmath59 matrix @xmath60 , where @xmath61 is the transfer matrix constructed by gathering the gcv s of @xmath58 incoming links .", "the network code is called _ feasible _ if every transfer matrix to a sink node has rank @xmath0 over @xmath12 .", "the system is called _ coherent _ if @xmath62 is known to each sink node ; otherwise , called _"], ["this section introduces the wiretap network model with packet errors and the nested coset coding scheme in secure network coding @xcite .", "then , we define the universal security performance in terms of the _ universal equivocation _ and the _ universal @xmath15-strong security _ on the wiretap network model .", "we also define the universal error correction capability of secure network coding . from now on ,", "only one sink node is assumed without loss of generality .", "in addition , we focus on the fundamental case of coherent systems in this paper due to the space constraint . but , as in @xcite , all analysis in this paper can be easily adapted to the case of noncoherent systems .", "following @xcite , assume that in the setup of , there is a wiretapper who has access to packets transmitted on any @xmath63 links .", "let @xmath64 be the set of @xmath65 links observed by the wiretapper .", "then the packets observed by the wiretapper are given by @xmath66 , where rows of @xmath67 are the gcv s associated with the links in @xmath64 .    in the scenario @xcite", ", the source node first regards an @xmath11-dimensional column vector space @xmath48 as @xmath10 , and fix @xmath2 for @xmath68 .", "let @xmath69 \\!\\in\\ !", "\\f_{q^m}^l$ ] be the secret message , and assume that @xmath3 are uniformly distributed over @xmath70 and mutually independent . under the wiretapper s observation , the source node wants to transmit @xmath7 without information leakage to the wiretapper . to protect @xmath7 from the wiretapper , the source node encodes @xmath7 to a transmitted vector @xmath71\\!\\in\\!\\f_{q^m}^n$ ] of @xmath0 packets by applying the _ nested coset coding scheme", "@xcite on @xmath7 . in @xcite , its special case", "is called a _ secret sharing scheme based on linear codes_.    [ def : nestedcoding ] let @xmath72 be a linear code over @xmath10 ( @xmath26 ) , and @xmath73 be its subcode with dimension @xmath74 over @xmath10 .", "let @xmath75 be an arbitrary isomorphism .", "for a secret message @xmath76 , we choose @xmath18 from a coset @xmath77 uniformly at random and independently of @xmath7 .", "then , the source node finally transmit @xmath18 over the network coded network .", "includes the ozarow - wyner coset coding scheme @xcite as a special case with @xmath78 .", "hence , when we set @xmath78 , this is the secure network coding based on ozarow - wyner coset coding scheme @xcite .", "corresponding to @xmath18 transmitted from the source node , the sink node receives a vector of @xmath58 packets @xmath79 . here", "we extend the basic network model described in to incorporate packet errors and rank deficiency of the transfer matrix @xmath80 of the sink node .", "suppose that at most @xmath81 errors can occur in any of links , causing the corresponding packets to become corrupted .", "then , as @xcite , @xmath20 can be expressed by @xmath82 where @xmath83 is the @xmath81 error packets , and @xmath84 is the transfer matrix of @xmath85 .", "we define @xmath86 as the rank deficiency of @xmath62 . in this setup , we want to decode @xmath7 correctly from @xmath20 .", "if the network is free of errors and the network code used is feasible , @xmath18 can be always reconstructed from @xmath87 as described in .", "then , the coset @xmath88 , and hence @xmath7 , is uniquely determined from @xmath18 from .      the security performance of secure network coding in the above model was measured by the following criterion @xcite .    [ def : nonuniversalperformance ] the minimum uncertainty @xmath89 of @xmath7 given @xmath90 for all possible @xmath64 s ( @xmath65 ) in the network is called _ equivocation _ , defined as @xmath91 .    as defined in", ", @xmath89 depends on the underlying network code . in @xcite , @xmath89 for @xmath92", "was expressed in terms of the relative network generalized hamming weight ( rnghw ) of @xmath93 and @xmath94 .", "the rnghw is the value determined according to gcv s of all links in the network .", "hence , the rnghw can not determine the equivocation over random linear network code @xcite .", "here , we extend by requiring the independence of the underlying network code , as follows .", "[ def : universalperformance ] the _ universal equivocation _", "@xmath14 is the minimum uncertainty of @xmath7 given @xmath95 for all @xmath96 , defined as @xmath97    as defined in , @xmath14 does not depend on the set of @xmath64 s in the network .", "silva et al.s universal secure network coding scheme based on mrd codes @xcite achieves @xmath98 in provided @xmath8 .    defines the security for the whole components of a message @xmath99 $ ] . here", "we focus on the security for every part of @xmath7 , and give the following definition .", "[ def : universalalpha ] let @xmath100 be a tuple for a subset @xmath101 .", "we say that a secure network coding scheme attains the _", "universal @xmath15-strong security _ if we have @xmath102    as @xcite , a scheme with universal @xmath15-strong security does not leak any @xmath103 components of @xmath7 even if at most @xmath104 links are observed by the wiretapper .", "moreover , this guarantee holds over any underlying network code as @xmath14 .", "we note that if a scheme achieves the @xmath15-strong security , the universal equivocation @xmath14 for @xmath105 must be @xmath106 as shown in .", "however , the converse does not always hold .", "the scheme in @xcite achieves @xmath107 provided @xmath108 by nested coset coding with mrd codes .", "the universal strongly security in @xcite is a special case of with @xmath109 .      in the model described in", ", the error correction capability of secure network coding , guaranteed over any underlying network code , is defined as follows .", "a secure network coding scheme is called _ universally @xmath81-error-@xmath110-erasure - correcting _ ,", "if @xmath111 @xmath7 can be uniquely determined from @xmath20 against @xmath81 errors over any underlying network code with at most @xmath110 rank deficiency .", "silva et al.s scheme ( * ? ? ?", "* section vi ) is universally @xmath81-error-@xmath110-erasure - correcting when the minimum rank distance @xcite of @xmath93 is greater than @xmath112 ."], ["this section introduce the _ relative dimension / intersection profile _ ( rdip ) and the _ relative generalized rank weight _ ( rgrw ) of linear codes . in the following sections ,", "these parameters are used to characterize the universal security performance and the universal error correction capability of secure network coding .", "we first define the _ relative dimension / intersection profile _ ( rdip ) of linear codes as follows .", "[ def : rdip ] let @xmath72 be a linear code and @xmath113 be its subcode .", "then , the @xmath114-th relative dimension / intersection profile ( rdip ) of @xmath93 and @xmath94 is the greatest difference between dimensions over @xmath10 of intersections , defined as @xmath115 for @xmath116 .", "next , we define the _ relative generalized rank weight _ ( rgrw ) of linear codes as follows .", "[ def : rgrw ] let @xmath72 be a linear code and @xmath113 be its subcode .", "then , the @xmath114-th relative generalized rank weight ( rgrw ) of @xmath93 and @xmath94 is defined by @xmath117 for @xmath118 .    the relative dimension / length profile and the relative generalized hamming weight introduced in @xcite", "are equivalent to and ( [ eq : defrgrw ] ) with @xmath119 and @xmath120 replaced by suitable smaller sets , respectively .", "this subsection introduces some basic properties of the rdip and the rgrw , and also shows the relation between the rgrw and the rank distance @xcite .", "these will be used for expressions of the universal security performance and the universal error correction capability of secure network coding .", "first , we introduce the following theorem and lemma about the rdip and the rgrw .", "[ thm : monotonerdip ] let @xmath72 be a linear code and @xmath73 be its subcode .", "then , the @xmath114-th rdip @xmath121 is nondecreasing with @xmath114 from @xmath122 to @xmath123 , and @xmath124 holds .", "@xmath122 and @xmath123 , are obvious from .", "recall that @xmath125 for @xmath126 from ( * ? ? ?", "* lemma 1 ) .", "this implies that for any subspace @xmath127 , there always exist some @xmath128 s satisfying @xmath129 and @xmath130 .", "this yields @xmath131 .", "next we show that the increment at each step is at most @xmath132 .", "consider arbitrary subspaces @xmath133 such that @xmath134 and @xmath135 .", "let @xmath136 ; @xmath137 .", "since @xmath138 and @xmath73 , we have @xmath139 and hence @xmath140 .", "[ lma : rgrw ] let @xmath72 be a linear code and @xmath73 be its subcode . then , the @xmath114-th rgrw @xmath141 is strictly increasing with @xmath114 .", "moreover , @xmath142 and @xmath143 where @xmath144 .", "first we have @xmath145 from , we have @xmath146 .", "we thus have @xmath147 therefore the rgrw is strictly increasing with @xmath114 and thus @xmath148 is established .", "next , we show the relation between the rank distance @xcite and the rgrw .", "let @xmath149 be an @xmath12-linear isomorphism that expands an element of @xmath10 as a column vector over @xmath12 with respect to some fixed basis for @xmath10 over @xmath12 .", "then , we define the _ rank over @xmath12 _ of a vector @xmath150 \\in \\f_{q^m}^n$ ] , denoted by @xmath151 , as the rank of @xmath52 matrix @xmath152 $ ] over @xmath12 .", "the rank distance @xcite between two vectors @xmath153 is given by @xmath154 .", "the minimum rank distance @xcite of a code @xmath29 is given as @xmath155 .", "for a subspace @xmath41 , we define by @xmath156 the sum of subspaces @xmath157 .", "[ lma : xxxx ] for a subspace @xmath38 with @xmath158 , we have @xmath159 .", "let @xmath160 \\!\\in\\ !", "v$ ] be a nonzero vector , which implies @xmath161 .", "let @xmath162_{i , j=1}^{m , n } \\!\\in\\ ! \\f_{q^m}^{m \\times n}$ ] , @xmath163 . each vector in @xmath164 is represented by an @xmath10-linear combination of @xmath165 , and hence @xmath166 .    for @xmath167 , @xmath168 , we have @xmath169 .", "this implies that there always exists some @xmath170 with @xmath171 satisfying @xmath172 \\!\\in\\ !", "\\f_{q^m}^n , g_j   \\!\\neq\\ ! 0 , \\label{eq : transform}\\end{aligned}\\ ] ] where @xmath173 are linearly independent over @xmath12 , and note that @xmath174 represents the elementary column operation on @xmath175 $ ] . also for @xmath176 , @xmath177 , we have @xmath178 ( @xmath179 ) .", "hence , for @xmath170 satisfying , we also have @xmath180 \\!\\in\\ !", "\\f_{q^m}^n$ ] for all @xmath181 .", "thus , by the elementary column operation on @xmath182 over @xmath12 , represented by @xmath174 , we get @xmath183 . by eliminating zero columns from @xmath183", ", we obtain a matrix @xmath184_{i , j=1}^{m , d_r(v)}$ ] , @xmath185 , where @xmath186 .", "let @xmath187 @xmath188 be the submatrix consisting of the first @xmath30 rows of @xmath189 . since @xmath190 and @xmath173 are linearly independent , @xmath191 is the generator matrix of @xmath192 $ ] gabidulin code and @xmath193 @xcite .", "thus , @xmath194 is nonsingular , and hence we have @xmath195 .", "therefore , @xmath196 .", "[ lma : rankdistance ] for a code @xmath72 and its subcode @xmath73 , the first rgrw can be represented as @xmath197 .", "@xmath198 can be represented as @xmath199 for any subspace @xmath38 with @xmath200 , there always exists some @xmath201 satisfying @xmath202 , because we have @xmath203 and @xmath204 . also , for subspaces @xmath205 and @xmath206 with @xmath200 , if @xmath205 is the smallest space in @xmath120 including @xmath207 , then @xmath208 @xcite .", "thus can be rewritten as @xmath209 where the last equality of is obtained by @xmath210 , and @xmath211 from @xmath212 .", "for subspaces @xmath207 and @xmath213 , we have @xmath214 .", "therefore , can be rewritten as follows .", "@xmath215 +    immediately yields the following corollary .", "[ coro : rankdistance ] for a linear code @xmath29 , @xmath216 holds .", "this shows that @xmath217 is a generalization of @xmath218 .", "now we present the following proposition that generalizes the singleton - type bound of the rank distance @xcite .", "[ prop : generalizedsingleton ] let @xmath72 be a linear code and @xmath73 be its subcode .", "then , the rgrw of @xmath93 and @xmath94 is upper bounded by @xmath219 for @xmath220 .", "we can consider that @xmath94 is a systematic code without loss of generality .", "that is , the first @xmath221 coordinates of each basis of @xmath94 is one of canonical bases of @xmath222 .", "let @xmath223 be a linear code such that @xmath93 is a direct sum of @xmath94 and @xmath224 . then , after suitable permutation of coordinates , a basis of @xmath224 can be chosen such that its first @xmath221 coordinates are zero .", "then , the effective length @xcite of a code @xmath224 is less than or equal to @xmath225 .", "hence we have @xmath226 from the singleton - type bound for rank metric @xcite .    here", "we write @xmath227 for the sake of simplicity . recall that @xmath228 from , and @xmath229 holds from .", "we shall use the mathematical induction on @xmath81 .", "we see that is true for @xmath230 .", "assume that for some @xmath231 , @xmath232 is true .", "then , by the monotonicity shown in , @xmath233 holds .", "thus , it is proved by mathematical induction that holds for @xmath234 .", "lastly , we prove by the above discussion about the rgrw of @xmath224 and @xmath235 . for an arbitrary fixed subspace @xmath38 , we have @xmath236 , because @xmath93 is a direct sum of @xmath224 and @xmath94 .", "hence , @xmath237 holds , and we have @xmath238 for @xmath239 from .", "therefore , from the foregoing proof , we have @xmath240 for @xmath239 , and the proposition is proved .", "immediately yields the following corollary .", "[ coro : mrdrgrw ] for a linear code @xmath32 , @xmath241 for @xmath242 .", "the equality holds for all @xmath114 if and only if @xmath29 is an mrd code ."], ["in this section , we express @xmath14 and @xmath15 given in in terms of the rdip and rgrw . from now on , we use the following definition .    for @xmath243", ", we define @xmath244 .    recall that if an @xmath10-linear space @xmath38 admits a basis in @xmath245 then @xmath246 @xcite , which implies @xmath247    first , we give the following theorem for the universal equivocation @xmath14 given in    [ thm : equivocation ] consider the nested coset coding in . then , the universal equivocation @xmath14 of @xmath248 is given by @xmath249    let @xmath250 be an arbitrary matrix . by the chain rule @xcite", ", we have the following equation for the conditional entropy of @xmath7 given @xmath95 : @xmath251 then , from ( * ? ? ?", "* proof of lemma 4.2 ) , we have @xmath252 by substituting these equations into , we have @xmath253 by we have @xmath254 thus , by and , the universal equivocation @xmath14 is given as follows .", "@xmath255 +    [ ex1 ] the existing schemes @xcite used mrd codes as @xmath256 and @xmath257 , where @xmath8 . by , we have @xmath258 for any @xmath259 .", "this implies @xmath260 for @xmath261 .    on the other hand , @xmath262 by .", "since @xmath263 for any @xmath264 by , we have @xmath265 . by , @xmath266 , @xmath267 for @xmath268 .    by", ", we see that @xmath269 for @xmath270 in the schemes @xcite .", "we then have the following corollary by the rgrw .", "shows that the wiretapper obtain no information of @xmath7 from any @xmath271 links .", "[ prop : perfectsecrecy ] consider the nested coset coding in .", "then , the wiretapper must observe at least @xmath272 links to obtain the mutual information @xmath273 ( @xmath274 ) between @xmath7 and observed packets .    from", ", the smallest number @xmath63 of tapped links satisfying @xmath275 @xmath276 is @xmath277 from ( * ? ?", "* lemma 1 ) and , this equation can be rewritten as follows .", "@xmath278 +    although the message @xmath7 has been assumed to be uniformly distributed over @xmath70 in , the following proposition reveals that the wiretapper still obtain no information of @xmath7 from any @xmath271 links even if @xmath7 is arbitrarily distributed .", "[ coro : distribution ] fix the transfer matrix @xmath279 to the wiretapper .", "suppose that the wiretapper obtain no information of @xmath7 from @xmath95 when @xmath7 is uniformly distributed over @xmath70 as described in .", "then , even if @xmath7 is chosen according to an arbitrary distribution over @xmath70 , the wiretapper still obtain no information of @xmath7 from @xmath95 , that is , @xmath280 .", "when we assume that @xmath7 is arbitrarily distributed over @xmath70 , @xmath281 is upper bounded as follows from ( * ? ? ?", "* proof of lemma 6 ) and ( * ? ? ? * proof of lemma 4.2 ) .", "@xmath282 also , since @xmath18 is uniformly distributed over a coset @xmath283 for fixed @xmath7 , we have @xmath284 . for the dimension of a subspace @xmath285", ", we have @xmath286 where @xmath287 is a generator matrix of @xmath93 .", "hence we have @xmath288 .", "we thus have @xmath289 for any distribution of @xmath7 .", "by @xmath290 and we can see that the equality holds if @xmath7 is uniformly distributed .", "therefore , for fixed @xmath279 , if @xmath280 holds for uniformly distributed @xmath7 , then the right hand side of is zero , which implies that @xmath280 also holds for arbitrarily distributed @xmath7 from the nonnegativity of mutual information @xcite .", "lastly , we express @xmath15 in in terms of the rgrw . for a subset @xmath291 and a vector @xmath292\\in\\f_{q^m}^n$ ] , let @xmath293 be a vector of length @xmath294 over @xmath10 , obtained by removing the @xmath81-th components @xmath295 for @xmath296 .", "for example for @xmath297 and @xmath298 $ ] ( @xmath299 ) , we have @xmath300 $ ] .", "the _ punctured code _", "@xmath301 of a code @xmath302 is given by @xmath303 .", "the _ shortened code _", "@xmath304 of a code @xmath305 is defined by @xmath306 \\in \\c , c_i = 0 \\text { for } i \\notin \\mathcal{j } \\right\\}$ ] .", "for example for @xmath307,[1,1,0],[1,0,1],[0,1,1]\\}$ ] ( @xmath308 ) and @xmath309 , we have @xmath310,[1,1]\\}$ ] .", "we then have the following theorem for the universal @xmath15-strong security defined in .", "[ thm : universalalpha ] let @xmath311 .", "fix @xmath93 , @xmath94 and @xmath312 in and consider the corresponding nested coset coding scheme in . by using @xmath93 , @xmath94 and @xmath312 , define @xmath313 : s \\in \\f_{q^m}^l\\text {   and } x\\in\\psi(s)\\right\\ } \\subseteq \\f_{q^m}^{l+n}.\\end{aligned}\\ ] ] for each index @xmath314 , we define a punctured code @xmath315 of @xmath316 as @xmath317 , and a shortened code @xmath318 of @xmath316 as @xmath319 .", "then , the value @xmath15 in is given by @xmath320    define @xmath321 : \\vec{c}_2 \\in \\c_2\\}\\subseteq\\f_{q^m}^{l+n}$ ] . since @xmath73 , @xmath322 is also a subcode of @xmath316 .", "thus , in terms of @xmath316 and @xmath322 , we can see that the vector @xmath323\\in\\f_{q^m}^{l+n}$ ] is generated by a nested coset coding scheme of @xmath316 and @xmath322 from @xmath7 .", "then , from the definition of @xmath316 and @xmath322 , we can see that @xmath318 is a subcode of @xmath315 with dimension @xmath324 over @xmath10 for each @xmath325 .", "let @xmath326 and @xmath327 $ ] for each @xmath328 .", "for @xmath329 define a coset @xmath330 :   s_{\\mathcal{l}\\backslash\\{i\\ } } \\in\\f_{q^m}^{l-1 }   \\text { and }   x \\in \\psi(s)\\right\\ } \\in\\d_{1,i}/\\d_{2,i}.\\end{aligned}\\ ] ] here we define @xmath331)= [ s_{\\mathcal{l}\\backslash\\{i\\}},x ] \\in \\d_{1,i}$ ] .", "recall that @xmath3 are mutually independent and uniformly distributed over @xmath10 .", "thus , considering a nested coset coding scheme that generates @xmath332 from a secret message @xmath333 with @xmath334 , we can see that @xmath335 is chosen uniformly at random from @xmath336 .", "therefore , we have @xmath337 for any @xmath338 whenever @xmath339 from .    for an arbitrary subset @xmath340 ,", "define a matrix @xmath341 that consists of @xmath342 rows of an @xmath343 identity matrix , satisfying @xmath344^{\\rm t } \\!=\\ ! f_\\mathcal{r", "} s_{\\mathcal{l}\\backslash\\{i\\}}^{\\rm t}$ ] . for an arbitrary matrix @xmath345 ( @xmath346 ) , set @xmath347 $ ] .", "then , from the foregoing proof , we have @xmath348 whenever @xmath349 . since @xmath350 is equivalent to from ( * ? ? ?", "* prop.5 ) , we have by selecting the minimum value of @xmath351 for @xmath352 .", "[ ex2 ] the scheme proposed in @xcite used a systematic mrd code as @xmath316 ( not @xmath93 ) , where @xmath108 .", "we proved @xmath353 in ( * ? ? ? * proof of theorem 4 ) . by , we see that the scheme @xcite attains the universal @xmath16-strong security in the sense of , while @xcite proved it by adapting the proof argument in @xcite .    as shown in , no information of @xmath7 is leaked from less than @xmath354 tapped links even if @xmath7 is arbitrarily distributed .", "in contrast , @xmath7 must be uniformly distributed over @xmath70 to establish .", "this is because elements of @xmath7 need to be treated as extra random packets , as in strongly secure network coding schemes @xcite ."], ["this section derives the universal error correction capability by the approach of ( * ? ? ?", "* section iii ) . recall that the received packets @xmath20 is given by @xmath355 in the setup of , and that @xmath18 is chosen from the coset @xmath283 corresponding to @xmath7 by the nested coset coding in . from now on ,", "we write @xmath356 for the sake of simplicity .", "first , we define the _ discrepancy _ @xcite between @xmath23 and @xmath20 by @xmath357 where the second equality is derived from ( * ? ? ?", "* lemma 4 ) .", "this definition of @xmath358 represents the minimum number @xmath359 of error packets @xmath85 required to be injected in order to transform at least one element of @xmath23 into @xmath20 , as ( * ? ? ?", "* eq.(9 ) ) .", "let @xmath363 and let @xmath374 .", "then , @xmath375 from .", "let @xmath376 and @xmath377 be vectors satisfying @xmath378 . from the proof of (", "* theorem 6 ) , we can always find two vectors @xmath379 such that @xmath380 , @xmath381 and @xmath382 .", "taking @xmath383 , we have @xmath384 and @xmath385 .", "we thus obtain @xmath386 and @xmath387 from . on the other hand , since @xmath388 , we have @xmath389 for any @xmath390 from from", ". therefore , @xmath391 and @xmath392 hold .", "[ prop : deltacorrection ] a nested coset coding scheme with @xmath248 is guaranteed to determine the unique coset @xmath23 against any @xmath81 packet errors for any fixed @xmath62 if and only if @xmath395 .            [ thm : errorcorrectioncap ] consider the nested coset coding in .", "then , the scheme is a universally ( simultaneously for all @xmath80 with rank deficiency at most @xmath110 ) @xmath81-error-@xmath110-erasure - correcting secure network coding if and only if @xmath398 .    for the rank deficiency @xmath399 , we have @xmath400 , and there always exists @xmath401 depending on @xmath402 such that the equality holds .", "thus , from , we have @xmath403 & \\!=\\ ! \\min \\left\\ { d_r(x,\\vec{0 } ) : x \\in \\c_1 , x \\notin \\c_2 \\right\\}-\\rho\\\\ & \\!=\\ !", "m_{r,1}(\\c_1,\\c_2 ) -\\rho .", "\\quad \\text{(by \\lma{lma : rankdistance})}\\end{aligned}\\ ] ] therefore , we have @xmath404 for @xmath405 , and hence we obtain @xmath406 @xmath407 .", "[ ex3 ] the existing scheme @xcite used mrd codes as @xmath248 , where @xmath8 .", "then , by , we have @xmath408 . since @xmath409 for any @xmath410 by and @xmath411 , we have @xmath412 .", "thus , by and , the scheme is universally @xmath81-error-@xmath110-erasure - correcting when @xmath413 , as shown in ( * ? ? ?", "* theorem 11 ) .", "h.  chen , r.  cramer , s.  goldwasser , r.  de  haan , and v.  vaikuntanathan , `` secure computation from random error correcting codes , '' in _ proc .", "eurocrypt 2007 _ , ser .", "lecture notes in computer science , vol .", "4515.1em plus 0.5em minus 0.4emspringer - verlag , 2007 , pp . 291310 .", "t.  ho , m.  mdard , r.  koetter , d.  r. karger , m.  effros , j.  shi , and b.  leong , `` a random linear network coding approach to multicast , '' _ ieee trans .", "inf . theory _ ,", "52 , no .", "10 , pp . 44134430 , oct ."]]}
{"article_id": "1603.04792", "article_text": ["ever since databases have been able to store basket data , many techniques have been proposed to extract useful insights for analysts .", "one of the first , association rule mining  @xcite , also remains one of the most intuitive .", "association rules are often used to summarize consumer trends in a transaction set or as input to a classifier  @xcite .", "the problem is the very high number of rules , typically in the order of millions . that is exacerbated by the lack of thorough studies of which of the many interestingness measures for ranking rules  @xcite is most appropriate for which application domain .", "we present , a framework to compare the outcome of different interestingness measures applied to association rules generated in the food retail domain .", "relies on a flexible architecture and on  @xcite , our parallel and distributed pattern mining algorithm that runs on mapreduce .", "the use of real datasets and a close collaboration with experienced domain experts from intermarch , one of the largest retailers in france , has led us to selecting the most relevant measures to rank association rules in the food retail domain .", "our dataset contains @xmath0 million receipts from stores in all of france , gathered over one year , 2013 .", "mining this data results in a huge number of rules .", "for example , using a minimum support of mines frequent rules of the form _ customer segment _", "_ product category_. out of these , have a confidence of @xmath2 or higher .", "table  [ tab : eyecatcher ] shows a ranking of the top-10 rules according to 3 different interestingness measures proposed in  @xcite .", "if we denote rules as @xmath3 , _ confidence _ is the probability to observe @xmath4 given that we observed @xmath5 , i.e. , @xmath6 .", "_ piatetsky - shapiro _", "@xcite combines how @xmath5 and @xmath4 occur together with how they would if they were independent , i.e. , @xmath7 .", "_ pearson s _ @xmath8 , measures how unlikely observations of @xmath5 and @xmath4 are independent .", "this very small example already shows that these measures result in different rule rankings .", "[ cols= \" > , < , > , < , > , < \" , ]      +", "we now report the results of a user study with domain experts from intermarch .", "the goal of this study is to assess the ability of interestingness measures to rank association rules according to the needs of an analyst . as explained in section  [ sec : xp : empirical ] , we identified 6 families of measures , and selected a representative of each group for the user study ( table  [ tab : groupsummary ] ) .", "we rely on the expertise of our industrial partner to determine , for each analysis scenario , which family produces the most interesting results .", "this experiment involved 2 experienced analysts from the marketing department of intermarch .", "we setup and let analysts select targets multiple times in order to populate the web application s database with association rules ( section  [ sec : exploitation ] ) .", "we let our analysts interact with without any time restriction , and collect their feedback in a free text form .", "each analyst firstly has to pick a mining scenario among ` demo_assoc ` , ` prod_assoc_t ` , or ` prod_assoc_c ` .", "then she picks a target category or a target product in the taxonomy . in ` prod_assoc_t `  and ` prod_assoc_c ` , she also has the option to filter out rules whose antecedent products are not from the same category as the target .", "finally , she chooses one of our 6 ranking measures to sort association rules .", "neither the name of the measure nor its computed values for association rules are revealed , because we wanted analysts to evaluate rankings without knowing how they were produced .", "resulting association rules are ranked according to a selected measure .", "each rule is displayed with its support , confidence and recall , such that analysts can evaluate it at a glance .", "for each scenario , our analysts are asked which representative measure highlights the most interesting results ( as detailed below , in all cases a few of them were chosen ) .", "once the analyst selects a target , _ all _ matching rules are returned .", "the initial motivation of this choice was to determine how many results are worth displaying and are actually examined by the analysts . according to the follow - up interview with the analysts , they carefully considered the first ten results , and screened up to a hundred more .", "interestingly , analysts mentioned that they also scrolled down to the bottom of the list in order to see which customer segments are not akin to buying the selected category .", "for example , when browsing demographic association rules , they expected to find \\{_50 - 64 _ } @xmath1 _ pet food _ among top results , but also expected \\{_<35 , paris _ } @xmath1 _ pet food _ among bottom results .", "this confirms that all rules should remain accessible .", "this also indicates that while interestingness measures favor strong associations , it would also be interesting to highlight _", "anti_-rules , as those can also convey useful information .", "we let marketing experts explore all 3 scenarios and express their preference towards groups of measures .", "in the ` demo_assoc`case , @xmath9 and @xmath10 were both highly appreciated .", "@xmath9 favors rules such as @xmath11 oise@xmath12 _ flat and carbonated drinks_. these rules are very specific and thus have a very high confidence ( 31,58 % in this particular case ) .", "however , this comes at the cost of recall ( 0,08 % ) .", "experts value _ confidence _ much more than _ recall _ , as their priority is finding rules that they consider reliable .", "a low support is not necessarily an issue , and can lead to the discovery of surprising niche rules that can be exploited nonetheless . as discussed in section  [ sec : annotation ] , @xmath10 offers a more balanced trade - off between confidence and recall , and prioritizes rules such as @xmath13 < 35 , * , * @xmath12 _ baby food _ ( confidence 8,57 % , recall 37,61% ) .", "these rules are interesting because they capture a large fraction of the sales of a given category , but are less reliable and generally less surprising . @xmath14 and", "@xmath15 were considered as less interesting than @xmath9 and @xmath10 respectively .", "their results offer similar trade - offs , but with lower confidence each time .", "@xmath16 and @xmath17 were considered unusable because of their very low confidence .    when experimenting with ` prod_assoc `", ", we observed a slightly different behavior . by default , the analysts favored @xmath9 and @xmath14 because of the confidence of their results .", "then , we offered the analysts the possibility of filtering the rules to only keep the ones in which the antecedent contains products from the same category as the target .", "this led to analysts favoring @xmath10 and @xmath16 .", "this difference is caused by an important but implicit criterion : the ability of a measure to filter out very popular products .", "for example , the rule \\{_vanilla cream , emmental_}@xmath1 _ chocolate cream _ usually appears just above its shorter version \\{_vanilla cream_}@xmath1 _ chocolate cream _ , because the first one has a confidence of @xmath18 and the second @xmath19 .", "however , experts prefer the second one , because _ emmental _ ( cheese ) is among the heavy hitters in stores .", "its addition to the rule is hence considered insignificant .", "this `` noise '' generally increases with _", "recall_. hence , when no filtering is available , @xmath9 is selected , but analysts prefer the _ recall _ and _ confidence _ trade - off provided by @xmath10 and @xmath16 .", "again , @xmath15 suffered from its proximity to @xmath10 with lower confidence , while @xmath17 s confidence was too low .", "in all cases , analysts mentioned @xmath17 as uninteresting overall because it selects rules of low _", "confidence_. in general , sorting by decreasing _ lift _", "( which is close to sorting by decreasing _ confidence _ ) is the preferred choice . combined with the minimum support threshold used in the mining phase ,", "this ranking promotes rules that are considered reliable .", "however , the preference of the analysts changes when filters are available to narrow down the set of rules to specific product categories . in this case", ", they favor the compromise between _ confidence _ and _ support _ offered , for instance , by the _ piatetsky - shapiro _ s measure  @xcite .", "to the best of our knowledge , targets datasets which are orders of magnitude bigger ( and sparser ) than those tested in existing work on ranking association rules .", "this paper is also the first to complement an algorithmic comparative analysis with a user study involving domain experts .", "the definition of quality of association rules is a well - studied topic in statistics and data mining , summarized in  @xcite . in this survey ,", "geng _ et al .", "_ review as many as 38 measures for association and classification rules .", "they also discuss 4 sets of properties like symmetry or monotony , and how each of them highlights different meanings of `` rule quality '' , such as novelty and generality .", "however , we observe no correlation between these properties and the groups of measures discovered using .    these 38 measures are compared in  @xcite .", "authors consider the case of extracting and ranking temporal rules ( _ _ event a__@xmath1_event b _ ) from the execution traces of java programs .", "each measure is evaluated in its ability to rank highly rules known from a ground truth ( java library specification ) .", "we observe that the measures scoring the highest are all from the groups identified in this work as @xmath9 and @xmath10 , which were also favored by our analysts .", "there are however some counterexamples , with measures from @xmath9 scoring poorly .", "the authors then use a statistical approach to build a partial ordering of measures quality .", "this results in the formation of measure equivalence classes .", "however , the semantic of these classes is based on the principle of dominance in the evaluation , and not on the comparison of the rankings themselves .", "hence , the equivalence classes obtained do not match our groups . the main difference between and  @xcite", "is the absence of a ground truth of interesting rules for our dataset .", "consequently , our evaluation of measures is first comparative , with 4 correlations measures covering both the top of the ranking and the entire ranked list .", "we then build groups of measures to reduce the number of options presented to expert analysts in the user study .", "the differences in the results obtained also highlight the importance of performing domain - specific studies , as the properties of data and the expectations of analysts vary significantly .", "the closest work to ours is herbs  @xcite .", "herbs relies on a different and smaller set of measures to cluster rule rankings .", "authors perform an analysis of the properties of measures , in addition to an experimental study .", "the datasets used are from the health and astronomy domains .", "each of them contains at most transactions and leads to the extraction of 49 to rules .", "rankings are then compared between all pairs of measures using kendall s @xmath20 correlation measure averaged over all datasets .", "the largest group of measures identified , which includes confidence , is quite similar to @xmath9", ". however , there are also significant differences . for instance , we find @xmath14 and @xmath17 to be very different , while  @xcite considers the measures of this group similar . the authors observe a weak resemblance between the theoretical and experimental analysis of the measures .", "the main similarity between  @xcite and is the reliance on a pairwise correlation measure followed by a hierarchical clustering to detect groups of measures .", "is entirely focused on retail data , which has different properties and contains millions of transactions and rules .", "is also more exhaustive in the analysis of measures : we consider more interestingness measures , and 4 different ranking correlation measures instead of 1 .", "this allows us to discover more subtle differences in a more specific domain .", "finally , we perform a user study to assess the quality of each group according to experts from the retail industry .    our use of the _ p - value _ ( via _ pearson s @xmath8 test _ ) in the evaluation of rule interestingness is borrowed from  @xcite .", "a low _ p - value _ shows a correlation between a rule s antecedent and consequent .", "the use of _ fisher _ s exact test on association rules is inspired by  @xcite", ". both of these works aim at finding highly - correlated itemsets , which requires the analyst to set a threshold on the @xmath21-value .", "this is common practice in biology , but less meaningful in the retail industry . in  @xcite ,", "also propose an exploration framework where rules are grouped by consequent , then traversed by progressively adding items to the antecedent .", "the framework provides hints to help guess how each additional item would make a difference .", "such a framework is suitable to some of the scenarios we consider and could be integrated in a future version of  .", "in this paper , we present , a framework for mining association rules from large - scale retail data .", "we defined 3 mining scenarios allowing analysts to extract associations between user segments and product categories , or products themselves .", "given a scenario , builds a dataset of transactions and mines in parallel association rules containing targets selected by the analysts .", "our main contribution is the study of 34 interestingness measures for association rules .", "we first performed an analytical and an empirical comparison between different rule rankings and grouped measures into 6 families .", "resulting groups were then evaluated in a user study involving retail experts .", "we concluded that _", "lift _ and _ piatetsky - shapiro _ best fit the needs of the analysts , as they ensure a high confidence .", "we foresee 3 directions of improvement for . the first one is related to the architecture .", "is currently implemented using batch processing and on - disk storage . while mining is already fast , i / o operations introduces some latency between the definition of a mining scenario and the display of results .", "we are currently migrating to an in - memory dataset representation , using spark  @xcite , to allow faster target selection and lower response time .", "a second improvement is the extraction of negative results ( _ anti_-rules ) .", "that is particularly true for rules containing customer segments .", "we hence need to determine how negative rules should be ranked in order to properly adjust their proportion in the outcome . finally , while quality measures are crucial to select the most interesting results for the analysts , we would like to introduce diversity in displaying rules and study its impact on analysts satisfaction .", "p. suganthan g.c .", ", c. sun , k. gayatri k. , h. zhang , f. yang , n. rampalli , s. prasad , e. arcaute , g. krishnan , r. deep , v. raghavendra , a. doan why big data industrial systems need rules and what we can do about it . in _ proc .", "sigmod _ , pages 265276 , 2015 .", "v.  k. vavilapalli , a.  c. murthy , c.  douglas , s.  agarwal , m.  konar , r.  evans , t.  graves , j.  lowe , h.  shah , s.  seth , b.  saha , c.  curino , o.  omalley , s.  radia , b.  reed , and e.  baldeschwieler .", "apache hadoop yarn : yet another resource negotiator . in _ proc .", "socc _ , pages 5:15:16 , 2013 ."], "abstract_text": ["<S> understanding customer buying patterns is of great interest to the retail industry and has shown to benefit a wide variety of goals ranging from managing stocks to implementing loyalty programs . </S>", "<S> association rule mining is a common technique for extracting correlations such as _ people in the south of france buy ros wine _ or _ </S>", "<S> customers who buy pat also buy salted butter and sour bread . _ unfortunately , sifting through a high number of buying patterns is not useful in practice , because of the predominance of popular products in the top rules . as a result , </S>", "<S> a number of `` interestingness '' measures ( over 30 ) have been proposed to rank rules . </S>", "<S> however , there is no agreement on which measures are more appropriate for retail data . </S>", "<S> moreover , since pattern mining algorithms output thousands of association rules for each product , the ability for an analyst to rely on ranking measures to identify the most interesting ones is crucial . in this paper , </S>", "<S> we develop  ( comparative analysis of patterns ) , a framework that provides analysts with the ability to compare the outcome of interestingness measures applied to buying patterns in the retail industry . </S>", "<S> we report on how we used  to compare 34 measures applied to over 1,800 stores of intermarch , one of the largest food retailers in france . </S>"], "labels": null, "section_names": ["introduction", "user study", "related work", "summary and evolutions"], "sections": [["ever since databases have been able to store basket data , many techniques have been proposed to extract useful insights for analysts .", "one of the first , association rule mining  @xcite , also remains one of the most intuitive .", "association rules are often used to summarize consumer trends in a transaction set or as input to a classifier  @xcite .", "the problem is the very high number of rules , typically in the order of millions . that is exacerbated by the lack of thorough studies of which of the many interestingness measures for ranking rules  @xcite is most appropriate for which application domain .", "we present , a framework to compare the outcome of different interestingness measures applied to association rules generated in the food retail domain .", "relies on a flexible architecture and on  @xcite , our parallel and distributed pattern mining algorithm that runs on mapreduce .", "the use of real datasets and a close collaboration with experienced domain experts from intermarch , one of the largest retailers in france , has led us to selecting the most relevant measures to rank association rules in the food retail domain .", "our dataset contains @xmath0 million receipts from stores in all of france , gathered over one year , 2013 .", "mining this data results in a huge number of rules .", "for example , using a minimum support of mines frequent rules of the form _ customer segment _", "_ product category_. out of these , have a confidence of @xmath2 or higher .", "table  [ tab : eyecatcher ] shows a ranking of the top-10 rules according to 3 different interestingness measures proposed in  @xcite .", "if we denote rules as @xmath3 , _ confidence _ is the probability to observe @xmath4 given that we observed @xmath5 , i.e. , @xmath6 .", "_ piatetsky - shapiro _", "@xcite combines how @xmath5 and @xmath4 occur together with how they would if they were independent , i.e. , @xmath7 .", "_ pearson s _ @xmath8 , measures how unlikely observations of @xmath5 and @xmath4 are independent .", "this very small example already shows that these measures result in different rule rankings .", "[ cols= \" > , < , > , < , > , < \" , ]      +"], ["we now report the results of a user study with domain experts from intermarch .", "the goal of this study is to assess the ability of interestingness measures to rank association rules according to the needs of an analyst . as explained in section  [ sec : xp : empirical ] , we identified 6 families of measures , and selected a representative of each group for the user study ( table  [ tab : groupsummary ] ) .", "we rely on the expertise of our industrial partner to determine , for each analysis scenario , which family produces the most interesting results .", "this experiment involved 2 experienced analysts from the marketing department of intermarch .", "we setup and let analysts select targets multiple times in order to populate the web application s database with association rules ( section  [ sec : exploitation ] ) .", "we let our analysts interact with without any time restriction , and collect their feedback in a free text form .", "each analyst firstly has to pick a mining scenario among ` demo_assoc ` , ` prod_assoc_t ` , or ` prod_assoc_c ` .", "then she picks a target category or a target product in the taxonomy . in ` prod_assoc_t `  and ` prod_assoc_c ` , she also has the option to filter out rules whose antecedent products are not from the same category as the target .", "finally , she chooses one of our 6 ranking measures to sort association rules .", "neither the name of the measure nor its computed values for association rules are revealed , because we wanted analysts to evaluate rankings without knowing how they were produced .", "resulting association rules are ranked according to a selected measure .", "each rule is displayed with its support , confidence and recall , such that analysts can evaluate it at a glance .", "for each scenario , our analysts are asked which representative measure highlights the most interesting results ( as detailed below , in all cases a few of them were chosen ) .", "once the analyst selects a target , _ all _ matching rules are returned .", "the initial motivation of this choice was to determine how many results are worth displaying and are actually examined by the analysts . according to the follow - up interview with the analysts , they carefully considered the first ten results , and screened up to a hundred more .", "interestingly , analysts mentioned that they also scrolled down to the bottom of the list in order to see which customer segments are not akin to buying the selected category .", "for example , when browsing demographic association rules , they expected to find \\{_50 - 64 _ } @xmath1 _ pet food _ among top results , but also expected \\{_<35 , paris _ } @xmath1 _ pet food _ among bottom results .", "this confirms that all rules should remain accessible .", "this also indicates that while interestingness measures favor strong associations , it would also be interesting to highlight _", "anti_-rules , as those can also convey useful information .", "we let marketing experts explore all 3 scenarios and express their preference towards groups of measures .", "in the ` demo_assoc`case , @xmath9 and @xmath10 were both highly appreciated .", "@xmath9 favors rules such as @xmath11 oise@xmath12 _ flat and carbonated drinks_. these rules are very specific and thus have a very high confidence ( 31,58 % in this particular case ) .", "however , this comes at the cost of recall ( 0,08 % ) .", "experts value _ confidence _ much more than _ recall _ , as their priority is finding rules that they consider reliable .", "a low support is not necessarily an issue , and can lead to the discovery of surprising niche rules that can be exploited nonetheless . as discussed in section  [ sec : annotation ] , @xmath10 offers a more balanced trade - off between confidence and recall , and prioritizes rules such as @xmath13 < 35 , * , * @xmath12 _ baby food _ ( confidence 8,57 % , recall 37,61% ) .", "these rules are interesting because they capture a large fraction of the sales of a given category , but are less reliable and generally less surprising . @xmath14 and", "@xmath15 were considered as less interesting than @xmath9 and @xmath10 respectively .", "their results offer similar trade - offs , but with lower confidence each time .", "@xmath16 and @xmath17 were considered unusable because of their very low confidence .    when experimenting with ` prod_assoc `", ", we observed a slightly different behavior . by default , the analysts favored @xmath9 and @xmath14 because of the confidence of their results .", "then , we offered the analysts the possibility of filtering the rules to only keep the ones in which the antecedent contains products from the same category as the target .", "this led to analysts favoring @xmath10 and @xmath16 .", "this difference is caused by an important but implicit criterion : the ability of a measure to filter out very popular products .", "for example , the rule \\{_vanilla cream , emmental_}@xmath1 _ chocolate cream _ usually appears just above its shorter version \\{_vanilla cream_}@xmath1 _ chocolate cream _ , because the first one has a confidence of @xmath18 and the second @xmath19 .", "however , experts prefer the second one , because _ emmental _ ( cheese ) is among the heavy hitters in stores .", "its addition to the rule is hence considered insignificant .", "this `` noise '' generally increases with _", "recall_. hence , when no filtering is available , @xmath9 is selected , but analysts prefer the _ recall _ and _ confidence _ trade - off provided by @xmath10 and @xmath16 .", "again , @xmath15 suffered from its proximity to @xmath10 with lower confidence , while @xmath17 s confidence was too low .", "in all cases , analysts mentioned @xmath17 as uninteresting overall because it selects rules of low _", "confidence_. in general , sorting by decreasing _ lift _", "( which is close to sorting by decreasing _ confidence _ ) is the preferred choice . combined with the minimum support threshold used in the mining phase ,", "this ranking promotes rules that are considered reliable .", "however , the preference of the analysts changes when filters are available to narrow down the set of rules to specific product categories . in this case", ", they favor the compromise between _ confidence _ and _ support _ offered , for instance , by the _ piatetsky - shapiro _ s measure  @xcite ."], ["to the best of our knowledge , targets datasets which are orders of magnitude bigger ( and sparser ) than those tested in existing work on ranking association rules .", "this paper is also the first to complement an algorithmic comparative analysis with a user study involving domain experts .", "the definition of quality of association rules is a well - studied topic in statistics and data mining , summarized in  @xcite . in this survey ,", "geng _ et al .", "_ review as many as 38 measures for association and classification rules .", "they also discuss 4 sets of properties like symmetry or monotony , and how each of them highlights different meanings of `` rule quality '' , such as novelty and generality .", "however , we observe no correlation between these properties and the groups of measures discovered using .    these 38 measures are compared in  @xcite .", "authors consider the case of extracting and ranking temporal rules ( _ _ event a__@xmath1_event b _ ) from the execution traces of java programs .", "each measure is evaluated in its ability to rank highly rules known from a ground truth ( java library specification ) .", "we observe that the measures scoring the highest are all from the groups identified in this work as @xmath9 and @xmath10 , which were also favored by our analysts .", "there are however some counterexamples , with measures from @xmath9 scoring poorly .", "the authors then use a statistical approach to build a partial ordering of measures quality .", "this results in the formation of measure equivalence classes .", "however , the semantic of these classes is based on the principle of dominance in the evaluation , and not on the comparison of the rankings themselves .", "hence , the equivalence classes obtained do not match our groups . the main difference between and  @xcite", "is the absence of a ground truth of interesting rules for our dataset .", "consequently , our evaluation of measures is first comparative , with 4 correlations measures covering both the top of the ranking and the entire ranked list .", "we then build groups of measures to reduce the number of options presented to expert analysts in the user study .", "the differences in the results obtained also highlight the importance of performing domain - specific studies , as the properties of data and the expectations of analysts vary significantly .", "the closest work to ours is herbs  @xcite .", "herbs relies on a different and smaller set of measures to cluster rule rankings .", "authors perform an analysis of the properties of measures , in addition to an experimental study .", "the datasets used are from the health and astronomy domains .", "each of them contains at most transactions and leads to the extraction of 49 to rules .", "rankings are then compared between all pairs of measures using kendall s @xmath20 correlation measure averaged over all datasets .", "the largest group of measures identified , which includes confidence , is quite similar to @xmath9", ". however , there are also significant differences . for instance , we find @xmath14 and @xmath17 to be very different , while  @xcite considers the measures of this group similar . the authors observe a weak resemblance between the theoretical and experimental analysis of the measures .", "the main similarity between  @xcite and is the reliance on a pairwise correlation measure followed by a hierarchical clustering to detect groups of measures .", "is entirely focused on retail data , which has different properties and contains millions of transactions and rules .", "is also more exhaustive in the analysis of measures : we consider more interestingness measures , and 4 different ranking correlation measures instead of 1 .", "this allows us to discover more subtle differences in a more specific domain .", "finally , we perform a user study to assess the quality of each group according to experts from the retail industry .    our use of the _ p - value _ ( via _ pearson s @xmath8 test _ ) in the evaluation of rule interestingness is borrowed from  @xcite .", "a low _ p - value _ shows a correlation between a rule s antecedent and consequent .", "the use of _ fisher _ s exact test on association rules is inspired by  @xcite", ". both of these works aim at finding highly - correlated itemsets , which requires the analyst to set a threshold on the @xmath21-value .", "this is common practice in biology , but less meaningful in the retail industry . in  @xcite ,", "also propose an exploration framework where rules are grouped by consequent , then traversed by progressively adding items to the antecedent .", "the framework provides hints to help guess how each additional item would make a difference .", "such a framework is suitable to some of the scenarios we consider and could be integrated in a future version of  ."], ["in this paper , we present , a framework for mining association rules from large - scale retail data .", "we defined 3 mining scenarios allowing analysts to extract associations between user segments and product categories , or products themselves .", "given a scenario , builds a dataset of transactions and mines in parallel association rules containing targets selected by the analysts .", "our main contribution is the study of 34 interestingness measures for association rules .", "we first performed an analytical and an empirical comparison between different rule rankings and grouped measures into 6 families .", "resulting groups were then evaluated in a user study involving retail experts .", "we concluded that _", "lift _ and _ piatetsky - shapiro _ best fit the needs of the analysts , as they ensure a high confidence .", "we foresee 3 directions of improvement for . the first one is related to the architecture .", "is currently implemented using batch processing and on - disk storage . while mining is already fast , i / o operations introduces some latency between the definition of a mining scenario and the display of results .", "we are currently migrating to an in - memory dataset representation , using spark  @xcite , to allow faster target selection and lower response time .", "a second improvement is the extraction of negative results ( _ anti_-rules ) .", "that is particularly true for rules containing customer segments .", "we hence need to determine how negative rules should be ranked in order to properly adjust their proportion in the outcome . finally , while quality measures are crucial to select the most interesting results for the analysts , we would like to introduce diversity in displaying rules and study its impact on analysts satisfaction .", "p. suganthan g.c .", ", c. sun , k. gayatri k. , h. zhang , f. yang , n. rampalli , s. prasad , e. arcaute , g. krishnan , r. deep , v. raghavendra , a. doan why big data industrial systems need rules and what we can do about it . in _ proc .", "sigmod _ , pages 265276 , 2015 .", "v.  k. vavilapalli , a.  c. murthy , c.  douglas , s.  agarwal , m.  konar , r.  evans , t.  graves , j.  lowe , h.  shah , s.  seth , b.  saha , c.  curino , o.  omalley , s.  radia , b.  reed , and e.  baldeschwieler .", "apache hadoop yarn : yet another resource negotiator . in _ proc .", "socc _ , pages 5:15:16 , 2013 ."]]}
{"article_id": "1104.1824", "article_text": ["the trend for massively parallel computation is moving from the more common multi - core cpus towards gpus for several significant reasons @xcite .", "one important reason for such a trend in recent years include the low consumption in terms of power of gpus compared to setting up machines and infrastructure which will utilize multiple cpus in order to obtain the same level of parallelization and performance @xcite .", "another more important reason is that gpus are architectured for _ massively parallel computations _ since unlike most general purpose multicore cpus , a large part of the architecture of gpus are devoted to parallel execution of arithmetic operations , and not on control and caching just like in cpus @xcite .", "arithmetic operations are at the heart of many basic operations as well as scientific computations , and these are performed with larger speedups when done in parallel as compared to performing them sequentially . in order to perform these arithmetic operations on the gpu", ", there is a set of techniques called @xmath2 ( general purpose computations on the gpu ) coined by mark harris in 2002 which allows programmers to do computations on gpus and not be limited to just graphics and video processing alone @xcite .", "_ membrane computing _ or its more specific counterpart , a _ p system _ , is a turing complete computing model ( for several p system variants ) that perform computations nondeterministically , exhausting all possible computations at any given time .", "this type of unconventional model of computation was introduced by gheorghe pun in 1998 and takes inspiration and abstraction , similar to other members of _ natural computing _", "( e.g. dna / molecular computing , neural networks , quantum computing ) , from nature @xcite .", "specifically , p systems try to mimic the constitution and dynamics of the living cell : the multitude of elements inside it , and their interactions within themselves and their environment , or outside the cell s @xmath3 ( the cell s outermost membrane ) . before proceeding ,", "it is important to clarify what is meant when it is said that nature @xmath4 , particularly life or the cell : computation in this case involves reading information from memory from past or present stimuli , rewrite and retrieve this data as a stimuli from the environment , process the gathered data and act accordingly due to this processing @xcite .", "thus , we try to extend the classical meaning of computation presented by allan turing .", "sn p systems differ from other types of p systems precisely because they are @xmath5 and the working alphabet contains only _ one object type_. these characteristics , among others , are meant to capture the workings of a special type of cell known as the @xmath6 .", "neurons , such as those in the human brain , communicate or compute by sending indistinct signals more commonly known as action potential or _ spikes _ @xcite .", "@xmath7 is then communicated and encoded not by the spikes themselves , since the spikes are unrecognizable from one another , but by ( a ) the time elapsed between spikes , as well as ( b ) the number of spikes sent / received from one neuron to another , oftentimes under a certain time interval @xcite .", "it has been shown that sn p systems , given their nature , are representable by matrices @xcite .", "this representation allows design and implementation of an sn p system simulator using parallel devices such as gpus .", "since the time p systems were presented , many simulators and software applications have been produced @xcite . in terms of", "_ high performance computing _", ", many p system simulators have been also designed for clusters of computers @xcite , for reconfigurable hardware as in fpgas @xcite , and even for gpus @xcite .", "all of these efforts have shown that parallel architectures are well - suited in performance to simulate p systems .", "however , these previous works on hardware are designed to simulate _ cell - like _ p system variants , which are among the first p system variants to have been introduced .", "thus , the efficient simulation of snp systems is a new challenge that requires novel attempts .", "a matrix representation of sn p systems is quite intuitive and natural due to their graph - like configurations and properties ( as will be further shown in the succeeding sections such as in subsection [ computesnp ] ) .    on the other hand , _", "linear algebra _ operations have been efficiently implemented on parallel platforms and devices in the past years .", "for instance , there is a large number of algorithms implementing @xmath8 and @xmath9 operations on the gpu .", "these algorithms offer huge performance since dense linear algebra readily maps to the data - parallel architecture of gpus @xcite .", "it would thus seem then that a matrix represented sn p system simulator implementation on highly parallel computing devices such as gpus be a natural confluence of the earlier points made .", "the matrix representation of sn p systems bridges the gap between the theoretical yet still computationally powerful sn p systems and the applicative and more tangible gpus , via an sn p system simulator .", "the design and implementation of the simulator , including the algorithms deviced , architectural considerations , are then implemented using cuda .", "the compute unified device architecture ( cuda ) programming model , launched by nvidia in mid-2007 , is a hardware and software architecture for issuing and managing computations on their most recent gpu families ( g80 family onward ) , making the gpu operate as a highly parallel computing device @xcite .", "cuda programming model extends the widely known ansi c programming language ( among other languages which can interface with cuda ) , allowing programmers to easily design the code to be executed on the gpu , avoiding the use of low - level graphical primitives .", "cuda also provides other benefits for the programmer such as abstracted and automated scaling of the parallel executed code .", "this paper starts out by introducing and defining the type of snp system that will be simulated . afterwards the nvidia cuda model and architecture are discussed , baring the scalability and parallelization cuda offers .", "next , the design of the simulator , constraints and considerations , as well as the details of the algorithms used to realize the snp system are discussed .", "the simulation results are presented next , as well as observations and analysis of these results .", "the paper ends by providing the conclusions and future work .", "the objective of this work is to continue the creation of p system simulators , in this particular case an sn p system , using highly parallel devices such as gpus .", "fidelity to the computing model ( the type of snp system in this paper ) is a part of this objective .", "the type of snp systems focused on by this paper ( scope ) are those without delays i.e. those that spike or transmit signals the moment they are able to do so @xcite .", "variants which allow for delays before a neuron produces a spike , are also available @xcite .", "an snp system without delay is of the form :    [ snpdefn ] @xmath10 where :    1 .", "@xmath11 is the alphabet made up of only one object , the system spike @xmath12 .", "2 .   @xmath13 are @xmath14 number of neurons of the form @xmath15 where : 1 .", "@xmath16 gives the initial number of @xmath12s i.e. spikes contained in neuron @xmath17 2 .", "@xmath18 is a finite set of rules of with two forms : 1 .", "@xmath19 , are known as _ spiking rules _", ", where @xmath20 is a regular expression over @xmath12 , and @xmath21 , such that @xmath22 number of spikes are produced , one for each adjacent neuron with @xmath17 as the originating neuron and @xmath23 .", "@xmath24 , are known as _ forgetting rules _ , for @xmath25 , such that for each rule @xmath26 of type ( b-1 ) from @xmath18 , @xmath27 .", "@xmath28 , a special case of ( b-1 ) where @xmath20 = @xmath29 , @xmath30 .", "@xmath31 are the synapses i.e. connection between neurons .", "@xmath32 are the input and output neurons , respectively .", "furthermore , rules of type ( b-1 ) are applied if @xmath17 contains @xmath33 spikes , @xmath34 and @xmath30 . using this type of rule", "uses up or consumes @xmath33 spikes from the neuron , producing a spike to each of the neurons connected to it via a forward pointing arrow i.e. away from the neuron . in this manner , for rules of type ( b-2 )", "if @xmath17 contains @xmath35 spikes , then @xmath35 spikes are forgotten or removed once the rule is used .", "the non - determinism of sn p systems comes with the fact that more than one rule of the several types are applicable at a given time , given enough spikes .", "the rule to be used is chosen non - deterministically in the neuron . however , only one rule can be applied or used at a given time @xcite . the neurons in an sn p system operate in parallel and in unison , under a global clock @xcite . for figure [ snp_ex ] no input neuron is present , but neuron 3 is the output neuron , hence the arrow pointing towards the environment , outside the snp system .    the sn p system in figure [ snp_ex ] is @xmath36 , a 3 neuron system whose neurons are labeled ( neuron 1/@xmath37 to neuron 3/@xmath38 ) and whose rules have a total system ordering from ( 1 ) to ( 5 ) .", "neuron 1/@xmath37 can be seen to have an initial number of spikes equal to 2 ( hence the @xmath39 seen inside it ) .", "there is @xmath40 input neuron , but @xmath38 is the output neuron , as seen by the arrow pointing towards the environment ( not to another neuron ) .", "more formally , @xmath36 can be represented as follows :    @xmath41 where @xmath42 , @xmath43 , ( neurons 2 to 3 and their @xmath44s and @xmath18s can be similarly shown ) , @xmath45 are the synapses for @xmath36 , @xmath46 .", "this sn p system generates all numbers in the set @xmath0 - \\{@xmath1 } , hence it does nt @xmath47 , which can be easily verified by applying the rules in @xmath36 , and checking the spikes produced by the output neuron @xmath38 .", "this generated set is the result of the computation in @xmath36 .    , generating all numbers in @xmath0 - \\{@xmath1 } , from @xcite . ]", "a matrix representation of an sn p system makes use of the following vectors and matrix definitions @xcite .", "it is important to note that , just as in figure [ snp_ex ] , a total ordering of rules is considered .", "_ configuration vector _", "@xmath48 is the vector containing all spikes in every neuron on the @xmath49 computation step / time , where @xmath50 is the initial vector containing all spikes in the system at the beginning of the computation . for @xmath36 ( in figure [ snp_ex ] )", "the initial configuration vector is @xmath51 .    _", "spiking vector _ shows at a given configuration @xmath48 , if a rule is applicable ( has value _ 1 _ ) or not ( has value _ 0 _ instead ) . for @xmath36", "we have the spiking vector @xmath52 given @xmath50 .", "note that a 2nd spiking vector , @xmath52 , is possible if we use rule ( 2 ) over rule ( 1 ) instead ( but not both at the same time , hence we can not have a vector equal to @xmath53 , so this @xmath54 is invalid ) .", "@xmath55 in this case means that only one among several applicable rules is used and thus represented in the spiking vector .", "we can have all the possible vectors composed of @xmath56s and @xmath1s with length equal to the number of rules , but have only some of them be valid , given by @xmath57 later at subsection [ siminspect ] .    _ spiking transition matrix _", "@xmath58 is a matrix comprised of @xmath59 elements where @xmath59 is given as    [ defi - snp - mat ] @xmath60    for @xmath36 , the @xmath58 is as follows :    @xmath61    in such a scheme , rows represent rules and columns represent neurons .", "finally , the following equation provides the configuration vector at the @xmath62 step , given the configuration vector and spiking vector at the @xmath49 step , and @xmath58 :    @xmath63", "nvidia , a well known manufacturer of gpus , released in 2007 the cuda programming model and architecture @xcite . using extensions of the widely known c language ,", "a programmer can write parallel code which will then execute in multiple threads within multiple thread blocks , each contained within a grid of ( thread ) blocks .", "these grids belong to a single device i.e. a single gpu .", "each device/ gpu has multiple cores , each capable of running its own _ block of threads _", "the program run in the cuda model scales up or down , depending on the number of cores the programmer currently has in a device .", "this scaling is done in a manner that is abstracted from the user , and is efficiently handled by the architecture as well .", "automatic and efficient scaling is shown in figure [ cuda_scale ] .", "parallelized code will run faster with more cores than with fewer ones @xcite .", "code alongside the parallel execution of the @xmath64 function on the @xmath65 side , from @xcite . ]", "figure [ cuda_model ] shows another important feature of the cuda model : the host and the device parts .", "the host controls the execution flow while the device is a highly - parallel co - processor .", "device pertains to the gpu / s of the system , while the host pertains to the cpu / s .", "a function known as a _ kernel function _", ", is a function called from the host but executed in the device .    a general model for creating a cuda enabled program", "is shown in listing [ cuda - code ] .    ....", "//allocate memory on gpu e.g. cudamalloc ( ( void**)&dev_a , n * sizeof(int )    //populate arrays   . .", "//copy arrays from host to device e.g. cudamemcpy ( dev_a , a , n * sizeof(int ) ,   cudamemcpyhosttodevice )    //call kernel ( gpu ) function e.g. add<<<n , 1 > > > ( dev_a , dev_b , dev_c ) ;    // copy arrays from device to host e.g. cudamemcpy ( c , dev_c , n * sizeof ( int ) ,   cudamemcpydevicetohost )    //display results    //free memory e.g. cudafree ( dev_a ) ; ....    lines 2 and 21 , implement cuda versions of the standard c language functions e.g. the standard c function @xmath66 has the cuda c function counterpart being @xmath67 , and the standard c function @xmath68 has @xmath69 as its cuda c counterpart .", "lines 8 and 15 show a cuda c specific function , namely @xmath70 , which , given an input of pointers ( from listing [ cuda - code ] host code pointers are single letter variables such as @xmath12 and @xmath71,while device code variable counterparts are prefixed by @xmath72 such as @xmath73 and @xmath74 ) and the size to copy ( as computed by the @xmath75 function ) , moves data from host to device ( parameter @xmath76 ) or device to host ( parameter @xmath77 ) .", "a kernel function call uses the triple @xmath78 and @xmath79 operator , in this case the kernel function    @xmath80@xmath81 .", "this function adds the values , per element ( and each element is associated to 1 thread ) , of the variables @xmath73 and @xmath82 sent to the device , collected in variable @xmath74 before being sent back to the host / cpu .", "the variable @xmath83 in this case allows the programmer to specify @xmath83 number of threads which will execute the @xmath84 kernel function in parallel , with 1 specifying only one block of thread for all @xmath83 threads .", "since the kernel function is executed in parallel in the device , the function needs to have its inputs initially moved from the cpu / host to the device , and then back from the device to the host after computation for the results .", "this movement of data back and forth should be minimized in order to obtain more efficient , in terms of time , execution . implementing an equation such as ( [ next - config ] ) , which involves multiplication and addition between vectors and a matrix , can be done in parallel with the previous considerations in mind . in this case ,", "@xmath48 , @xmath54 , and @xmath58 are loaded , manipulated , and pre - processed within the host code , before being sent to the kernel function which will perform computations on these function arguments in parallel . to represent @xmath48 , @xmath54 , and @xmath58 ,", "text files are created to house each input , whereby each element of the vector or matrix is entered in the file in order , from left to right , with a blank space in between as a delimiter .", "the matrix however is entered in row - major ( a linear array of all the elements , rows first , then columns ) order format i.e. for the matrix @xmath58 seen in ( [ snp_mat ] ) , the row - major order version is simply @xmath85    row major ordering is a well - known ordering and representation of matrices for their linear as well as parallel manipulation in corresponding algorithms @xcite .", "once all computations are done for the @xmath62 configuration , the result of equation ( [ next - config ] ) are then collected and moved from the device back to the host , where they can once again be operated on by the host / cpu .", "it is also important to note that these operations in the host / cpu provide logic and control of the data / inputs , while the device / gpu provides the arithmetic or computational muscle , the laborious task of working on multiple data at a given time in parallel , hence the current dichotomy of the cuda programming model @xcite .", "the gpu acts as a _ co - processor _ of the central processor .", "this division of labor is observed in listing [ cuda - code ] .      once all 3 initial and necessary inputs are loaded ,", "as is to be expected from equation [ next - config ] , the device is first instructed to perform multiplication between the spiking vector @xmath54 and the matrix @xmath58 . to further simplify computations at this point ,", "the vectors are treated and automatically formatted by the host code to appear as single row matrices , since vectors can be considered as such .", "multiplication is done per element ( one element is in one thread of the device / gpu ) , and then the products are collected and summed to produce a single element of the resulting vector / single row matrix .", "once multiplication of the @xmath54 and @xmath58 is done , the result is added to the @xmath48 , once again element per element , with each element belonging to one thread , executed at the same time as the others .    for this simulator ,", "the host code consists largely of the programming language _ python _ , a well - known high- level , object oriented programming ( oop ) language .", "the reason for using a high - level language such as python is because the initial inputs , as well as succeeding ones resulting from exhaustively applying the rules and equation ( [ next - config ] ) require manipulation of the vector / matrix elements or values as _", "strings_. the strings are then concatenated , checked on ( if they conform to the form ( b-3 ) for example ) by the host , as well as manipulated in ways which will be elaborated in the following sections along with the discussion of the algorithm for producing all possible and valid @xmath54s and @xmath48s given initial conditions .", "the host code / python part thus implements the logic and control as mentioned earlier , while in it , the device / gpu code which is written in c executes the parallel parts of the simulator for cuda to be utilized .", "the current snp simulator , which is based on the type of snp systems without time delays , is capable of implementing rules of the form ( b-3 ) i.e. whenever the regular expression @xmath20 is equivalent to the regular expression @xmath86 in that rule .", "rules are entered in the same manner as the earlier mentioned vectors and matrix , as blank space delimited values ( from one rule to the other , belonging to the same neuron ) and $ delimited ( from one neuron to the other ) .", "thus for the snp system @xmath87 shown earlier , the file @xmath88 containing the blank space and _ $ _ delimited values is as follows : @xmath89    that is , rule ( 1 ) from figure [ snp_ex ] has the value @xmath90 in the file @xmath88 ( though rule ( 1 ) is nt of the form ( b-3 ) it nevertheless consumes a spike since its regular expression is of the same regular expression type as the rest of the rules of @xmath87 ) .", "another implementation consideration was the use of @xmath91 in python , since unlike dictionaries or tuples , lists in python are _ mutable _ , which is a direct requirement of the vector / matrix element manipulation to be performed later on ( concatenation mostly ) .", "hence a @xmath92 is represented as @xmath93 $ ] in python .", "that is , at the @xmath49 configuration of the system , the number of spikes of neuron 1 are given by accessing the index ( starting at zero ) of the configuration vector python @xmath94 variable @xmath95 , in this case if @xmath96\\ ] ]    then @xmath97 = 2 $ ] gives the number of spikes available at that time for neuron 1 , @xmath98 = 1 $ ] for neuron 2 , and so on .", "the file @xmath88 , which contains the ordered list of neurons and the rules that comprise each of them , is represented as a _ list of sub- lists _ in the python / host code . for snp system @xmath87 and from ( [ rules ] ) we have the following : @xmath99 , [ 1 ] , [ 1 , 2 ] ] \\ ] ]    neuron 1 s rules are given by accessing the sub - lists of @xmath88 ( again , starting at index zero ) i.e. rule ( 1 ) is given by @xmath100 [ 0 ] = 2 $ ] and rule ( 4 ) is given by @xmath101 [ 1 ] = 1 $ ] .", "finally , we have the input file @xmath102 , which holds the python @xmath94 version of ( [ row - maj ] ) .", "the general algorithm is shown in algorithm [ sim - algo ] .", "each line in algorithm [ sim - algo ] mentions which part / s the simulator code runs in , either in the device * ( device ) * or in the host * ( host ) * part .", "step @xmath103 of algorithm [ sim - algo ] makes the algorithm stop with _ 2 stopping criteria _ to do this :    one is when there are no more available spikes in the system ( hence a zero value for a configuration vector ) , and the second one being the fact that all previously generated configuration vectors have been produced in an earlier time or computation , hence using them again in part i of algorithm [ sim - algo ] would be pointless , since a redundant , infinite loop will only be formed .", "input files : @xmath95 , @xmath104 .", "i. * ( host ) * load input files . note that @xmath102 and @xmath88 need only be loaded once since they are unchanging , @xmath50 is loaded once , and then @xmath48s are loaded afterwards .", "( host ) * determine if a rule / element in @xmath88 is applicable based on its corresponding spike value in @xmath95 , and then generate all valid and possible spiking vectors in a list of lists @xmath105 given the 3 initial inputs .", "* ( device ) * from part ii .", ", run the kernel function on @xmath105 , which contains all the valid and possible spiking vectors for the current @xmath95 and @xmath88", ". this will generate the succeeding @xmath48s and their corresponding @xmath54s .    \\iv . *", "( host+device ) * repeat steps i to iv ( except instead of loading @xmath50 as @xmath95 , use the generated @xmath48s in iii ) until a zero configuration vector ( vector with only zeros as elements ) or further @xmath48s produced are repetitions of a @xmath48 produced at an earlier time .", "( stopping criteria in subsection [ snp - sim - algo ] )    another important point to notice is that either of the stopping criterion from [ snp - sim - algo ] could allow for a deeply nested computation tree , one that can continue executing for a significantly lengthy amount of time even with a multi - core cpu and even the more parallelized gpu .", "the more detailed algorithm for part @xmath106 of algorithm [ sim - algo ] is as follows .", "recall from the definition of an snp system ( definitin [ snpdefn ] ) that we have @xmath14 number of @xmath107s .", "we related @xmath14 to our implementation by noticing the cardinality of the python list @xmath88 .", "@xmath108    where    @xmath109    means the total number of rules in the @xmath110 neuron which satisfy the regular expresion @xmath20 in ( b-3 ) . @xmath14", "gives the total number of neurons , while @xmath57 gives the expected number of @xmath111 and @xmath112 @xmath54s which should be produced in a given configuration .", "we also define @xmath113 as both the largest and last integer value in the sub - list ( neuron ) created in step ii of algorithm [ sim - algo ] and further detailed in algorithm [ sim - algo2 ] , which tells us how many elements of that neuron satisfy @xmath20 .    during the exposition of the algorithm , the previous python lists ( from their vector / matrix counterparts in earlier sections ) ( [ confvec ] ) and ( [ rule - list ] ) will be utilized . for part @xmath106 algorithm [ sim - algo ]", "we have a sub - algorithm ( algorithm [ sim - algo2 ] ) for generating all valid and possible spiking vectors given input files @xmath102 , @xmath95 , and @xmath88 .", "create a list @xmath114 , a copy of @xmath88 , marking each element of @xmath114 in increasing order of @xmath0 , as long as the element / s satisfy the rule s regular expression @xmath20 of a rule ( given by list @xmath88 ) .", "elements that do nt satisfy @xmath20 are marked with 0 .", "+    to generate all possible and valid spiking vectors from @xmath114 , we go through each neuron i.e. all elements of @xmath114 , since we know a priori @xmath14 as well as the number of elements per neuron which satisfy @xmath20 .", "we only need to iterate through each neuron / element of @xmath114 , @xmath113 times .", "( from ii-1 ) .", "we then produce a new list , @xmath115 , which is made up of a sub - list of strings from all possible and valid _", "\\{1,0 } _ strings i.e. spiking vectors per neuron . +    to obtain all possible and valid _", "\\{1,0 } _ strings ( @xmath54s ) , given that there are multiple strings to be concatenated ( as in @xmath115 s case ) , pairing up the neurons first , in order , and then exhaustively distributing every element of the first neuron to the elements of the 2nd one in the pair .", "these paired - distributed strings will be stored in a new list , @xmath116 .", "algorithm [ sim - algo2 ] ends once all _ \\{1,0 } _ have been paired up to one another . as an illustration of algorithm [ sim - algo2 ] , consider ( [ confvec ] ) , ( [ rule - list ] ) , and ( [ snp_mat ] ) as inputs to our snp system simulator .", "the following details the production of all valid and possible spiking vectors using algorithm [ sim - algo2 ] .    initially from ii-1 of algorithm [ sim - algo2 ]", ", we have    @xmath117 , [ 1 ] , [ 1 , 2 ] ] $ ] .    proceeding to illustrate ii-2 we have the following passes .", "1st pass : @xmath118 , [ 1 ] , [ 1 , 2 ] ] $ ] + _ remark / s _ : previously , @xmath119 [ 0 ] $ ] was equal to 2 , but now has been changed to 1 , since it satisfies @xmath20 ( @xmath120 = 2 $ ] w / c is equal to 2 , the number of spikes consumed by that rule).@xmath121    2nd pass : @xmath118 , [ 1 ] , [ 1 , 2 ] ] $ ] + _ remark / s _ : previously @xmath122 [ 1 ] = 2 $ ] , which has now been changed ( incidentally ) to 2 as well , since it s the 2nd element of @xmath37 which satisfies @xmath20 .", "3rd pass : @xmath118 , [ 1 ] , [ 1 , 2 ] ] $ ] + _", "remark / s _ : 1st ( and only ) element of neuron 2 which satisfies @xmath20 .", "4th pass : @xmath118 , [ 1 ] , [ 1 , 2 ] ] $ ] + _", "remark / s _ : same as the 1st pass    5th pass : @xmath118 , [ 1 ] , [ 1 , 0 ] ] $ ] + _ remark / s _ : element @xmath123 [ 1 ] $ ] , or the 2nd element / rule of neuron 3 does nt satisfy @xmath20 .    final result : @xmath118 , [ 1 ] , [ 1 , 0 ] ] $ ]    at this point we have the following , based on the earlier definitions :    @xmath14 = 3 ( 3 neurons in total , one per element / value of @xmath95 )    @xmath124    @xmath57 tells us the number of valid strings of _ _ 1__s and _ _", "0__s i.e. @xmath54s , which need to be produced later , for a given @xmath48 which in this case is @xmath125 .", "there are only 2 valid @xmath54s / spiking vectors from ( [ confvec ] ) and the rules given in ( [ rule - list ] ) encoded in the python list @xmath88 .", "these @xmath54s are @xmath126 @xmath127 in order to produce all @xmath54s in an algorithmic way as is done in algorithm [ sim - algo2 ] , it s important to notice that first , _ all possible and valid _ @xmath54s ( made up of _ _ 1__s and _ _", "0__s ) per @xmath107 have to be produced first , which is facilitated by ii-1 of algorithm [ sim - algo2 ] and its output ( the current value of the list @xmath114 ) .", "continuing the illustration of ii-1 , and illustrating ii-2 this time , we iterate over neuron 1 twice , since its @xmath113 = 2 , i.e. neuron 1 has only 2 elements which satisfy @xmath20 , and consequently , it is its 2nd element ,    @xmath122 [ 1 ] = 2.$ ]    for neuron 1 , our first pass along its elements / list is as follows .", "its 1st element ,    @xmath122 [ 0 ] = 1 $ ]    is the first element to satisfy @xmath20 , hence it requires a _ 1 _ in its place , and _ 0 _ in the others .", "we therefore produce the string _10_ for it .", "next , the 2nd element satisfies @xmath20 and it too , deserves a _ 1 _ , while the rest get _ _ 0__s .", "we produce the string _01_ for it .    the new list , @xmath115 , collecting the strings produced for neuron 1 therefore becomes    @xmath128 ] $ ]    following these procedures , for neuron 2 we get @xmath115 to be as follows :    @xmath128 , [ 1 ] ] $ ]    since neuron 2 which has only one element only has 1 possible and valid string , the string 1 . finally , for neuron 3 , we get @xmath115 to be    @xmath128 , [ 1 ] , [ 10 ] ] $ ]    in neuron 3 , we iterated over it only once because @xmath113 , the number of elements it has which satisfy @xmath20 , is equal to 1 only .", "observe that the sublist    @xmath129 = [ 10 , 01 ] $ ]    is equal to all possible and valid \\{_1,0 _ } strings for neuron 1 , given rules in ( [ rule - list ] ) and the number of spikes in @xmath130 .", "illustrating ii-3 of algorithm [ sim - algo2 ] , given the valid and possible \\{_1,0 _ } strings ( spiking vectors ) for neurons 1 , 2 , and 3 ( separated per neuron - column ) from ( [ confvec ] ) and ( [ rule - list ] ) and from the illustration of ii-2 , all possible and valid list of \\{_1,0 _ } string / s for neuron 1 : [ 10,01 ] , neuron 2 : [ 1 ] , and neuron 3 : [ 10 ] .", "first , pair the strings of neurons 1 and 2 , and then distribute them exhaustively to the other neuron s possible and valid strings , concatenating them in the process ( since they are considered as @xmath131 in python ) .    10 + 1 @xmath132 101    01 + and    10    01 + 1 @xmath132 011    now we have to create a new list from @xmath115 , which will house the concatenations we ll be doing . in this case ,    @xmath133 $ ]    next , we pair up @xmath116 and the possible and valid strings of neuron 3    101 + 10 @xmath132 10110    011 + and    101    011 + 10 @xmath132 01110    eventually turning @xmath116 into    @xmath134 $ ]    the final output of the sub - algorithm for the generation of all valid and possible spiking vectors is a list ,    @xmath134 $ ]    as mentioned earlier , @xmath57 = 2 is the number of valid and possible @xmath54s to be expected from @xmath88 , @xmath135 , and @xmath50 = [ 2,1,1 ] in @xmath36 .", "thus @xmath116 is the list of all possible and valid spiking vectors given ( [ confvec ] ) and ( [ rule - list ] ) in this illustration .", "furthermore , @xmath116 includes all possible and valid spiking vectors for a given neuron in a given configuration of an sn p system with all its rules and synapses ( interconnections ) .", "part ii-3 is done ( @xmath136 ) times , albeit exhaustively still so , between the two lists / neurons in the pair .", "the snp system simulator ( combination of python and cuda c ) implements the algorithms in section [ sect - snp - algo ] earlier .", "a sample simulation run with the snp system @xmath36 is shown below ( most of the output has been truncated due to space constraints ) with @xmath50 = [ 2,1,1 ]    .... * * * * sn p system simulation run starts here * * * * spiking transition matrix :   ...     rules of the form a^n", "/ a^m - > a or a^n ->a loaded :   [ ' 2 ' , ' 2 ' , ' $ ' , ' 1 ' , ' $ ' , ' 1 ' , ' 2 ' ]     initial configuration vector : 211    number of neurons for the sn p system is 3   neuron 1   rules criterion / criteria and total order   ...      tmplist =   [ [ ' 10 ' , ' 01 ' ] , [ ' 1 ' ] , [ ' 10 ' ] ]    all valid spiking vectors : allvalidspikvec =   [ [ ' 10110 ' , ' 01110 ' ] ]   all generated cks are allgenck = [ ' 2 - 1 - 1 ' , ' 2 - 1 - 2 ' , ' 1 - 1 - 2 ' ]    end of c0   * *   * *   * *    initial total ck list is     [ ' 2 - 1 - 1 ' , ' 2 - 1 - 2 ' , ' 1 - 1 - 2 ' ]   current confvec : 212   all generated cks are allgenck = [ ' 2 - 1 - 1 ' , ' 2 - 1 - 2 ' , ' 1 - 1 - 2 ' , ' 2 - 1 - 3 ' , ' 1 - 1 - 3 ' ]   * *   * *   * *   current confvec : 112   all generated cks are allgenck = [ ' 2 - 1 - 1 ' , ' 2 - 1 - 2 ' , ' 1 - 1 - 2 ' , ' 2 - 1 - 3 ' , ' 1 - 1 - 3 ' , ' 2 - 0 - 2 ' , ' 2 - 0 - 1 ' ]   * *   * *   ...    current confvec : 109 all generated cks are allgenck = [ ' 2 - 1 - 1 ' , ' 2 - 1 - 2 ' , ...   ' 1 - 0 - 7 ' , ' 0 - 1 - 9 ' , ' 1 - 0 - 8 ' , ' 1 - 0 - 9 ' ]    * * * * * *    no more cks to use ( infinite loop / s otherwise ) . stop . * * * * sn p system simulation run ends here * * * * ....    that is , the computation tree for snp system @xmath36 with @xmath50 = [ 2,1,1 ] went down as deep as @xmath137 . at that point ,", "all configuration vectors for all possible and valid spiking vectors have been produced .", "the python list variable @xmath138 collects all the @xmath48s produced . in algorithm [ sim - algo2 ]", "all the values of @xmath116 are added to @xmath138 .", "the final value of @xmath138 for the above simulation run is     + _ allgenck = [ 2 - 1 - 1 , 2 - 1 - 2 , 1 - 1 - 2 , 2 - 1 - 3 , 1 - 1 - 3 , 2 - 0 - 2 , 2 - 0 - 1 , 2 - 1 - 4 , 1 - 1 - 4 , 2 - 0 - 3 , 1 - 1 - 1 , 0 - 1 - 2 , 0 - 1 - 1 , 2 - 1 - 5 , 1 - 1 - 5 , 2 - 0 - 4 , 0 - 1 - 3 , 1 - 0 - 2 , 1 - 0 - 1 , 2 - 1 - 6 , 1 - 1 - 6 , 2 - 0 - 5 , 0 - 1 - 4 , 1 - 0 - 3 , 1 - 0 - 0 , 2 - 1 - 7 , 1 - 1 - 7 , 2 - 0 - 6 , 0 - 1 - 5 , 1 - 0 - 4 , 2 - 1 - 8 , 1 - 1 - 8 , 2 - 0 - 7 , 0 - 1 - 6 , 1 - 0 - 5 , 2 - 1 - 9 , 1 - 1 - 9 , 2 - 0 - 8 , 0 - 1 - 7 , 1 - 0 - 6 , 2 - 1 - 10 , 1 - 1 - 10 , 2 - 0 - 9 , 0 - 1 - 8 , 1 - 0 - 7 , 0 - 1 - 9 , 1 - 0 - 8 , 1 - 0 - 9 ] _     + it s also noteworthy that the simulation for @xmath36 did nt stop at the 1st stopping criteria ( arriving at a zero vector i.e. @xmath48 = [ 0,0,0 ] ) since @xmath36 generates all natural counting numbers greater than 1 , hence a loop ( an infinite one ) is to be expected .", "the simulation run shown above stopped with the 2nd stopping criteria from section [ sect - snp - algo ] .", "thus the simulation was able to exhaust all possible configuration vectors and their spiking vectors , stopping only since a repetition of an earlier generated @xmath95/@xmath48 would introduce a loop ( triggering the 2nd stopping criteria in subsection [ snp - sim - algo ] ) .", "graphically ( though not shown exhaustively ) the computation tree for @xmath36 is shown in figure [ c211_tree ] .", "with @xmath50 = [ 2 , 1 , 1 ] ]    the @xmath139 followed by ( ... ) are the @xmath139 that went deeper i.e. produced more @xmath48s than figure [ c211_tree ] has shown .", "using a highly parallel computing device such as a gpu , and the nvidia cuda programming model , an snp system simulator was successfully designed and implemented as per the objective of this work .", "the simulator was shown to model the workings of an sn p system without delay using the system s matrix representation .", "the use of a high level programming language such as python for host tasks , mainly for logic and string representation and manipulation of values ( vector / matrix elements ) has provided the necessary expressivity to implement the algorithms created to produce and exhaust all possible and valid configuration and spiking vectors .", "for the device tasks , cuda allowed the manipulation of the nvidia cuda enabled gpu which took care of repetitive and highly parallel computations ( vector - matrix addition and multiplication essentially ) .", "future versions of the snp system simulator will focus on several improvements .", "these improvements include the use of an optimized algorithm for matrix computations on the gpu without requiring the input matrix to be transformed into a square matrix ( this is currently handled by the simulator by padding zeros to an otherwise non - square matrix input ) .", "another improvement would be the simulation of systems not of the form ( b-3 ) .", "byte - compiling the python / host part of the code to improve performance as well as metrics to further enhance and measure execution time are desirable as well .", "finally , deeper understanding of the cuda architecture , such as inter- thread / block communication , for very large systems with equally large matrices , is required .", "these improvements as well as the current version of the simulator should also be run in a machine or setup with higher versions of gpus supporting nvidia cuda .", "francis cabarle is supported by the dost - erdt scholarship program .", "henry adorna is funded by the dost - erdt research grant and the alexan professorial chair of the up diliman department of computer science , university of the philippines diliman .", "they would also like to acknowledge the algorithms and complexity laboratory for the use of apple imacs with nvidia cuda enabled gpus for this work .", "miguel a. martnez  del ", "amor is supported by `` proyecto de excelencia con investigador de reconocida vala '' of the `` junta de andaluca '' under grant p08-tic04200 , and the support of the project tin200913192 of the `` ministerio de educacin y ciencia '' of spain , both co - financed by feder funds .", "finally , they would also like to thank the valuable insights of mr .", "neil ibo .", "x. zeng , h. adorna , m. a. martnez - del - amor , l. pan , m. prez - jimnez , `` matrix representation of spiking neural p systems '' , _", "11th international conference on membrane computing _ , jena , germany , aug .", "2010 .", "cecilia , j.m .", "garca , g.d .", "guerrero , m.a .", "martnez - del - amor , i. prez - hurtado , m.j .", "prez - jimnez , `` simulating a p system based efficient solution to sat by using gpus '' , _ journal of logic and algebraic programming _ ,", "vol 79 , issue 6 , pp .", "317 - 325 , apr . 2010 .", "cecilia , j.m .", "garca , g.d .", "guerrero , m.a .", "martnez - del - amor , i. prez - hurtado , m.j .", "prez - jimnez , `` simulation of p systems with active membranes on cuda '' , _ briefings in bioinformatics _ ,", "vol 11 , issue 3 , pp .", "313 - 322 , mar .", "d. daz , c. graciani , m.a .", "gutirrez , i. prez - hurtado , m.j .", "prez - jimnez .", "software for p systems . in gh .", "pun , g. rozenberg , a. salomaa ( eds . ) _ the oxford handbook of membrane computing _ , oxford university press , oxford ( u.k . ) , chapter 17 , pp .", "437 - 454 , 2009 .", "v. nguyen , d. kearney , g. gioiosa . a region - oriented hardware implementation for membrane computing applications and its integration into reconfig - p . _ lecture notes in computer science _", ", 5957 , 385 - 409 , 2010 .", "k. fatahalian , j. sugerman , p. hanrahan , `` understanding the efficiency of gpu algorithms for matrix - matrix multiplication '' , _ in proceedings of the acm siggraph / eurographics conference on graphics hardware ( hwws 04 ) _ , acm , ny , usa , pp", ". 133 - 137 , 2004"], "abstract_text": ["<S> we present in this paper our work regarding simulating a type of p system known as a spiking neural p system ( snp system ) using graphics processing units ( gpus ) . </S>", "<S> gpus , because of their architectural optimization for parallel computations , are well - suited for highly parallelizable problems . due to the advent of general purpose gpu computing in recent years , gpus are not limited to graphics and video processing alone , but include computationally intensive scientific and mathematical applications as well </S>", "<S> . moreover p systems , including snp systems , are inherently and maximally parallel computing models whose inspirations are taken from the functioning and dynamics of a living cell . </S>", "<S> in particular , snp systems try to give a modest but formal representation of a special type of cell known as the neuron and their interactions with one another . </S>", "<S> the nature of snp systems allowed their representation as matrices , which is a crucial step in simulating them on highly parallel devices such as gpus . </S>", "<S> the highly parallel nature of snp systems necessitate the use of hardware intended for parallel computations . </S>", "<S> the simulation algorithms , design considerations , and implementation are presented . finally , simulation results , observations , and analyses using an snp system that generates all numbers in @xmath0 - \\{@xmath1 } are discussed , as well as recommendations for future work .    </S>", "<S> * key words : * membrane computing , parallel computing , gpu computing </S>"], "labels": null, "section_names": ["introduction", "spiking neural p systems", "the nvidia cuda architecture", "simulator design and implementation", "simulation results, observations, and analyses", "conclusions and future work", " acknowledgments"], "sections": [["the trend for massively parallel computation is moving from the more common multi - core cpus towards gpus for several significant reasons @xcite .", "one important reason for such a trend in recent years include the low consumption in terms of power of gpus compared to setting up machines and infrastructure which will utilize multiple cpus in order to obtain the same level of parallelization and performance @xcite .", "another more important reason is that gpus are architectured for _ massively parallel computations _ since unlike most general purpose multicore cpus , a large part of the architecture of gpus are devoted to parallel execution of arithmetic operations , and not on control and caching just like in cpus @xcite .", "arithmetic operations are at the heart of many basic operations as well as scientific computations , and these are performed with larger speedups when done in parallel as compared to performing them sequentially . in order to perform these arithmetic operations on the gpu", ", there is a set of techniques called @xmath2 ( general purpose computations on the gpu ) coined by mark harris in 2002 which allows programmers to do computations on gpus and not be limited to just graphics and video processing alone @xcite .", "_ membrane computing _ or its more specific counterpart , a _ p system _ , is a turing complete computing model ( for several p system variants ) that perform computations nondeterministically , exhausting all possible computations at any given time .", "this type of unconventional model of computation was introduced by gheorghe pun in 1998 and takes inspiration and abstraction , similar to other members of _ natural computing _", "( e.g. dna / molecular computing , neural networks , quantum computing ) , from nature @xcite .", "specifically , p systems try to mimic the constitution and dynamics of the living cell : the multitude of elements inside it , and their interactions within themselves and their environment , or outside the cell s @xmath3 ( the cell s outermost membrane ) . before proceeding ,", "it is important to clarify what is meant when it is said that nature @xmath4 , particularly life or the cell : computation in this case involves reading information from memory from past or present stimuli , rewrite and retrieve this data as a stimuli from the environment , process the gathered data and act accordingly due to this processing @xcite .", "thus , we try to extend the classical meaning of computation presented by allan turing .", "sn p systems differ from other types of p systems precisely because they are @xmath5 and the working alphabet contains only _ one object type_. these characteristics , among others , are meant to capture the workings of a special type of cell known as the @xmath6 .", "neurons , such as those in the human brain , communicate or compute by sending indistinct signals more commonly known as action potential or _ spikes _ @xcite .", "@xmath7 is then communicated and encoded not by the spikes themselves , since the spikes are unrecognizable from one another , but by ( a ) the time elapsed between spikes , as well as ( b ) the number of spikes sent / received from one neuron to another , oftentimes under a certain time interval @xcite .", "it has been shown that sn p systems , given their nature , are representable by matrices @xcite .", "this representation allows design and implementation of an sn p system simulator using parallel devices such as gpus .", "since the time p systems were presented , many simulators and software applications have been produced @xcite . in terms of", "_ high performance computing _", ", many p system simulators have been also designed for clusters of computers @xcite , for reconfigurable hardware as in fpgas @xcite , and even for gpus @xcite .", "all of these efforts have shown that parallel architectures are well - suited in performance to simulate p systems .", "however , these previous works on hardware are designed to simulate _ cell - like _ p system variants , which are among the first p system variants to have been introduced .", "thus , the efficient simulation of snp systems is a new challenge that requires novel attempts .", "a matrix representation of sn p systems is quite intuitive and natural due to their graph - like configurations and properties ( as will be further shown in the succeeding sections such as in subsection [ computesnp ] ) .    on the other hand , _", "linear algebra _ operations have been efficiently implemented on parallel platforms and devices in the past years .", "for instance , there is a large number of algorithms implementing @xmath8 and @xmath9 operations on the gpu .", "these algorithms offer huge performance since dense linear algebra readily maps to the data - parallel architecture of gpus @xcite .", "it would thus seem then that a matrix represented sn p system simulator implementation on highly parallel computing devices such as gpus be a natural confluence of the earlier points made .", "the matrix representation of sn p systems bridges the gap between the theoretical yet still computationally powerful sn p systems and the applicative and more tangible gpus , via an sn p system simulator .", "the design and implementation of the simulator , including the algorithms deviced , architectural considerations , are then implemented using cuda .", "the compute unified device architecture ( cuda ) programming model , launched by nvidia in mid-2007 , is a hardware and software architecture for issuing and managing computations on their most recent gpu families ( g80 family onward ) , making the gpu operate as a highly parallel computing device @xcite .", "cuda programming model extends the widely known ansi c programming language ( among other languages which can interface with cuda ) , allowing programmers to easily design the code to be executed on the gpu , avoiding the use of low - level graphical primitives .", "cuda also provides other benefits for the programmer such as abstracted and automated scaling of the parallel executed code .", "this paper starts out by introducing and defining the type of snp system that will be simulated . afterwards the nvidia cuda model and architecture are discussed , baring the scalability and parallelization cuda offers .", "next , the design of the simulator , constraints and considerations , as well as the details of the algorithms used to realize the snp system are discussed .", "the simulation results are presented next , as well as observations and analysis of these results .", "the paper ends by providing the conclusions and future work .", "the objective of this work is to continue the creation of p system simulators , in this particular case an sn p system , using highly parallel devices such as gpus .", "fidelity to the computing model ( the type of snp system in this paper ) is a part of this objective ."], ["the type of snp systems focused on by this paper ( scope ) are those without delays i.e. those that spike or transmit signals the moment they are able to do so @xcite .", "variants which allow for delays before a neuron produces a spike , are also available @xcite .", "an snp system without delay is of the form :    [ snpdefn ] @xmath10 where :    1 .", "@xmath11 is the alphabet made up of only one object , the system spike @xmath12 .", "2 .   @xmath13 are @xmath14 number of neurons of the form @xmath15 where : 1 .", "@xmath16 gives the initial number of @xmath12s i.e. spikes contained in neuron @xmath17 2 .", "@xmath18 is a finite set of rules of with two forms : 1 .", "@xmath19 , are known as _ spiking rules _", ", where @xmath20 is a regular expression over @xmath12 , and @xmath21 , such that @xmath22 number of spikes are produced , one for each adjacent neuron with @xmath17 as the originating neuron and @xmath23 .", "@xmath24 , are known as _ forgetting rules _ , for @xmath25 , such that for each rule @xmath26 of type ( b-1 ) from @xmath18 , @xmath27 .", "@xmath28 , a special case of ( b-1 ) where @xmath20 = @xmath29 , @xmath30 .", "@xmath31 are the synapses i.e. connection between neurons .", "@xmath32 are the input and output neurons , respectively .", "furthermore , rules of type ( b-1 ) are applied if @xmath17 contains @xmath33 spikes , @xmath34 and @xmath30 . using this type of rule", "uses up or consumes @xmath33 spikes from the neuron , producing a spike to each of the neurons connected to it via a forward pointing arrow i.e. away from the neuron . in this manner , for rules of type ( b-2 )", "if @xmath17 contains @xmath35 spikes , then @xmath35 spikes are forgotten or removed once the rule is used .", "the non - determinism of sn p systems comes with the fact that more than one rule of the several types are applicable at a given time , given enough spikes .", "the rule to be used is chosen non - deterministically in the neuron . however , only one rule can be applied or used at a given time @xcite . the neurons in an sn p system operate in parallel and in unison , under a global clock @xcite . for figure [ snp_ex ] no input neuron is present , but neuron 3 is the output neuron , hence the arrow pointing towards the environment , outside the snp system .    the sn p system in figure [ snp_ex ] is @xmath36 , a 3 neuron system whose neurons are labeled ( neuron 1/@xmath37 to neuron 3/@xmath38 ) and whose rules have a total system ordering from ( 1 ) to ( 5 ) .", "neuron 1/@xmath37 can be seen to have an initial number of spikes equal to 2 ( hence the @xmath39 seen inside it ) .", "there is @xmath40 input neuron , but @xmath38 is the output neuron , as seen by the arrow pointing towards the environment ( not to another neuron ) .", "more formally , @xmath36 can be represented as follows :    @xmath41 where @xmath42 , @xmath43 , ( neurons 2 to 3 and their @xmath44s and @xmath18s can be similarly shown ) , @xmath45 are the synapses for @xmath36 , @xmath46 .", "this sn p system generates all numbers in the set @xmath0 - \\{@xmath1 } , hence it does nt @xmath47 , which can be easily verified by applying the rules in @xmath36 , and checking the spikes produced by the output neuron @xmath38 .", "this generated set is the result of the computation in @xmath36 .    , generating all numbers in @xmath0 - \\{@xmath1 } , from @xcite . ]", "a matrix representation of an sn p system makes use of the following vectors and matrix definitions @xcite .", "it is important to note that , just as in figure [ snp_ex ] , a total ordering of rules is considered .", "_ configuration vector _", "@xmath48 is the vector containing all spikes in every neuron on the @xmath49 computation step / time , where @xmath50 is the initial vector containing all spikes in the system at the beginning of the computation . for @xmath36 ( in figure [ snp_ex ] )", "the initial configuration vector is @xmath51 .    _", "spiking vector _ shows at a given configuration @xmath48 , if a rule is applicable ( has value _ 1 _ ) or not ( has value _ 0 _ instead ) . for @xmath36", "we have the spiking vector @xmath52 given @xmath50 .", "note that a 2nd spiking vector , @xmath52 , is possible if we use rule ( 2 ) over rule ( 1 ) instead ( but not both at the same time , hence we can not have a vector equal to @xmath53 , so this @xmath54 is invalid ) .", "@xmath55 in this case means that only one among several applicable rules is used and thus represented in the spiking vector .", "we can have all the possible vectors composed of @xmath56s and @xmath1s with length equal to the number of rules , but have only some of them be valid , given by @xmath57 later at subsection [ siminspect ] .    _ spiking transition matrix _", "@xmath58 is a matrix comprised of @xmath59 elements where @xmath59 is given as    [ defi - snp - mat ] @xmath60    for @xmath36 , the @xmath58 is as follows :    @xmath61    in such a scheme , rows represent rules and columns represent neurons .", "finally , the following equation provides the configuration vector at the @xmath62 step , given the configuration vector and spiking vector at the @xmath49 step , and @xmath58 :    @xmath63"], ["nvidia , a well known manufacturer of gpus , released in 2007 the cuda programming model and architecture @xcite . using extensions of the widely known c language ,", "a programmer can write parallel code which will then execute in multiple threads within multiple thread blocks , each contained within a grid of ( thread ) blocks .", "these grids belong to a single device i.e. a single gpu .", "each device/ gpu has multiple cores , each capable of running its own _ block of threads _", "the program run in the cuda model scales up or down , depending on the number of cores the programmer currently has in a device .", "this scaling is done in a manner that is abstracted from the user , and is efficiently handled by the architecture as well .", "automatic and efficient scaling is shown in figure [ cuda_scale ] .", "parallelized code will run faster with more cores than with fewer ones @xcite .", "code alongside the parallel execution of the @xmath64 function on the @xmath65 side , from @xcite . ]", "figure [ cuda_model ] shows another important feature of the cuda model : the host and the device parts .", "the host controls the execution flow while the device is a highly - parallel co - processor .", "device pertains to the gpu / s of the system , while the host pertains to the cpu / s .", "a function known as a _ kernel function _", ", is a function called from the host but executed in the device .    a general model for creating a cuda enabled program", "is shown in listing [ cuda - code ] .    ....", "//allocate memory on gpu e.g. cudamalloc ( ( void**)&dev_a , n * sizeof(int )    //populate arrays   . .", "//copy arrays from host to device e.g. cudamemcpy ( dev_a , a , n * sizeof(int ) ,   cudamemcpyhosttodevice )    //call kernel ( gpu ) function e.g. add<<<n , 1 > > > ( dev_a , dev_b , dev_c ) ;    // copy arrays from device to host e.g. cudamemcpy ( c , dev_c , n * sizeof ( int ) ,   cudamemcpydevicetohost )    //display results    //free memory e.g. cudafree ( dev_a ) ; ....    lines 2 and 21 , implement cuda versions of the standard c language functions e.g. the standard c function @xmath66 has the cuda c function counterpart being @xmath67 , and the standard c function @xmath68 has @xmath69 as its cuda c counterpart .", "lines 8 and 15 show a cuda c specific function , namely @xmath70 , which , given an input of pointers ( from listing [ cuda - code ] host code pointers are single letter variables such as @xmath12 and @xmath71,while device code variable counterparts are prefixed by @xmath72 such as @xmath73 and @xmath74 ) and the size to copy ( as computed by the @xmath75 function ) , moves data from host to device ( parameter @xmath76 ) or device to host ( parameter @xmath77 ) .", "a kernel function call uses the triple @xmath78 and @xmath79 operator , in this case the kernel function    @xmath80@xmath81 .", "this function adds the values , per element ( and each element is associated to 1 thread ) , of the variables @xmath73 and @xmath82 sent to the device , collected in variable @xmath74 before being sent back to the host / cpu .", "the variable @xmath83 in this case allows the programmer to specify @xmath83 number of threads which will execute the @xmath84 kernel function in parallel , with 1 specifying only one block of thread for all @xmath83 threads .", "since the kernel function is executed in parallel in the device , the function needs to have its inputs initially moved from the cpu / host to the device , and then back from the device to the host after computation for the results .", "this movement of data back and forth should be minimized in order to obtain more efficient , in terms of time , execution . implementing an equation such as ( [ next - config ] ) , which involves multiplication and addition between vectors and a matrix , can be done in parallel with the previous considerations in mind . in this case ,", "@xmath48 , @xmath54 , and @xmath58 are loaded , manipulated , and pre - processed within the host code , before being sent to the kernel function which will perform computations on these function arguments in parallel . to represent @xmath48 , @xmath54 , and @xmath58 ,", "text files are created to house each input , whereby each element of the vector or matrix is entered in the file in order , from left to right , with a blank space in between as a delimiter .", "the matrix however is entered in row - major ( a linear array of all the elements , rows first , then columns ) order format i.e. for the matrix @xmath58 seen in ( [ snp_mat ] ) , the row - major order version is simply @xmath85    row major ordering is a well - known ordering and representation of matrices for their linear as well as parallel manipulation in corresponding algorithms @xcite .", "once all computations are done for the @xmath62 configuration , the result of equation ( [ next - config ] ) are then collected and moved from the device back to the host , where they can once again be operated on by the host / cpu .", "it is also important to note that these operations in the host / cpu provide logic and control of the data / inputs , while the device / gpu provides the arithmetic or computational muscle , the laborious task of working on multiple data at a given time in parallel , hence the current dichotomy of the cuda programming model @xcite .", "the gpu acts as a _ co - processor _ of the central processor .", "this division of labor is observed in listing [ cuda - code ] .      once all 3 initial and necessary inputs are loaded ,", "as is to be expected from equation [ next - config ] , the device is first instructed to perform multiplication between the spiking vector @xmath54 and the matrix @xmath58 . to further simplify computations at this point ,", "the vectors are treated and automatically formatted by the host code to appear as single row matrices , since vectors can be considered as such .", "multiplication is done per element ( one element is in one thread of the device / gpu ) , and then the products are collected and summed to produce a single element of the resulting vector / single row matrix .", "once multiplication of the @xmath54 and @xmath58 is done , the result is added to the @xmath48 , once again element per element , with each element belonging to one thread , executed at the same time as the others .    for this simulator ,", "the host code consists largely of the programming language _ python _ , a well - known high- level , object oriented programming ( oop ) language .", "the reason for using a high - level language such as python is because the initial inputs , as well as succeeding ones resulting from exhaustively applying the rules and equation ( [ next - config ] ) require manipulation of the vector / matrix elements or values as _", "strings_. the strings are then concatenated , checked on ( if they conform to the form ( b-3 ) for example ) by the host , as well as manipulated in ways which will be elaborated in the following sections along with the discussion of the algorithm for producing all possible and valid @xmath54s and @xmath48s given initial conditions .", "the host code / python part thus implements the logic and control as mentioned earlier , while in it , the device / gpu code which is written in c executes the parallel parts of the simulator for cuda to be utilized ."], ["the current snp simulator , which is based on the type of snp systems without time delays , is capable of implementing rules of the form ( b-3 ) i.e. whenever the regular expression @xmath20 is equivalent to the regular expression @xmath86 in that rule .", "rules are entered in the same manner as the earlier mentioned vectors and matrix , as blank space delimited values ( from one rule to the other , belonging to the same neuron ) and $ delimited ( from one neuron to the other ) .", "thus for the snp system @xmath87 shown earlier , the file @xmath88 containing the blank space and _ $ _ delimited values is as follows : @xmath89    that is , rule ( 1 ) from figure [ snp_ex ] has the value @xmath90 in the file @xmath88 ( though rule ( 1 ) is nt of the form ( b-3 ) it nevertheless consumes a spike since its regular expression is of the same regular expression type as the rest of the rules of @xmath87 ) .", "another implementation consideration was the use of @xmath91 in python , since unlike dictionaries or tuples , lists in python are _ mutable _ , which is a direct requirement of the vector / matrix element manipulation to be performed later on ( concatenation mostly ) .", "hence a @xmath92 is represented as @xmath93 $ ] in python .", "that is , at the @xmath49 configuration of the system , the number of spikes of neuron 1 are given by accessing the index ( starting at zero ) of the configuration vector python @xmath94 variable @xmath95 , in this case if @xmath96\\ ] ]    then @xmath97 = 2 $ ] gives the number of spikes available at that time for neuron 1 , @xmath98 = 1 $ ] for neuron 2 , and so on .", "the file @xmath88 , which contains the ordered list of neurons and the rules that comprise each of them , is represented as a _ list of sub- lists _ in the python / host code . for snp system @xmath87 and from ( [ rules ] ) we have the following : @xmath99 , [ 1 ] , [ 1 , 2 ] ] \\ ] ]    neuron 1 s rules are given by accessing the sub - lists of @xmath88 ( again , starting at index zero ) i.e. rule ( 1 ) is given by @xmath100 [ 0 ] = 2 $ ] and rule ( 4 ) is given by @xmath101 [ 1 ] = 1 $ ] .", "finally , we have the input file @xmath102 , which holds the python @xmath94 version of ( [ row - maj ] ) .", "the general algorithm is shown in algorithm [ sim - algo ] .", "each line in algorithm [ sim - algo ] mentions which part / s the simulator code runs in , either in the device * ( device ) * or in the host * ( host ) * part .", "step @xmath103 of algorithm [ sim - algo ] makes the algorithm stop with _ 2 stopping criteria _ to do this :    one is when there are no more available spikes in the system ( hence a zero value for a configuration vector ) , and the second one being the fact that all previously generated configuration vectors have been produced in an earlier time or computation , hence using them again in part i of algorithm [ sim - algo ] would be pointless , since a redundant , infinite loop will only be formed .", "input files : @xmath95 , @xmath104 .", "i. * ( host ) * load input files . note that @xmath102 and @xmath88 need only be loaded once since they are unchanging , @xmath50 is loaded once , and then @xmath48s are loaded afterwards .", "( host ) * determine if a rule / element in @xmath88 is applicable based on its corresponding spike value in @xmath95 , and then generate all valid and possible spiking vectors in a list of lists @xmath105 given the 3 initial inputs .", "* ( device ) * from part ii .", ", run the kernel function on @xmath105 , which contains all the valid and possible spiking vectors for the current @xmath95 and @xmath88", ". this will generate the succeeding @xmath48s and their corresponding @xmath54s .    \\iv . *", "( host+device ) * repeat steps i to iv ( except instead of loading @xmath50 as @xmath95 , use the generated @xmath48s in iii ) until a zero configuration vector ( vector with only zeros as elements ) or further @xmath48s produced are repetitions of a @xmath48 produced at an earlier time .", "( stopping criteria in subsection [ snp - sim - algo ] )    another important point to notice is that either of the stopping criterion from [ snp - sim - algo ] could allow for a deeply nested computation tree , one that can continue executing for a significantly lengthy amount of time even with a multi - core cpu and even the more parallelized gpu .", "the more detailed algorithm for part @xmath106 of algorithm [ sim - algo ] is as follows .", "recall from the definition of an snp system ( definitin [ snpdefn ] ) that we have @xmath14 number of @xmath107s .", "we related @xmath14 to our implementation by noticing the cardinality of the python list @xmath88 .", "@xmath108    where    @xmath109    means the total number of rules in the @xmath110 neuron which satisfy the regular expresion @xmath20 in ( b-3 ) . @xmath14", "gives the total number of neurons , while @xmath57 gives the expected number of @xmath111 and @xmath112 @xmath54s which should be produced in a given configuration .", "we also define @xmath113 as both the largest and last integer value in the sub - list ( neuron ) created in step ii of algorithm [ sim - algo ] and further detailed in algorithm [ sim - algo2 ] , which tells us how many elements of that neuron satisfy @xmath20 .    during the exposition of the algorithm , the previous python lists ( from their vector / matrix counterparts in earlier sections ) ( [ confvec ] ) and ( [ rule - list ] ) will be utilized . for part @xmath106 algorithm [ sim - algo ]", "we have a sub - algorithm ( algorithm [ sim - algo2 ] ) for generating all valid and possible spiking vectors given input files @xmath102 , @xmath95 , and @xmath88 .", "create a list @xmath114 , a copy of @xmath88 , marking each element of @xmath114 in increasing order of @xmath0 , as long as the element / s satisfy the rule s regular expression @xmath20 of a rule ( given by list @xmath88 ) .", "elements that do nt satisfy @xmath20 are marked with 0 .", "+    to generate all possible and valid spiking vectors from @xmath114 , we go through each neuron i.e. all elements of @xmath114 , since we know a priori @xmath14 as well as the number of elements per neuron which satisfy @xmath20 .", "we only need to iterate through each neuron / element of @xmath114 , @xmath113 times .", "( from ii-1 ) .", "we then produce a new list , @xmath115 , which is made up of a sub - list of strings from all possible and valid _", "\\{1,0 } _ strings i.e. spiking vectors per neuron . +    to obtain all possible and valid _", "\\{1,0 } _ strings ( @xmath54s ) , given that there are multiple strings to be concatenated ( as in @xmath115 s case ) , pairing up the neurons first , in order , and then exhaustively distributing every element of the first neuron to the elements of the 2nd one in the pair .", "these paired - distributed strings will be stored in a new list , @xmath116 .", "algorithm [ sim - algo2 ] ends once all _ \\{1,0 } _ have been paired up to one another . as an illustration of algorithm [ sim - algo2 ] , consider ( [ confvec ] ) , ( [ rule - list ] ) , and ( [ snp_mat ] ) as inputs to our snp system simulator .", "the following details the production of all valid and possible spiking vectors using algorithm [ sim - algo2 ] .    initially from ii-1 of algorithm [ sim - algo2 ]", ", we have    @xmath117 , [ 1 ] , [ 1 , 2 ] ] $ ] .    proceeding to illustrate ii-2 we have the following passes .", "1st pass : @xmath118 , [ 1 ] , [ 1 , 2 ] ] $ ] + _ remark / s _ : previously , @xmath119 [ 0 ] $ ] was equal to 2 , but now has been changed to 1 , since it satisfies @xmath20 ( @xmath120 = 2 $ ] w / c is equal to 2 , the number of spikes consumed by that rule).@xmath121    2nd pass : @xmath118 , [ 1 ] , [ 1 , 2 ] ] $ ] + _ remark / s _ : previously @xmath122 [ 1 ] = 2 $ ] , which has now been changed ( incidentally ) to 2 as well , since it s the 2nd element of @xmath37 which satisfies @xmath20 .", "3rd pass : @xmath118 , [ 1 ] , [ 1 , 2 ] ] $ ] + _", "remark / s _ : 1st ( and only ) element of neuron 2 which satisfies @xmath20 .", "4th pass : @xmath118 , [ 1 ] , [ 1 , 2 ] ] $ ] + _", "remark / s _ : same as the 1st pass    5th pass : @xmath118 , [ 1 ] , [ 1 , 0 ] ] $ ] + _ remark / s _ : element @xmath123 [ 1 ] $ ] , or the 2nd element / rule of neuron 3 does nt satisfy @xmath20 .    final result : @xmath118 , [ 1 ] , [ 1 , 0 ] ] $ ]    at this point we have the following , based on the earlier definitions :    @xmath14 = 3 ( 3 neurons in total , one per element / value of @xmath95 )    @xmath124    @xmath57 tells us the number of valid strings of _ _ 1__s and _ _", "0__s i.e. @xmath54s , which need to be produced later , for a given @xmath48 which in this case is @xmath125 .", "there are only 2 valid @xmath54s / spiking vectors from ( [ confvec ] ) and the rules given in ( [ rule - list ] ) encoded in the python list @xmath88 .", "these @xmath54s are @xmath126 @xmath127 in order to produce all @xmath54s in an algorithmic way as is done in algorithm [ sim - algo2 ] , it s important to notice that first , _ all possible and valid _ @xmath54s ( made up of _ _ 1__s and _ _", "0__s ) per @xmath107 have to be produced first , which is facilitated by ii-1 of algorithm [ sim - algo2 ] and its output ( the current value of the list @xmath114 ) .", "continuing the illustration of ii-1 , and illustrating ii-2 this time , we iterate over neuron 1 twice , since its @xmath113 = 2 , i.e. neuron 1 has only 2 elements which satisfy @xmath20 , and consequently , it is its 2nd element ,    @xmath122 [ 1 ] = 2.$ ]    for neuron 1 , our first pass along its elements / list is as follows .", "its 1st element ,    @xmath122 [ 0 ] = 1 $ ]    is the first element to satisfy @xmath20 , hence it requires a _ 1 _ in its place , and _ 0 _ in the others .", "we therefore produce the string _10_ for it .", "next , the 2nd element satisfies @xmath20 and it too , deserves a _ 1 _ , while the rest get _ _ 0__s .", "we produce the string _01_ for it .    the new list , @xmath115 , collecting the strings produced for neuron 1 therefore becomes    @xmath128 ] $ ]    following these procedures , for neuron 2 we get @xmath115 to be as follows :    @xmath128 , [ 1 ] ] $ ]    since neuron 2 which has only one element only has 1 possible and valid string , the string 1 . finally , for neuron 3 , we get @xmath115 to be    @xmath128 , [ 1 ] , [ 10 ] ] $ ]    in neuron 3 , we iterated over it only once because @xmath113 , the number of elements it has which satisfy @xmath20 , is equal to 1 only .", "observe that the sublist    @xmath129 = [ 10 , 01 ] $ ]    is equal to all possible and valid \\{_1,0 _ } strings for neuron 1 , given rules in ( [ rule - list ] ) and the number of spikes in @xmath130 .", "illustrating ii-3 of algorithm [ sim - algo2 ] , given the valid and possible \\{_1,0 _ } strings ( spiking vectors ) for neurons 1 , 2 , and 3 ( separated per neuron - column ) from ( [ confvec ] ) and ( [ rule - list ] ) and from the illustration of ii-2 , all possible and valid list of \\{_1,0 _ } string / s for neuron 1 : [ 10,01 ] , neuron 2 : [ 1 ] , and neuron 3 : [ 10 ] .", "first , pair the strings of neurons 1 and 2 , and then distribute them exhaustively to the other neuron s possible and valid strings , concatenating them in the process ( since they are considered as @xmath131 in python ) .    10 + 1 @xmath132 101    01 + and    10    01 + 1 @xmath132 011    now we have to create a new list from @xmath115 , which will house the concatenations we ll be doing . in this case ,    @xmath133 $ ]    next , we pair up @xmath116 and the possible and valid strings of neuron 3    101 + 10 @xmath132 10110    011 + and    101    011 + 10 @xmath132 01110    eventually turning @xmath116 into    @xmath134 $ ]    the final output of the sub - algorithm for the generation of all valid and possible spiking vectors is a list ,    @xmath134 $ ]    as mentioned earlier , @xmath57 = 2 is the number of valid and possible @xmath54s to be expected from @xmath88 , @xmath135 , and @xmath50 = [ 2,1,1 ] in @xmath36 .", "thus @xmath116 is the list of all possible and valid spiking vectors given ( [ confvec ] ) and ( [ rule - list ] ) in this illustration .", "furthermore , @xmath116 includes all possible and valid spiking vectors for a given neuron in a given configuration of an sn p system with all its rules and synapses ( interconnections ) .", "part ii-3 is done ( @xmath136 ) times , albeit exhaustively still so , between the two lists / neurons in the pair ."], ["the snp system simulator ( combination of python and cuda c ) implements the algorithms in section [ sect - snp - algo ] earlier .", "a sample simulation run with the snp system @xmath36 is shown below ( most of the output has been truncated due to space constraints ) with @xmath50 = [ 2,1,1 ]    .... * * * * sn p system simulation run starts here * * * * spiking transition matrix :   ...     rules of the form a^n", "/ a^m - > a or a^n ->a loaded :   [ ' 2 ' , ' 2 ' , ' $ ' , ' 1 ' , ' $ ' , ' 1 ' , ' 2 ' ]     initial configuration vector : 211    number of neurons for the sn p system is 3   neuron 1   rules criterion / criteria and total order   ...      tmplist =   [ [ ' 10 ' , ' 01 ' ] , [ ' 1 ' ] , [ ' 10 ' ] ]    all valid spiking vectors : allvalidspikvec =   [ [ ' 10110 ' , ' 01110 ' ] ]   all generated cks are allgenck = [ ' 2 - 1 - 1 ' , ' 2 - 1 - 2 ' , ' 1 - 1 - 2 ' ]    end of c0   * *   * *   * *    initial total ck list is     [ ' 2 - 1 - 1 ' , ' 2 - 1 - 2 ' , ' 1 - 1 - 2 ' ]   current confvec : 212   all generated cks are allgenck = [ ' 2 - 1 - 1 ' , ' 2 - 1 - 2 ' , ' 1 - 1 - 2 ' , ' 2 - 1 - 3 ' , ' 1 - 1 - 3 ' ]   * *   * *   * *   current confvec : 112   all generated cks are allgenck = [ ' 2 - 1 - 1 ' , ' 2 - 1 - 2 ' , ' 1 - 1 - 2 ' , ' 2 - 1 - 3 ' , ' 1 - 1 - 3 ' , ' 2 - 0 - 2 ' , ' 2 - 0 - 1 ' ]   * *   * *   ...    current confvec : 109 all generated cks are allgenck = [ ' 2 - 1 - 1 ' , ' 2 - 1 - 2 ' , ...   ' 1 - 0 - 7 ' , ' 0 - 1 - 9 ' , ' 1 - 0 - 8 ' , ' 1 - 0 - 9 ' ]    * * * * * *    no more cks to use ( infinite loop / s otherwise ) . stop . * * * * sn p system simulation run ends here * * * * ....    that is , the computation tree for snp system @xmath36 with @xmath50 = [ 2,1,1 ] went down as deep as @xmath137 . at that point ,", "all configuration vectors for all possible and valid spiking vectors have been produced .", "the python list variable @xmath138 collects all the @xmath48s produced . in algorithm [ sim - algo2 ]", "all the values of @xmath116 are added to @xmath138 .", "the final value of @xmath138 for the above simulation run is     + _ allgenck = [ 2 - 1 - 1 , 2 - 1 - 2 , 1 - 1 - 2 , 2 - 1 - 3 , 1 - 1 - 3 , 2 - 0 - 2 , 2 - 0 - 1 , 2 - 1 - 4 , 1 - 1 - 4 , 2 - 0 - 3 , 1 - 1 - 1 , 0 - 1 - 2 , 0 - 1 - 1 , 2 - 1 - 5 , 1 - 1 - 5 , 2 - 0 - 4 , 0 - 1 - 3 , 1 - 0 - 2 , 1 - 0 - 1 , 2 - 1 - 6 , 1 - 1 - 6 , 2 - 0 - 5 , 0 - 1 - 4 , 1 - 0 - 3 , 1 - 0 - 0 , 2 - 1 - 7 , 1 - 1 - 7 , 2 - 0 - 6 , 0 - 1 - 5 , 1 - 0 - 4 , 2 - 1 - 8 , 1 - 1 - 8 , 2 - 0 - 7 , 0 - 1 - 6 , 1 - 0 - 5 , 2 - 1 - 9 , 1 - 1 - 9 , 2 - 0 - 8 , 0 - 1 - 7 , 1 - 0 - 6 , 2 - 1 - 10 , 1 - 1 - 10 , 2 - 0 - 9 , 0 - 1 - 8 , 1 - 0 - 7 , 0 - 1 - 9 , 1 - 0 - 8 , 1 - 0 - 9 ] _     + it s also noteworthy that the simulation for @xmath36 did nt stop at the 1st stopping criteria ( arriving at a zero vector i.e. @xmath48 = [ 0,0,0 ] ) since @xmath36 generates all natural counting numbers greater than 1 , hence a loop ( an infinite one ) is to be expected .", "the simulation run shown above stopped with the 2nd stopping criteria from section [ sect - snp - algo ] .", "thus the simulation was able to exhaust all possible configuration vectors and their spiking vectors , stopping only since a repetition of an earlier generated @xmath95/@xmath48 would introduce a loop ( triggering the 2nd stopping criteria in subsection [ snp - sim - algo ] ) .", "graphically ( though not shown exhaustively ) the computation tree for @xmath36 is shown in figure [ c211_tree ] .", "with @xmath50 = [ 2 , 1 , 1 ] ]    the @xmath139 followed by ( ... ) are the @xmath139 that went deeper i.e. produced more @xmath48s than figure [ c211_tree ] has shown ."], ["using a highly parallel computing device such as a gpu , and the nvidia cuda programming model , an snp system simulator was successfully designed and implemented as per the objective of this work .", "the simulator was shown to model the workings of an sn p system without delay using the system s matrix representation .", "the use of a high level programming language such as python for host tasks , mainly for logic and string representation and manipulation of values ( vector / matrix elements ) has provided the necessary expressivity to implement the algorithms created to produce and exhaust all possible and valid configuration and spiking vectors .", "for the device tasks , cuda allowed the manipulation of the nvidia cuda enabled gpu which took care of repetitive and highly parallel computations ( vector - matrix addition and multiplication essentially ) .", "future versions of the snp system simulator will focus on several improvements .", "these improvements include the use of an optimized algorithm for matrix computations on the gpu without requiring the input matrix to be transformed into a square matrix ( this is currently handled by the simulator by padding zeros to an otherwise non - square matrix input ) .", "another improvement would be the simulation of systems not of the form ( b-3 ) .", "byte - compiling the python / host part of the code to improve performance as well as metrics to further enhance and measure execution time are desirable as well .", "finally , deeper understanding of the cuda architecture , such as inter- thread / block communication , for very large systems with equally large matrices , is required .", "these improvements as well as the current version of the simulator should also be run in a machine or setup with higher versions of gpus supporting nvidia cuda ."], ["francis cabarle is supported by the dost - erdt scholarship program .", "henry adorna is funded by the dost - erdt research grant and the alexan professorial chair of the up diliman department of computer science , university of the philippines diliman .", "they would also like to acknowledge the algorithms and complexity laboratory for the use of apple imacs with nvidia cuda enabled gpus for this work .", "miguel a. martnez  del ", "amor is supported by `` proyecto de excelencia con investigador de reconocida vala '' of the `` junta de andaluca '' under grant p08-tic04200 , and the support of the project tin200913192 of the `` ministerio de educacin y ciencia '' of spain , both co - financed by feder funds .", "finally , they would also like to thank the valuable insights of mr .", "neil ibo .", "x. zeng , h. adorna , m. a. martnez - del - amor , l. pan , m. prez - jimnez , `` matrix representation of spiking neural p systems '' , _", "11th international conference on membrane computing _ , jena , germany , aug .", "2010 .", "cecilia , j.m .", "garca , g.d .", "guerrero , m.a .", "martnez - del - amor , i. prez - hurtado , m.j .", "prez - jimnez , `` simulating a p system based efficient solution to sat by using gpus '' , _ journal of logic and algebraic programming _ ,", "vol 79 , issue 6 , pp .", "317 - 325 , apr . 2010 .", "cecilia , j.m .", "garca , g.d .", "guerrero , m.a .", "martnez - del - amor , i. prez - hurtado , m.j .", "prez - jimnez , `` simulation of p systems with active membranes on cuda '' , _ briefings in bioinformatics _ ,", "vol 11 , issue 3 , pp .", "313 - 322 , mar .", "d. daz , c. graciani , m.a .", "gutirrez , i. prez - hurtado , m.j .", "prez - jimnez .", "software for p systems . in gh .", "pun , g. rozenberg , a. salomaa ( eds . ) _ the oxford handbook of membrane computing _ , oxford university press , oxford ( u.k . ) , chapter 17 , pp .", "437 - 454 , 2009 .", "v. nguyen , d. kearney , g. gioiosa . a region - oriented hardware implementation for membrane computing applications and its integration into reconfig - p . _ lecture notes in computer science _", ", 5957 , 385 - 409 , 2010 .", "k. fatahalian , j. sugerman , p. hanrahan , `` understanding the efficiency of gpu algorithms for matrix - matrix multiplication '' , _ in proceedings of the acm siggraph / eurographics conference on graphics hardware ( hwws 04 ) _ , acm , ny , usa , pp", ". 133 - 137 , 2004"]]}
{"article_id": "1004.0056", "article_text": ["partial orders are a principle tool for modelling `` true concurrency '' semantics of concurrent systems ( cf .", "@xcite ) . they are utilized to develop powerful partial - order based automatic verification techniques , e.g. , _ partial order reduction _ for model checking concurrent software ( see , e.g. , ( * ? ? ?", "* chapter 10 ) and @xcite ) .", "partial orders are also equipped with _", "traces _ , their powerful formal language counterpart , proposed by mazurkiewicz @xcite . in _ the book of traces _", "@xcite , trace theory has been used to tackle problems from diverse areas including formal language theory , combinatorics , graph theory , algebra , logic , and _", "concurrency theory_.    however , while partial orders and traces can sufficiently model the  earlier than \" relationship , janicki and koutny argued that it is problematic to use a single partial order to specify both the  earlier than \" and the  not later than \" relationships @xcite .", "this motivates them to develop the theory of _ relational structures _", ", where a pair of relations is used to capture concurrent behaviors .", "the most well - known among the classes of relational structures proposed by janicki and koutny is the class of _ stratified order structures _", "( _ so - structures _ ) @xcite .", "a so - structure is a triple @xmath0 , where @xmath1 and @xmath2 are binary relations on @xmath3 .", "they were invented to model both the  earlier than \" ( the relation @xmath1 ) and  not later than \" ( the relation @xmath2 ) relationships , under the assumption that system runs are described by _ stratified partial orders _ , i.e. , step sequences .", "they have been successfully applied to model inhibitor and priority systems , asynchronous races , synthesis problems , etc .", "( see for example @xcite and others ) .", "_ com__bined _ trace _ ( _ comtrace _ ) notion , introduced by janicki and koutny @xcite , generalizes the trace notion by utilizing step sequences instead of words .", "first the set of all possible steps that generates step sequences are identified by a relation @xmath4 , which is called _", "simultaneity_. second a congruence relation is determined by a relation @xmath5 , which is called _ serializability _ and in general _ not _ symmetric . then a comtrace is defined as a finite set of congruent step sequences .", "comtraces were introduced as a formal language representation of so - structures to provide an operational semantics for petri nets with inhibitor arcs .", "unfortunately , comtraces have been less often known and applied than so - structures , even though in many cases they appear to be more natural .", "we believe one reason is that the comtrace notion was too succinctly discussed in @xcite without a full treatment dedicated to comtrace theory .", "motivated by this , janicki and the author have devoted our recent effort on the study of comtraces @xcite , yet there are too many different aspects to explore and the truth is we can barely scratch the surface . in particular , the huge amount of results from trace theory ( e.g. , from @xcite ) desperately needs to be generalized to comtraces .", "these tasks are often non - trivial since we are required to develop intuition and novel techniques to deal with the complex interactions of the `` earlier than '' and `` not later than '' relations .", "this paper gives a novel characterization of comtraces using labeled so - structures .", "such definition is interesting for the following reasons .", "first , it defines exactly the class of labeled so - structures that can be represented by comtraces .", "it is worth noting that this point is particularly important .", "even though it was shown in @xcite that every comtrace can be represented by a labeled so - structure , the converse could not be shown because a class of labeled so - structures that defines precisely the class of comtraces was not known .", "the closest to our characterization is the _ combined dependency graph _", "( _ cd - graph _ )", "notion ( analogous to _ dependence graph _ representation of traces ) introduced recently by kleijn and koutny @xcite , but again a theorem showing that combined dependency graphs can be represented by comtraces was not given .", "our approach is quite different and based on some new ideas discussed in section 4 of this paper .", "second , even though the step sequence definition of comtraces is more suitable when dealing with formal language aspects of comtraces , the labeled so - structure representation is more suitable for a variety of powerful order - theoretic results and techniques available to us ( cf .", "@xcite ) .", "finally , the labeled so - structure definition of comtrace can be easily extended to _", "infinite comtraces _ , which describe nonterminating concurrent processes .", "the labeled poset representation of infinite traces is already successfully applied in both theory and practice , e.g. , @xcite .", "although such definition is equivalent to the one using quotient monoid over infinite words @xcite , we believe that infinite labeled posets are sometimes simpler . indeed the celebrated work by thiagarajan and walukiewicz ( cf .", "@xcite ) on linear temporal logic for traces utilizes the labeled poset characterization of infinite traces , where _ configurations _ of a trace are conveniently defined as _ finite downward closed subsets _ of the labeled poset representation .", "we will not analyze infinite comtraces or logics for comtraces in this paper , but these are fruitful directions to explore using the results from this paper .", "the paper is organized as follows . in section 2 ,", "we recall some preliminary definitions and notations . in section 3", ", we give a concise exposition of the theory of so - structures and comtraces by janicki and koutny @xcite . in section 4", ", we give our definition of comtraces using labeled so - structure and some remarks on how we arrived at such definition . in section 5 , we prove a representation theorem showing that our comtrace definition and the one by janicki and koutny are indeed equivalent ; then using this theorem , we prove another representation theorem showing that our definition is also equivalent to the cd - graph definition from @xcite . in section 6", ", we define _ composition _", "operators for our comtrace representation and for cd - graphs . finally , in section 7 , some final remarks and future works are presented .", "the _ powerset _ of a set @xmath3 will be denoted by @xmath6 , i.e. @xmath7 . the set of all _ non - empty _ subsets of @xmath3 will be denoted by @xmath8 . in other words ,", "we let @xmath10 denote the _ identity relation _ on a set @xmath3 .", "if @xmath11 and @xmath12 are binary relations on a set @xmath3 ( i.e. , @xmath13 ) , then their _ composition _", "@xmath14 is defined as @xmath15 .", "we also define @xmath16 the relations @xmath17 and @xmath18 are called the _ ( irreflexive ) transitive closure _ and _ reflexive transitive closure _ of @xmath11 respectively .", "a binary relation @xmath19 is an _ equivalence relation _", "relation on @xmath3 if and only if ( iff ) @xmath11 is _ reflexive _ , _ symmetric _ and _ transitive_. if @xmath11 is an equivalence relation , then for every @xmath20 , the set @xmath21_r { \\triangleq}{\\{y\\;|\\;y\\;r\\;x \\wedge y\\in x\\}}$ ] is the equivalence class of @xmath22 with respect to @xmath11 .", "we also define @xmath23_r\\mid x\\in x\\}}$ ] , i.e. , the set of all equivalence classes of @xmath3 under @xmath11 .", "we drop the subscript and write @xmath21 $ ] when @xmath11 is clear from the context .", "a binary relation @xmath24 is a _", "partial order _", "iff @xmath11 is _ irreflexive _ and _ transitive_. the pair @xmath25 in this case is called a _ partially ordered set _ ( _ poset _ ) .", "the pair @xmath25 is called a _ finite poset _ if @xmath3 is finite . for convenience ,", "we define : @xmath26    a poset @xmath25 is _ total _", "iff @xmath27 is empty ; and _ stratified _ iff @xmath28 is an equivalence relation . evidently every total order is stratified .      for every finite set @xmath3 ,", "a set @xmath29 can be seen as an alphabet .", "the elements of @xmath30 are called _ steps _ and the elements of @xmath31 are called _ step sequences_. for example , if the set of possible steps is @xmath32 , then @xmath33 is a step sequence .", "the triple @xmath34 , where @xmath35 denotes the step sequence concatenation operator ( usually omitted ) and @xmath36 denotes the empty step sequence , is a monoid .", "let @xmath37 be a step sequence .", "we define @xmath38 , the number of occurrences of an event @xmath39 in @xmath40 , as @xmath41 , where @xmath42 denotes the cardinality of the set @xmath3", ". then we can construct its unique _ enumerated step sequence _ @xmath43 as + @xmath44 + we will call such @xmath45 an _ event occurrence _ of @xmath46 . for instance , if we let @xmath47 , then @xmath48    we let @xmath49 denote the set of all event occurrences in all steps of @xmath50 .", "for example , when @xmath47 , @xmath51 we also define @xmath52 to be the function that returns the _ label _ of @xmath53 for each @xmath54 .", "for example , if @xmath55 , then @xmath56 .", "hence , from an enumerated step sequence @xmath57 , we can uniquely reconstruct its step sequence @xmath58    for each @xmath59 , we let @xmath60 denote the consecutive number of a step where @xmath53 belongs , i.e. , if @xmath61 then @xmath62 . for our example , @xmath63 , @xmath64 , etc .", "it is important to observe that step sequences and stratified orders are interchangeable concepts . given a step sequence @xmath65 , define the binary relation @xmath66 on @xmath67 as + @xmath68    intuitively , @xmath69 simply means @xmath53 occurs before @xmath70 on the step sequence @xmath65 .", "thus , @xmath71 iff @xmath72 ; and @xmath73 iff @xmath74 . obviously , the relation @xmath66 is a stratified order and we will call it the stratified order _ generated by the step sequence _ @xmath65 .    conversely , let @xmath75 be a stratified order on a set @xmath76 . the set @xmath76", "can be represented as a sequence of equivalence classes @xmath77 ( @xmath78 ) such that @xmath79 the sequence @xmath80 is called the step sequence _ representing _ @xmath75 .", "a detailed discussion on this connection between stratified orders and step sequences can be found in @xcite .", "in this section , we review the janicki  koutny theory of stratified order structures and comtraces from @xcite .", "the reader is also referred to @xcite for an excellent introductory survey on the subject with many motivating examples .", "a _ relational structure _ is a triple @xmath81 , where @xmath3 is a set and @xmath82 , @xmath83 are binary relations on @xmath3 .", "a relational structure @xmath84 is an _ extension _ of @xmath85 , denoted as @xmath86 , iff @xmath87 , @xmath88 and @xmath89 .    a _ stratified order structure _ ( _ so - structure _ ) is a relational structure @xmath90 such that for all @xmath91 , the following hold : @xmath92 when @xmath3 is finite ,", "@xmath12 is called a _ finite so - structure_. [ def : sos ]    the axioms  imply that @xmath93 is a partial order and @xmath94 the axioms and imply @xmath95 is a _", "strict preorder_. the relation @xmath1 is called _ causality _ and represents the  earlier than \" relationship while the relation @xmath2 is called _ weak causality _ and represents the  not later than \" relationship .", "the axioms  model the mutual relationship between  earlier than \" and  not later than \" relations , provided that _ the system runs are stratified orders_. historically , the name `` stratified order structure '' came from the fact that stratified orders can be seen as a special kind of so - structures .    for every stratified poset @xmath96 ,", "the triple @xmath97 is a so - structure.[prop : soss ]    we next recall the notion of _ stratified order extension_. this concept is extremely important since the relationship between stratified orders and so - structures is exactly analogous to the one between total orders and partial orders .    let @xmath98 be a so - structure .", "a _ stratified _", "order @xmath75 on @xmath3 is a _", "stratified extension _ of @xmath12 if and only if @xmath99 .", "the set of all stratified extensions of @xmath12 is denoted as @xmath100 .", "[ def : extsos ]    szpilrajn s theorem @xcite states that every poset can be reconstructed by taking the intersection of all of its total order extensions .", "janicki and koutny showed that a similar result holds for so - structures and stratified extensions :    let @xmath101 be a so - structure .", "then + @xmath102 [ theo : szpstrat ]    using this theorem , we can show the following properties relating so - structures and their stratified extensions .", "for every so - structure @xmath101 ,    1 .", "@xmath103 2 .", "@xmath104    [ cor : szpstrat ]    * 1 .", "* see ( * ? ? ?", "* theorem 3.6 ) .", "* 2 . * follows from * 1 . * and theorem  [ theo : szpstrat ] .", "_ comtraces _ were introduced in @xcite as a generalization of traces to represent so - structures .", "the _ comtrace congruence _ is defined via two relations _ simultaneity _ and _", "serializability_.    let @xmath105 be a finite set ( of events ) and let @xmath106 be two relations called _ serializability _ and _ simultaneity _ respectively and the relation @xmath4 is irreflexive and symmetric .", "the triple @xmath107 is called a _ comtrace alphabet_. [ def : comalpha ]    intuitively , if @xmath108 then @xmath39 and @xmath109 can occur simultaneously ( or be a part of a _ synchronous _ occurrence in the sense of @xcite ) , while @xmath110 means that @xmath39 and @xmath109 may occur simultaneously or @xmath39 may occur before @xmath109 .", "we define @xmath111 , the set of all possible _ steps _ , to be the set of all cliques of the graph @xmath112 , i.e. , + @xmath113    for a comtrace alphabet @xmath114 , we define @xmath115 to be the relation comprising all pairs @xmath116 of step sequences such that + @xmath117 + where @xmath118 and @xmath119 , @xmath120 , @xmath121 are steps satisfying @xmath122 and @xmath123 .", "we define _ comtrace congruence _ @xmath124 .", "we define the comtrace concatenation operator @xmath125 as @xmath126\\circledast[t ] { \\triangleq}[r\\ast t]$ ] .", "the quotient monoid @xmath127)$ ] is called the monoid of _ comtraces _ over @xmath128 .", "[ def : commonoid ]    note that since @xmath5 is irreflexive , @xmath129 implies that @xmath130 .", "we will omit the subscript @xmath128 from the comtrace congruence @xmath131 , and write @xmath132 and @xmath133 when it causes no ambiguity . to shorten our notations , we often write @xmath134_{\\theta}$ ] or @xmath134 $ ] instead of @xmath134_{{\\equiv}_{\\theta}}$ ] to denote the comtrace generated by the step sequence @xmath135 over @xmath128 .", "let @xmath136 where @xmath39 , @xmath109 and @xmath137 are three atomic operations , where + @xmath138 + assume simultaneous reading is allowed .", "then only @xmath109 and @xmath137 can be performed simultaneously , and the simultaneous execution of @xmath109 and @xmath137 gives the same outcome as executing @xmath109 followed by @xmath137 .", "we can then define the comtrace alphabet @xmath114 , where @xmath139 and @xmath140 .", "this yields @xmath141 .", "thus , @xmath142 = { \\bigl\\ { \\{a\\}\\{b , c\\},\\{a\\}\\{b\\}\\{c\\}\\bigr\\}}$ ] is a comtrace .", "but @xmath143 .", "[ ex : comtrace1 ]    even though traces are quotient monoids over sequences and comtraces are quotient monoids over step sequences , traces can be regarded as special kinds of comtraces when the relation @xmath144 . for a more detailed discussion on this connection between traces and comtraces ,", "the reader is referred to @xcite .", "let @xmath145 .", "we define the relations @xmath146 as :    1 .", "@xmath147 2 .   @xmath148 .", "[ def : s2inv ]    it is worth noting that the structure @xmath149 is exactly the _ cd - graph _ ( cf . definition  [ def : comdag ] ) that represents the comtrace @xmath150 $ ] .", "this gives us some intuition on how koutny and kleijn constructed the cd - graph definition in @xcite .", "we also observe that @xmath151 is usually _ not _ a so - structure since @xmath152 and @xmath153 describe only basic `` local '' causality and weak causality invariants of the event occurrences of @xmath65 by considering pairwise serializable relationships of event occurrences .", "hence , @xmath152 and @xmath153 might not capture `` global '' invariants that can be inferred from  of definition  [ def : sos ] . to ensure all invariants", "are included , we need the following @xmath154-closure operator .    for every relational structure @xmath155 we define @xmath156 as + @xmath157 [ def : so - cl ]    intuitively @xmath154-closure is a generalization of transitive closure for relations to relational structures .", "the motivation is that for appropriate relations @xmath82 and @xmath83 ( see assertion ( 3 ) of proposition  [ prop : so - cl ] ) , the relational structure @xmath158 is a so - structure .", "the @xmath154-closure operator satisfies the following properties :    let @xmath155 be a relational structure .    1 .", "if @xmath83 is irreflexive then @xmath159 .", "2 .   @xmath160 .", "@xmath156 is a so - structure if and only if @xmath161 is irreflexive .", "4 .   if @xmath12 is a so - structure then @xmath162 . 5 .", "if @xmath12 be a so - structure and @xmath163 , then @xmath164 and @xmath165 is a so - structure .", "[ prop : so - cl ]    given a step sequence @xmath145 and its respective comtrace @xmath166\\in { \\mathbb{s}}_{\\theta}^*/\\!\\equiv$ ] , we define the relational structures @xmath167 as : + @xmath168.[def : s2sos ]    the relational structure @xmath167 is called the _ so - structure defined by the comtrace _ @xmath166 $ ] , where @xmath169 , @xmath170 and @xmath171 are used to denote the event occurrence set , causality relation and weak causality relation induced by the comtrace @xmath172 respectively . the following nontrivial theorem and its corollary justifies the name by showing that step sequences in a comtrace @xmath172 are exactly stratified extension of the so - structure @xmath173 , and that @xmath167 is uniquely defined for the comtrace @xmath172 regardless of the choice of @xmath174 .    for each @xmath175 ,", "the relational structure @xmath167 is a so - structure and @xmath176 .", "[ theo : com2sos ]    for all @xmath177 ,    1 .", "@xmath178 2 .", "@xmath179    [ cor : com2sos ]", "even though theorem  [ theo : com2sos ] shows that each comtrace can be represented uniquely by a labeled so - structure , it does not give us an explicit definition of how these labeled so - structures look like .", "in this section , we will give an exact definition of labeled so - structures that represent comtraces . to provide us with more intuition , we first recall how mazurkiewicz traces", "can be characterized as labeled posets .", "a _ trace concurrent alphabet _ is a pair @xmath180 , where @xmath181 is a symmetric irreflexive binary relation on the finite set @xmath105 .", "a _ trace congruence _", "@xmath182 can then be defined as the smallest equivalence relation such that for all sequences @xmath183 , if @xmath184 , then @xmath185 .", "the elements of @xmath186 are called _", "traces_.    traces can also be defined alternatively as posets whose elements are labeled with symbols of a concurrent alphabet @xmath180 satisfying certain conditions .    given a binary relation @xmath187 , the _ covering relation _ of @xmath11 is defined as @xmath188 .", "an alternative definition of mazurkiewicz trace is :    a trace over a concurrent alphabet @xmath180 is a finite labeled poset @xmath189 , where @xmath190 is a labeling function , such that for all @xmath191 ,    1 .", "@xmath192 , and 2 .", "@xmath193 .", "[ def : ltraces ]    a trace in this definition is only identified unique up to _ label - preserving isomorphism_. the first condition says that immediately causally related event occurrences must be labeled with dependent events .", "the second condition ensures that any two event occurrences with dependent labels must be causally related .", "the first condition is particularly important since two immediately causally related event occurrences will occur next to each other in at least one of its linear extensions .", "this is the key to relate definition  [ def : ltraces ] with quotient monoid definition of traces .", "thus , we would like to establish a similar relationship for comtraces .", "an immediate technical difficulty is that weak causality might be cyclic , so the notion of `` immediate weak causality '' does not make sense .", "however , we can still deal with cycles of a so - structure by taking advantage of the following simple fact : _ the weak causality relation is a strict preorder_.    let @xmath101 be a so - structure .", "we define the relation @xmath194 as + @xmath195    since @xmath95 is a strict preorder , it follows that @xmath196 is an equivalence relation .", "the relation @xmath196 will be called the _ @xmath95-cycle equivalence relation _ and an element of the quotient set @xmath197 will be called a _", "@xmath95-cycle equivalence class_. we then define the following binary relations @xmath198 and @xmath199 on the quotient set @xmath197 as @xmath200 \\widehat{{\\prec } } [ \\beta ] { \\stackrel{\\textit{\\scriptsize{df}}}{\\iff}\\ } ( [ \\alpha]\\times[\\beta])\\;\\cap { \\prec}\\not=\\emptyset \\hspace*{3mm}\\text { and } \\hspace*{3 mm } [ \\alpha ] \\widehat{{\\sqsubset } } [ \\beta ] { \\stackrel{\\textit{\\scriptsize{df}}}{\\iff}\\ } ( [ \\alpha]\\times[\\beta])\\;\\cap { \\sqsubset}\\not=\\emptyset \\label{qsos}\\end{aligned}\\ ] ]    using this quotient construction , every so - structure , whose weak causality relation might be cyclic , can be uniquely represented by an _", "quotient so - structure .", "the relational structure @xmath201 is a so - structure , the relation @xmath199 is a partial order , and for all @xmath202 ,    1 .", "@xmath203 \\widehat{{\\prec } } [ \\beta]$ ] 2 .", "@xmath204 \\widehat{{\\sqsubset } } [ \\beta ] \\vee ( \\alpha\\not=\\beta \\wedge [ \\alpha]=[\\beta])$ ]    follows from definition  [ def : sos ] .    using ( [ qsos ] ) and theorem  [ theo : szpstrat ] , it is not hard to prove the following simple yet useful properties of @xmath95-cycle equivalence classes .", "let @xmath205 be a so - structure .", "we use @xmath65 and @xmath206 to denote some step sequences over @xmath8 .", "then for all @xmath191 ,    1 .", "@xmath207=[\\beta ] \\iff   \\forall \\lhd \\in ext(s),\\ ; \\alpha \\simeq_{\\lhd } \\beta$ ] 2 .", "@xmath208v$ ] 3 .", "@xmath207 { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\ ; } } [ \\beta ] \\implies \\exists\\lhd \\in ext(s),\\;\\omega_{\\lhd } = u[\\alpha][\\beta]v$ ]    [ prop : covlsos ]    each @xmath95-cycle equivalence class is what juhs , lorenz and mauser called a", "_ synchronous step _", "they also used equivalence classes to capture synchronous steps but only for the special class of _ synchronous closed _ so - structures , where @xmath209 is an equivalence relation .", "we extend their ideas by using @xmath95-cycle equivalence classes to capture what we will call _ non - serializable sets _ in arbitrary so - structures .", "the name is justified in assertion ( 1 ) of proposition  [ prop : covlsos ] stating that two elements belong to the same non - serializable set of a so - structure @xmath12 iff they must be executed simultaneously in every stratified extension of @xmath12 .", "furthermore , we show in assertion ( 2 ) that all elements of a non - serializable set must occur together as a single step in at least one stratified extension of @xmath12 .", "assertion ( 3 ) gives a sufficient condition for two non - serializable sets to occur as consecutive steps in at least one stratified extension of @xmath12 .", "before we proceed to define comtrace using labeled so - structure , we need to define _ label - preserving isomorphisms _ for labeled so - structures more formally .", "a tuple @xmath210 is a _ labeled relational structure _", "iff @xmath211 is a relational structure and @xmath212 is a function with domain @xmath3 .", "if @xmath211 is a so - structure , then @xmath85 is a _ labeled so - structure_.    given two labeled relational structures @xmath213 and @xmath214 , we write @xmath215 to denote that @xmath216 and @xmath217 are _ label - preserving isomorphic _ ( _ lp - isomorphic _ ) . in other words , there is a bijection @xmath218 such that for all @xmath219 ,    1 .", "@xmath220 2 .", "@xmath221 3 .", "@xmath222    such function @xmath223 is called a _", "label - preserving isomorphism _", "( _ lp - isomorphism _ ) .", "note that all notations , definitions and results for so - structures are applicable to labeled so - structures .", "we also write @xmath224 $ ] or @xmath225 } $ ] to denote the lp - isomorphic class of a labeled relational structure @xmath226 .", "we will not distinguish an lp - isomorphic class @xmath227 } $ ] with a single labeled relational structure @xmath85 when it does not cause ambiguity .", "we are now ready to give an alternative definition for comtraces .", "to avoid confusion with the comtrace notion by janicki and koutny in @xcite , we will use the term _ lsos - comtrace _ to denote a comtrace defined using our definition .    given a comtrace alphabet @xmath114 , a _", "lsos - comtrace _ over @xmath128 is ( an lp - isomorphic class of ) a finite labeled so - structure @xmath228 } $ ] such that @xmath190 and for all @xmath191 ,    1 .", "@xmath207 ( { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\cap \\hat{{\\prec } } ) [ \\beta ] \\implies \\lambda([\\alpha])\\times \\lambda([\\beta ] ) \\nsubseteq ser$ ] 2 .", "@xmath207 ( { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\setminus \\hat{{\\prec } } ) [ \\beta ] \\implies \\lambda([\\beta])\\times \\lambda([\\alpha ] ) \\nsubseteq ser$ ] 3 .", "@xmath229)},\\ ; a\\cup b = [ \\alpha ] \\implies \\lambda(a)\\times \\lambda(b ) \\not \\subseteq ser$ ] 4 .", "@xmath230 5 .", "@xmath231    we write @xmath232 to denote the class of all lsos - comtraces over @xmath128 .", "[ def : lcomtrace ]    let @xmath136 , @xmath233 and @xmath234 .", "then we have @xmath235 .", "the lp - isomorphic class of the labeled so - structure @xmath236 depicted in figure  [ fig : f1 ] ( the dotted edges denote @xmath95 relation and the solid edges denote both @xmath93 and @xmath95 relations ) is a lsos - comtrace .", "the graph in figure  [ fig : f2 ] represents the labeled quotient so - structure @xmath237 of @xmath85 , where we define @xmath238 .", "@xmath239[f-]{a } \\ar@{-->}[dr]\\ar@/^/[rr ] \\ar@/_1.5pc/ [ ddrr ] &      & * + [ o][f-]{c}\\ar@/_/ @{-->}[dd]\\\\           & * + [ o][f-]{c } \\ar[ur]\\ar[dr ]        & \\\\ * + [ o][f-]{b } \\ar[ur]\\ar@/_/[rr]\\ar@/^1.5pc/ [ uurr ]   &    & * + [ o][f-]{b}\\ar@/_/ @{-->}[uu ] } $ ]    @xmath240{a } \\ar@{-->}[dr]\\ar@/^1pc/[drr ] &     & \\\\           & * + [ f-]{c } \\ar[r ]       & * + [ f-]{b , c}\\\\ * + [ f-]{b } \\ar[ur]\\ar@/_1pc/[urr ]   & & } $ ]    the lsos - comtrace @xmath224 $ ] actually corresponds to the comtrace @xmath241 $ ] , and we will show this relationship formally in section  [ sec : representation ] . [ ex : comtrace2 ]    definition  [ def : lcomtrace ] can be extended to define _ infinite comtrace _ as follows . instead of asking @xmath3 to be finite , we require a labeled so - structure to be _ initially finite _", "@xcite ) , i.e. , @xmath242 is finite for all @xmath243 .", "the initially - finiteness not only gives us a sensible interpretation that every event only causually depends on finitely many events , but also guarantees that the covering relations of @xmath244 and @xmath245 are well - defined .", "since each lsos - comtrace is defined as a class of lp - isomorphic labeled so - structures , dealing with lsos - comtrace might seem tricky .", "the _ no autoconcurrency _ property , i.e. , the relation @xmath5 is irreflexive , gives us a _ canonical _ way to enumerate the events of a lsos - comtrace very similar to how the events of a comtrace are enumerated .    given a step sequence @xmath246 and any function @xmath223 defined on @xmath247 , we define @xmath248 , i.e. , the step sequence derived from @xmath135 by applying the function @xmath223 successively on each @xmath249 . note that @xmath250 denotes the _ image _ of @xmath251 under @xmath223 .", "given a lsos - comtrace @xmath252 } $ ] over a comtrace alphabet @xmath114 , a stratified order @xmath253 can be seen as a step sequence @xmath254 .    1 .", "for every @xmath255 ( @xmath256 ) , @xmath257 2 .", "@xmath258 .", "[ prop : validss ]    proposition  [ prop : validss ] ensures that @xmath259 is a valid step sequence over @xmath128 .", "recall that @xmath260 denotes the enumerated step sequence of @xmath65 and @xmath261 denotes the set of event occurrences .", "bijection _ @xmath262 as + @xmath263    by proposition  [ prop : validss ] , the function @xmath264 is well - defined .", "moreover , we can show that @xmath264 is uniquely determined by @xmath85 regardless of the choice of @xmath253 .    given @xmath265 , let @xmath266 and @xmath267", ". then @xmath268 .", "[ prop : uniquexi ]    henceforth , we will ignore subscripts and reserve the notation @xmath269 to denote the kind of mappings as defined above .", "we then define the _ enumerated so - structure _ of @xmath85 to be the labeled so - structure @xmath270 , where @xmath271 for @xmath259 and @xmath253 ; and the relations @xmath272 are defined as + @xmath273    clearly , the enumerated so - structure @xmath274 can be uniquely determined from @xmath85 using the preceding definition . from our construction , we can easily show the following important relationships :    1 .", "@xmath275 and @xmath85 are lp - isomorphic under the mapping @xmath269 .", "2 .   the labeled so - structures @xmath276 and @xmath277 are lp - isomorphic under the mapping @xmath269 and @xmath278 .", "[ prop : isoext ]    in other words , the mapping @xmath279 plays the role of both the lp - isomorphism from @xmath275 to @xmath85 and the lp - isomorphism from the stratified extension @xmath280 of @xmath275 to the stratified extension @xmath96 of @xmath85 .", "these relationships can be best captured using the commutative diagram on the right .", "@xmath281_{\\displaystyle \\xi } \\ar@{^{(}->}[d]_{\\displaystyle id_{\\sigma } }        & ( x,{\\prec},{\\sqsubset},\\lambda ) \\ar@{^{(}->}[d]^{\\displaystyle id_{x } }     \\\\    ( \\sigma,\\lhd_{u},\\lhd_{u}^{\\frown},\\ell ) \\ar@{->}[r]^{\\displaystyle \\xi } &   ( x,\\lhd,\\lhd^{\\frown},\\lambda)}$ ]    we can even observe further that two lsos - comtraces are identical if and only if they define the same enumerated so - structure . henceforth , we will call an enumerated so - structure defined by a lsos - comtrace @xmath85 _ the canonical representation _ of @xmath85 .", "recently , inspired by the dependency graph notion for mazurkiewicz traces ( cf .", "* chapter 2 ) ) , kleijn and koutny claimed without proof that their _ combined dependency graph _ notion is another alternative way to define comtraces @xcite . in section  [ sec : representation ] , we will give a detailed proof of their claim .", "given an comtrace alphabet @xmath282 , a _ combined dependency graph _", "( _ cd - graph _ ) over @xmath128 is ( a lp - isomorphic class of ) a finite labeled relational structure @xmath283 } $ ] such that @xmath190 , the relations @xmath284 are irreflexive , @xmath285 is a so - structure , and for all @xmath286 ,    @xmath287    @xmath288    @xmath289    @xmath290 + we will write @xmath291 to denote the class of all cd - graphs over @xmath128 . [ def : comdag ]    cd - graphs can be seen as reduced graph - theoretic representations for lsos - comtraces , where some arcs that can be recovered using @xmath154-closure are omitted .", "it is interesting to observe that the non - serializable sets of a cd - graph are exactly the _ strongly connected components _ of the directed graph @xmath292 and can easily be found in time @xmath293 using any standard algorithm ( cf .", "* section 22.5 ) ) .", "+    cd - graphs were called _ dependence comdags _ in @xcite . but this name could be misleading since the directed graph @xmath292 is not necessarily acyclic .", "for example , the graph on the right is the cd - graph that corresponds to the lsos - comtrace from figure  [ fig : f1 ] , but it is not acyclic .", "( here , we use the dotted edges to denote @xmath294 and the solid edges to denote _ only _ @xmath295 . ) thus , we use the name `` combined dependency graph '' instead .", "@xmath296[f-]{a } \\ar@{-->}[dr ] \\ar@/_1.5pc/ [ ddrr ] &     & * + [ o][f-]{c}\\ar@/_/ @{-->}[dd]\\\\           & * + [ o][f-]{c } \\ar[ur]\\ar[dr ]        & \\\\ * + [ o][f-]{b } \\ar[ur]\\ar@/_/[rr]\\ar@/^1.5pc/ [ uurr ]   &    & * + [ o][f-]{b}\\ar@/_/ @{-->}[uu ] } $ ]", "this section contains the main technical contribution of this paper by showing that for a given comtrace alphabet @xmath128 , @xmath297 , @xmath232 and @xmath291 are three equivalent ways of talking about the same class of objects .", "we will next prove the first representation theorem which establishes the representation mappings between @xmath297 and @xmath232 .", "let @xmath298 and @xmath299 be stratified order structures such that @xmath300", ". then @xmath301 .", "[ prop : stratsubset ]    follows from theorem  [ theo : szpstrat ] .    for the next two lemmata , we let @xmath85 be a lsos - comtrace over a comtrace alphabet @xmath114 .", "let @xmath302 be the canonical representation of @xmath85 .", "let @xmath303 and @xmath304 . since @xmath65 is a valid step sequence in @xmath31 ( by proposition  [ prop : validss ] ) , we can construct @xmath305 } = ( \\sigma_{u},{\\prec}_{[u]},{\\sqsubset}_{[u]})$ ] from definition  [ def : s2sos ] .", "our goal is to show that the stratified order @xmath305}$ ] defined by the comtrace @xmath150 $ ] is exactly @xmath306 .", "@xmath305 } \\subseteq ( \\sigma_{u},{\\prec}_{0},{\\sqsubset}_{0})$ ] .", "[ lem : l1 ]    by proposition  [ prop : so - cl ] , to show @xmath305 } = ( \\sigma_{u},{\\prec}_{u},{\\sqsubset}_{u})^{\\lozenge } \\subseteq ( \\sigma_{u},{\\prec}_{0},{\\sqsubset}_{0})$ ] , it suffices to show that @xmath307 .", "since @xmath274 is the canonical representation of @xmath85 , it is important to observe that @xmath308 .", "( @xmath309 ) : assume @xmath310 . then from definition  [ def : s2inv ] , @xmath311 .", "since @xmath312 , it follows from definition  [ def : lcomtrace ] that @xmath313 or @xmath314 .", "suppose for a contradiction that @xmath314 , then by theorem  [ theo : szpstrat ] , @xmath315 .", "but since we assume that @xmath303 , it follows that @xmath316 and @xmath317 , a contradiction .", "hence , we have shown @xmath313 .", "( @xmath318 ) : can be shown in a similar way .    @xmath305 } \\supseteq ( \\sigma_{u},{\\prec}_{0},{\\sqsubset}_{0})$ ] . [", "lem : l2 ]    in this proof , we will include subscripts for equivalence classes to avoid confusing the elements from quotient set @xmath319 with the elements from the quotient comtrace monoid @xmath320 .", "in other words , we write @xmath207_{\\equiv_{{\\sqsubset}_{0}}}$ ] to denote an element of the quotient set @xmath319 , and write @xmath150_{\\theta}$ ] to denote the comtrace generated by @xmath65 .", "let @xmath321 .", "to show @xmath305 } \\supseteq s'$ ] , by proposition  [ prop : stratsubset ] , it suffices to show @xmath322 } ) \\subseteq ext(s')$ ] . from theorem  [ theo :", "com2sos ] , we know that @xmath322_{\\theta } } ) = { \\{\\lhd_w\\mid w\\in [ u]_{\\theta}\\}}$ ] .", "thus we only need to show that for all @xmath323_{\\theta}$ ] , @xmath324 .", "we observe that from @xmath65 , by definition  [ def : commonoid ] , we can generate all the step sequences in the comtrace @xmath150_{\\theta}$ ] in stages using the following recursive definition : @xmath325    since the set @xmath150_{\\theta}$ ] is finite , @xmath150_{\\theta}=d^n(u)$ ] for some stage @xmath326 . for the rest of the proof , we will prove by induction on @xmath327 that for all @xmath328 , if @xmath329 then @xmath330 . + * base case : * when @xmath331 , @xmath332 . since @xmath333", ", it follows from proposition  [ prop : isoext ] that @xmath334 .", "+ * inductive case : * when @xmath335 , let @xmath40 be an element of @xmath336 .", "then either @xmath337 or @xmath338 . for the former case , by inductive hypothesis , @xmath339 . for the latter case", ", there must be some element @xmath340 such that @xmath341 or @xmath342 . by induction hypothesis ,", "we already known @xmath343 .", "we want to show that @xmath324 .", "there are two cases to consider : + _ * case ( i ) : * _ + when @xmath341 , by definition  [ def : commonoid ] , there are some @xmath344 and steps @xmath345 such that @xmath346 and @xmath347 where @xmath119 , @xmath120 , @xmath121 satisfy @xmath348 and @xmath349 and @xmath129 .", "let @xmath350 and @xmath351 be enumerated step sequences of @xmath206 and @xmath40 respectively .", "suppose for a contradiction that @xmath352 .", "by definition  [ def : extsos ] , there are @xmath353 and @xmath354 such that @xmath355 .", "we now consider the quotient set @xmath356 . by proposition  [ prop : covlsos ]", "( 1 ) , @xmath357 . since @xmath355", ", it follows that @xmath207_{\\equiv_{{\\sqsubset}_{0}}}\\hat{{\\sqsubset}}_{0 } [ \\beta]_{\\equiv_{{\\sqsubset}_{0}}}$ ] .", "thus , from the fact that @xmath358 is partial order , there must exists a chain @xmath200_{\\equiv_{{\\sqsubset}_{0}}}=\\ ; [ \\gamma_1]_{\\equiv_{{\\sqsubset}_{0 } } } \\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\ ; [ \\gamma_2]_{\\equiv_{{\\sqsubset}_{0}}}\\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\ ; \\ldots\\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\;[\\gamma_k]_{\\equiv_{{\\sqsubset}_{0}}}\\;= [ \\beta]_{\\equiv_{{\\sqsubset}_{0 } } }   \\label{eq : chain1}\\end{aligned}\\ ] ]    then by theorem  [ theo : szpstrat ] and the fact that @xmath343 , we know that @xmath359 for all @xmath255 . in other words , since the chain implies that every @xmath360 must always occur between @xmath53 and @xmath70 in all stratified extensions of @xmath361 and @xmath362 , we also have @xmath359 . hence , by proposition  [ prop : covlsos ] ( 1 ) , we have @xmath363_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{a}}$ ] for all @xmath255 , @xmath256 . also from of definition  [ def : lcomtrace ] and that @xmath364 , we know that for each @xmath360 , either @xmath363_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{b}}$ ] or @xmath363_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{c}}$ ]", ". now we note that the first element on the chain @xmath365_{\\equiv_{{\\sqsubset}_{0}}}=[\\alpha]_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{c}}$ ] and the last element on the chain @xmath366_{\\equiv_{{\\sqsubset}_{0}}}=[\\beta]_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{b}}$ ] .", "thus , there exist two consecutive elements @xmath363_{\\equiv_{{\\sqsubset}_{0}}}$ ] and @xmath367_{\\equiv_{{\\sqsubset}_{0}}}$ ] on the chain such that @xmath363_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{c}}$ ] and @xmath367_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{b}}$ ] .", "but then it follows that    * @xmath367_{\\equiv_{{\\sqsubset}_{0}}}\\times [ \\gamma_i]_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq ser$ ] and @xmath363_{\\equiv_{{\\sqsubset}_{0 } } } { { \\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\ ; } } [ \\gamma_{i+1}]_{\\equiv_{{\\sqsubset}_{0}}}$ ] * @xmath368_{\\equiv_{{\\sqsubset}_{0 } } } \\hat{{\\prec}}_{0 } [ \\gamma_{i+1}]_{\\equiv_{{\\sqsubset}_{0}}}\\bigr)$ ] since @xmath343 and @xmath369    these contradict of definition  [ def : lcomtrace ] since @xmath274 is a lsos - comtrace .", "+ _ * case ( ii ) : * _ + when @xmath342 , by definition  [ def : commonoid ] , there are some @xmath344 and steps @xmath345 such that @xmath370 and @xmath371 where @xmath119 , @xmath120 , @xmath121 satisfy @xmath348 and @xmath349 and @xmath129 .", "let @xmath372 and @xmath373 be enumerated step sequences of @xmath206 and @xmath40 respectively .", "suppose for a contradiction that @xmath352 . by definition  [ def : extsos ] , there are @xmath374 and @xmath375 such that @xmath313 . by proposition  [ prop : covlsos ]", "( 1 ) , @xmath357 . thus , using a dual argument to the proof of case ( i ) , we can build a chain @xmath200_{\\equiv_{{\\sqsubset}_{0}}}=\\ ; [ \\gamma_1]_{\\equiv_{{\\sqsubset}_{0 } } } \\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\ ; [ \\gamma_2]_{\\equiv_{{\\sqsubset}_{0}}}\\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\ ; \\ldots\\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\;[\\gamma_k]_{\\equiv_{{\\sqsubset}_{0}}}\\;= [ \\beta]_{\\equiv_{{\\sqsubset}_{0 } } } \\end{aligned}\\ ] ] we then argue that there are two consecutive elements on the chain such that @xmath363_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{b}}$ ] and @xmath367_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{c}}$ ] , which implies    1 .", "@xmath376_{\\equiv_{{\\sqsubset}_{0}}}\\times [ \\gamma_{i+1}]_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq ser$ ] and @xmath363_{\\equiv_{{\\sqsubset}_{0 } } } { { \\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\ ; } } [ \\gamma_{i+1}]_{\\equiv_{{\\sqsubset}_{0}}}$ ] 2 .", "@xmath363_{\\equiv_{{\\sqsubset}_{0 } } } \\hat{{\\prec}}_{0 } [ \\gamma_{i+1}]_{\\equiv_{{\\sqsubset}_{0}}}$ ] since @xmath343 and @xmath377    these contradict of definition  [ def : lcomtrace ] .", "we also need to show that the labeled so - structure defined from each comtrace is indeed a lsos - comtrace . in other words ,", "we need to show the following lemma .", "let @xmath114 be a comtrace alphabet . given a step sequence @xmath378 , the lp - isomorphic class @xmath379},{\\prec}_{[u]},{\\sqsubset}_{[u]},\\ell \\right ] } $ ]", "is a lsos - comtrace over @xmath128 .", "[ lem : l3 ]    the proof of this lemma is straightforward by checking that @xmath379},{\\prec}_{[u]},{\\sqsubset}_{[u]},\\ell \\right ] } $ ] satisfies all conditions .", "let @xmath128 be a comtrace alphabet .    1 .", "the mapping @xmath380 is defined as + @xmath381 } , $ ] + where the function @xmath382 is defined in section  [ sec : steps ] and @xmath383 is the so - structure defined by the comtrace @xmath172 from definition  [ def : s2sos ] .", "the mapping @xmath384 is defined as + @xmath385    [ def : repmaps ]    intuitively , the mapping @xmath386 is used to convert a comtrace to lsos - comtrace while the mapping @xmath387 is used to transform a lsos - comtrace into a comtrace .", "the fact that @xmath386 and @xmath387 are valid representation mappings for @xmath388 and @xmath232 will be shown in the following theorem .", "let @xmath128 be a comtrace alphabet .    1 .   for every @xmath389 , @xmath390 .", "2 .   for every @xmath391 , @xmath392 .", "[ theo : rep ]    * 1 . *", "the fact that @xmath393 follows from lemma  [ lem : l3 ] .", "now for a given @xmath389 , we have @xmath394 .", "thus , it follows that @xmath395    * 2 . * assume @xmath270 is the canonical representation of @xmath85 .", "observe that since @xmath396 , we have @xmath397    let @xmath398 .", "we will next show that @xmath399 and @xmath400 } $ ] .", "fix an arbitrary @xmath401 , from lemmas [ lem : l1 ] and [ lem : l2 ] , @xmath305 } = ( \\sigma,{\\prec}_{0},{\\sqsubset}_{0})$ ] . from theorem  [ theo : com2sos ] ,", "@xmath402})\\bigr\\ } } = [ u]$ ] .", "and the rest follows .", "the theorem says that the mappings @xmath386 and @xmath387 are inverses of each other and hence are both _", "bijective_.      using theorem  [ theo : rep ] , we are going to show that the _ combined dependency graph _ notion proposed in @xcite is another correct alternative definition for comtraces .", "first we need to define several representation mappings that are needed for our proofs .", "let @xmath128 be a comtrace alphabet .    1 .", "the mapping @xmath403 is defined as + @xmath404 + where @xmath65 is any step sequence in @xmath172 and @xmath405 and @xmath406 are defined as in definition  [ def : s2inv ] .", "the mapping @xmath407 is defined as @xmath408 .", "the mapping @xmath409 is defined as + @xmath410 .", "[ def : drepmaps ]    before proceeding futher , we want to make sure that :    1 .", "@xmath407 is a well - defined function . 2 .", "@xmath403 is a well - defined function .", "given a cd - graph @xmath411 } \\in { \\mathsf{cdg}}(\\theta)$ ] , let @xmath412 } = d_1^{\\lozenge}$ ] . we know that @xmath85 is uniquely defined , since by definition  [ def : comdag ] , @xmath413 is a so - structure , and so - structures are fixed points of @xmath154-closure ( by proposition  [ prop : so - cl ] ( 4 ) ) .", "we will next show that @xmath85 is a lsos - comtrace by verifying the conditions  of definition  [ def : lcomtrace ] .", "conditions and are exactly and .", ": suppose for contradiction that there exist two distinct non - serializable sets @xmath207,[\\beta]\\subset x$ ] such that @xmath207({{\\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\cap \\hat{{\\prec}})[\\beta]$ ] and @xmath414)\\times \\lambda([\\beta ] ) \\subseteq ser$ ] .", "clearly , this implies that @xmath415 , and thus by the @xmath154-closure definition , @xmath70 is reachable from @xmath53 on the directed graph @xmath416 , where @xmath417 .", "now we consider a shortest path @xmath418 + @xmath419 + on @xmath420 that connects @xmath53 to @xmath70 .", "we will prove by induction on @xmath421 that there exist two consecutive @xmath422 and @xmath423 on @xmath418 such that @xmath424 $ ] and @xmath425 $ ] and @xmath426 , which contradicts with @xmath414)\\times \\lambda([\\beta ] ) \\subseteq ser$ ] .", "+ * base case : * when @xmath427 , then @xmath428 . since @xmath207({{\\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\cap \\hat{{\\prec}})[\\beta]$ ] , we have @xmath429 , which by implies @xmath430 . +", "* inductive case : * when @xmath431 , we consider @xmath432 and @xmath433 .", "if @xmath434 $ ] and @xmath435 $ ] , then by @xmath207({{\\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\cap \\hat{{\\prec}})[\\beta]$ ] , we have @xmath436 , which immediately yields @xmath437 . otherwise , we have @xmath438\\cup [ \\beta]$ ] or @xmath439 $ ] .", "for the first case , we get @xmath207\\hat{{\\sqsubset}}[\\delta_2]\\hat{{\\sqsubset}}[\\beta]$ ] , which contradicts that @xmath207{{\\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}[\\beta]$ ] . for the latter case", ", we can apply induction hypothesis on the path @xmath440 .    and can also be shown similarly using `` shortest path '' argument as above .", "these proofs are easier since we only need to consider paths with edges in @xmath294 .", "* 2 . * by the proof of (", "* lemma 4.7 ) , for any two step sequences @xmath50 and @xmath65 in @xmath441 , we have @xmath442 iff @xmath443 ) = { \\mathsf{ct2dep}}([t])$ ] .", "thus the mapping @xmath444 is well - defined .", "the mapping @xmath407 is injective .", "[ lem : dlinj ]    assume that @xmath445 , such that @xmath446 $ ] . since @xmath154-closure operator does not change the labeling function", ", we can assume that @xmath447 $ ] and @xmath448 .", "we will next show that @xmath449 .", "( @xmath450 ) : let @xmath286 such that @xmath451 .", "suppose for a contradiction that @xmath452 . since @xmath451 , by , @xmath453 .", "thus , by , @xmath454 .", "but since @xmath448 , it follows that @xmath455 ( by proposition  [ prop : so - cl ] ) .", "thus , @xmath456 and @xmath457 , a contradiction .", "( @xmath458 ) : can be proved similarly .    by reversing the role of @xmath459 and @xmath460 , we have @xmath461 .", "thus , we conclude @xmath462 .", "we are now ready to show the following representation theorem which ensures that @xmath463 and @xmath464 are valid representation mappings for @xmath232 and @xmath291 .", "let @xmath128 be a comtrace alphabet .    1 .   for every @xmath465 , @xmath466 .", "2 .   for every @xmath391 , @xmath467 .", "[ theo : deprep ]    * 1 . *", "let @xmath468 and let @xmath469 .", "suppose for a contradiction that @xmath470 and @xmath471 . from how @xmath386 is defined , @xmath472 .", "thus , it follows that @xmath473 . but this contradicts the injectivity of @xmath464 from lemma  [ lem : dlinj ] .", "let @xmath391 and let @xmath474 .", "suppose for a contradiction that @xmath475 and @xmath476 . since @xmath477 , if we let @xmath478 , then @xmath479 .", "thus , we have shown that @xmath480 and @xmath481 , contradicting theorem  [ theo : rep ] ( 2 ) .", "this theorem shows that lsos - comtraces and cd - graphs are equivalent representations for comtraces .", "the main advantage of cd - graph definition is its simplicity while the lsos - comtrace definition is stronger and more convenient to prove properties about labeled so - structures that represent comtraces .", "we do not need to prove another representation theorem for cd - graphs and comtraces since their representation mappings are simply the composition of the representation mappings from theorems [ theo : rep ] and [ theo : deprep ] .", "recall for a comtrace monoid @xmath127)$ ] , the comtrace operator @xmath125 is defined as @xmath126\\circledast[t ] = [ r\\ast t]$ ] .", "we will construct analogous composition operators for lsos - comtraces and cd - graphs .", "we will then show that lsos - comtraces ( cd - graphs ) over a comtrace alphabet @xmath128 together with its composition operator form a monoid isomorphic to the comtrace monoid @xmath127)$ ] .", "given two sets @xmath482 and @xmath483 , we write @xmath484 to denote the _ disjoint union _ of @xmath482 and @xmath483 .", "such disjoint union can be easily obtained by renaming the elements in @xmath482 and @xmath483 so that @xmath485 .", "we define the lsos - comtrace composition operator as follows .", "let @xmath486 and @xmath487 be lsos - comtraces over an alphabet @xmath114 , where @xmath488 } $ ] .", "the _ composition _ @xmath489 of @xmath486 and @xmath487 is defined as ( a lp - isomorphic class of ) a labeled so - structure @xmath490 } $ ] such that @xmath491 , @xmath492 , and @xmath493 , where + @xmath494 + @xmath495    observe that the operator is well - defined since we can easily check that :    for every @xmath496 , @xmath497 .", "we will next show that this composition operator @xmath498 properly corresponds to the operator @xmath125 of the comtrace monoid over @xmath128 .", "let @xmath128 be a comtrace alphabet", ". then    1 .   for every @xmath499 , @xmath500 2 .   for every @xmath501 , @xmath502    [ prop : hom1 ]    * 1 . *", "assume @xmath503 } $ ] , @xmath504 } $ ] and @xmath505 } $ ] .", "we can pick @xmath506 and @xmath507 . then observe that a stratified order @xmath75 satisfying @xmath508 is an extension of @xmath509 .", "thus , by theorem  [ theo : rep ] , we have @xmath510\\circledast [ { \\mathsf{map}}(\\lambda_2,\\lhd_2 ) ] =   [ { \\mathsf{map}}(\\lambda,\\lhd ) ] = { \\mathsf{lct2ct}}(q)$ ] as desired .", "* 2 . * without loss of generality", ", we can assume that @xmath511 $ ] , @xmath512 $ ] and @xmath513 = \\mathbf{r}\\circledast \\mathbf{t}$ ] , where @xmath514 .", "by appropriate reindexing , we can also assume that @xmath515 .", "under these assumptions , let @xmath516 } $ ] , @xmath517 } $ ] and @xmath518 } $ ] , where @xmath519 is simply the standard labeling functions .", "it will now suffice to show that @xmath520 .", "( @xmath521 ) : let @xmath522 . by definitions [ def : s2inv ] and [ def : s2sos ]", ", we have + @xmath523 + @xmath524 + thus , by proposition  [ prop : so - cl ] ( 5 ) , we have @xmath525 as desired . furthermore , by proposition  [ prop : so - cl ] ( 5 ) , @xmath526 is a so - structure .", "( @xmath527 ) : by definitions [ def : s2inv ] and [ def : s2sos ] , we have @xmath528 and @xmath529 . since we already know", "@xmath526 is a so - structure , it follows from proposition  [ prop : so - cl ] ( 5 ) that + @xmath530    let @xmath531 denote the lp - isomorphic class @xmath532 $ ] .", "then we observe that @xmath533 ) = \\mathbb{i}$ ] and @xmath534 $ ] . by proposition  [ prop : hom1 ] and theorem  [ theo :", "rep ] , the structure @xmath535 is isomorphic to the monoid @xmath536)$ ] under the isomorphisms @xmath380 and @xmath384 .", "thus , the triple @xmath537 is also a monoid .", "we can summarize these facts in the following theorem :    the mappings @xmath386 and @xmath387 are monoid isomorphisms between two monoids @xmath536)$ ] and @xmath535 .", "similarly , we can also define a composition operator for cd - graphs .", "let @xmath538 and @xmath539 be cd - graphs over an alphabet @xmath114 , where @xmath540 } $ ] .", "the _ composition _ @xmath541 of @xmath538 and @xmath539 is defined as ( a lp - isomorphic class of ) a labeled so - structure @xmath542 } $ ] such that @xmath491 , @xmath492 , and + @xmath543 + @xmath544    from this definition , it is straightforward to show the following propositions , which we will state without proofs .    for every @xmath545 , @xmath546 .", "let @xmath128 be a comtrace alphabet", ". then    1 .   for every @xmath499 , @xmath547 2 .   for every @xmath548 , @xmath549    [ prop : hom2 ]    putting the two preceding propositions and theorem  [ theo : deprep ] together ,", "we conclude :    the mappings @xmath463 and @xmath464 are monoid isomorphisms between two monoids @xmath535 and @xmath550 .", "the simple yet useful construction we used extensively in this paper is to build a quotient so - structure modulo the @xmath95-cycle equivalence relation .", "intuitively , each @xmath95-cycle equivalence class consists of all the events that must be executed simultaneously with one another and hence can be seen as a single `` composite event '' .", "the resulting quotient so - structure is technically easier to handle since both relations of the quotient so - structure are acyclic . from this construction", ", we were able to give a labeled so - structure definition for comtraces similar to the labeled poset definition for traces .", "this quotient construction also explicitly reveals the following connection : a step on a step sequence @xmath135 is not serializable with respect to the relation @xmath5 of a comtrace alphabet if and only if it corresponds to a @xmath95-cycle equivalence class of the lsos - comtrace representing the comtrace @xmath134 $ ] ( cf .", "proposition  [ prop : covlsos ] ) .", "we have also formally shown that the quotient monoid of comtraces , the monoid of lsos - comtraces and the monoid of cd - graphs _ over the same comtrace alphabet _ are indeed isomorphic by establishing monoid isomorphisms between them .", "these three models are formal linguistic , order - theoretic , and graph - theoretic respectively , which allows us to apply a variety of tools and techniques .", "an immediate future task is to develop a framework similar to the one in this paper for _ generalized comtraces _ , proposed and developed in @xcite .", "generalized comtraces extend comtraces with the ability to model events that can be executed _ earlier than or later than but never simultaneously_. another direction is to define and analyze infinite comtraces ( and generalized comtraces ) in a spirit similar to the works on infinite traces , e.g. , @xcite .", "it is also promising to use infinite lsos - comtraces and cd - graphs to develop logics for comtraces similarly to what have been done for traces ( cf .", "@xcite ) .", "i am grateful to prof .", "ryszard janicki for introducing me comtrace theory .", "i also thank the mathematics institute of warsaw university and the theoretical computer science group of jagiellonian university for their supports during my visits .", "it was during these visits that the ideas from this paper emerge .", "this work is financially supported by the ontario graduate scholarship and the natural sciences and engineering research council of canada .", "the anonymous referees are thanked for their valuable comments that help improving the readability of this paper .", "19 e. clarke , o. grumberg and d. peled , _ model checking _ , mit press , cambridge , 1999 .", "t. h. cormen , c. e. leiserson and r. l. rivest , _ introduction to algorithms _ , second edition , mit press , 2001 .", "b. a. davey and h. a. priestley , , cambridge university press 2002 .", "v. diekert , on the concatenation of infinite traces , proc . of stacs ,", "_ lncs _ 480 ( 1991 ) , 105117 .", "v. diekert and g. rozenberg ( eds . ) , .", "world scientific 1995 .", "v. diekert and y mtivier , , _ handbook of formal languages , vol .", "3 : beyond words _ , pp 457 - 533 , springer 1997 .", "v. diekert , m. horsch , m. kufleitner , on first - order fragments for mazurkiewicz traces , _ fundam .", "_ 80(1 - 3 ) : 1 - 29 , 2007 .", "j. esparza and k. heljanko , _ unfoldings  a partial - order approach to model checking _ , springer 2008 .", "a. farzan and p. madhusudan , causal dataflow analysis for concurrent programs , proc . of cav ,", "_ lncs _ 4144 ( 2006 ) : 315328 .", "a. farzan and p. madhusudan , causal atomicity , proc . of tacas 2007 ,", "_ lncs _ 4424 ( 2007 ) , 102116 .", "p. c. fishburn , _ interval orders and interval graphs _", ", j. wiley 1985 , new york .", "h. gaifman and v. pratt , partial order models of concurrency and the computation of function , _ proc . of lics87", "_ , pp . 7285 .", "p. gastin , infinite traces , proc . of semantics of systems of concurrent processes , _", "lncs _ 469 ( 1990 ) , 277308 .", "t. gazagnaire , b. genest , l. hlout , p. s. thiagarajan , s. yang , causal message sequence charts , _ theor .", "410(41 ) : 40944110 , 2009 .", "r. janicki , relational structures model of concurrency .", ", 45(4 ) : 279320 , 2008 .", "r. janicki and m. koutny , invariants and paradigms of concurrency theory , proc . of _ parle _ 91 , _ lncs _ 506 , springer 1991 , pp . 5974 .", "r. janicki and m. koutny , structure of concurrency , , 112(1 ) : 552 , 1993 .", "r. janicki and m. koutny , semantics of inhibitor nets , _ information and computation _ , 123(1 ) : 116 , 1995 .", "r. janicki and m. koutny , fundamentals of modelling concurrency using discrete relational structures , _ acta informatica _ ,", "34 : 367388 , 1997 .", "r. janicki and m. koutny , on causality semantics of nets with priorities , _ fundamenta informaticae _ 34 : 222255 , 1999 .", "r. janicki and d. t. m. l , modelling concurrency with quotient monoids , proc of petri nets 2008 , _ lncs _ 5062 , springer 2008 , pp .", "251269 .", "r. janicki and d. t. m. l , modelling concurrency with comtraces and generalized comtraces , submitted in 2009 .", "available at : http://arxiv.org/abs/0907.1722    g. juhs , r. lorenz , s. mauser , causal semantics of algebraic petri nets distinguishing concurrency and synchronicity , _ fundamenta informatica _", "86(3 ) : 255 - 298 , 2008 .", "g. juhs , r. lorenz , s. mauser , synchronous + concurrent + sequential = earlier than + not later than , proc . of acsd06 , turku ,", "finland 2006 , pp .", "261 - 272 , ieee press .", "h. c. m. kleijn and m. koutny , process semantics of general inhibitor nets , _ information and computation _ , 190:1869 , 2004 .", "j. kleijn and m. koutny , formal languages and concurrent behaviour , _ studies in computational intelligence _ , 113:125 - 182 , 2008 .", "d. t. m. l , studies in comtrace monoids , master thesis , dept . of computing and software , mcmaster university , canada , august 2008 .", "a.  mazurkiewicz , concurrent program schemes and their interpretation , tr daimi pb-78 , comp .", "science depart .", ", aarhus university , 1977 .", "v. pratt , modeling concurrency with partial orders , _ international journal of parallel programming _", ", 15(1):3371 , 1986 .", "e.  szpilrajn , sur lextension de lordre partiel , _ fund .", "mathematicae _ 16 , 386389 , 1930 .", "p. s. thiagarajan and i. walukiewicz , an expressively complete linear time temporal logic for mazurkiewicz traces , _ inf .", "comput . _ 179(2 ) : 230249 , 2002 .", "* 1 . * ( @xmath551 ) : since @xmath207=[\\beta]$ ] , we know that @xmath552 or @xmath553 .", "the former case is trivial . for the latter case , by theorem  [ theo : szpstrat ]", ", we have @xmath554 and @xmath555 . but", "this implies that @xmath556 .", "( @xmath557 ) : the case when @xmath552 is trivial .", "assume that @xmath558 and @xmath556 .", "thus , by theorem  [ theo : szpstrat ] , @xmath559 and @xmath457 . but", "this means @xmath53 and @xmath70 belong to the same equivalence class .", "* suppose for a contradiction that all @xmath560 can not be written in the form of @xmath561v$ ] .", "this implies that there exists some @xmath562 $ ] such that for all @xmath560 , @xmath563 . but by theorem  [ theo : szpstrat ] , this yields @xmath564 and @xmath565 , contradicting with @xmath566 $ ] .", "assume @xmath207 { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\ ; } } [ \\beta]$ ] .", "suppose for a contradiction that there does not exist @xmath560 such that @xmath561[\\beta]v$ ] for some step sequences @xmath65 and @xmath206 .", "then , by theorem  [ theo : szpstrat ] , there must exist some @xmath567\\cup[\\beta])$ ] , such that @xmath568 .", "since @xmath569\\cup[\\beta]$ ] , this yields @xmath207{\\sqsubset}[\\gamma ] { \\sqsubset}[\\beta]$ ] , which contradicts that @xmath207 { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\ ; } } [ \\beta]$ ] .", "assume @xmath570 and @xmath558 .", "thus , @xmath571 .", "thus , by corollary  [ cor : szpstrat ] ( 2 ) , we have @xmath572 .", "hence , by of definition  [ def : lcomtrace ] , @xmath573 . since @xmath4 is irreflexive , this also shows that any two distinct @xmath53 and @xmath70 in @xmath251 have different labels .", "thus , @xmath257 for all @xmath255 .", "* 2 . * from the proof of * 1 . * , we know that @xmath570 and @xmath558 implies @xmath573 . thus , @xmath574 for all @xmath255 .", "observe that from proposition  [ prop : validss ] , we have @xmath575 .", "it remains to show that @xmath576 for all @xmath577 .", "suppose for a contradiction that @xmath578 for some @xmath577 .", "from the definition above , there are two distinct elements @xmath202 , such that @xmath579 and @xmath580 and @xmath581 . since @xmath5 is irreflexive , @xmath582 .", "thus , by of definition  [ def : lcomtrace ] , @xmath583 or @xmath584 . without loss of generality , we assume @xmath583 and that @xmath585 for some event @xmath586 .    again by of definition  [ def : lcomtrace ] , we know that elements having the same label are totally ordered by @xmath93 .", "thus , if @xmath587 is the number of elements in @xmath3 labeled by @xmath39 , then we have @xmath588 and @xmath589 .", "but then @xmath590 implies that @xmath591 , while @xmath592 implies that @xmath593 , which is absurd .", "let @xmath594},{\\prec}_{[u]},{\\sqsubset}_{[u]},l \\bigr)$ ] . from theorem  [ theo : com2sos ]", ", @xmath85 is a labeled so - structure .", "it only remains to show that @xmath85 satisfies conditions  of definition  [ def : lcomtrace ] .", ": assume @xmath207 ( { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\cap \\hat{{\\prec } } ) [ \\beta]$ ] and suppose for a contradiction that @xmath414)\\times \\lambda([\\beta ] ) \\subseteq ser$ ] .", "then from proposition  [ prop : covlsos ] ( 3 ) , there exists @xmath595 such that @xmath596[\\beta]w$ ] . from theorem  [ theo : com2sos ] , since we have @xmath597\\bigr\\ } } = ext(s_{[u]})$ ] , it follows that @xmath598[\\beta]w ) \\in [ u]$ ] . but @xmath414)\\times \\lambda([\\beta ] ) \\subseteq ser$ ] implies @xmath599\\cup [ \\beta]v ) \\in [ u]$ ] .", "hence , @xmath600\\cup [ \\beta]v$ ] is also a stratified extension of @xmath85 , which contradicts that @xmath207 \\hat{{\\prec } } [ \\beta]$ ] . using a similar argument", ", we can show using proposition  [ prop : covlsos ] ( 1,4 ) and using proposition  [ prop : covlsos ] ( 1,2 ) .      :", "since @xmath601 , it follows from corollary  [ cor : szpstrat ] that there exists @xmath595 where @xmath602 . since @xmath603\\bigr\\ } } = ext(s_{[u]})$ ] , there exists a sequence @xmath604 $ ] such that @xmath605 .", "this implies @xmath53 and @xmath70 belong to the same step in @xmath606 .", "thus , we have @xmath573 ."], "abstract_text": ["<S> this paper defines a class of labeled stratified order structures that characterizes exactly the notion of _ _ com__bined _ traces _ </S>", "<S> ( i.e. , _ comtraces _ ) proposed by janicki and koutny in 1995 . </S>", "<S> our main technical contributions are the representation theorems showing that comtrace quotient monoid , _ combined dependency graph _ </S>", "<S> ( kleijn and koutny 2008 ) and our labeled stratified order structure characterization are three different and yet equivalent ways to represent comtraces . </S>", "<S> + * keywords . </S>", "<S> * causality theory of concurrency , combined traces monoids , step sequences , stratified order structures , label - preserving isomorphism . </S>"], "labels": null, "section_names": ["introduction", "notations", "stratified order structures and combined traces", "comtraces as labeled stratified order structures", "representation theorems", "composition operators", "conclusion", "proof of proposition[prop:covlsos]", "proof of proposition[prop:validss]", "proof of proposition[prop:uniquexi]", "proof of lemma[lem:l3]"], "sections": [["partial orders are a principle tool for modelling `` true concurrency '' semantics of concurrent systems ( cf .", "@xcite ) . they are utilized to develop powerful partial - order based automatic verification techniques , e.g. , _ partial order reduction _ for model checking concurrent software ( see , e.g. , ( * ? ? ?", "* chapter 10 ) and @xcite ) .", "partial orders are also equipped with _", "traces _ , their powerful formal language counterpart , proposed by mazurkiewicz @xcite . in _ the book of traces _", "@xcite , trace theory has been used to tackle problems from diverse areas including formal language theory , combinatorics , graph theory , algebra , logic , and _", "concurrency theory_.    however , while partial orders and traces can sufficiently model the  earlier than \" relationship , janicki and koutny argued that it is problematic to use a single partial order to specify both the  earlier than \" and the  not later than \" relationships @xcite .", "this motivates them to develop the theory of _ relational structures _", ", where a pair of relations is used to capture concurrent behaviors .", "the most well - known among the classes of relational structures proposed by janicki and koutny is the class of _ stratified order structures _", "( _ so - structures _ ) @xcite .", "a so - structure is a triple @xmath0 , where @xmath1 and @xmath2 are binary relations on @xmath3 .", "they were invented to model both the  earlier than \" ( the relation @xmath1 ) and  not later than \" ( the relation @xmath2 ) relationships , under the assumption that system runs are described by _ stratified partial orders _ , i.e. , step sequences .", "they have been successfully applied to model inhibitor and priority systems , asynchronous races , synthesis problems , etc .", "( see for example @xcite and others ) .", "_ com__bined _ trace _ ( _ comtrace _ ) notion , introduced by janicki and koutny @xcite , generalizes the trace notion by utilizing step sequences instead of words .", "first the set of all possible steps that generates step sequences are identified by a relation @xmath4 , which is called _", "simultaneity_. second a congruence relation is determined by a relation @xmath5 , which is called _ serializability _ and in general _ not _ symmetric . then a comtrace is defined as a finite set of congruent step sequences .", "comtraces were introduced as a formal language representation of so - structures to provide an operational semantics for petri nets with inhibitor arcs .", "unfortunately , comtraces have been less often known and applied than so - structures , even though in many cases they appear to be more natural .", "we believe one reason is that the comtrace notion was too succinctly discussed in @xcite without a full treatment dedicated to comtrace theory .", "motivated by this , janicki and the author have devoted our recent effort on the study of comtraces @xcite , yet there are too many different aspects to explore and the truth is we can barely scratch the surface . in particular , the huge amount of results from trace theory ( e.g. , from @xcite ) desperately needs to be generalized to comtraces .", "these tasks are often non - trivial since we are required to develop intuition and novel techniques to deal with the complex interactions of the `` earlier than '' and `` not later than '' relations .", "this paper gives a novel characterization of comtraces using labeled so - structures .", "such definition is interesting for the following reasons .", "first , it defines exactly the class of labeled so - structures that can be represented by comtraces .", "it is worth noting that this point is particularly important .", "even though it was shown in @xcite that every comtrace can be represented by a labeled so - structure , the converse could not be shown because a class of labeled so - structures that defines precisely the class of comtraces was not known .", "the closest to our characterization is the _ combined dependency graph _", "( _ cd - graph _ )", "notion ( analogous to _ dependence graph _ representation of traces ) introduced recently by kleijn and koutny @xcite , but again a theorem showing that combined dependency graphs can be represented by comtraces was not given .", "our approach is quite different and based on some new ideas discussed in section 4 of this paper .", "second , even though the step sequence definition of comtraces is more suitable when dealing with formal language aspects of comtraces , the labeled so - structure representation is more suitable for a variety of powerful order - theoretic results and techniques available to us ( cf .", "@xcite ) .", "finally , the labeled so - structure definition of comtrace can be easily extended to _", "infinite comtraces _ , which describe nonterminating concurrent processes .", "the labeled poset representation of infinite traces is already successfully applied in both theory and practice , e.g. , @xcite .", "although such definition is equivalent to the one using quotient monoid over infinite words @xcite , we believe that infinite labeled posets are sometimes simpler . indeed the celebrated work by thiagarajan and walukiewicz ( cf .", "@xcite ) on linear temporal logic for traces utilizes the labeled poset characterization of infinite traces , where _ configurations _ of a trace are conveniently defined as _ finite downward closed subsets _ of the labeled poset representation .", "we will not analyze infinite comtraces or logics for comtraces in this paper , but these are fruitful directions to explore using the results from this paper .", "the paper is organized as follows . in section 2 ,", "we recall some preliminary definitions and notations . in section 3", ", we give a concise exposition of the theory of so - structures and comtraces by janicki and koutny @xcite . in section 4", ", we give our definition of comtraces using labeled so - structure and some remarks on how we arrived at such definition . in section 5 , we prove a representation theorem showing that our comtrace definition and the one by janicki and koutny are indeed equivalent ; then using this theorem , we prove another representation theorem showing that our definition is also equivalent to the cd - graph definition from @xcite . in section 6", ", we define _ composition _", "operators for our comtrace representation and for cd - graphs . finally , in section 7 , some final remarks and future works are presented ."], ["the _ powerset _ of a set @xmath3 will be denoted by @xmath6 , i.e. @xmath7 . the set of all _ non - empty _ subsets of @xmath3 will be denoted by @xmath8 . in other words ,", "we let @xmath10 denote the _ identity relation _ on a set @xmath3 .", "if @xmath11 and @xmath12 are binary relations on a set @xmath3 ( i.e. , @xmath13 ) , then their _ composition _", "@xmath14 is defined as @xmath15 .", "we also define @xmath16 the relations @xmath17 and @xmath18 are called the _ ( irreflexive ) transitive closure _ and _ reflexive transitive closure _ of @xmath11 respectively .", "a binary relation @xmath19 is an _ equivalence relation _", "relation on @xmath3 if and only if ( iff ) @xmath11 is _ reflexive _ , _ symmetric _ and _ transitive_. if @xmath11 is an equivalence relation , then for every @xmath20 , the set @xmath21_r { \\triangleq}{\\{y\\;|\\;y\\;r\\;x \\wedge y\\in x\\}}$ ] is the equivalence class of @xmath22 with respect to @xmath11 .", "we also define @xmath23_r\\mid x\\in x\\}}$ ] , i.e. , the set of all equivalence classes of @xmath3 under @xmath11 .", "we drop the subscript and write @xmath21 $ ] when @xmath11 is clear from the context .", "a binary relation @xmath24 is a _", "partial order _", "iff @xmath11 is _ irreflexive _ and _ transitive_. the pair @xmath25 in this case is called a _ partially ordered set _ ( _ poset _ ) .", "the pair @xmath25 is called a _ finite poset _ if @xmath3 is finite . for convenience ,", "we define : @xmath26    a poset @xmath25 is _ total _", "iff @xmath27 is empty ; and _ stratified _ iff @xmath28 is an equivalence relation . evidently every total order is stratified .      for every finite set @xmath3 ,", "a set @xmath29 can be seen as an alphabet .", "the elements of @xmath30 are called _ steps _ and the elements of @xmath31 are called _ step sequences_. for example , if the set of possible steps is @xmath32 , then @xmath33 is a step sequence .", "the triple @xmath34 , where @xmath35 denotes the step sequence concatenation operator ( usually omitted ) and @xmath36 denotes the empty step sequence , is a monoid .", "let @xmath37 be a step sequence .", "we define @xmath38 , the number of occurrences of an event @xmath39 in @xmath40 , as @xmath41 , where @xmath42 denotes the cardinality of the set @xmath3", ". then we can construct its unique _ enumerated step sequence _ @xmath43 as + @xmath44 + we will call such @xmath45 an _ event occurrence _ of @xmath46 . for instance , if we let @xmath47 , then @xmath48    we let @xmath49 denote the set of all event occurrences in all steps of @xmath50 .", "for example , when @xmath47 , @xmath51 we also define @xmath52 to be the function that returns the _ label _ of @xmath53 for each @xmath54 .", "for example , if @xmath55 , then @xmath56 .", "hence , from an enumerated step sequence @xmath57 , we can uniquely reconstruct its step sequence @xmath58    for each @xmath59 , we let @xmath60 denote the consecutive number of a step where @xmath53 belongs , i.e. , if @xmath61 then @xmath62 . for our example , @xmath63 , @xmath64 , etc .", "it is important to observe that step sequences and stratified orders are interchangeable concepts . given a step sequence @xmath65 , define the binary relation @xmath66 on @xmath67 as + @xmath68    intuitively , @xmath69 simply means @xmath53 occurs before @xmath70 on the step sequence @xmath65 .", "thus , @xmath71 iff @xmath72 ; and @xmath73 iff @xmath74 . obviously , the relation @xmath66 is a stratified order and we will call it the stratified order _ generated by the step sequence _ @xmath65 .    conversely , let @xmath75 be a stratified order on a set @xmath76 . the set @xmath76", "can be represented as a sequence of equivalence classes @xmath77 ( @xmath78 ) such that @xmath79 the sequence @xmath80 is called the step sequence _ representing _ @xmath75 .", "a detailed discussion on this connection between stratified orders and step sequences can be found in @xcite ."], ["in this section , we review the janicki  koutny theory of stratified order structures and comtraces from @xcite .", "the reader is also referred to @xcite for an excellent introductory survey on the subject with many motivating examples .", "a _ relational structure _ is a triple @xmath81 , where @xmath3 is a set and @xmath82 , @xmath83 are binary relations on @xmath3 .", "a relational structure @xmath84 is an _ extension _ of @xmath85 , denoted as @xmath86 , iff @xmath87 , @xmath88 and @xmath89 .    a _ stratified order structure _ ( _ so - structure _ ) is a relational structure @xmath90 such that for all @xmath91 , the following hold : @xmath92 when @xmath3 is finite ,", "@xmath12 is called a _ finite so - structure_. [ def : sos ]    the axioms  imply that @xmath93 is a partial order and @xmath94 the axioms and imply @xmath95 is a _", "strict preorder_. the relation @xmath1 is called _ causality _ and represents the  earlier than \" relationship while the relation @xmath2 is called _ weak causality _ and represents the  not later than \" relationship .", "the axioms  model the mutual relationship between  earlier than \" and  not later than \" relations , provided that _ the system runs are stratified orders_. historically , the name `` stratified order structure '' came from the fact that stratified orders can be seen as a special kind of so - structures .    for every stratified poset @xmath96 ,", "the triple @xmath97 is a so - structure.[prop : soss ]    we next recall the notion of _ stratified order extension_. this concept is extremely important since the relationship between stratified orders and so - structures is exactly analogous to the one between total orders and partial orders .    let @xmath98 be a so - structure .", "a _ stratified _", "order @xmath75 on @xmath3 is a _", "stratified extension _ of @xmath12 if and only if @xmath99 .", "the set of all stratified extensions of @xmath12 is denoted as @xmath100 .", "[ def : extsos ]    szpilrajn s theorem @xcite states that every poset can be reconstructed by taking the intersection of all of its total order extensions .", "janicki and koutny showed that a similar result holds for so - structures and stratified extensions :    let @xmath101 be a so - structure .", "then + @xmath102 [ theo : szpstrat ]    using this theorem , we can show the following properties relating so - structures and their stratified extensions .", "for every so - structure @xmath101 ,    1 .", "@xmath103 2 .", "@xmath104    [ cor : szpstrat ]    * 1 .", "* see ( * ? ? ?", "* theorem 3.6 ) .", "* 2 . * follows from * 1 . * and theorem  [ theo : szpstrat ] .", "_ comtraces _ were introduced in @xcite as a generalization of traces to represent so - structures .", "the _ comtrace congruence _ is defined via two relations _ simultaneity _ and _", "serializability_.    let @xmath105 be a finite set ( of events ) and let @xmath106 be two relations called _ serializability _ and _ simultaneity _ respectively and the relation @xmath4 is irreflexive and symmetric .", "the triple @xmath107 is called a _ comtrace alphabet_. [ def : comalpha ]    intuitively , if @xmath108 then @xmath39 and @xmath109 can occur simultaneously ( or be a part of a _ synchronous _ occurrence in the sense of @xcite ) , while @xmath110 means that @xmath39 and @xmath109 may occur simultaneously or @xmath39 may occur before @xmath109 .", "we define @xmath111 , the set of all possible _ steps _ , to be the set of all cliques of the graph @xmath112 , i.e. , + @xmath113    for a comtrace alphabet @xmath114 , we define @xmath115 to be the relation comprising all pairs @xmath116 of step sequences such that + @xmath117 + where @xmath118 and @xmath119 , @xmath120 , @xmath121 are steps satisfying @xmath122 and @xmath123 .", "we define _ comtrace congruence _ @xmath124 .", "we define the comtrace concatenation operator @xmath125 as @xmath126\\circledast[t ] { \\triangleq}[r\\ast t]$ ] .", "the quotient monoid @xmath127)$ ] is called the monoid of _ comtraces _ over @xmath128 .", "[ def : commonoid ]    note that since @xmath5 is irreflexive , @xmath129 implies that @xmath130 .", "we will omit the subscript @xmath128 from the comtrace congruence @xmath131 , and write @xmath132 and @xmath133 when it causes no ambiguity . to shorten our notations , we often write @xmath134_{\\theta}$ ] or @xmath134 $ ] instead of @xmath134_{{\\equiv}_{\\theta}}$ ] to denote the comtrace generated by the step sequence @xmath135 over @xmath128 .", "let @xmath136 where @xmath39 , @xmath109 and @xmath137 are three atomic operations , where + @xmath138 + assume simultaneous reading is allowed .", "then only @xmath109 and @xmath137 can be performed simultaneously , and the simultaneous execution of @xmath109 and @xmath137 gives the same outcome as executing @xmath109 followed by @xmath137 .", "we can then define the comtrace alphabet @xmath114 , where @xmath139 and @xmath140 .", "this yields @xmath141 .", "thus , @xmath142 = { \\bigl\\ { \\{a\\}\\{b , c\\},\\{a\\}\\{b\\}\\{c\\}\\bigr\\}}$ ] is a comtrace .", "but @xmath143 .", "[ ex : comtrace1 ]    even though traces are quotient monoids over sequences and comtraces are quotient monoids over step sequences , traces can be regarded as special kinds of comtraces when the relation @xmath144 . for a more detailed discussion on this connection between traces and comtraces ,", "the reader is referred to @xcite .", "let @xmath145 .", "we define the relations @xmath146 as :    1 .", "@xmath147 2 .   @xmath148 .", "[ def : s2inv ]    it is worth noting that the structure @xmath149 is exactly the _ cd - graph _ ( cf . definition  [ def : comdag ] ) that represents the comtrace @xmath150 $ ] .", "this gives us some intuition on how koutny and kleijn constructed the cd - graph definition in @xcite .", "we also observe that @xmath151 is usually _ not _ a so - structure since @xmath152 and @xmath153 describe only basic `` local '' causality and weak causality invariants of the event occurrences of @xmath65 by considering pairwise serializable relationships of event occurrences .", "hence , @xmath152 and @xmath153 might not capture `` global '' invariants that can be inferred from  of definition  [ def : sos ] . to ensure all invariants", "are included , we need the following @xmath154-closure operator .    for every relational structure @xmath155 we define @xmath156 as + @xmath157 [ def : so - cl ]    intuitively @xmath154-closure is a generalization of transitive closure for relations to relational structures .", "the motivation is that for appropriate relations @xmath82 and @xmath83 ( see assertion ( 3 ) of proposition  [ prop : so - cl ] ) , the relational structure @xmath158 is a so - structure .", "the @xmath154-closure operator satisfies the following properties :    let @xmath155 be a relational structure .    1 .", "if @xmath83 is irreflexive then @xmath159 .", "2 .   @xmath160 .", "@xmath156 is a so - structure if and only if @xmath161 is irreflexive .", "4 .   if @xmath12 is a so - structure then @xmath162 . 5 .", "if @xmath12 be a so - structure and @xmath163 , then @xmath164 and @xmath165 is a so - structure .", "[ prop : so - cl ]    given a step sequence @xmath145 and its respective comtrace @xmath166\\in { \\mathbb{s}}_{\\theta}^*/\\!\\equiv$ ] , we define the relational structures @xmath167 as : + @xmath168.[def : s2sos ]    the relational structure @xmath167 is called the _ so - structure defined by the comtrace _ @xmath166 $ ] , where @xmath169 , @xmath170 and @xmath171 are used to denote the event occurrence set , causality relation and weak causality relation induced by the comtrace @xmath172 respectively . the following nontrivial theorem and its corollary justifies the name by showing that step sequences in a comtrace @xmath172 are exactly stratified extension of the so - structure @xmath173 , and that @xmath167 is uniquely defined for the comtrace @xmath172 regardless of the choice of @xmath174 .    for each @xmath175 ,", "the relational structure @xmath167 is a so - structure and @xmath176 .", "[ theo : com2sos ]    for all @xmath177 ,    1 .", "@xmath178 2 .", "@xmath179    [ cor : com2sos ]"], ["even though theorem  [ theo : com2sos ] shows that each comtrace can be represented uniquely by a labeled so - structure , it does not give us an explicit definition of how these labeled so - structures look like .", "in this section , we will give an exact definition of labeled so - structures that represent comtraces . to provide us with more intuition , we first recall how mazurkiewicz traces", "can be characterized as labeled posets .", "a _ trace concurrent alphabet _ is a pair @xmath180 , where @xmath181 is a symmetric irreflexive binary relation on the finite set @xmath105 .", "a _ trace congruence _", "@xmath182 can then be defined as the smallest equivalence relation such that for all sequences @xmath183 , if @xmath184 , then @xmath185 .", "the elements of @xmath186 are called _", "traces_.    traces can also be defined alternatively as posets whose elements are labeled with symbols of a concurrent alphabet @xmath180 satisfying certain conditions .    given a binary relation @xmath187 , the _ covering relation _ of @xmath11 is defined as @xmath188 .", "an alternative definition of mazurkiewicz trace is :    a trace over a concurrent alphabet @xmath180 is a finite labeled poset @xmath189 , where @xmath190 is a labeling function , such that for all @xmath191 ,    1 .", "@xmath192 , and 2 .", "@xmath193 .", "[ def : ltraces ]    a trace in this definition is only identified unique up to _ label - preserving isomorphism_. the first condition says that immediately causally related event occurrences must be labeled with dependent events .", "the second condition ensures that any two event occurrences with dependent labels must be causally related .", "the first condition is particularly important since two immediately causally related event occurrences will occur next to each other in at least one of its linear extensions .", "this is the key to relate definition  [ def : ltraces ] with quotient monoid definition of traces .", "thus , we would like to establish a similar relationship for comtraces .", "an immediate technical difficulty is that weak causality might be cyclic , so the notion of `` immediate weak causality '' does not make sense .", "however , we can still deal with cycles of a so - structure by taking advantage of the following simple fact : _ the weak causality relation is a strict preorder_.    let @xmath101 be a so - structure .", "we define the relation @xmath194 as + @xmath195    since @xmath95 is a strict preorder , it follows that @xmath196 is an equivalence relation .", "the relation @xmath196 will be called the _ @xmath95-cycle equivalence relation _ and an element of the quotient set @xmath197 will be called a _", "@xmath95-cycle equivalence class_. we then define the following binary relations @xmath198 and @xmath199 on the quotient set @xmath197 as @xmath200 \\widehat{{\\prec } } [ \\beta ] { \\stackrel{\\textit{\\scriptsize{df}}}{\\iff}\\ } ( [ \\alpha]\\times[\\beta])\\;\\cap { \\prec}\\not=\\emptyset \\hspace*{3mm}\\text { and } \\hspace*{3 mm } [ \\alpha ] \\widehat{{\\sqsubset } } [ \\beta ] { \\stackrel{\\textit{\\scriptsize{df}}}{\\iff}\\ } ( [ \\alpha]\\times[\\beta])\\;\\cap { \\sqsubset}\\not=\\emptyset \\label{qsos}\\end{aligned}\\ ] ]    using this quotient construction , every so - structure , whose weak causality relation might be cyclic , can be uniquely represented by an _", "quotient so - structure .", "the relational structure @xmath201 is a so - structure , the relation @xmath199 is a partial order , and for all @xmath202 ,    1 .", "@xmath203 \\widehat{{\\prec } } [ \\beta]$ ] 2 .", "@xmath204 \\widehat{{\\sqsubset } } [ \\beta ] \\vee ( \\alpha\\not=\\beta \\wedge [ \\alpha]=[\\beta])$ ]    follows from definition  [ def : sos ] .    using ( [ qsos ] ) and theorem  [ theo : szpstrat ] , it is not hard to prove the following simple yet useful properties of @xmath95-cycle equivalence classes .", "let @xmath205 be a so - structure .", "we use @xmath65 and @xmath206 to denote some step sequences over @xmath8 .", "then for all @xmath191 ,    1 .", "@xmath207=[\\beta ] \\iff   \\forall \\lhd \\in ext(s),\\ ; \\alpha \\simeq_{\\lhd } \\beta$ ] 2 .", "@xmath208v$ ] 3 .", "@xmath207 { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\ ; } } [ \\beta ] \\implies \\exists\\lhd \\in ext(s),\\;\\omega_{\\lhd } = u[\\alpha][\\beta]v$ ]    [ prop : covlsos ]    each @xmath95-cycle equivalence class is what juhs , lorenz and mauser called a", "_ synchronous step _", "they also used equivalence classes to capture synchronous steps but only for the special class of _ synchronous closed _ so - structures , where @xmath209 is an equivalence relation .", "we extend their ideas by using @xmath95-cycle equivalence classes to capture what we will call _ non - serializable sets _ in arbitrary so - structures .", "the name is justified in assertion ( 1 ) of proposition  [ prop : covlsos ] stating that two elements belong to the same non - serializable set of a so - structure @xmath12 iff they must be executed simultaneously in every stratified extension of @xmath12 .", "furthermore , we show in assertion ( 2 ) that all elements of a non - serializable set must occur together as a single step in at least one stratified extension of @xmath12 .", "assertion ( 3 ) gives a sufficient condition for two non - serializable sets to occur as consecutive steps in at least one stratified extension of @xmath12 .", "before we proceed to define comtrace using labeled so - structure , we need to define _ label - preserving isomorphisms _ for labeled so - structures more formally .", "a tuple @xmath210 is a _ labeled relational structure _", "iff @xmath211 is a relational structure and @xmath212 is a function with domain @xmath3 .", "if @xmath211 is a so - structure , then @xmath85 is a _ labeled so - structure_.    given two labeled relational structures @xmath213 and @xmath214 , we write @xmath215 to denote that @xmath216 and @xmath217 are _ label - preserving isomorphic _ ( _ lp - isomorphic _ ) . in other words , there is a bijection @xmath218 such that for all @xmath219 ,    1 .", "@xmath220 2 .", "@xmath221 3 .", "@xmath222    such function @xmath223 is called a _", "label - preserving isomorphism _", "( _ lp - isomorphism _ ) .", "note that all notations , definitions and results for so - structures are applicable to labeled so - structures .", "we also write @xmath224 $ ] or @xmath225 } $ ] to denote the lp - isomorphic class of a labeled relational structure @xmath226 .", "we will not distinguish an lp - isomorphic class @xmath227 } $ ] with a single labeled relational structure @xmath85 when it does not cause ambiguity .", "we are now ready to give an alternative definition for comtraces .", "to avoid confusion with the comtrace notion by janicki and koutny in @xcite , we will use the term _ lsos - comtrace _ to denote a comtrace defined using our definition .    given a comtrace alphabet @xmath114 , a _", "lsos - comtrace _ over @xmath128 is ( an lp - isomorphic class of ) a finite labeled so - structure @xmath228 } $ ] such that @xmath190 and for all @xmath191 ,    1 .", "@xmath207 ( { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\cap \\hat{{\\prec } } ) [ \\beta ] \\implies \\lambda([\\alpha])\\times \\lambda([\\beta ] ) \\nsubseteq ser$ ] 2 .", "@xmath207 ( { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\setminus \\hat{{\\prec } } ) [ \\beta ] \\implies \\lambda([\\beta])\\times \\lambda([\\alpha ] ) \\nsubseteq ser$ ] 3 .", "@xmath229)},\\ ; a\\cup b = [ \\alpha ] \\implies \\lambda(a)\\times \\lambda(b ) \\not \\subseteq ser$ ] 4 .", "@xmath230 5 .", "@xmath231    we write @xmath232 to denote the class of all lsos - comtraces over @xmath128 .", "[ def : lcomtrace ]    let @xmath136 , @xmath233 and @xmath234 .", "then we have @xmath235 .", "the lp - isomorphic class of the labeled so - structure @xmath236 depicted in figure  [ fig : f1 ] ( the dotted edges denote @xmath95 relation and the solid edges denote both @xmath93 and @xmath95 relations ) is a lsos - comtrace .", "the graph in figure  [ fig : f2 ] represents the labeled quotient so - structure @xmath237 of @xmath85 , where we define @xmath238 .", "@xmath239[f-]{a } \\ar@{-->}[dr]\\ar@/^/[rr ] \\ar@/_1.5pc/ [ ddrr ] &      & * + [ o][f-]{c}\\ar@/_/ @{-->}[dd]\\\\           & * + [ o][f-]{c } \\ar[ur]\\ar[dr ]        & \\\\ * + [ o][f-]{b } \\ar[ur]\\ar@/_/[rr]\\ar@/^1.5pc/ [ uurr ]   &    & * + [ o][f-]{b}\\ar@/_/ @{-->}[uu ] } $ ]    @xmath240{a } \\ar@{-->}[dr]\\ar@/^1pc/[drr ] &     & \\\\           & * + [ f-]{c } \\ar[r ]       & * + [ f-]{b , c}\\\\ * + [ f-]{b } \\ar[ur]\\ar@/_1pc/[urr ]   & & } $ ]    the lsos - comtrace @xmath224 $ ] actually corresponds to the comtrace @xmath241 $ ] , and we will show this relationship formally in section  [ sec : representation ] . [ ex : comtrace2 ]    definition  [ def : lcomtrace ] can be extended to define _ infinite comtrace _ as follows . instead of asking @xmath3 to be finite , we require a labeled so - structure to be _ initially finite _", "@xcite ) , i.e. , @xmath242 is finite for all @xmath243 .", "the initially - finiteness not only gives us a sensible interpretation that every event only causually depends on finitely many events , but also guarantees that the covering relations of @xmath244 and @xmath245 are well - defined .", "since each lsos - comtrace is defined as a class of lp - isomorphic labeled so - structures , dealing with lsos - comtrace might seem tricky .", "the _ no autoconcurrency _ property , i.e. , the relation @xmath5 is irreflexive , gives us a _ canonical _ way to enumerate the events of a lsos - comtrace very similar to how the events of a comtrace are enumerated .    given a step sequence @xmath246 and any function @xmath223 defined on @xmath247 , we define @xmath248 , i.e. , the step sequence derived from @xmath135 by applying the function @xmath223 successively on each @xmath249 . note that @xmath250 denotes the _ image _ of @xmath251 under @xmath223 .", "given a lsos - comtrace @xmath252 } $ ] over a comtrace alphabet @xmath114 , a stratified order @xmath253 can be seen as a step sequence @xmath254 .    1 .", "for every @xmath255 ( @xmath256 ) , @xmath257 2 .", "@xmath258 .", "[ prop : validss ]    proposition  [ prop : validss ] ensures that @xmath259 is a valid step sequence over @xmath128 .", "recall that @xmath260 denotes the enumerated step sequence of @xmath65 and @xmath261 denotes the set of event occurrences .", "bijection _ @xmath262 as + @xmath263    by proposition  [ prop : validss ] , the function @xmath264 is well - defined .", "moreover , we can show that @xmath264 is uniquely determined by @xmath85 regardless of the choice of @xmath253 .    given @xmath265 , let @xmath266 and @xmath267", ". then @xmath268 .", "[ prop : uniquexi ]    henceforth , we will ignore subscripts and reserve the notation @xmath269 to denote the kind of mappings as defined above .", "we then define the _ enumerated so - structure _ of @xmath85 to be the labeled so - structure @xmath270 , where @xmath271 for @xmath259 and @xmath253 ; and the relations @xmath272 are defined as + @xmath273    clearly , the enumerated so - structure @xmath274 can be uniquely determined from @xmath85 using the preceding definition . from our construction , we can easily show the following important relationships :    1 .", "@xmath275 and @xmath85 are lp - isomorphic under the mapping @xmath269 .", "2 .   the labeled so - structures @xmath276 and @xmath277 are lp - isomorphic under the mapping @xmath269 and @xmath278 .", "[ prop : isoext ]    in other words , the mapping @xmath279 plays the role of both the lp - isomorphism from @xmath275 to @xmath85 and the lp - isomorphism from the stratified extension @xmath280 of @xmath275 to the stratified extension @xmath96 of @xmath85 .", "these relationships can be best captured using the commutative diagram on the right .", "@xmath281_{\\displaystyle \\xi } \\ar@{^{(}->}[d]_{\\displaystyle id_{\\sigma } }        & ( x,{\\prec},{\\sqsubset},\\lambda ) \\ar@{^{(}->}[d]^{\\displaystyle id_{x } }     \\\\    ( \\sigma,\\lhd_{u},\\lhd_{u}^{\\frown},\\ell ) \\ar@{->}[r]^{\\displaystyle \\xi } &   ( x,\\lhd,\\lhd^{\\frown},\\lambda)}$ ]    we can even observe further that two lsos - comtraces are identical if and only if they define the same enumerated so - structure . henceforth , we will call an enumerated so - structure defined by a lsos - comtrace @xmath85 _ the canonical representation _ of @xmath85 .", "recently , inspired by the dependency graph notion for mazurkiewicz traces ( cf .", "* chapter 2 ) ) , kleijn and koutny claimed without proof that their _ combined dependency graph _ notion is another alternative way to define comtraces @xcite . in section  [ sec : representation ] , we will give a detailed proof of their claim .", "given an comtrace alphabet @xmath282 , a _ combined dependency graph _", "( _ cd - graph _ ) over @xmath128 is ( a lp - isomorphic class of ) a finite labeled relational structure @xmath283 } $ ] such that @xmath190 , the relations @xmath284 are irreflexive , @xmath285 is a so - structure , and for all @xmath286 ,    @xmath287    @xmath288    @xmath289    @xmath290 + we will write @xmath291 to denote the class of all cd - graphs over @xmath128 . [ def : comdag ]    cd - graphs can be seen as reduced graph - theoretic representations for lsos - comtraces , where some arcs that can be recovered using @xmath154-closure are omitted .", "it is interesting to observe that the non - serializable sets of a cd - graph are exactly the _ strongly connected components _ of the directed graph @xmath292 and can easily be found in time @xmath293 using any standard algorithm ( cf .", "* section 22.5 ) ) .", "+    cd - graphs were called _ dependence comdags _ in @xcite . but this name could be misleading since the directed graph @xmath292 is not necessarily acyclic .", "for example , the graph on the right is the cd - graph that corresponds to the lsos - comtrace from figure  [ fig : f1 ] , but it is not acyclic .", "( here , we use the dotted edges to denote @xmath294 and the solid edges to denote _ only _ @xmath295 . ) thus , we use the name `` combined dependency graph '' instead .", "@xmath296[f-]{a } \\ar@{-->}[dr ] \\ar@/_1.5pc/ [ ddrr ] &     & * + [ o][f-]{c}\\ar@/_/ @{-->}[dd]\\\\           & * + [ o][f-]{c } \\ar[ur]\\ar[dr ]        & \\\\ * + [ o][f-]{b } \\ar[ur]\\ar@/_/[rr]\\ar@/^1.5pc/ [ uurr ]   &    & * + [ o][f-]{b}\\ar@/_/ @{-->}[uu ] } $ ]"], ["this section contains the main technical contribution of this paper by showing that for a given comtrace alphabet @xmath128 , @xmath297 , @xmath232 and @xmath291 are three equivalent ways of talking about the same class of objects .", "we will next prove the first representation theorem which establishes the representation mappings between @xmath297 and @xmath232 .", "let @xmath298 and @xmath299 be stratified order structures such that @xmath300", ". then @xmath301 .", "[ prop : stratsubset ]    follows from theorem  [ theo : szpstrat ] .    for the next two lemmata , we let @xmath85 be a lsos - comtrace over a comtrace alphabet @xmath114 .", "let @xmath302 be the canonical representation of @xmath85 .", "let @xmath303 and @xmath304 . since @xmath65 is a valid step sequence in @xmath31 ( by proposition  [ prop : validss ] ) , we can construct @xmath305 } = ( \\sigma_{u},{\\prec}_{[u]},{\\sqsubset}_{[u]})$ ] from definition  [ def : s2sos ] .", "our goal is to show that the stratified order @xmath305}$ ] defined by the comtrace @xmath150 $ ] is exactly @xmath306 .", "@xmath305 } \\subseteq ( \\sigma_{u},{\\prec}_{0},{\\sqsubset}_{0})$ ] .", "[ lem : l1 ]    by proposition  [ prop : so - cl ] , to show @xmath305 } = ( \\sigma_{u},{\\prec}_{u},{\\sqsubset}_{u})^{\\lozenge } \\subseteq ( \\sigma_{u},{\\prec}_{0},{\\sqsubset}_{0})$ ] , it suffices to show that @xmath307 .", "since @xmath274 is the canonical representation of @xmath85 , it is important to observe that @xmath308 .", "( @xmath309 ) : assume @xmath310 . then from definition  [ def : s2inv ] , @xmath311 .", "since @xmath312 , it follows from definition  [ def : lcomtrace ] that @xmath313 or @xmath314 .", "suppose for a contradiction that @xmath314 , then by theorem  [ theo : szpstrat ] , @xmath315 .", "but since we assume that @xmath303 , it follows that @xmath316 and @xmath317 , a contradiction .", "hence , we have shown @xmath313 .", "( @xmath318 ) : can be shown in a similar way .    @xmath305 } \\supseteq ( \\sigma_{u},{\\prec}_{0},{\\sqsubset}_{0})$ ] . [", "lem : l2 ]    in this proof , we will include subscripts for equivalence classes to avoid confusing the elements from quotient set @xmath319 with the elements from the quotient comtrace monoid @xmath320 .", "in other words , we write @xmath207_{\\equiv_{{\\sqsubset}_{0}}}$ ] to denote an element of the quotient set @xmath319 , and write @xmath150_{\\theta}$ ] to denote the comtrace generated by @xmath65 .", "let @xmath321 .", "to show @xmath305 } \\supseteq s'$ ] , by proposition  [ prop : stratsubset ] , it suffices to show @xmath322 } ) \\subseteq ext(s')$ ] . from theorem  [ theo :", "com2sos ] , we know that @xmath322_{\\theta } } ) = { \\{\\lhd_w\\mid w\\in [ u]_{\\theta}\\}}$ ] .", "thus we only need to show that for all @xmath323_{\\theta}$ ] , @xmath324 .", "we observe that from @xmath65 , by definition  [ def : commonoid ] , we can generate all the step sequences in the comtrace @xmath150_{\\theta}$ ] in stages using the following recursive definition : @xmath325    since the set @xmath150_{\\theta}$ ] is finite , @xmath150_{\\theta}=d^n(u)$ ] for some stage @xmath326 . for the rest of the proof , we will prove by induction on @xmath327 that for all @xmath328 , if @xmath329 then @xmath330 . + * base case : * when @xmath331 , @xmath332 . since @xmath333", ", it follows from proposition  [ prop : isoext ] that @xmath334 .", "+ * inductive case : * when @xmath335 , let @xmath40 be an element of @xmath336 .", "then either @xmath337 or @xmath338 . for the former case , by inductive hypothesis , @xmath339 . for the latter case", ", there must be some element @xmath340 such that @xmath341 or @xmath342 . by induction hypothesis ,", "we already known @xmath343 .", "we want to show that @xmath324 .", "there are two cases to consider : + _ * case ( i ) : * _ + when @xmath341 , by definition  [ def : commonoid ] , there are some @xmath344 and steps @xmath345 such that @xmath346 and @xmath347 where @xmath119 , @xmath120 , @xmath121 satisfy @xmath348 and @xmath349 and @xmath129 .", "let @xmath350 and @xmath351 be enumerated step sequences of @xmath206 and @xmath40 respectively .", "suppose for a contradiction that @xmath352 .", "by definition  [ def : extsos ] , there are @xmath353 and @xmath354 such that @xmath355 .", "we now consider the quotient set @xmath356 . by proposition  [ prop : covlsos ]", "( 1 ) , @xmath357 . since @xmath355", ", it follows that @xmath207_{\\equiv_{{\\sqsubset}_{0}}}\\hat{{\\sqsubset}}_{0 } [ \\beta]_{\\equiv_{{\\sqsubset}_{0}}}$ ] .", "thus , from the fact that @xmath358 is partial order , there must exists a chain @xmath200_{\\equiv_{{\\sqsubset}_{0}}}=\\ ; [ \\gamma_1]_{\\equiv_{{\\sqsubset}_{0 } } } \\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\ ; [ \\gamma_2]_{\\equiv_{{\\sqsubset}_{0}}}\\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\ ; \\ldots\\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\;[\\gamma_k]_{\\equiv_{{\\sqsubset}_{0}}}\\;= [ \\beta]_{\\equiv_{{\\sqsubset}_{0 } } }   \\label{eq : chain1}\\end{aligned}\\ ] ]    then by theorem  [ theo : szpstrat ] and the fact that @xmath343 , we know that @xmath359 for all @xmath255 . in other words , since the chain implies that every @xmath360 must always occur between @xmath53 and @xmath70 in all stratified extensions of @xmath361 and @xmath362 , we also have @xmath359 . hence , by proposition  [ prop : covlsos ] ( 1 ) , we have @xmath363_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{a}}$ ] for all @xmath255 , @xmath256 . also from of definition  [ def : lcomtrace ] and that @xmath364 , we know that for each @xmath360 , either @xmath363_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{b}}$ ] or @xmath363_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{c}}$ ]", ". now we note that the first element on the chain @xmath365_{\\equiv_{{\\sqsubset}_{0}}}=[\\alpha]_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{c}}$ ] and the last element on the chain @xmath366_{\\equiv_{{\\sqsubset}_{0}}}=[\\beta]_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{b}}$ ] .", "thus , there exist two consecutive elements @xmath363_{\\equiv_{{\\sqsubset}_{0}}}$ ] and @xmath367_{\\equiv_{{\\sqsubset}_{0}}}$ ] on the chain such that @xmath363_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{c}}$ ] and @xmath367_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{b}}$ ] .", "but then it follows that    * @xmath367_{\\equiv_{{\\sqsubset}_{0}}}\\times [ \\gamma_i]_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq ser$ ] and @xmath363_{\\equiv_{{\\sqsubset}_{0 } } } { { \\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\ ; } } [ \\gamma_{i+1}]_{\\equiv_{{\\sqsubset}_{0}}}$ ] * @xmath368_{\\equiv_{{\\sqsubset}_{0 } } } \\hat{{\\prec}}_{0 } [ \\gamma_{i+1}]_{\\equiv_{{\\sqsubset}_{0}}}\\bigr)$ ] since @xmath343 and @xmath369    these contradict of definition  [ def : lcomtrace ] since @xmath274 is a lsos - comtrace .", "+ _ * case ( ii ) : * _ + when @xmath342 , by definition  [ def : commonoid ] , there are some @xmath344 and steps @xmath345 such that @xmath370 and @xmath371 where @xmath119 , @xmath120 , @xmath121 satisfy @xmath348 and @xmath349 and @xmath129 .", "let @xmath372 and @xmath373 be enumerated step sequences of @xmath206 and @xmath40 respectively .", "suppose for a contradiction that @xmath352 . by definition  [ def : extsos ] , there are @xmath374 and @xmath375 such that @xmath313 . by proposition  [ prop : covlsos ]", "( 1 ) , @xmath357 . thus , using a dual argument to the proof of case ( i ) , we can build a chain @xmath200_{\\equiv_{{\\sqsubset}_{0}}}=\\ ; [ \\gamma_1]_{\\equiv_{{\\sqsubset}_{0 } } } \\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\ ; [ \\gamma_2]_{\\equiv_{{\\sqsubset}_{0}}}\\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\ ; \\ldots\\;{{\\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\;}}\\;[\\gamma_k]_{\\equiv_{{\\sqsubset}_{0}}}\\;= [ \\beta]_{\\equiv_{{\\sqsubset}_{0 } } } \\end{aligned}\\ ] ] we then argue that there are two consecutive elements on the chain such that @xmath363_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{b}}$ ] and @xmath367_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq { \\overline{c}}$ ] , which implies    1 .", "@xmath376_{\\equiv_{{\\sqsubset}_{0}}}\\times [ \\gamma_{i+1}]_{\\equiv_{{\\sqsubset}_{0}}}\\subseteq ser$ ] and @xmath363_{\\equiv_{{\\sqsubset}_{0 } } } { { \\hat{{\\sqsubset}}_{0}}^{\\mathsf{cov}\\ ; } } [ \\gamma_{i+1}]_{\\equiv_{{\\sqsubset}_{0}}}$ ] 2 .", "@xmath363_{\\equiv_{{\\sqsubset}_{0 } } } \\hat{{\\prec}}_{0 } [ \\gamma_{i+1}]_{\\equiv_{{\\sqsubset}_{0}}}$ ] since @xmath343 and @xmath377    these contradict of definition  [ def : lcomtrace ] .", "we also need to show that the labeled so - structure defined from each comtrace is indeed a lsos - comtrace . in other words ,", "we need to show the following lemma .", "let @xmath114 be a comtrace alphabet . given a step sequence @xmath378 , the lp - isomorphic class @xmath379},{\\prec}_{[u]},{\\sqsubset}_{[u]},\\ell \\right ] } $ ]", "is a lsos - comtrace over @xmath128 .", "[ lem : l3 ]    the proof of this lemma is straightforward by checking that @xmath379},{\\prec}_{[u]},{\\sqsubset}_{[u]},\\ell \\right ] } $ ] satisfies all conditions .", "let @xmath128 be a comtrace alphabet .    1 .", "the mapping @xmath380 is defined as + @xmath381 } , $ ] + where the function @xmath382 is defined in section  [ sec : steps ] and @xmath383 is the so - structure defined by the comtrace @xmath172 from definition  [ def : s2sos ] .", "the mapping @xmath384 is defined as + @xmath385    [ def : repmaps ]    intuitively , the mapping @xmath386 is used to convert a comtrace to lsos - comtrace while the mapping @xmath387 is used to transform a lsos - comtrace into a comtrace .", "the fact that @xmath386 and @xmath387 are valid representation mappings for @xmath388 and @xmath232 will be shown in the following theorem .", "let @xmath128 be a comtrace alphabet .    1 .   for every @xmath389 , @xmath390 .", "2 .   for every @xmath391 , @xmath392 .", "[ theo : rep ]    * 1 . *", "the fact that @xmath393 follows from lemma  [ lem : l3 ] .", "now for a given @xmath389 , we have @xmath394 .", "thus , it follows that @xmath395    * 2 . * assume @xmath270 is the canonical representation of @xmath85 .", "observe that since @xmath396 , we have @xmath397    let @xmath398 .", "we will next show that @xmath399 and @xmath400 } $ ] .", "fix an arbitrary @xmath401 , from lemmas [ lem : l1 ] and [ lem : l2 ] , @xmath305 } = ( \\sigma,{\\prec}_{0},{\\sqsubset}_{0})$ ] . from theorem  [ theo : com2sos ] ,", "@xmath402})\\bigr\\ } } = [ u]$ ] .", "and the rest follows .", "the theorem says that the mappings @xmath386 and @xmath387 are inverses of each other and hence are both _", "bijective_.      using theorem  [ theo : rep ] , we are going to show that the _ combined dependency graph _ notion proposed in @xcite is another correct alternative definition for comtraces .", "first we need to define several representation mappings that are needed for our proofs .", "let @xmath128 be a comtrace alphabet .    1 .", "the mapping @xmath403 is defined as + @xmath404 + where @xmath65 is any step sequence in @xmath172 and @xmath405 and @xmath406 are defined as in definition  [ def : s2inv ] .", "the mapping @xmath407 is defined as @xmath408 .", "the mapping @xmath409 is defined as + @xmath410 .", "[ def : drepmaps ]    before proceeding futher , we want to make sure that :    1 .", "@xmath407 is a well - defined function . 2 .", "@xmath403 is a well - defined function .", "given a cd - graph @xmath411 } \\in { \\mathsf{cdg}}(\\theta)$ ] , let @xmath412 } = d_1^{\\lozenge}$ ] . we know that @xmath85 is uniquely defined , since by definition  [ def : comdag ] , @xmath413 is a so - structure , and so - structures are fixed points of @xmath154-closure ( by proposition  [ prop : so - cl ] ( 4 ) ) .", "we will next show that @xmath85 is a lsos - comtrace by verifying the conditions  of definition  [ def : lcomtrace ] .", "conditions and are exactly and .", ": suppose for contradiction that there exist two distinct non - serializable sets @xmath207,[\\beta]\\subset x$ ] such that @xmath207({{\\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\cap \\hat{{\\prec}})[\\beta]$ ] and @xmath414)\\times \\lambda([\\beta ] ) \\subseteq ser$ ] .", "clearly , this implies that @xmath415 , and thus by the @xmath154-closure definition , @xmath70 is reachable from @xmath53 on the directed graph @xmath416 , where @xmath417 .", "now we consider a shortest path @xmath418 + @xmath419 + on @xmath420 that connects @xmath53 to @xmath70 .", "we will prove by induction on @xmath421 that there exist two consecutive @xmath422 and @xmath423 on @xmath418 such that @xmath424 $ ] and @xmath425 $ ] and @xmath426 , which contradicts with @xmath414)\\times \\lambda([\\beta ] ) \\subseteq ser$ ] .", "+ * base case : * when @xmath427 , then @xmath428 . since @xmath207({{\\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\cap \\hat{{\\prec}})[\\beta]$ ] , we have @xmath429 , which by implies @xmath430 . +", "* inductive case : * when @xmath431 , we consider @xmath432 and @xmath433 .", "if @xmath434 $ ] and @xmath435 $ ] , then by @xmath207({{\\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\cap \\hat{{\\prec}})[\\beta]$ ] , we have @xmath436 , which immediately yields @xmath437 . otherwise , we have @xmath438\\cup [ \\beta]$ ] or @xmath439 $ ] .", "for the first case , we get @xmath207\\hat{{\\sqsubset}}[\\delta_2]\\hat{{\\sqsubset}}[\\beta]$ ] , which contradicts that @xmath207{{\\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}[\\beta]$ ] . for the latter case", ", we can apply induction hypothesis on the path @xmath440 .    and can also be shown similarly using `` shortest path '' argument as above .", "these proofs are easier since we only need to consider paths with edges in @xmath294 .", "* 2 . * by the proof of (", "* lemma 4.7 ) , for any two step sequences @xmath50 and @xmath65 in @xmath441 , we have @xmath442 iff @xmath443 ) = { \\mathsf{ct2dep}}([t])$ ] .", "thus the mapping @xmath444 is well - defined .", "the mapping @xmath407 is injective .", "[ lem : dlinj ]    assume that @xmath445 , such that @xmath446 $ ] . since @xmath154-closure operator does not change the labeling function", ", we can assume that @xmath447 $ ] and @xmath448 .", "we will next show that @xmath449 .", "( @xmath450 ) : let @xmath286 such that @xmath451 .", "suppose for a contradiction that @xmath452 . since @xmath451 , by , @xmath453 .", "thus , by , @xmath454 .", "but since @xmath448 , it follows that @xmath455 ( by proposition  [ prop : so - cl ] ) .", "thus , @xmath456 and @xmath457 , a contradiction .", "( @xmath458 ) : can be proved similarly .    by reversing the role of @xmath459 and @xmath460 , we have @xmath461 .", "thus , we conclude @xmath462 .", "we are now ready to show the following representation theorem which ensures that @xmath463 and @xmath464 are valid representation mappings for @xmath232 and @xmath291 .", "let @xmath128 be a comtrace alphabet .    1 .   for every @xmath465 , @xmath466 .", "2 .   for every @xmath391 , @xmath467 .", "[ theo : deprep ]    * 1 . *", "let @xmath468 and let @xmath469 .", "suppose for a contradiction that @xmath470 and @xmath471 . from how @xmath386 is defined , @xmath472 .", "thus , it follows that @xmath473 . but this contradicts the injectivity of @xmath464 from lemma  [ lem : dlinj ] .", "let @xmath391 and let @xmath474 .", "suppose for a contradiction that @xmath475 and @xmath476 . since @xmath477 , if we let @xmath478 , then @xmath479 .", "thus , we have shown that @xmath480 and @xmath481 , contradicting theorem  [ theo : rep ] ( 2 ) .", "this theorem shows that lsos - comtraces and cd - graphs are equivalent representations for comtraces .", "the main advantage of cd - graph definition is its simplicity while the lsos - comtrace definition is stronger and more convenient to prove properties about labeled so - structures that represent comtraces .", "we do not need to prove another representation theorem for cd - graphs and comtraces since their representation mappings are simply the composition of the representation mappings from theorems [ theo : rep ] and [ theo : deprep ] ."], ["recall for a comtrace monoid @xmath127)$ ] , the comtrace operator @xmath125 is defined as @xmath126\\circledast[t ] = [ r\\ast t]$ ] .", "we will construct analogous composition operators for lsos - comtraces and cd - graphs .", "we will then show that lsos - comtraces ( cd - graphs ) over a comtrace alphabet @xmath128 together with its composition operator form a monoid isomorphic to the comtrace monoid @xmath127)$ ] .", "given two sets @xmath482 and @xmath483 , we write @xmath484 to denote the _ disjoint union _ of @xmath482 and @xmath483 .", "such disjoint union can be easily obtained by renaming the elements in @xmath482 and @xmath483 so that @xmath485 .", "we define the lsos - comtrace composition operator as follows .", "let @xmath486 and @xmath487 be lsos - comtraces over an alphabet @xmath114 , where @xmath488 } $ ] .", "the _ composition _ @xmath489 of @xmath486 and @xmath487 is defined as ( a lp - isomorphic class of ) a labeled so - structure @xmath490 } $ ] such that @xmath491 , @xmath492 , and @xmath493 , where + @xmath494 + @xmath495    observe that the operator is well - defined since we can easily check that :    for every @xmath496 , @xmath497 .", "we will next show that this composition operator @xmath498 properly corresponds to the operator @xmath125 of the comtrace monoid over @xmath128 .", "let @xmath128 be a comtrace alphabet", ". then    1 .   for every @xmath499 , @xmath500 2 .   for every @xmath501 , @xmath502    [ prop : hom1 ]    * 1 . *", "assume @xmath503 } $ ] , @xmath504 } $ ] and @xmath505 } $ ] .", "we can pick @xmath506 and @xmath507 . then observe that a stratified order @xmath75 satisfying @xmath508 is an extension of @xmath509 .", "thus , by theorem  [ theo : rep ] , we have @xmath510\\circledast [ { \\mathsf{map}}(\\lambda_2,\\lhd_2 ) ] =   [ { \\mathsf{map}}(\\lambda,\\lhd ) ] = { \\mathsf{lct2ct}}(q)$ ] as desired .", "* 2 . * without loss of generality", ", we can assume that @xmath511 $ ] , @xmath512 $ ] and @xmath513 = \\mathbf{r}\\circledast \\mathbf{t}$ ] , where @xmath514 .", "by appropriate reindexing , we can also assume that @xmath515 .", "under these assumptions , let @xmath516 } $ ] , @xmath517 } $ ] and @xmath518 } $ ] , where @xmath519 is simply the standard labeling functions .", "it will now suffice to show that @xmath520 .", "( @xmath521 ) : let @xmath522 . by definitions [ def : s2inv ] and [ def : s2sos ]", ", we have + @xmath523 + @xmath524 + thus , by proposition  [ prop : so - cl ] ( 5 ) , we have @xmath525 as desired . furthermore , by proposition  [ prop : so - cl ] ( 5 ) , @xmath526 is a so - structure .", "( @xmath527 ) : by definitions [ def : s2inv ] and [ def : s2sos ] , we have @xmath528 and @xmath529 . since we already know", "@xmath526 is a so - structure , it follows from proposition  [ prop : so - cl ] ( 5 ) that + @xmath530    let @xmath531 denote the lp - isomorphic class @xmath532 $ ] .", "then we observe that @xmath533 ) = \\mathbb{i}$ ] and @xmath534 $ ] . by proposition  [ prop : hom1 ] and theorem  [ theo :", "rep ] , the structure @xmath535 is isomorphic to the monoid @xmath536)$ ] under the isomorphisms @xmath380 and @xmath384 .", "thus , the triple @xmath537 is also a monoid .", "we can summarize these facts in the following theorem :    the mappings @xmath386 and @xmath387 are monoid isomorphisms between two monoids @xmath536)$ ] and @xmath535 .", "similarly , we can also define a composition operator for cd - graphs .", "let @xmath538 and @xmath539 be cd - graphs over an alphabet @xmath114 , where @xmath540 } $ ] .", "the _ composition _ @xmath541 of @xmath538 and @xmath539 is defined as ( a lp - isomorphic class of ) a labeled so - structure @xmath542 } $ ] such that @xmath491 , @xmath492 , and + @xmath543 + @xmath544    from this definition , it is straightforward to show the following propositions , which we will state without proofs .    for every @xmath545 , @xmath546 .", "let @xmath128 be a comtrace alphabet", ". then    1 .   for every @xmath499 , @xmath547 2 .   for every @xmath548 , @xmath549    [ prop : hom2 ]    putting the two preceding propositions and theorem  [ theo : deprep ] together ,", "we conclude :    the mappings @xmath463 and @xmath464 are monoid isomorphisms between two monoids @xmath535 and @xmath550 ."], ["the simple yet useful construction we used extensively in this paper is to build a quotient so - structure modulo the @xmath95-cycle equivalence relation .", "intuitively , each @xmath95-cycle equivalence class consists of all the events that must be executed simultaneously with one another and hence can be seen as a single `` composite event '' .", "the resulting quotient so - structure is technically easier to handle since both relations of the quotient so - structure are acyclic . from this construction", ", we were able to give a labeled so - structure definition for comtraces similar to the labeled poset definition for traces .", "this quotient construction also explicitly reveals the following connection : a step on a step sequence @xmath135 is not serializable with respect to the relation @xmath5 of a comtrace alphabet if and only if it corresponds to a @xmath95-cycle equivalence class of the lsos - comtrace representing the comtrace @xmath134 $ ] ( cf .", "proposition  [ prop : covlsos ] ) .", "we have also formally shown that the quotient monoid of comtraces , the monoid of lsos - comtraces and the monoid of cd - graphs _ over the same comtrace alphabet _ are indeed isomorphic by establishing monoid isomorphisms between them .", "these three models are formal linguistic , order - theoretic , and graph - theoretic respectively , which allows us to apply a variety of tools and techniques .", "an immediate future task is to develop a framework similar to the one in this paper for _ generalized comtraces _ , proposed and developed in @xcite .", "generalized comtraces extend comtraces with the ability to model events that can be executed _ earlier than or later than but never simultaneously_. another direction is to define and analyze infinite comtraces ( and generalized comtraces ) in a spirit similar to the works on infinite traces , e.g. , @xcite .", "it is also promising to use infinite lsos - comtraces and cd - graphs to develop logics for comtraces similarly to what have been done for traces ( cf .", "@xcite ) .", "i am grateful to prof .", "ryszard janicki for introducing me comtrace theory .", "i also thank the mathematics institute of warsaw university and the theoretical computer science group of jagiellonian university for their supports during my visits .", "it was during these visits that the ideas from this paper emerge .", "this work is financially supported by the ontario graduate scholarship and the natural sciences and engineering research council of canada .", "the anonymous referees are thanked for their valuable comments that help improving the readability of this paper .", "19 e. clarke , o. grumberg and d. peled , _ model checking _ , mit press , cambridge , 1999 .", "t. h. cormen , c. e. leiserson and r. l. rivest , _ introduction to algorithms _ , second edition , mit press , 2001 .", "b. a. davey and h. a. priestley , , cambridge university press 2002 .", "v. diekert , on the concatenation of infinite traces , proc . of stacs ,", "_ lncs _ 480 ( 1991 ) , 105117 .", "v. diekert and g. rozenberg ( eds . ) , .", "world scientific 1995 .", "v. diekert and y mtivier , , _ handbook of formal languages , vol .", "3 : beyond words _ , pp 457 - 533 , springer 1997 .", "v. diekert , m. horsch , m. kufleitner , on first - order fragments for mazurkiewicz traces , _ fundam .", "_ 80(1 - 3 ) : 1 - 29 , 2007 .", "j. esparza and k. heljanko , _ unfoldings  a partial - order approach to model checking _ , springer 2008 .", "a. farzan and p. madhusudan , causal dataflow analysis for concurrent programs , proc . of cav ,", "_ lncs _ 4144 ( 2006 ) : 315328 .", "a. farzan and p. madhusudan , causal atomicity , proc . of tacas 2007 ,", "_ lncs _ 4424 ( 2007 ) , 102116 .", "p. c. fishburn , _ interval orders and interval graphs _", ", j. wiley 1985 , new york .", "h. gaifman and v. pratt , partial order models of concurrency and the computation of function , _ proc . of lics87", "_ , pp . 7285 .", "p. gastin , infinite traces , proc . of semantics of systems of concurrent processes , _", "lncs _ 469 ( 1990 ) , 277308 .", "t. gazagnaire , b. genest , l. hlout , p. s. thiagarajan , s. yang , causal message sequence charts , _ theor .", "410(41 ) : 40944110 , 2009 .", "r. janicki , relational structures model of concurrency .", ", 45(4 ) : 279320 , 2008 .", "r. janicki and m. koutny , invariants and paradigms of concurrency theory , proc . of _ parle _ 91 , _ lncs _ 506 , springer 1991 , pp . 5974 .", "r. janicki and m. koutny , structure of concurrency , , 112(1 ) : 552 , 1993 .", "r. janicki and m. koutny , semantics of inhibitor nets , _ information and computation _ , 123(1 ) : 116 , 1995 .", "r. janicki and m. koutny , fundamentals of modelling concurrency using discrete relational structures , _ acta informatica _ ,", "34 : 367388 , 1997 .", "r. janicki and m. koutny , on causality semantics of nets with priorities , _ fundamenta informaticae _ 34 : 222255 , 1999 .", "r. janicki and d. t. m. l , modelling concurrency with quotient monoids , proc of petri nets 2008 , _ lncs _ 5062 , springer 2008 , pp .", "251269 .", "r. janicki and d. t. m. l , modelling concurrency with comtraces and generalized comtraces , submitted in 2009 .", "available at : http://arxiv.org/abs/0907.1722    g. juhs , r. lorenz , s. mauser , causal semantics of algebraic petri nets distinguishing concurrency and synchronicity , _ fundamenta informatica _", "86(3 ) : 255 - 298 , 2008 .", "g. juhs , r. lorenz , s. mauser , synchronous + concurrent + sequential = earlier than + not later than , proc . of acsd06 , turku ,", "finland 2006 , pp .", "261 - 272 , ieee press .", "h. c. m. kleijn and m. koutny , process semantics of general inhibitor nets , _ information and computation _ , 190:1869 , 2004 .", "j. kleijn and m. koutny , formal languages and concurrent behaviour , _ studies in computational intelligence _ , 113:125 - 182 , 2008 .", "d. t. m. l , studies in comtrace monoids , master thesis , dept . of computing and software , mcmaster university , canada , august 2008 .", "a.  mazurkiewicz , concurrent program schemes and their interpretation , tr daimi pb-78 , comp .", "science depart .", ", aarhus university , 1977 .", "v. pratt , modeling concurrency with partial orders , _ international journal of parallel programming _", ", 15(1):3371 , 1986 .", "e.  szpilrajn , sur lextension de lordre partiel , _ fund .", "mathematicae _ 16 , 386389 , 1930 .", "p. s. thiagarajan and i. walukiewicz , an expressively complete linear time temporal logic for mazurkiewicz traces , _ inf .", "comput . _ 179(2 ) : 230249 , 2002 ."], ["* 1 . * ( @xmath551 ) : since @xmath207=[\\beta]$ ] , we know that @xmath552 or @xmath553 .", "the former case is trivial . for the latter case , by theorem  [ theo : szpstrat ]", ", we have @xmath554 and @xmath555 . but", "this implies that @xmath556 .", "( @xmath557 ) : the case when @xmath552 is trivial .", "assume that @xmath558 and @xmath556 .", "thus , by theorem  [ theo : szpstrat ] , @xmath559 and @xmath457 . but", "this means @xmath53 and @xmath70 belong to the same equivalence class .", "* suppose for a contradiction that all @xmath560 can not be written in the form of @xmath561v$ ] .", "this implies that there exists some @xmath562 $ ] such that for all @xmath560 , @xmath563 . but by theorem  [ theo : szpstrat ] , this yields @xmath564 and @xmath565 , contradicting with @xmath566 $ ] .", "assume @xmath207 { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\ ; } } [ \\beta]$ ] .", "suppose for a contradiction that there does not exist @xmath560 such that @xmath561[\\beta]v$ ] for some step sequences @xmath65 and @xmath206 .", "then , by theorem  [ theo : szpstrat ] , there must exist some @xmath567\\cup[\\beta])$ ] , such that @xmath568 .", "since @xmath569\\cup[\\beta]$ ] , this yields @xmath207{\\sqsubset}[\\gamma ] { \\sqsubset}[\\beta]$ ] , which contradicts that @xmath207 { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\ ; } } [ \\beta]$ ] ."], ["assume @xmath570 and @xmath558 .", "thus , @xmath571 .", "thus , by corollary  [ cor : szpstrat ] ( 2 ) , we have @xmath572 .", "hence , by of definition  [ def : lcomtrace ] , @xmath573 . since @xmath4 is irreflexive , this also shows that any two distinct @xmath53 and @xmath70 in @xmath251 have different labels .", "thus , @xmath257 for all @xmath255 .", "* 2 . * from the proof of * 1 . * , we know that @xmath570 and @xmath558 implies @xmath573 . thus , @xmath574 for all @xmath255 ."], ["observe that from proposition  [ prop : validss ] , we have @xmath575 .", "it remains to show that @xmath576 for all @xmath577 .", "suppose for a contradiction that @xmath578 for some @xmath577 .", "from the definition above , there are two distinct elements @xmath202 , such that @xmath579 and @xmath580 and @xmath581 . since @xmath5 is irreflexive , @xmath582 .", "thus , by of definition  [ def : lcomtrace ] , @xmath583 or @xmath584 . without loss of generality , we assume @xmath583 and that @xmath585 for some event @xmath586 .    again by of definition  [ def : lcomtrace ] , we know that elements having the same label are totally ordered by @xmath93 .", "thus , if @xmath587 is the number of elements in @xmath3 labeled by @xmath39 , then we have @xmath588 and @xmath589 .", "but then @xmath590 implies that @xmath591 , while @xmath592 implies that @xmath593 , which is absurd ."], ["let @xmath594},{\\prec}_{[u]},{\\sqsubset}_{[u]},l \\bigr)$ ] . from theorem  [ theo : com2sos ]", ", @xmath85 is a labeled so - structure .", "it only remains to show that @xmath85 satisfies conditions  of definition  [ def : lcomtrace ] .", ": assume @xmath207 ( { { \\hat{{\\sqsubset}}}^{\\mathsf{cov}\\;}}\\cap \\hat{{\\prec } } ) [ \\beta]$ ] and suppose for a contradiction that @xmath414)\\times \\lambda([\\beta ] ) \\subseteq ser$ ] .", "then from proposition  [ prop : covlsos ] ( 3 ) , there exists @xmath595 such that @xmath596[\\beta]w$ ] . from theorem  [ theo : com2sos ] , since we have @xmath597\\bigr\\ } } = ext(s_{[u]})$ ] , it follows that @xmath598[\\beta]w ) \\in [ u]$ ] . but @xmath414)\\times \\lambda([\\beta ] ) \\subseteq ser$ ] implies @xmath599\\cup [ \\beta]v ) \\in [ u]$ ] .", "hence , @xmath600\\cup [ \\beta]v$ ] is also a stratified extension of @xmath85 , which contradicts that @xmath207 \\hat{{\\prec } } [ \\beta]$ ] . using a similar argument", ", we can show using proposition  [ prop : covlsos ] ( 1,4 ) and using proposition  [ prop : covlsos ] ( 1,2 ) .      :", "since @xmath601 , it follows from corollary  [ cor : szpstrat ] that there exists @xmath595 where @xmath602 . since @xmath603\\bigr\\ } } = ext(s_{[u]})$ ] , there exists a sequence @xmath604 $ ] such that @xmath605 .", "this implies @xmath53 and @xmath70 belong to the same step in @xmath606 .", "thus , we have @xmath573 ."]]}
{"article_id": "1503.08946", "article_text": ["we are living in the age of `` big data '' , generally characterized by a series of `` vs '' .", "data are generated at an unprecedented _ volume _ by scientific instruments observing the macrocosm  @xcite and the microcosm  @xcite , or by humans connected around - the - clock to mobile platforms such as facebook and twitter .", "these data come in a _ variety _ of formats , ranging from delimited text to semi - structured json and multi - dimensional binaries such as fits .", "the volume and variety of `` big data '' pose serious problems to traditional database systems .", "before it is even possible to execute queries over a dataset , a relational schema has to be defined and data have to be loaded inside the database .", "schema definition imposes a strict structure on the data , which is expected to remain stable .", "however , this is rarely the case for rapidly evolving datasets represented using key - value and other semi - structured data formats , e.g. , json .", "data loading is a schema - driven process in which data are duplicated in the internal database representation to allow for efficient processing . even though storage is relatively cheap ,", "generating and storing multiple copies of the same data can easily become a bottleneck for massive datasets .", "moreover , it is quite often the case that many of the attributes in the schema are never used in queries .    motivated by the flexibility of nosql systems to access schema - less data and by the hadoop functionality to directly process data in any format ,", "we have recently witnessed a sustained effort to bring these capabilities inside relational database management systems ( rdbms ) . starting with version 9.3 ,", "postgresql includes support for json data type and corresponding functions .", "vertica flex zone and sinew  @xcite implement flex table and column reservoir , respectively , for storing key - value data serialized as maps in a blob column . in both systems", ", certain keys can be promoted to individual columns , in storage as well as in a dynamically evolving schema . with regards to directly processing raw data ,", "several query - driven extensions have been proposed to the loading and external table  @xcite mechanisms . instead of loading all the columns before querying , in adaptive partial loading  @xcite data", "are loaded only at query time , and only the attributes required by the query .", "this idea is further extended in invisible loading  @xcite , where only a fragment of the queried columns are loaded , and in nodb  @xcite , data vaults  @xcite , sds / q  @xcite , and raw  @xcite , where columns are loaded only in memory , but not into the database .", "scanraw  @xcite is a super - scalar pipeline operator that loads data speculatively , only when spare i / o resources are available .", "while these techniques enhance the rdbms flexibility to process schema - less raw data , they have several shortcomings , as the following examples show .", "* example 1 : twitter data . *", "the twitter api provides access to several objects in json format through a well - defined interface .", "the schema of the objects is , however , not well - defined , since it includes `` nullable '' attributes and nested objects .", "the state - of - the - art rdbms solution to process semi - structured json data  @xcite is to first load the objects as tuples in a blob column .", "essentially , this entails complete data duplication , even though many of the object attributes are never used .", "the internal representation consists of a map of key - values that is serialized / deserialized into / from persistent storage .", "the map can be directly queried from sql based on the keys , treated as virtual attributes . as an optimization , certain columns  chosen by the user or by the system based on appearance frequency  are promoted to physical status .", "the decision on which columns to materialize is only an heuristic , quite often sub - optimal .", "* example 2 : sloan digital sky survey ( sdss ) data .", "* sdss is a decade - long astronomy project having the goal to build a catalog of all the astrophysical objects in the observable universe .", "images of the sky are taken by a high - resolution telescope , typically in binary fits format .", "the catalog data summarize quantities measured from the images for every detected object .", "the catalog is stored as binary fits tables .", "additionally , the catalog data are loaded into an rdbms and made available through standard sql queries .", "the loading process replicates multi - terabyte data three times  in ascii csv and internal database representation  and it can take several days  if not weeks  @xcite . in order to evaluate the effectiveness of the loading , we extract a workload of 1 million sql queries executed over the sdss catalog in 2014 .", "the most frequent table in the workload is ` photoprimary ` , which appears in more than 70% of the queries . `", "photoprimary ` has 509 attributes , out of which only 74 are referenced in queries .", "this means that 435 attributes are replicated three times without ever being used  a significantly sub - optimal storage utilization", ".    * problem statement . * inspired by the above examples , in this paper , we study the raw data processing with partial loading problem .", "_ given a dataset in some raw format , a query workload , and a limited database storage budget , find what data to load in the database such that the overall workload execution time is minimized .", "_ this is a standard database optimization problem with bounded constraints , similar to vertical partitioning in physical database design  @xcite .", "however , while physical design investigates what non - overlapping partitions to build over internal database data , we focus on what data to load , i.e. , replicate , in a columnar database with support for multiple storage formats .", "existing solutions for loading and raw data processing are not adequate for our problem .", "complete loading not only requires a significant amount of storage and takes a prohibitively long time , but is also unnecessary for many workloads .", "pure raw data processing solutions  @xcite are not adequate either , because parsing semi - structured json data repeatedly is time - consuming . moreover ,", "accessing data from the database is clearly optimal in the case of workloads with tens of queries .", "the drawback of query - driven , adaptive loading methods  @xcite is that they are greedy , workload - agnostic .", "loading is decided based upon each query individually .", "it is easy to imagine a query order in which the first queries access non - frequent attributes that fill the storage budget , but have limited impact on the overall workload execution time .", "* contributions . * to the best of our knowledge , this is the first paper that incorporates query workload in raw data processing .", "this allows us to model raw data processing with partial loading as fully - replicated binary vertical partitioning .", "our contributions are guided by this equivalence .", "they can be summarized as follows :    we provide a linear mixed integer programming optimization formulation that we prove to be np - hard and inapproximable .", "we design a two - stage heuristic that combines the concepts of query coverage and attribute usage frequency .", "the heuristic comes within close range of the optimal solution in a fraction of the time .", "we extend the optimization formulation and the heuristic to a restricted type of pipelined raw data processing . in the pipelined scenario , data access and extraction", "are executed concurrently .", "we evaluate the performance of the heuristic and the accuracy of the optimization formulation over three real data formats ", "csv , fits , and json  processed with a state - of - the - art pipelined operator for raw data processing .", "the results confirm the superior performance of the proposed heuristic over related vertical partitioning algorithms and the accuracy of the formulation in capturing the execution details of a real operator .", "* outline . * the paper is organized as follows .", "raw data processing , the formal statement of the problem , and an illustrative example are introduced in the preliminaries ( section  [ sec : preliminaries ] ) .", "the mixed integer programming formulation and the proof that the formulation is np - hard are given in section  [ sec : mip ] .", "the proposed heuristic is presented in detail in section  [ sec : heuristic ] .", "the extension to pipelined raw data processing is discussed in section  [ sec : pipeline ] .", "extensive experiments that evaluate the heuristic and verify the accuracy of the optimization formulation over three real data formats are presented in section  [ sec : experiments ] .", "related work on vertical partitioning and raw data processing is briefly discussed in section  [ sec : rel - work ] , while section  [ sec : conclusions ] concludes the paper .", "in this section , we introduce query processing over raw data . then , we provide a formal problem statement and an illustrative example .", "query processing over raw data is depicted in figure  [ fig : scanraw ] .", "the input to the process is a raw file from a non - volatile storage device , e.g. , disk or ssd , a schema that can include optional attributes , a procedure to extract tuples with the given schema from the raw file , and a driver query .", "the output is a tuple representation that can be processed by the query engine and , possibly , is materialized ( i.e. , loaded ) on the same storage device . in the ` read ` stage , data are read from the original raw file , page - by - page , using the file system s functionality . without additional information about the structure or the content  stored inside the file or in some external structure ", "the entire file has to be read the first time it is accessed . `", "extract ` transforms tuples ", "one per line  from raw format into the processing representation , based on the schema provided and using the extraction procedure given as input to the process .", "there are two stages in ` extract``tokenize ` and ` parse ` .", "[ fig : scanraw ]    ` tokenize ` identifies the schema attributes and outputs a vector containing the starting position for every attribute in the tuple  or a subset , if the driver query does not access all the attributes . in ` parse ` , attributes are converted from raw format to the corresponding binary type and mapped to the processing representation of the tuple ", "the record in a row - store , or the array in column - stores , respectively .", "multiple records or column arrays are grouped into a chunk  the unit of processing . at the end of `", "extract ` , data are loaded in memory and ready for query processing .", "multiple paths can be taken at this point . in external tables", "@xcite , data are passed to the query engine and discarded afterwards . in nodb", "@xcite and in - memory databases  @xcite , data are kept in memory for subsequent processing . in standard database loading  @xcite", ", data are first written to the database and only then query processing starts .", "scanraw  @xcite invokes ` write ` concurrently with the query execution , only when spare i / o - bandwidth is available .", "the interaction between ` read ` and ` write ` is carefully scheduled in order to minimize interference .", "consider a relational schema @xmath0 and an instantiation of it that contains @xmath1 tuples .", "semi - structured json data can be mapped to the relational model by linearizing nested constructs  @xcite . in order to execute queries over @xmath2", ", tuples have to be read in memory and converted from the storage format into the processing representation .", "two timing components correspond to this process .", "@xmath3 is the time to read data from storage into memory .", "@xmath3 can be computed straightforwardly for a given schema and storage bandwidth @xmath4 .", "a constraint specific to raw file processing  and row - store databases , for that matter  is that all the attributes are read in a query  even when not required .", "@xmath5 is the second timing component .", "it corresponds to the conversion time . for every attribute @xmath6 in the schema ,", "the conversion is characterized by two parameters , defined at tuple level .", "the _ tokenizing time @xmath7 _ is the time to locate the attribute in a tuple in storage format .", "the _ parsing time @xmath8 _ is the time to convert the attribute from storage format into processing representation .", "a limited amount of storage @xmath9 is available for storing data converted into the processing representation .", "this eliminates the conversion and replaces it with an i / o process that operates at column level  only complete columns can be saved in the processing format .", "the time to read an attribute @xmath6 in processing representation , @xmath10 , can be determined when the type of the attribute and @xmath1 are known .", ".query access pattern to raw data attributes .", "[ cols=\"<,^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ tbl : mip : params ]    while written as a single constraint , @xmath11 decomposes into three separate constraints  one corresponding to each `` @xmath12 '' operator  for a total of @xmath13 constraints .", "@xmath14 is a reduced form of @xmath11 , applicable to query processing .", "the largest number of constraints , i.e. , @xmath15 , in the mip formulation are of type @xmath16 .", "they enforce that it is not possible to tokenize an attribute in a tuple without tokenizing all the preceding schema attributes in the same tuple .", "@xmath16 applies strictly to raw files without direct access to individual attributes .", "constraint @xmath17 guarantees that every attribute accessed in a query is either extracted from the raw file or read from the processing representation .", "there are @xmath18 binary 0/1 variables in the linear mip formulation , where @xmath19 is the number of queries in the workload and @xmath20 is the number of attributes in the schema .", "solving the mip directly is , thus , impractical for workloads with tens of queries over schemas with hundreds of attributes , unless the number of variables in the search space can be reduced .", "we prove that this is not possible by providing a reduction from a well - known np - hard problem to a restricted instance of the mip formulation .", "moreover , we also show that no approximation exists .", "[ def : k - elem - cover ] given a set of @xmath20 elements @xmath21 , @xmath19 subsets @xmath22 of @xmath2 , such that @xmath23 , and a value @xmath24 , the objective in the k - element cover problem is to find a size @xmath24 subset @xmath25 of @xmath2 that covers the largest number of subsets @xmath26 , i.e. , @xmath27 , @xmath28 .", "for the example in table  [ tbl : prelim : problem : example ] , @xmath29 is the single 2-element cover solution ( covering @xmath30 ) .", "while many 3-element cover solutions exist , they all cover only one query .", "the k - element cover problem is a restricted instance of the mip formulation , in which parameters @xmath7 , @xmath8 , and the loading and reading time to / from database are set to zero , i.e. , @xmath31 , while the raw data reading time is set to one , i.e. , @xmath32 .", "the objective function is reduced to counting how many times raw data have to be accessed .", "the bounding constraint limits the number of attributes that can be loaded , i.e. , @xmath33 , while the functional constraints determine the value of the other variables .", "the optimal solution is given by the configuration that minimizes the number of queries accessing raw data .", "a query does not access raw data when the @xmath34 variables corresponding to its attributes are all set to one .", "when the entire workload is considered , this equates to finding those attributes that cover the largest number of queries , i.e. , finding the k - attribute cover of the workload .", "given this reduction , it suffices to prove that k - element cover is np - hard for the mip formulation to have only exponential - time solutions .", "we provide a reduction to the well - known minimum k - set coverage problem  @xcite that proves k - element cover is np - hard .    [ def : min - k - set - cover ] given a set of @xmath20 elements @xmath21 , @xmath19 subsets @xmath22 of @xmath2 , such that @xmath23 , and a value @xmath24 , the objective in the minimum k - set coverage problem is to choose @xmath24 sets @xmath35 from @xmath36 whose union has the smallest cardinality , i.e. , @xmath37 .", "set @xmath21 and @xmath19 subsets @xmath22 of @xmath2 ; number @xmath38 of sets @xmath26 to choose in minimum set coverage minimum number @xmath24 of elements from @xmath2 covered by choosing @xmath38 subsets from @xmath36 @xmath39 = * _ k - element cover_*(@xmath36 , @xmath40 ) * if * @xmath41 * then * * return * @xmath40    algorithm  [ alg : reduction - cover - coverage ] gives a reduction from k - element cover to minimum k - set coverage .", "the solution to minimum k - set coverage is obtained by invoking k - element cover for any number of elements in @xmath2 and returning the smallest such number for which the solution to k - element cover contains at least @xmath38 subsets of @xmath36 .", "since we know that minimum k - set coverage is np - hard  @xcite and the solution is obtained by solving k - element cover , it implies that k - element cover can not be any simpler , i.e. , k - element cover is also np - hard .", "the following theorem formalizes this argument .", "[ thm : reduction ] the reduction from k - element cover to minimum k - set coverage given in algorithm  [ alg : reduction - cover - coverage ] is correct and complete .    * _ proof . _", "* in order to prove the theorem , we have to show that if the answer to the minimum k - set coverage problem is @xmath42 , algorithm  [ alg : reduction - cover - coverage ] returns @xmath42 and if algorithm  [ alg : reduction - cover - coverage ] returns @xmath42 , the answer to minimum k - set coverage problem is @xmath42 .", "we start with the first implication .", "let the optimal solution to the minimum k - set coverage problem be @xmath43 and @xmath44 be the set of elements in @xmath2 covered by @xmath45 , where @xmath46 and @xmath47 .", "suppose there exists a subset @xmath48 of @xmath2 and let the sets covered by @xmath48 be @xmath49 , where @xmath50 and @xmath51 .", "when @xmath51 , the union of any @xmath38 sets in @xmath49 is no larger than @xmath52 , which is smaller than @xmath42 .", "we get a contradiction .", "thus , there is no subset of @xmath2 whose size is smaller than @xmath42 that covers at least @xmath38 sets in @xmath36 . as a result ,", "algorithm  [ alg : reduction - cover - coverage ] does not return when @xmath53 . by the problem definition , we can use @xmath54 to cover at least @xmath38 sets @xmath45 .", "therefore , algorithm  [ alg : reduction - cover - coverage ] returns when @xmath55 .    for the second implication ,", "let the elements chosen by the _ * k - element cover * _ function be @xmath54 and @xmath45 be the sets covered by @xmath54 , where @xmath47 .", "suppose the optimal solution to minimum k - set coverage is @xmath49 , which covers elements @xmath48 , where @xmath56 and @xmath57 .", "then , @xmath49 is the answer to the function @xmath58 . in this case ,", "algorithm  [ alg : reduction - cover - coverage ] returns @xmath52 before @xmath42 .", "we have a contradiction .", "therefore , we know that the optimal solution to minimum k - set coverage can not be smaller than @xmath59 .", "we can choose any @xmath38 sets in @xmath45 as the solution to the minimum k - set coverage problem .", "the union of the sets in @xmath45 is not larger than @xmath59 .", "therefore , the second implication holds .    based on these two implications ,", "we conclude that the reduction is correct .", "the fact that algorithm  [ alg : reduction - cover - coverage ] has linear time complexity @xmath60 guarantees the completeness of the reduction .", "@xmath61    [ thm :", "reduction : corolar ] the mip formulation is np - hard and can not be approximated unless np - complete problems can be solved in randomized sub - exponential time .", "the np - hardness is a direct consequence of the reduction to the k - element cover problem and theorem  [ thm : reduction ] .", "in addition ,  @xcite and  @xcite prove that minimum k - set coverage can not be approximated within an absolute error of @xmath62 , for any @xmath63 , unless p = np .", "consequently , the mip formulation can not be approximated .", "in this section , we propose a novel heuristic algorithm for raw data processing with partial loading that has as a starting point a greedy solution for the k - element cover problem .", "the algorithm also includes elements from vertical partitioning  a connection we establish in the paper .", "the central idea is to combine _ query coverage _ with _ attribute usage frequency _ in order to determine the best attributes to load . at a high level", ", query coverage aims at reducing the number of queries that require access to the raw data , while usage frequency aims at eliminating the repetitive extraction of the heavily - used attributes .", "our algorithm reconciles between these two conflicting criteria by optimally dividing the available loading budget across them , based on the format of the raw data and the query workload .", "the solution found by the algorithm is guaranteed to be as good as the solution corresponding to each criterion , considered separately .    in the following ,", "we make the connection with vertical partitioning clear .", "then , we present separate algorithms based on query coverage and attribute usage frequency .", "these algorithms are combined into the proposed heuristic algorithm for raw data processing with partial loading .", "we conclude the section with a detailed comparison between the proposed heuristic and algorithms designed specifically for vertical partitioning .      vertical partitioning  @xcite of a relational schema", "@xmath64 splits the schema into multiple schemas  possibly overlapping  each containing a subset of the columns in @xmath2 . for example", ", @xmath65 is the atomic non - overlapping vertical partitioning of @xmath2 in which each column is associated with a separate partition .", "tuple integrity can be maintained either by sorting all the partitions in the same order , i.e. , positional equivalence , or by pre - pending a tuple identifier ( @xmath66 ) column to every partition .", "vertical partitioning reduces the amount of data that have to be accessed by queries that operate on a small subset of columns since only the required columns have to be scanned  when they form a partition .", "however , tuple reconstruction  @xcite can become problematic when integrity is enforced through @xmath66 values because of joins between partitions .", "this interplay between having partitions that contain only the required columns and access confined to a minimum number of partitions , i.e. , a minimum number of joins , is the objective function to minimize in vertical partitioning .", "the process is always workload - driven .", "raw data processing with partial loading can be mapped to _ fully - replicated binary vertical partitioning _ as follows .", "the complete raw data containing all the attributes in schema @xmath2 represent the _ raw partition_. the second partition  _ loaded partition _ ", "is given by the attributes loaded in processing representation .", "these are a subset of the attributes in @xmath2 .", "the storage allocated to the loaded partition is bounded .", "the asymmetric nature of the two partitions differentiates raw data processing from standard vertical partitioning .", "the raw partition provides access to all the attributes , at the cost of tokenizing and parsing .", "the loaded partition provides faster access to a reduced set of attributes . in vertical partitioning , all the partitions are equivalent .", "while having only two partitions may be regarded as a simplification , all the top - down algorithms we are aware of  @xcite apply binary splits recursively in order to find the optimal partitions .", "the structure of raw data processing with partial loading limits the number of splits to one .", "workload @xmath67 ; storage budget @xmath9 set of attributes @xmath68 to be loaded in processing representation    [ a2:l1 ] @xmath69 ; @xmath70 [ a2:l2 ] [ a2:l3 ] @xmath71 [ a2:l4 ] * if * @xmath72 * then * * break * [ a2:l5 ] @xmath73 [ a2:l6 ] @xmath74 [ a2:l8 ] @xmath75    a query that can be processed without accessing the raw data is said to be _ covered_. in other words , all the attributes accessed by the query are loaded in processing representation .", "these are the queries whose attributes are contained in the solution to the k - element cover problem .", "intuitively , increasing the number of covered queries results in a reduction to the objective function , i.e. , total query workload execution time , since only the required attributes are accessed .", "moreover , access to the raw data and conversion are completely eliminated .", "however , given a limited storage budget , it is computationally infeasible to find the optimal set of attributes to load  the k - element cover problem is np - hard and can not be approximated ( corollary  [ thm : reduction : corolar ] ) .", "thus , heuristic algorithms are required .", "we design a standard greedy algorithm for the k - element cover problem that maximizes the number of covered queries within a limited storage budget .", "the pseudo - code is given in algorithm  [ alg : query - coverage ] .", "the solution @xmath76 and the covered queries @xmath77 are initialized with the empty set in line  [ a2:l1 ] . as long as the storage budget is not exhausted ( line  [ a2:l2 ] ) and the value of the objective function @xmath78 decreases ( line  [ a2:l4 ] ) , a query to be covered is selected at each step of the algorithm ( line  [ a2:l3 ] ) .", "the criterion we use for selection is the reduction in the cost function normalized by the storage budget , i.e. , we select the query that provides the largest reduction in cost , while using the smallest storage .", "this criterion gives preference to queries that access a smaller number of attributes and is consistent with our idea of maximizing the number of covered queries .", "an alternative selection criterion is to drop the cost function and select the query that requires the least number of attributes to be added to the solution .", "the algorithm is guaranteed to stop when no storage budget is available or all the queries are covered .", "* example .", "* we illustrate how the _ * query coverage * _ algorithm works on the workload in table  [ tbl : prelim : problem : example ] . without loss of generality ,", "assume that all the attributes have the same size and the time to access raw data is considerably larger than the extraction time and the time to read data from processing representation , respectively .", "these is a common situation in practice , specific to delimited text file formats , e.g. , csv .", "let the storage budget be large enough to load three attributes , i.e. , @xmath79 .", "in the first step , only queries @xmath30 , @xmath80 , and @xmath81 are considered for coverage in line  [ a2:l3 ] , due to the storage constraint .", "while the same objective function value is obtained for each query , @xmath30 is selected for loading because it provides the largest normalized reduction , i.e. , @xmath82 .", "the other two queries have a normalized reduction of @xmath83 , where @xmath3 is the time to read the raw data . in the second step of the algorithm , @xmath84 .", "this also turns to be the last step since no other query can be covered in the given storage budget .", "notice that , although @xmath80 and @xmath81 make better use of the budget , the overall objective function value is hardly different , as long as reading raw data is the dominating cost component .", "workload @xmath22 of @xmath2 ; storage budget @xmath9 ; set of loaded attributes @xmath85 set of attributes @xmath86 to be loaded in processing representation    @xmath87 [ a3:l2 ] [ a3:l3 ] @xmath88", "@xmath89 @xmath75    the query coverage strategy operates at query granularity .", "an attribute is always considered as part of the subset of attributes accessed by the query . it is never considered individually .", "this is problematic for at least two reasons .", "first , the storage budget can be under - utilized , since a situation where storage is available but no query can be covered , can appear during execution .", "second , a frequently - used attribute or an attribute with a time - consuming extraction may not get loaded if , for example , is part of only long queries .", "the assumption that accessing raw data is the dominant cost factor does not hold in this case .", "we address these deficiencies of the query coverage strategy by introducing a simple greedy algorithm that handles attributes individually . as the name implies", ", the intuition behind the attribute usage frequency algorithm is to load those attributes that appear frequently in queries .", "the rationale is to eliminate the extraction stages that incur the largest cost in the objective function .", "the pseudo - code for the attribute usage frequency strategy is given in algorithm  [ alg : freq ] .", "in addition to the workload and the storage budget , a set of attributes already loaded in the processing representation is passed as argument . at each step", "( line  [ a3:l3 ] ) , the attribute that generates the largest decrease in the objective function is loaded . in this case , the algorithm stops only when the entire storage budget is exhausted ( line  [ a3:l2 ] ) .", "* example .", "* we illustrate how the _ * attribute usage frequency * _ algorithm works by continuing the example started in the query coverage section .", "recall that only two attributes @xmath90 out of a total of three are loaded .", "@xmath91 is chosen as the remaining attribute to be loaded since it appears in five queries , the largest number between unloaded attributes .", "given that all the attributes have the same size and there is no cost for tuple reconstruction , @xmath92 is the optimal loading configuration for the example in table  [ tbl : prelim : problem : example ] .", "workload @xmath67 ; storage budget @xmath9 set of attributes @xmath68 to be loaded in processing representation    @xmath93 [ a4:l2 ] [ a4:l3 ] @xmath94 [ a4:l4 ] @xmath95 @xmath96 @xmath97 @xmath98", "@xmath99 @xmath100    the heuristic algorithm for raw data processing with partial loading unifies the query coverage and attribute usage frequency algorithms . the pseudo - code is depicted in algorithm  [ alg : combined ] . given a storage budget @xmath9 , _ * query coverage * _", "is invoked first ( line  [ a4:l3 ] ) . _", "* attribute usage frequency * _ ( line  [ a4:l4 ] ) takes as input the result produced by _ * query coverage * _ and the unused budget @xmath101 . instead of invoking these algorithms only once , with the given storage budget @xmath9", ", we consider a series of allocations .", "@xmath9 is divided in @xmath102 increments ( line  [ a4:l2 ] ) .", "each algorithm is assigned anywhere from @xmath103 to @xmath9 storage , in @xmath102 increments .", "a solution is computed for each of these configurations .", "the heuristic algorithm returns the solution with the minimum objective .", "the increment @xmath102 controls the complexity of the algorithm .", "specifically , the smaller @xmath102 is , the larger the number of invocations to the component algorithms .", "notice , though , that as long as @xmath104 remains constant with respect to @xmath19 and @xmath20 , the complexity of the heuristic remains @xmath105 .    the rationale for using several budget allocations between query coverage and attribute usage frequency lies in the limited view they take for solving the optimization formulation .", "query coverage assumes that the access to the raw data is the most expensive cost component , i.e. , processing is i / o - bound , while attribute usage frequency focuses exclusively on the extraction , i.e. , processing is cpu - bound .", "however , the actual processing is heavily - dependent on the format of the data and the characteristics of the system .", "for example , binary formats , e.g. , fits , do not require extraction , while hierarchical text formats , e.g. , json , require complex parsing . moreover , the extraction complexity varies largely across data types .", "the proposed heuristic algorithm recognizes these impediments and solves many instances of the optimization formulation in order to identify the optimal solution .      as discussed in section  [ sec : heuristic : vertical - part ] , raw data processing with partial loading is a special case of vertical partitioning  binary vertical partitioning with full replication .", "however , there is a fundamental difference between the problem addressed in this paper and standard vertical partitioning .", "the amount of storage allocated to partitions is not a parameter in vertical partitioning because it is constant and independent of the layout  all the partitions use the same storage , plus - minus metadata .", "the bounded storage constraint is what makes raw data processing with partial loading a considerably more complicated problem , to which standard vertical partitioning algorithms are not directly applicable .", "a comprehensive comparison of vertical partitioning methods is given in  @xcite .", "with few exceptions  @xcite , vertical partitioning algorithms consider only the non - replicated case . when replication is considered , it is only partial replication .", "the bounded scenario  limited storage budget for replicated attributes  is discussed only in  @xcite . at a high level", ", vertical partitioning algorithms can be classified along several axes  @xcite .", "we discuss the two most relevant axes for the proposed heuristic .", "based on the direction in which partitions are built , we have top - down and bottom - up algorithms .", "a top - down algorithm  @xcite starts with the complete schema and , at each step , splits it into two partitioned schemas .", "the process is repeated recursively for each resulting schema .", "a bottom - up algorithm  @xcite starts with a series of schemas , e.g. , one for each attribute or one for each subset of attributes accessed in a query , and , at each step , merges a pair of schemas into a new single schema . in both cases ,", "the process stops when the objective function can not be improved further .", "a second classification axis is given by the granularity at which the algorithm works .", "an attribute - level algorithm  @xcite considers the attributes independent of the queries in which they appear .", "the interaction between attributes across queries still plays a significant role , though . a query or transaction - level algorithm  @xcite works at query granularity .", "a partition contains either all or none of the attributes accessed in a query .", "based on the classification of vertical partitioning algorithms , the proposed heuristic qualifies primarily as a top - down query - level attribute - level algorithm .", "however , the recursion is only one - level deep , with the loaded partition at the bottom .", "the partitioning process consists of multiple steps , though . at each step , a new partition extracted from the raw data", "is merged into the loaded partition  similar to a bottom - up algorithm .", "the query coverage algorithm gives the query granularity characteristic to the proposed heuristic , while attribute usage frequency provides the attribute - level property .", "overall , the proposed heuristic combines ideas from several classes of vertical partitioning algorithms , adapting their optimal behavior to raw data processing with partial loading . an experimental comparison with specific algorithms is presented in the experiments ( section  [ sec : experiments ] ) and a discussion on their differences in the related work ( section  [ sec : rel - work ] ) .", "in this section , we discuss on the feasibility of mip optimization in the case of pipelined raw data processing with partial loading .", "we consider a super - scalar pipeline architecture in which raw data access and the extraction stages  tokenize and parse  can be executed concurrently by overlapping disk i / o and cpu processing .", "this architecture is introduced in  @xcite , where it is shown that , with a sufficiently large number of threads , raw data processing is an i / o - bound task .", "loading and accessing data from the processing representation are not considered as part of the pipeline since they can not be overlapped with raw data access due to i / o interference .", "we show that , in general , pipelined raw data processing with partial loading can not be modeled as a linear mip . however , we provide a linear formulation for a scenario that is common in practice , e.g. , binary fits and json format . in these cases , tokenization is atomic .", "it is executed for all or none of the attributes .", "this lets parsing as the single variable in the extraction stage .", "the mip formulation can not be solved efficiently , due to the large number of variables and constraints  much larger than in the sequential formulation .", "we handle this problem by applying a simple modification to the heuristic introduced in section  [ sec : heuristic ] that makes the algorithm feasible for pipelined processing .", "since raw data access and extraction are executed concurrently , the objective function corresponding to pipelined query processing has to include only the maximum of the two : @xmath106 this is the only modification to the mip formulation for sequential processing given in section  [ sec : mip ] .", "since the @xmath107 function is non - linear , solving the modified formulation becomes impossible with standard mip solvers , e.g. , cplex , which work only for linear problems .", "the only alternative is to eliminate the @xmath107 function and linearize the objective .", "however , this can not be achieved in the general case .", "it can be achieved , though , for specific types of raw data  binary formats that do not require tokenization , e.g. , fits , and text formats that require complete tuple tokenization , e.g. , json . as discussed in the introduction", ", these formats are used extensively in practice .", "queries over raw data can be classified into two categories based on the pipelined objective function in eq .", "( [ eq : obj : pipeline : max ] ) . in i / o - bound queries ,", "the time to access raw data is the dominant factor , i.e. , @xmath107 returns the first argument . in cpu - bound queries , the extraction time dominates , i.e.", ", @xmath107 returns the second argument .", "if the category of the query is known , @xmath107 can be immediately replaced with the correct argument and the mip formulation becomes linear .", "our approach is to incorporate the category of the query in the optimization as 0/1 variables . for each query @xmath40 , there is a variable for cpu - bound ( @xmath108 ) and one for io - bound ( @xmath109 ) . only one of them can take value 1 .", "moreover , these variables have to be paired with the variables for raw data access and extraction , respectively .", "variables of the form @xmath110 , @xmath111 , and @xmath112 correspond to the variables in table  [ tbl : mip : vars ] , in the case of a cpu - bound query . variables", "@xmath113 , @xmath114 , and @xmath115 are for the io - bound case , respectively .    with these variables , we can define the functional * constraints * for pipelined raw data processing : @xmath116 constraint @xmath117 forces a query to be either cpu - bound or io - bound .", "constraints @xmath118 tie the new family of cpu / io variables to their originals in the serial formulation . for example , the raw data is accessed in a cpu / io query only if it is accessed in the stand - alone query .", "the same holds for tokenizing / parsing a column @xmath119 in query @xmath40 .", "constraints @xmath120 and @xmath121 , respectively , tie the value of the cpu / io variables to the value of the corresponding query variable .", "for example , only when a query @xmath40 is cpu - bound , it makes sense for @xmath111 and @xmath112 to be allowed to take value 1 .", "if the query is io - bound , @xmath114 and @xmath115 can be set , but not @xmath111 and @xmath112 .    at this point", ", we have still not defined when a query is cpu - bound and when is io - bound .", "this depends on the relationship between the time to access the raw data and the time to extract the referenced attributes .", "while the parsing time is completely determined by the attributes accessed in the query , the tokenizing time is problematic since it depends not only on the attributes , but also on their position in the schema .", "for example , in the sdss ` photoprimary ` table containing 509 attributes , the time to tokenize the @xmath122 attribute is considerably smaller than the time to tokenize the @xmath123 attribute .", "moreover , there is no linear relationship between the position in the schema and the tokenize time . for this reason", ", we can not distinguish between cpu- and io - bound queries in the general case .", "however , if there is no tokenization  the case for binary formats such as fits  or the tokenization involves all the attributes in the schema  the case for hierarchical json format  we can define a threshold @xmath124 that allows us to classify queries .", "@xmath125 is given by the ratio between the time to access raw data less the constant tokenize time and the average time to parse an attribute .", "intuitively , @xmath125 gives the number of attributes that can be parsed in the time required to access the raw data .", "if a query has to parse more than @xmath125 attributes , it is cpu - bound .", "otherwise , it is io - bound .", "the threshold * constraints * @xmath126 and @xmath127 make these definitions formal : @xmath128 for the atomic tokenization to hold , constraint @xmath16 in the serial formulation has to be replaced with @xmath129 .", "the complete pipelined mip includes the constraints in the serial formulation ( eq .  ( [ eq : mip : formulation ] ) ) and the constraints @xmath130 .", "the linear * objective function * corresponding to query processing is re - written using the newly introduced variables as follows : @xmath131      since the number of variables and constraints increases with respect to the serial mip formulation , the task of a direct linear solver becomes even harder .", "it is also important to notice that the problem remains np - hard and can not be approximated since the reduction to the k - element cover still applies . in these conditions , heuristic algorithms are the only solution .", "we design a simple modification to the heuristic introduced in section  [ sec : heuristic ] specifically targeted at pipelined raw data processing .    given a configuration of attributes loaded in processing representation , the category of a query can be determined by evaluating the objective function .", "what is more important , though , is that the evolution of the query can be traced precisely as attributes get loaded .", "an i / o - bound query remains i / o - bound as long as not all of its corresponding attributes are loaded . at that point , it is not considered by the heuristic anymore .", "a cpu - bound query has the potential to become i / o - bound if the attributes that dominate the extraction get loaded .", "once i / o - bound , a query can not reverse to the cpu - bound state .", "thus , the only transitions a query can make are from cpu - bound to i / o - bound , and to loaded from there . if an io - bound query is not covered in the _ * query coverage * _ section of the heuristic , its contribution to the objective function can not be improved since it can not be completely covered by _ * attribute usage frequency*_. based on this observation , the only strategy to reduce the cost is to select attributes that appear in cpu - bound queries .", "we enforce this by limiting the selection of the attributes considered in line  [ a3:l3 ] of _ * attribute usage frequency * _ to those attributes that appear in at least one cpu - bound query .", "the objective of the experimental evaluation is to investigate the accuracy and performance of the proposed heuristic across a variety of datasets and workloads executed sequentially and pipelined . to this end", ", we explore the accuracy of predicting the execution time for complex workloads over three raw data formats ", "csv , fits , and json .", "additionally , the sensitivity of the heuristic is quantified with respect to the various configuration parameters .", "specifically , the experiments we design are targeted to answer the following questions :    what is the impact of each stage in the overall behavior of the heuristic ?", "how accurate is the heuristic with respect to the optimal solution ?", "with respect to vertical partitioning algorithms ?    how much faster is the heuristic compared to directly solving the mip formulation ? compared to other vertical partitioning algorithms ?    can the heuristic exploit pipeline processing in partitioning ?", "do the mip model and the heuristic reflect reality across a variety of raw data formats ?", "* implementation .", "* we implement the heuristic and all the other algorithms referenced in the paper in ` c++ ` .", "we follow the description and the parameter settings given in the original paper as closely as possible . the loading and query execution plans returned by the optimization routine", "are executed with the scanraw  @xcite operator for raw data processing .", "scanraw supports serial and pipelined execution .", "the real results returned by scanraw are used as reference .", "we use ibm cplex 12.6.1 to implement and solve the mip formulations .", "cplex supports parallel processing .", "the number of threads used in the optimization is determined dynamically at runtime .", "* we execute the experiments on a standard server with 2 amd opteron 6128 series 8-core processors ( 64 bit )  16 cores ", "64 gb of memory , and four 2 tb 7200 rpm sas hard - drives configured raid-0 in software .", "each processor has 12 mb l3 cache while each core has 128 kb l1 and 512 kb l2 local caches .", "the storage system supports 240 , 436 and 1600 mb / second minimum , average , and maximum read rates , respectively  based on the ubuntu disk utility .", "the cached and buffered read rates are 3 gb / second and 565 mb / second , respectively .", "ubuntu 14.04.2 smp @xmath132-bit with linux kernel 3.13.0 - 43 is the operating system .", "* methodology .", "* we perform all experiments at least 3 times and report the average value as the result .", "we enforce data to be read from disk by cleaning the file system buffers before the execution of every query in the workload .", "this is necessary in order to maintain the validity of the modeling parameters .", "* we use three types of real data formats in our experiments ", "csv , fits , and json .", "the csv and fits data are downloaded from the sdss project using the cas tool .", "they correspond to the complete schema of the ` photoprimary ` table , which contains 509 attributes .", "the csv and fits data are identical .", "only their representation is different .", "csv is delimited text , while fits is in binary format .", "there are 5 million rows in each of these files .", "csv is 22 gb in size , while fits is only 19 gb . json is a lightweight semi - structured key - value data format .", "the twitter api provides access to user tweets in this format .", "tweets have a hierarchical structure that can be flattened into a relational schema .", "we acquire 5,420,000 tweets by making requests to the twitter api .", "there are at most 155 attributes in a tweet .", "the size of the data is 19 gb .", "* workloads . *", "we extract a real workload of 1 million sql queries executed over the sdss catalog in 2014 . out of these , we select the most popular 100 queries over table ` photoprimary ` and their corresponding frequency . these represent approximately 70% of the 1 million queries .", "we use these 100 queries as our workload in the experiments over csv and fits data .", "the weight of a query is given by its relative frequency .", "furthermore , we extract a subset of the 32 most popular queries and generate a second workload .", "the maximum number of attributes referenced in both workloads is 74 .", "we create the workload for the tweets data synthetically since we can not find a real workload that accesses more than a dozen of attributes .", "the number of attributes in a query is sampled from a normal distribution centered at 20 and having a standard deviation of 20 .", "the attributes in a query are randomly selected out of all the attributes in the schema or , alternatively , out of a subset of the attributes .", "the smaller the subset , the more attributes are not accessed in any query .", "the same weight is assigned to all the queries in the workload .      in this set of experiments ,", "we evaluate the sensitivity of the proposed heuristic with respect to the parameters of the problem , specifically , the number of queries in the workload and the storage budget .", "we study the impact each stage in the heuristic has on the overall accuracy .", "we measure the error incurred by the heuristic with respect to the optimal solution computed by cplex and the decrease in execution time .", "we also compare against several top - down vertical partitioning algorithms .", "we use the sdss data and workload in our evaluation .", "we consider the following vertical partitioning algorithms in our comparison : agrawal  @xcite , navathe  @xcite , and chu  @xcite . the _ agrawal algorithm _", "@xcite is a pruning - based algorithm in which all the possible column groups are generated based on the attribute co - occurrence in the query workload . for each column group ,", "an interestingness measure is computed . since there is an exponential number of such column groups , only the `` interesting '' ones", "are considered as possible partitions .", "a column group is interesting if the interestingness measure , i.e. , cg - cost , is larger than a specified threshold . the interesting column groups are further ranked based on another measure , i.e. , vp - confidence , which quantifies the frequency with which the entire column group is referenced in queries .", "the attributes to load are determined by selecting column groups in the order given by vp - confidence , as long as the storage budget is not filled .", "while many strategies can be envisioned , our implementation is greedy .", "it chooses those attributes in a column group that are not already loaded and that minimize the objective function , one - at - a - time .", "the agrawal algorithm has exponential complexity @xmath133 since this is the number of potential column groups .", "this can be reduced by selecting the cg - cost threshold intelligently . however , this results in a corresponding accuracy decrease .", "trojan layouts  @xcite are a newer version of the agrawal algorithm in which a different interestingness measure is defined and the selection of the partitions from the interesting column groups is done using an optimal exponential algorithm . since the authors admit that `` finding the right trojan layouts for scientific data sets ( having hundreds of attributes ) , like sdss , becomes a difficult task to achieve ''  @xcite , we use the original agrawal algorithm in our implementation .    the _ navathe algorithm _", "@xcite starts with an affinity matrix that quantifies the frequency with which any pair of two attributes appear together in a query .", "the main step of the algorithm consists in finding a permutation of the rows and columns that groups attributes that co - occur together in queries .", "this extends upon the affinity of two attributes to a larger number of attributes .", "while finding the optimal permutation is exponential in the number of attributes , a quadratic greedy algorithm that starts with two random attributes and then chooses the best attribute to add and the best position , one - at - a - time , is given .", "these are computed based on a benefit function that is independent of the objective .", "the attributes are ordered on the benefit function in the resulting matrix .", "the final step of the algorithm consists in finding a split point along the attribute axis that generates two partitions with minimum objective function value across the query workload .", "an additional condition that we have to consider in our implementation is the storage budget  we find the optimal partition that also fits in the available storage space .    the _ chu algorithm _  @xcite considers only those partitions supported by at least one query in the workload , i.e. , a column group can be a partition only if it is accessed entirely by a query .", "moreover , a column group supported by a query is never split into smaller sub - parts .", "the algorithm enumerates all the column groups supported by any number of queries in the workload  from a single query", "to all the queries  and chooses the partition that minimizes the objective function .", "the remaining attributes  not supported by the query  form the second partition .", "this algorithm is exponential in the number of queries in the workload @xmath134 .", "the solution proposed in  @xcite is to limit the number number of query combinations to a relatively small constant , e.g. , 5 . in our implementation", ", we let the algorithm run for a limited amount of time , e.g. , one hour , and report the best result at that time  if the algorithm has not finished by that time .    *", "heuristic stage analysis .", "* figure  [ fig : combined_compare ] and  [ fig : error ] depict the impact each stage in the heuristic  query * coverage * and attribute usage * frequency *  has on the accuracy , when taken separately and together , i.e. , * heuristic*. we measure both the absolute value ( figure  [ fig : combined_compare ] ) and the relative error with respect to the optimal value ( figure  [ fig : error ] ) .", "we depict these values as a function of the storage budget , given as the number of attributes that can be loaded .", "we use the 32 queries workload .", "as expected , when the budget increases , the objective decreases . in terms of relative error , though , the heuristic is more accurate at the extremes  small budget or large budget . when the budget is medium , the error is the highest", "the reason for this behavior is that , at the extremes , the number of choices for loading is considerably smaller and the heuristic finds a good enough solution .", "when the storage budget is medium , there are many loading choices and the heuristic makes only local optimal decisions that do not necessarily add - up to a good global solution .", "the two - stage heuristic has better accuracy than each stage taken separately .", "this is more clear in the case of the difficult problems with medium budget . between the two separate stages ,", "none of them is dominating the other in all the cases .", "this proves that our integrated heuristic is the right choice since it always improves upon the best stage taken individually .    *", "serial heuristic accuracy .", "* figure  [ fig : sequential_100_obj ] depicts the accuracy as a function of the storage budget for several algorithms in the case of serial raw data processing .", "the workload composed of 100 queries is used . out of the heuristic algorithms , the proposed heuristic is the most accurate .", "as already mentioned , the largest error is incurred when the budget is medium . between the vertical partitioning algorithms , the query - level granularity algorithm  @xcite", "is the most accurate .", "the other two algorithms  @xcite do not improve as the storage budget increases .", "this is because they are attribute - level algorithms that are not optimized for covering queries .", "* serial heuristic execution time .", "* figure  [ fig : sequential_100_time ] depicts the execution time for the same scenario as in figure  [ fig : sequential_100_obj ] .", "it is clear that the proposed heuristic is always the fastest , even by three orders of magnitude in the best case .", "surprisingly , calculating the exact solution using cplex is faster than all the vertical partitioning algorithms almost in all the cases .", "if an algorithm does not finish after one hour , we stop it and take the best solution at that moment .", "this is the case for chu  @xcite and agrawal  @xcite .", "however , the solution returned by chu is accurate  a known fact from the original paper .", "* pipelined heuristic accuracy . *", "the objective function value for pipelined processing over fits data is depicted in figure  [ fig : pipeline_100_obj ] . the same 100 query workload is used .", "the only difference compared to the serial case is that cplex can not find the optimal solution in less than one hour .", "however , it manages to find a good - enough solution in most cases .", "the proposed heuristic achieves the best accuracy for all the storage budgets .", "* pipelined heuristic execution time . *", "the proposed heuristic is the only solution that achieves sub - second execution time for all the storage budgets ( figure  [ fig : pipeline_100_time ] ) .", "cplex finishes execution in the alloted time only when the budget is large .", "the number of variables and constraints in the pipeline mip formulation increase the search space beyond what the cplex algorithms can handle .", "we provide a series of case studies over different data formats in order to validate that the raw data processing architecture depicted in figure  [ fig : scanraw ] is general and the mip models corresponding to this architecture fit reality .", "we use the implementation of the architecture in the scanraw operator  @xcite as a baseline .", "for a given workload and loading plan , we measure the cumulative execution time after each query and compare the result with the estimation computed by the mip formulation .", "if the two match , this is a good indication that the mip formulation models reality accurately .", "the csv format maps directly to the raw data processing architecture . in order to apply the mip formulation ,", "the value of the parameters has to be calibrated for a given system and a given input file .", "the time to tokenize @xmath7 and parse @xmath8 an attribute are the only parameters that require discussion .", "this can be done by executing the two stages on a sample of the data and measuring the average value of the parameter for each attribute .", "as long as accurate estimates are obtained , the model will be accurate .", "figure  [ fig : csv ] confirms this on the sdss workload of 32 queries . in this case", ", there is a perfect match between the model and the scanraw execution .", "[ fig : csv ]      since fits is a binary format , there is no extraction phase , i.e. , tokenizing and parsing , in the architecture .", "moreover , data can be read directly in the processing representation , as long as the file access library provides such a functionality .", "cfitsio  the library we use in our implementation", " can read a range of values of an attribute in a pre - allocated memory buffer .", "however , we observed experimentally that , in order to access any attribute , there is a high startup time .", "essentially , the entire data are read in order to extract the attribute .", "the additional time is linear in the number of attributes .", "[ fig : fits ]    based on these observations  that may be specific to cfitsio ", "the following parameters have to be calibrated : the time to read the raw data corresponds to the startup time ; an extraction time proportional with the number of attributes in the query is the equivalent of @xmath8 .", "@xmath7 is set to zero .", "although pipelining is an option for fits data , due to the specifics of the cfitsio library , it is impossible to apply it .", "the result for the sdss data confirms that the model is a good fit for fits data since there is almost complete overlap in figure  [ fig : fits ] .      at first sight", ", it seems impossible to map json data on the raw data processing architecture and the mip model .", "looking deeper , we observe that json data processing is even simpler than csv processing .", "this is because every object is fully - tokenized and parsed in an internal map data structure , independent of the requested attributes .", "at least this is how the jsoncpp library works .", "+    once the map is built , it can be queried for any key in the schema . for schemas with a reduced number of hierarchical levels  the case for tweets ", "there is no difference in query time across levels . essentially , the query time is proportional only with the number of requested keys , independent of their existence or not . based on these observations", ", we set the model parameters as follows .", "@xmath7 is set to the average time to build the map divided by the maximum number of attributes in the schema .", "@xmath8 is set to the map data structure query time .", "since @xmath7 is a constant , the pipelined mip formulation applies to the json format .", "the results in figure  [ fig : json ] confirm the accuracy of the model over a workload of 32 queries executed in scanraw .     [", "fig : json ]      the experimental evaluation provides answers to each of the questions raised at the beginning of the section .", "the two - stage heuristic improves over each of the component parts .", "it is not clear which of the query coverage and attribute usage frequency is more accurate .", "using them together guarantees the best results .", "the proposed heuristic comes close to the optimal solution whenever the storage budget is either small or large .", "when many choices are available  the case for a medium budget  the accuracy decreases , but remains superior to the accuracy of the other vertical partitioning methods . in terms of execution time ,", "the proposed heuristic is the clear winner  by as much as three orders of magnitude .", "surprisingly , cplex outperforms the other heuristics in the serial case .", "this is not necessarily unexpected , given that these algorithms have been introduced more than two decades ago .", "the case studies confirm the applicability of the mip formulation model to several raw data formats .", "the mip model fits the reality almost perfectly both for serial and pipelined raw data processing .", "two lines of research are most relevant to the work presented in this paper  raw data processing and vertical partitioning as a physical database design technique .", "our contribution is to integrate workload information in raw data processing and model the problem as vertical partitioning optimization . to the best of our knowledge ,", "this is the first paper to consider the problem of optimal vertical partitioning for raw data processing with partial loading .    *", "raw data processing .", "* several methods have been proposed for processing raw data within a database engine .", "the vast majority of them bring enhancements to the external table functionality , already supported by several major database servers  @xcite .", "a common factor across many of these methods is that they do not consider loading converted data inside the database . at most ,", "data are cached in memory on a query - by - query basis .", "this is the approach taken in nodb  @xcite , data vaults  @xcite , sds / q  @xcite , raw  @xcite , and impala  @xcite .", "even when loading is an option , for example in adaptive partial loading  @xcite , invisible loading  @xcite , and scanraw  @xcite , the workload is not taken into account and the storage budget is unlimited . the decision on what to load is local to every query , thus , prone to be acutely sub - optimal over the entire workload .", "the heuristic developed in this paper requires workload knowledge and aims to identify the optimal data to load such that the execution time of the entire workload is minimized . as in standard database processing", ", loading is executed offline , before query execution . however , the decision on what data to load is intelligent and the time spent on loading is limited by the allocated storage budget .", "notice that the heuristic is applicable both to secondary storage - based loading as well as to one - time in - memory caching without subsequent replacement .    * vertical partitioning . * vertical partitioning has a long - standing history as a physical database design strategy , dating back to the 1970 s .", "many types of solutions have been proposed over the years , ranging from integer programming formulations to top - down and bottom - up heuristics that operate at the granularity of a query or of an attribute .", "a comparative analysis of several vertical partitioning algorithms is presented in  @xcite .", "the serial mip formulation for raw data processing is inspired from the formulations for vertical partitioning given in  @xcite . while both are non - linear , none of these formulations considers pipeline processing .", "we prove that even the linear mip formulation is np - hard .", "the scale of the previous results for solving mip optimizations have to be taken with a grain of salt , given the extensive enhancements to integer programming solvers over the past two decades . as explained in section  [ sec : heuristic : comparison - vertical ] , the proposed heuristic combines ideas from several classes of vertical partitioning algorithms , adapting their optimal behavior to raw data processing with partial loading .", "the top - down transaction - level algorithm given in  @xcite is the closest to the query coverage stage .", "while query coverage is a greedy algorithm , @xcite employs exhaustive search to find the solution .", "as the experimental results show , this is time - consuming .", "other top - down heuristics  @xcite consider the interaction between attributes across the queries in the workload .", "the partitioning is guided by a quantitative parameter that measures the strength of the interaction . in  @xcite", ", only the interaction between pairs of attributes is considered .", "the attribute usage frequency phase of the proposed heuristic treats each attribute individually , but only after query coverage is executed .", "the objective in  @xcite is to find a set of vertical partitions that are subsequently evaluated for index creation .", "since we select a single partitioning scheme , the process is less time - consuming .", "finally , the difference between the proposed heuristic and bottom - up algorithms  @xcite is that the latter can not guarantee that only two partitions are generated at the end .", "this is a requirement for raw data processing with partial loading .", "all these algorithms are offline .", "they are executed only once , before query processing , over a known workload .", "online vertical partitioning algorithms form a separate class . in  @xcite", ", the entire workload is known in advance . however , the order of the queries is fixed and the vertical partitioning evolves .", "another series of algorithms  @xcite operates over an unknown workload , given one query at a time .", "their goal is to gather evidence from the past workload in order to determine the optimal vertical partitioning at each query .", "in this paper , we study the problem of workload - driven raw data processing with partial loading .", "we model loading as binary vertical partitioning with full replication . based on this equivalence ,", "we provide a linear mixed integer programming optimization formulation that we prove to be np - hard and inapproximable .", "we design a two - stage heuristic that combines the concepts of query coverage and attribute usage frequency .", "the heuristic comes within close range of the optimal solution in a fraction of the time .", "we extend the optimization formulation and the heuristic to a restricted type of pipelined raw data processing . in the pipelined scenario ,", "data access and extraction are executed concurrently .", "we evaluate the performance of the heuristic and the accuracy of the optimization formulation over three real data formats ", "csv , fits , and json  processed with a state - of - the - art pipelined operator for raw data processing .", "the results confirm the superior performance of the proposed heuristic over related vertical partitioning algorithms and the accuracy of the formulation in capturing the execution details of a real operator .", "following the steps of database physical design , we envision several avenues to extend the proposed research in the future .", "we can move from the offline loading setting to online loading , where query processing and loading are intertwined .", "we can assume that the workload is known beforehand or it is given one query at a time .", "we can drop the strict requirement of atomic attribute loading and allow for portions  horizontal partitions  of an attribute to be loaded .", "finally , we can consider a multi - query processing environment in which raw data access and attribute extraction can be shared across several queries ."], "abstract_text": ["<S> traditional databases are not equipped with the adequate functionality to handle the volume and variety of `` big data '' . strict schema definition and data loading </S>", "<S> are prerequisites even for the most primitive query session . </S>", "<S> raw data processing has been proposed as a schema - on - demand alternative that provides instant access to the data . when loading is an option , it is driven exclusively by the current - running query , resulting in sub - optimal performance across a query workload . in this paper , we investigate the problem of workload - driven raw data processing with partial loading . </S>", "<S> we model loading as fully - replicated binary vertical partitioning . </S>", "<S> we provide a linear mixed integer programming optimization formulation that we prove to be np - hard . </S>", "<S> we design a two - stage heuristic that comes within close range of the optimal solution in a fraction of the time . </S>", "<S> we extend the optimization formulation and the heuristic to pipelined raw data processing , scenario in which data access and extraction are executed concurrently . </S>", "<S> we provide three case - studies over real data formats that confirm the accuracy of the model when implemented in a state - of - the - art pipelined operator for raw data processing . </S>"], "labels": null, "section_names": ["introduction", "preliminaries", "heuristic algorithm", "pipeline processing", "experimental evaluation", "related work", "conclusions and future work"], "sections": [["we are living in the age of `` big data '' , generally characterized by a series of `` vs '' .", "data are generated at an unprecedented _ volume _ by scientific instruments observing the macrocosm  @xcite and the microcosm  @xcite , or by humans connected around - the - clock to mobile platforms such as facebook and twitter .", "these data come in a _ variety _ of formats , ranging from delimited text to semi - structured json and multi - dimensional binaries such as fits .", "the volume and variety of `` big data '' pose serious problems to traditional database systems .", "before it is even possible to execute queries over a dataset , a relational schema has to be defined and data have to be loaded inside the database .", "schema definition imposes a strict structure on the data , which is expected to remain stable .", "however , this is rarely the case for rapidly evolving datasets represented using key - value and other semi - structured data formats , e.g. , json .", "data loading is a schema - driven process in which data are duplicated in the internal database representation to allow for efficient processing . even though storage is relatively cheap ,", "generating and storing multiple copies of the same data can easily become a bottleneck for massive datasets .", "moreover , it is quite often the case that many of the attributes in the schema are never used in queries .    motivated by the flexibility of nosql systems to access schema - less data and by the hadoop functionality to directly process data in any format ,", "we have recently witnessed a sustained effort to bring these capabilities inside relational database management systems ( rdbms ) . starting with version 9.3 ,", "postgresql includes support for json data type and corresponding functions .", "vertica flex zone and sinew  @xcite implement flex table and column reservoir , respectively , for storing key - value data serialized as maps in a blob column . in both systems", ", certain keys can be promoted to individual columns , in storage as well as in a dynamically evolving schema . with regards to directly processing raw data ,", "several query - driven extensions have been proposed to the loading and external table  @xcite mechanisms . instead of loading all the columns before querying , in adaptive partial loading  @xcite data", "are loaded only at query time , and only the attributes required by the query .", "this idea is further extended in invisible loading  @xcite , where only a fragment of the queried columns are loaded , and in nodb  @xcite , data vaults  @xcite , sds / q  @xcite , and raw  @xcite , where columns are loaded only in memory , but not into the database .", "scanraw  @xcite is a super - scalar pipeline operator that loads data speculatively , only when spare i / o resources are available .", "while these techniques enhance the rdbms flexibility to process schema - less raw data , they have several shortcomings , as the following examples show .", "* example 1 : twitter data . *", "the twitter api provides access to several objects in json format through a well - defined interface .", "the schema of the objects is , however , not well - defined , since it includes `` nullable '' attributes and nested objects .", "the state - of - the - art rdbms solution to process semi - structured json data  @xcite is to first load the objects as tuples in a blob column .", "essentially , this entails complete data duplication , even though many of the object attributes are never used .", "the internal representation consists of a map of key - values that is serialized / deserialized into / from persistent storage .", "the map can be directly queried from sql based on the keys , treated as virtual attributes . as an optimization , certain columns  chosen by the user or by the system based on appearance frequency  are promoted to physical status .", "the decision on which columns to materialize is only an heuristic , quite often sub - optimal .", "* example 2 : sloan digital sky survey ( sdss ) data .", "* sdss is a decade - long astronomy project having the goal to build a catalog of all the astrophysical objects in the observable universe .", "images of the sky are taken by a high - resolution telescope , typically in binary fits format .", "the catalog data summarize quantities measured from the images for every detected object .", "the catalog is stored as binary fits tables .", "additionally , the catalog data are loaded into an rdbms and made available through standard sql queries .", "the loading process replicates multi - terabyte data three times  in ascii csv and internal database representation  and it can take several days  if not weeks  @xcite . in order to evaluate the effectiveness of the loading , we extract a workload of 1 million sql queries executed over the sdss catalog in 2014 .", "the most frequent table in the workload is ` photoprimary ` , which appears in more than 70% of the queries . `", "photoprimary ` has 509 attributes , out of which only 74 are referenced in queries .", "this means that 435 attributes are replicated three times without ever being used  a significantly sub - optimal storage utilization", ".    * problem statement . * inspired by the above examples , in this paper , we study the raw data processing with partial loading problem .", "_ given a dataset in some raw format , a query workload , and a limited database storage budget , find what data to load in the database such that the overall workload execution time is minimized .", "_ this is a standard database optimization problem with bounded constraints , similar to vertical partitioning in physical database design  @xcite .", "however , while physical design investigates what non - overlapping partitions to build over internal database data , we focus on what data to load , i.e. , replicate , in a columnar database with support for multiple storage formats .", "existing solutions for loading and raw data processing are not adequate for our problem .", "complete loading not only requires a significant amount of storage and takes a prohibitively long time , but is also unnecessary for many workloads .", "pure raw data processing solutions  @xcite are not adequate either , because parsing semi - structured json data repeatedly is time - consuming . moreover ,", "accessing data from the database is clearly optimal in the case of workloads with tens of queries .", "the drawback of query - driven , adaptive loading methods  @xcite is that they are greedy , workload - agnostic .", "loading is decided based upon each query individually .", "it is easy to imagine a query order in which the first queries access non - frequent attributes that fill the storage budget , but have limited impact on the overall workload execution time .", "* contributions . * to the best of our knowledge , this is the first paper that incorporates query workload in raw data processing .", "this allows us to model raw data processing with partial loading as fully - replicated binary vertical partitioning .", "our contributions are guided by this equivalence .", "they can be summarized as follows :    we provide a linear mixed integer programming optimization formulation that we prove to be np - hard and inapproximable .", "we design a two - stage heuristic that combines the concepts of query coverage and attribute usage frequency .", "the heuristic comes within close range of the optimal solution in a fraction of the time .", "we extend the optimization formulation and the heuristic to a restricted type of pipelined raw data processing . in the pipelined scenario , data access and extraction", "are executed concurrently .", "we evaluate the performance of the heuristic and the accuracy of the optimization formulation over three real data formats ", "csv , fits , and json  processed with a state - of - the - art pipelined operator for raw data processing .", "the results confirm the superior performance of the proposed heuristic over related vertical partitioning algorithms and the accuracy of the formulation in capturing the execution details of a real operator .", "* outline . * the paper is organized as follows .", "raw data processing , the formal statement of the problem , and an illustrative example are introduced in the preliminaries ( section  [ sec : preliminaries ] ) .", "the mixed integer programming formulation and the proof that the formulation is np - hard are given in section  [ sec : mip ] .", "the proposed heuristic is presented in detail in section  [ sec : heuristic ] .", "the extension to pipelined raw data processing is discussed in section  [ sec : pipeline ] .", "extensive experiments that evaluate the heuristic and verify the accuracy of the optimization formulation over three real data formats are presented in section  [ sec : experiments ] .", "related work on vertical partitioning and raw data processing is briefly discussed in section  [ sec : rel - work ] , while section  [ sec : conclusions ] concludes the paper ."], ["in this section , we introduce query processing over raw data . then , we provide a formal problem statement and an illustrative example .", "query processing over raw data is depicted in figure  [ fig : scanraw ] .", "the input to the process is a raw file from a non - volatile storage device , e.g. , disk or ssd , a schema that can include optional attributes , a procedure to extract tuples with the given schema from the raw file , and a driver query .", "the output is a tuple representation that can be processed by the query engine and , possibly , is materialized ( i.e. , loaded ) on the same storage device . in the ` read ` stage , data are read from the original raw file , page - by - page , using the file system s functionality . without additional information about the structure or the content  stored inside the file or in some external structure ", "the entire file has to be read the first time it is accessed . `", "extract ` transforms tuples ", "one per line  from raw format into the processing representation , based on the schema provided and using the extraction procedure given as input to the process .", "there are two stages in ` extract``tokenize ` and ` parse ` .", "[ fig : scanraw ]    ` tokenize ` identifies the schema attributes and outputs a vector containing the starting position for every attribute in the tuple  or a subset , if the driver query does not access all the attributes . in ` parse ` , attributes are converted from raw format to the corresponding binary type and mapped to the processing representation of the tuple ", "the record in a row - store , or the array in column - stores , respectively .", "multiple records or column arrays are grouped into a chunk  the unit of processing . at the end of `", "extract ` , data are loaded in memory and ready for query processing .", "multiple paths can be taken at this point . in external tables", "@xcite , data are passed to the query engine and discarded afterwards . in nodb", "@xcite and in - memory databases  @xcite , data are kept in memory for subsequent processing . in standard database loading  @xcite", ", data are first written to the database and only then query processing starts .", "scanraw  @xcite invokes ` write ` concurrently with the query execution , only when spare i / o - bandwidth is available .", "the interaction between ` read ` and ` write ` is carefully scheduled in order to minimize interference .", "consider a relational schema @xmath0 and an instantiation of it that contains @xmath1 tuples .", "semi - structured json data can be mapped to the relational model by linearizing nested constructs  @xcite . in order to execute queries over @xmath2", ", tuples have to be read in memory and converted from the storage format into the processing representation .", "two timing components correspond to this process .", "@xmath3 is the time to read data from storage into memory .", "@xmath3 can be computed straightforwardly for a given schema and storage bandwidth @xmath4 .", "a constraint specific to raw file processing  and row - store databases , for that matter  is that all the attributes are read in a query  even when not required .", "@xmath5 is the second timing component .", "it corresponds to the conversion time . for every attribute @xmath6 in the schema ,", "the conversion is characterized by two parameters , defined at tuple level .", "the _ tokenizing time @xmath7 _ is the time to locate the attribute in a tuple in storage format .", "the _ parsing time @xmath8 _ is the time to convert the attribute from storage format into processing representation .", "a limited amount of storage @xmath9 is available for storing data converted into the processing representation .", "this eliminates the conversion and replaces it with an i / o process that operates at column level  only complete columns can be saved in the processing format .", "the time to read an attribute @xmath6 in processing representation , @xmath10 , can be determined when the type of the attribute and @xmath1 are known .", ".query access pattern to raw data attributes .", "[ cols=\"<,^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ tbl : mip : params ]    while written as a single constraint , @xmath11 decomposes into three separate constraints  one corresponding to each `` @xmath12 '' operator  for a total of @xmath13 constraints .", "@xmath14 is a reduced form of @xmath11 , applicable to query processing .", "the largest number of constraints , i.e. , @xmath15 , in the mip formulation are of type @xmath16 .", "they enforce that it is not possible to tokenize an attribute in a tuple without tokenizing all the preceding schema attributes in the same tuple .", "@xmath16 applies strictly to raw files without direct access to individual attributes .", "constraint @xmath17 guarantees that every attribute accessed in a query is either extracted from the raw file or read from the processing representation .", "there are @xmath18 binary 0/1 variables in the linear mip formulation , where @xmath19 is the number of queries in the workload and @xmath20 is the number of attributes in the schema .", "solving the mip directly is , thus , impractical for workloads with tens of queries over schemas with hundreds of attributes , unless the number of variables in the search space can be reduced .", "we prove that this is not possible by providing a reduction from a well - known np - hard problem to a restricted instance of the mip formulation .", "moreover , we also show that no approximation exists .", "[ def : k - elem - cover ] given a set of @xmath20 elements @xmath21 , @xmath19 subsets @xmath22 of @xmath2 , such that @xmath23 , and a value @xmath24 , the objective in the k - element cover problem is to find a size @xmath24 subset @xmath25 of @xmath2 that covers the largest number of subsets @xmath26 , i.e. , @xmath27 , @xmath28 .", "for the example in table  [ tbl : prelim : problem : example ] , @xmath29 is the single 2-element cover solution ( covering @xmath30 ) .", "while many 3-element cover solutions exist , they all cover only one query .", "the k - element cover problem is a restricted instance of the mip formulation , in which parameters @xmath7 , @xmath8 , and the loading and reading time to / from database are set to zero , i.e. , @xmath31 , while the raw data reading time is set to one , i.e. , @xmath32 .", "the objective function is reduced to counting how many times raw data have to be accessed .", "the bounding constraint limits the number of attributes that can be loaded , i.e. , @xmath33 , while the functional constraints determine the value of the other variables .", "the optimal solution is given by the configuration that minimizes the number of queries accessing raw data .", "a query does not access raw data when the @xmath34 variables corresponding to its attributes are all set to one .", "when the entire workload is considered , this equates to finding those attributes that cover the largest number of queries , i.e. , finding the k - attribute cover of the workload .", "given this reduction , it suffices to prove that k - element cover is np - hard for the mip formulation to have only exponential - time solutions .", "we provide a reduction to the well - known minimum k - set coverage problem  @xcite that proves k - element cover is np - hard .    [ def : min - k - set - cover ] given a set of @xmath20 elements @xmath21 , @xmath19 subsets @xmath22 of @xmath2 , such that @xmath23 , and a value @xmath24 , the objective in the minimum k - set coverage problem is to choose @xmath24 sets @xmath35 from @xmath36 whose union has the smallest cardinality , i.e. , @xmath37 .", "set @xmath21 and @xmath19 subsets @xmath22 of @xmath2 ; number @xmath38 of sets @xmath26 to choose in minimum set coverage minimum number @xmath24 of elements from @xmath2 covered by choosing @xmath38 subsets from @xmath36 @xmath39 = * _ k - element cover_*(@xmath36 , @xmath40 ) * if * @xmath41 * then * * return * @xmath40    algorithm  [ alg : reduction - cover - coverage ] gives a reduction from k - element cover to minimum k - set coverage .", "the solution to minimum k - set coverage is obtained by invoking k - element cover for any number of elements in @xmath2 and returning the smallest such number for which the solution to k - element cover contains at least @xmath38 subsets of @xmath36 .", "since we know that minimum k - set coverage is np - hard  @xcite and the solution is obtained by solving k - element cover , it implies that k - element cover can not be any simpler , i.e. , k - element cover is also np - hard .", "the following theorem formalizes this argument .", "[ thm : reduction ] the reduction from k - element cover to minimum k - set coverage given in algorithm  [ alg : reduction - cover - coverage ] is correct and complete .    * _ proof . _", "* in order to prove the theorem , we have to show that if the answer to the minimum k - set coverage problem is @xmath42 , algorithm  [ alg : reduction - cover - coverage ] returns @xmath42 and if algorithm  [ alg : reduction - cover - coverage ] returns @xmath42 , the answer to minimum k - set coverage problem is @xmath42 .", "we start with the first implication .", "let the optimal solution to the minimum k - set coverage problem be @xmath43 and @xmath44 be the set of elements in @xmath2 covered by @xmath45 , where @xmath46 and @xmath47 .", "suppose there exists a subset @xmath48 of @xmath2 and let the sets covered by @xmath48 be @xmath49 , where @xmath50 and @xmath51 .", "when @xmath51 , the union of any @xmath38 sets in @xmath49 is no larger than @xmath52 , which is smaller than @xmath42 .", "we get a contradiction .", "thus , there is no subset of @xmath2 whose size is smaller than @xmath42 that covers at least @xmath38 sets in @xmath36 . as a result ,", "algorithm  [ alg : reduction - cover - coverage ] does not return when @xmath53 . by the problem definition , we can use @xmath54 to cover at least @xmath38 sets @xmath45 .", "therefore , algorithm  [ alg : reduction - cover - coverage ] returns when @xmath55 .    for the second implication ,", "let the elements chosen by the _ * k - element cover * _ function be @xmath54 and @xmath45 be the sets covered by @xmath54 , where @xmath47 .", "suppose the optimal solution to minimum k - set coverage is @xmath49 , which covers elements @xmath48 , where @xmath56 and @xmath57 .", "then , @xmath49 is the answer to the function @xmath58 . in this case ,", "algorithm  [ alg : reduction - cover - coverage ] returns @xmath52 before @xmath42 .", "we have a contradiction .", "therefore , we know that the optimal solution to minimum k - set coverage can not be smaller than @xmath59 .", "we can choose any @xmath38 sets in @xmath45 as the solution to the minimum k - set coverage problem .", "the union of the sets in @xmath45 is not larger than @xmath59 .", "therefore , the second implication holds .    based on these two implications ,", "we conclude that the reduction is correct .", "the fact that algorithm  [ alg : reduction - cover - coverage ] has linear time complexity @xmath60 guarantees the completeness of the reduction .", "@xmath61    [ thm :", "reduction : corolar ] the mip formulation is np - hard and can not be approximated unless np - complete problems can be solved in randomized sub - exponential time .", "the np - hardness is a direct consequence of the reduction to the k - element cover problem and theorem  [ thm : reduction ] .", "in addition ,  @xcite and  @xcite prove that minimum k - set coverage can not be approximated within an absolute error of @xmath62 , for any @xmath63 , unless p = np .", "consequently , the mip formulation can not be approximated ."], ["in this section , we propose a novel heuristic algorithm for raw data processing with partial loading that has as a starting point a greedy solution for the k - element cover problem .", "the algorithm also includes elements from vertical partitioning  a connection we establish in the paper .", "the central idea is to combine _ query coverage _ with _ attribute usage frequency _ in order to determine the best attributes to load . at a high level", ", query coverage aims at reducing the number of queries that require access to the raw data , while usage frequency aims at eliminating the repetitive extraction of the heavily - used attributes .", "our algorithm reconciles between these two conflicting criteria by optimally dividing the available loading budget across them , based on the format of the raw data and the query workload .", "the solution found by the algorithm is guaranteed to be as good as the solution corresponding to each criterion , considered separately .    in the following ,", "we make the connection with vertical partitioning clear .", "then , we present separate algorithms based on query coverage and attribute usage frequency .", "these algorithms are combined into the proposed heuristic algorithm for raw data processing with partial loading .", "we conclude the section with a detailed comparison between the proposed heuristic and algorithms designed specifically for vertical partitioning .      vertical partitioning  @xcite of a relational schema", "@xmath64 splits the schema into multiple schemas  possibly overlapping  each containing a subset of the columns in @xmath2 . for example", ", @xmath65 is the atomic non - overlapping vertical partitioning of @xmath2 in which each column is associated with a separate partition .", "tuple integrity can be maintained either by sorting all the partitions in the same order , i.e. , positional equivalence , or by pre - pending a tuple identifier ( @xmath66 ) column to every partition .", "vertical partitioning reduces the amount of data that have to be accessed by queries that operate on a small subset of columns since only the required columns have to be scanned  when they form a partition .", "however , tuple reconstruction  @xcite can become problematic when integrity is enforced through @xmath66 values because of joins between partitions .", "this interplay between having partitions that contain only the required columns and access confined to a minimum number of partitions , i.e. , a minimum number of joins , is the objective function to minimize in vertical partitioning .", "the process is always workload - driven .", "raw data processing with partial loading can be mapped to _ fully - replicated binary vertical partitioning _ as follows .", "the complete raw data containing all the attributes in schema @xmath2 represent the _ raw partition_. the second partition  _ loaded partition _ ", "is given by the attributes loaded in processing representation .", "these are a subset of the attributes in @xmath2 .", "the storage allocated to the loaded partition is bounded .", "the asymmetric nature of the two partitions differentiates raw data processing from standard vertical partitioning .", "the raw partition provides access to all the attributes , at the cost of tokenizing and parsing .", "the loaded partition provides faster access to a reduced set of attributes . in vertical partitioning , all the partitions are equivalent .", "while having only two partitions may be regarded as a simplification , all the top - down algorithms we are aware of  @xcite apply binary splits recursively in order to find the optimal partitions .", "the structure of raw data processing with partial loading limits the number of splits to one .", "workload @xmath67 ; storage budget @xmath9 set of attributes @xmath68 to be loaded in processing representation    [ a2:l1 ] @xmath69 ; @xmath70 [ a2:l2 ] [ a2:l3 ] @xmath71 [ a2:l4 ] * if * @xmath72 * then * * break * [ a2:l5 ] @xmath73 [ a2:l6 ] @xmath74 [ a2:l8 ] @xmath75    a query that can be processed without accessing the raw data is said to be _ covered_. in other words , all the attributes accessed by the query are loaded in processing representation .", "these are the queries whose attributes are contained in the solution to the k - element cover problem .", "intuitively , increasing the number of covered queries results in a reduction to the objective function , i.e. , total query workload execution time , since only the required attributes are accessed .", "moreover , access to the raw data and conversion are completely eliminated .", "however , given a limited storage budget , it is computationally infeasible to find the optimal set of attributes to load  the k - element cover problem is np - hard and can not be approximated ( corollary  [ thm : reduction : corolar ] ) .", "thus , heuristic algorithms are required .", "we design a standard greedy algorithm for the k - element cover problem that maximizes the number of covered queries within a limited storage budget .", "the pseudo - code is given in algorithm  [ alg : query - coverage ] .", "the solution @xmath76 and the covered queries @xmath77 are initialized with the empty set in line  [ a2:l1 ] . as long as the storage budget is not exhausted ( line  [ a2:l2 ] ) and the value of the objective function @xmath78 decreases ( line  [ a2:l4 ] ) , a query to be covered is selected at each step of the algorithm ( line  [ a2:l3 ] ) .", "the criterion we use for selection is the reduction in the cost function normalized by the storage budget , i.e. , we select the query that provides the largest reduction in cost , while using the smallest storage .", "this criterion gives preference to queries that access a smaller number of attributes and is consistent with our idea of maximizing the number of covered queries .", "an alternative selection criterion is to drop the cost function and select the query that requires the least number of attributes to be added to the solution .", "the algorithm is guaranteed to stop when no storage budget is available or all the queries are covered .", "* example .", "* we illustrate how the _ * query coverage * _ algorithm works on the workload in table  [ tbl : prelim : problem : example ] . without loss of generality ,", "assume that all the attributes have the same size and the time to access raw data is considerably larger than the extraction time and the time to read data from processing representation , respectively .", "these is a common situation in practice , specific to delimited text file formats , e.g. , csv .", "let the storage budget be large enough to load three attributes , i.e. , @xmath79 .", "in the first step , only queries @xmath30 , @xmath80 , and @xmath81 are considered for coverage in line  [ a2:l3 ] , due to the storage constraint .", "while the same objective function value is obtained for each query , @xmath30 is selected for loading because it provides the largest normalized reduction , i.e. , @xmath82 .", "the other two queries have a normalized reduction of @xmath83 , where @xmath3 is the time to read the raw data . in the second step of the algorithm , @xmath84 .", "this also turns to be the last step since no other query can be covered in the given storage budget .", "notice that , although @xmath80 and @xmath81 make better use of the budget , the overall objective function value is hardly different , as long as reading raw data is the dominating cost component .", "workload @xmath22 of @xmath2 ; storage budget @xmath9 ; set of loaded attributes @xmath85 set of attributes @xmath86 to be loaded in processing representation    @xmath87 [ a3:l2 ] [ a3:l3 ] @xmath88", "@xmath89 @xmath75    the query coverage strategy operates at query granularity .", "an attribute is always considered as part of the subset of attributes accessed by the query . it is never considered individually .", "this is problematic for at least two reasons .", "first , the storage budget can be under - utilized , since a situation where storage is available but no query can be covered , can appear during execution .", "second , a frequently - used attribute or an attribute with a time - consuming extraction may not get loaded if , for example , is part of only long queries .", "the assumption that accessing raw data is the dominant cost factor does not hold in this case .", "we address these deficiencies of the query coverage strategy by introducing a simple greedy algorithm that handles attributes individually . as the name implies", ", the intuition behind the attribute usage frequency algorithm is to load those attributes that appear frequently in queries .", "the rationale is to eliminate the extraction stages that incur the largest cost in the objective function .", "the pseudo - code for the attribute usage frequency strategy is given in algorithm  [ alg : freq ] .", "in addition to the workload and the storage budget , a set of attributes already loaded in the processing representation is passed as argument . at each step", "( line  [ a3:l3 ] ) , the attribute that generates the largest decrease in the objective function is loaded . in this case , the algorithm stops only when the entire storage budget is exhausted ( line  [ a3:l2 ] ) .", "* example .", "* we illustrate how the _ * attribute usage frequency * _ algorithm works by continuing the example started in the query coverage section .", "recall that only two attributes @xmath90 out of a total of three are loaded .", "@xmath91 is chosen as the remaining attribute to be loaded since it appears in five queries , the largest number between unloaded attributes .", "given that all the attributes have the same size and there is no cost for tuple reconstruction , @xmath92 is the optimal loading configuration for the example in table  [ tbl : prelim : problem : example ] .", "workload @xmath67 ; storage budget @xmath9 set of attributes @xmath68 to be loaded in processing representation    @xmath93 [ a4:l2 ] [ a4:l3 ] @xmath94 [ a4:l4 ] @xmath95 @xmath96 @xmath97 @xmath98", "@xmath99 @xmath100    the heuristic algorithm for raw data processing with partial loading unifies the query coverage and attribute usage frequency algorithms . the pseudo - code is depicted in algorithm  [ alg : combined ] . given a storage budget @xmath9 , _ * query coverage * _", "is invoked first ( line  [ a4:l3 ] ) . _", "* attribute usage frequency * _ ( line  [ a4:l4 ] ) takes as input the result produced by _ * query coverage * _ and the unused budget @xmath101 . instead of invoking these algorithms only once , with the given storage budget @xmath9", ", we consider a series of allocations .", "@xmath9 is divided in @xmath102 increments ( line  [ a4:l2 ] ) .", "each algorithm is assigned anywhere from @xmath103 to @xmath9 storage , in @xmath102 increments .", "a solution is computed for each of these configurations .", "the heuristic algorithm returns the solution with the minimum objective .", "the increment @xmath102 controls the complexity of the algorithm .", "specifically , the smaller @xmath102 is , the larger the number of invocations to the component algorithms .", "notice , though , that as long as @xmath104 remains constant with respect to @xmath19 and @xmath20 , the complexity of the heuristic remains @xmath105 .    the rationale for using several budget allocations between query coverage and attribute usage frequency lies in the limited view they take for solving the optimization formulation .", "query coverage assumes that the access to the raw data is the most expensive cost component , i.e. , processing is i / o - bound , while attribute usage frequency focuses exclusively on the extraction , i.e. , processing is cpu - bound .", "however , the actual processing is heavily - dependent on the format of the data and the characteristics of the system .", "for example , binary formats , e.g. , fits , do not require extraction , while hierarchical text formats , e.g. , json , require complex parsing . moreover , the extraction complexity varies largely across data types .", "the proposed heuristic algorithm recognizes these impediments and solves many instances of the optimization formulation in order to identify the optimal solution .      as discussed in section  [ sec : heuristic : vertical - part ] , raw data processing with partial loading is a special case of vertical partitioning  binary vertical partitioning with full replication .", "however , there is a fundamental difference between the problem addressed in this paper and standard vertical partitioning .", "the amount of storage allocated to partitions is not a parameter in vertical partitioning because it is constant and independent of the layout  all the partitions use the same storage , plus - minus metadata .", "the bounded storage constraint is what makes raw data processing with partial loading a considerably more complicated problem , to which standard vertical partitioning algorithms are not directly applicable .", "a comprehensive comparison of vertical partitioning methods is given in  @xcite .", "with few exceptions  @xcite , vertical partitioning algorithms consider only the non - replicated case . when replication is considered , it is only partial replication .", "the bounded scenario  limited storage budget for replicated attributes  is discussed only in  @xcite . at a high level", ", vertical partitioning algorithms can be classified along several axes  @xcite .", "we discuss the two most relevant axes for the proposed heuristic .", "based on the direction in which partitions are built , we have top - down and bottom - up algorithms .", "a top - down algorithm  @xcite starts with the complete schema and , at each step , splits it into two partitioned schemas .", "the process is repeated recursively for each resulting schema .", "a bottom - up algorithm  @xcite starts with a series of schemas , e.g. , one for each attribute or one for each subset of attributes accessed in a query , and , at each step , merges a pair of schemas into a new single schema . in both cases ,", "the process stops when the objective function can not be improved further .", "a second classification axis is given by the granularity at which the algorithm works .", "an attribute - level algorithm  @xcite considers the attributes independent of the queries in which they appear .", "the interaction between attributes across queries still plays a significant role , though . a query or transaction - level algorithm  @xcite works at query granularity .", "a partition contains either all or none of the attributes accessed in a query .", "based on the classification of vertical partitioning algorithms , the proposed heuristic qualifies primarily as a top - down query - level attribute - level algorithm .", "however , the recursion is only one - level deep , with the loaded partition at the bottom .", "the partitioning process consists of multiple steps , though . at each step , a new partition extracted from the raw data", "is merged into the loaded partition  similar to a bottom - up algorithm .", "the query coverage algorithm gives the query granularity characteristic to the proposed heuristic , while attribute usage frequency provides the attribute - level property .", "overall , the proposed heuristic combines ideas from several classes of vertical partitioning algorithms , adapting their optimal behavior to raw data processing with partial loading . an experimental comparison with specific algorithms is presented in the experiments ( section  [ sec : experiments ] ) and a discussion on their differences in the related work ( section  [ sec : rel - work ] ) ."], ["in this section , we discuss on the feasibility of mip optimization in the case of pipelined raw data processing with partial loading .", "we consider a super - scalar pipeline architecture in which raw data access and the extraction stages  tokenize and parse  can be executed concurrently by overlapping disk i / o and cpu processing .", "this architecture is introduced in  @xcite , where it is shown that , with a sufficiently large number of threads , raw data processing is an i / o - bound task .", "loading and accessing data from the processing representation are not considered as part of the pipeline since they can not be overlapped with raw data access due to i / o interference .", "we show that , in general , pipelined raw data processing with partial loading can not be modeled as a linear mip . however , we provide a linear formulation for a scenario that is common in practice , e.g. , binary fits and json format . in these cases , tokenization is atomic .", "it is executed for all or none of the attributes .", "this lets parsing as the single variable in the extraction stage .", "the mip formulation can not be solved efficiently , due to the large number of variables and constraints  much larger than in the sequential formulation .", "we handle this problem by applying a simple modification to the heuristic introduced in section  [ sec : heuristic ] that makes the algorithm feasible for pipelined processing .", "since raw data access and extraction are executed concurrently , the objective function corresponding to pipelined query processing has to include only the maximum of the two : @xmath106 this is the only modification to the mip formulation for sequential processing given in section  [ sec : mip ] .", "since the @xmath107 function is non - linear , solving the modified formulation becomes impossible with standard mip solvers , e.g. , cplex , which work only for linear problems .", "the only alternative is to eliminate the @xmath107 function and linearize the objective .", "however , this can not be achieved in the general case .", "it can be achieved , though , for specific types of raw data  binary formats that do not require tokenization , e.g. , fits , and text formats that require complete tuple tokenization , e.g. , json . as discussed in the introduction", ", these formats are used extensively in practice .", "queries over raw data can be classified into two categories based on the pipelined objective function in eq .", "( [ eq : obj : pipeline : max ] ) . in i / o - bound queries ,", "the time to access raw data is the dominant factor , i.e. , @xmath107 returns the first argument . in cpu - bound queries , the extraction time dominates , i.e.", ", @xmath107 returns the second argument .", "if the category of the query is known , @xmath107 can be immediately replaced with the correct argument and the mip formulation becomes linear .", "our approach is to incorporate the category of the query in the optimization as 0/1 variables . for each query @xmath40 , there is a variable for cpu - bound ( @xmath108 ) and one for io - bound ( @xmath109 ) . only one of them can take value 1 .", "moreover , these variables have to be paired with the variables for raw data access and extraction , respectively .", "variables of the form @xmath110 , @xmath111 , and @xmath112 correspond to the variables in table  [ tbl : mip : vars ] , in the case of a cpu - bound query . variables", "@xmath113 , @xmath114 , and @xmath115 are for the io - bound case , respectively .    with these variables , we can define the functional * constraints * for pipelined raw data processing : @xmath116 constraint @xmath117 forces a query to be either cpu - bound or io - bound .", "constraints @xmath118 tie the new family of cpu / io variables to their originals in the serial formulation . for example , the raw data is accessed in a cpu / io query only if it is accessed in the stand - alone query .", "the same holds for tokenizing / parsing a column @xmath119 in query @xmath40 .", "constraints @xmath120 and @xmath121 , respectively , tie the value of the cpu / io variables to the value of the corresponding query variable .", "for example , only when a query @xmath40 is cpu - bound , it makes sense for @xmath111 and @xmath112 to be allowed to take value 1 .", "if the query is io - bound , @xmath114 and @xmath115 can be set , but not @xmath111 and @xmath112 .    at this point", ", we have still not defined when a query is cpu - bound and when is io - bound .", "this depends on the relationship between the time to access the raw data and the time to extract the referenced attributes .", "while the parsing time is completely determined by the attributes accessed in the query , the tokenizing time is problematic since it depends not only on the attributes , but also on their position in the schema .", "for example , in the sdss ` photoprimary ` table containing 509 attributes , the time to tokenize the @xmath122 attribute is considerably smaller than the time to tokenize the @xmath123 attribute .", "moreover , there is no linear relationship between the position in the schema and the tokenize time . for this reason", ", we can not distinguish between cpu- and io - bound queries in the general case .", "however , if there is no tokenization  the case for binary formats such as fits  or the tokenization involves all the attributes in the schema  the case for hierarchical json format  we can define a threshold @xmath124 that allows us to classify queries .", "@xmath125 is given by the ratio between the time to access raw data less the constant tokenize time and the average time to parse an attribute .", "intuitively , @xmath125 gives the number of attributes that can be parsed in the time required to access the raw data .", "if a query has to parse more than @xmath125 attributes , it is cpu - bound .", "otherwise , it is io - bound .", "the threshold * constraints * @xmath126 and @xmath127 make these definitions formal : @xmath128 for the atomic tokenization to hold , constraint @xmath16 in the serial formulation has to be replaced with @xmath129 .", "the complete pipelined mip includes the constraints in the serial formulation ( eq .  ( [ eq : mip : formulation ] ) ) and the constraints @xmath130 .", "the linear * objective function * corresponding to query processing is re - written using the newly introduced variables as follows : @xmath131      since the number of variables and constraints increases with respect to the serial mip formulation , the task of a direct linear solver becomes even harder .", "it is also important to notice that the problem remains np - hard and can not be approximated since the reduction to the k - element cover still applies . in these conditions , heuristic algorithms are the only solution .", "we design a simple modification to the heuristic introduced in section  [ sec : heuristic ] specifically targeted at pipelined raw data processing .    given a configuration of attributes loaded in processing representation , the category of a query can be determined by evaluating the objective function .", "what is more important , though , is that the evolution of the query can be traced precisely as attributes get loaded .", "an i / o - bound query remains i / o - bound as long as not all of its corresponding attributes are loaded . at that point , it is not considered by the heuristic anymore .", "a cpu - bound query has the potential to become i / o - bound if the attributes that dominate the extraction get loaded .", "once i / o - bound , a query can not reverse to the cpu - bound state .", "thus , the only transitions a query can make are from cpu - bound to i / o - bound , and to loaded from there . if an io - bound query is not covered in the _ * query coverage * _ section of the heuristic , its contribution to the objective function can not be improved since it can not be completely covered by _ * attribute usage frequency*_. based on this observation , the only strategy to reduce the cost is to select attributes that appear in cpu - bound queries .", "we enforce this by limiting the selection of the attributes considered in line  [ a3:l3 ] of _ * attribute usage frequency * _ to those attributes that appear in at least one cpu - bound query ."], ["the objective of the experimental evaluation is to investigate the accuracy and performance of the proposed heuristic across a variety of datasets and workloads executed sequentially and pipelined . to this end", ", we explore the accuracy of predicting the execution time for complex workloads over three raw data formats ", "csv , fits , and json .", "additionally , the sensitivity of the heuristic is quantified with respect to the various configuration parameters .", "specifically , the experiments we design are targeted to answer the following questions :    what is the impact of each stage in the overall behavior of the heuristic ?", "how accurate is the heuristic with respect to the optimal solution ?", "with respect to vertical partitioning algorithms ?    how much faster is the heuristic compared to directly solving the mip formulation ? compared to other vertical partitioning algorithms ?    can the heuristic exploit pipeline processing in partitioning ?", "do the mip model and the heuristic reflect reality across a variety of raw data formats ?", "* implementation .", "* we implement the heuristic and all the other algorithms referenced in the paper in ` c++ ` .", "we follow the description and the parameter settings given in the original paper as closely as possible . the loading and query execution plans returned by the optimization routine", "are executed with the scanraw  @xcite operator for raw data processing .", "scanraw supports serial and pipelined execution .", "the real results returned by scanraw are used as reference .", "we use ibm cplex 12.6.1 to implement and solve the mip formulations .", "cplex supports parallel processing .", "the number of threads used in the optimization is determined dynamically at runtime .", "* we execute the experiments on a standard server with 2 amd opteron 6128 series 8-core processors ( 64 bit )  16 cores ", "64 gb of memory , and four 2 tb 7200 rpm sas hard - drives configured raid-0 in software .", "each processor has 12 mb l3 cache while each core has 128 kb l1 and 512 kb l2 local caches .", "the storage system supports 240 , 436 and 1600 mb / second minimum , average , and maximum read rates , respectively  based on the ubuntu disk utility .", "the cached and buffered read rates are 3 gb / second and 565 mb / second , respectively .", "ubuntu 14.04.2 smp @xmath132-bit with linux kernel 3.13.0 - 43 is the operating system .", "* methodology .", "* we perform all experiments at least 3 times and report the average value as the result .", "we enforce data to be read from disk by cleaning the file system buffers before the execution of every query in the workload .", "this is necessary in order to maintain the validity of the modeling parameters .", "* we use three types of real data formats in our experiments ", "csv , fits , and json .", "the csv and fits data are downloaded from the sdss project using the cas tool .", "they correspond to the complete schema of the ` photoprimary ` table , which contains 509 attributes .", "the csv and fits data are identical .", "only their representation is different .", "csv is delimited text , while fits is in binary format .", "there are 5 million rows in each of these files .", "csv is 22 gb in size , while fits is only 19 gb . json is a lightweight semi - structured key - value data format .", "the twitter api provides access to user tweets in this format .", "tweets have a hierarchical structure that can be flattened into a relational schema .", "we acquire 5,420,000 tweets by making requests to the twitter api .", "there are at most 155 attributes in a tweet .", "the size of the data is 19 gb .", "* workloads . *", "we extract a real workload of 1 million sql queries executed over the sdss catalog in 2014 . out of these , we select the most popular 100 queries over table ` photoprimary ` and their corresponding frequency . these represent approximately 70% of the 1 million queries .", "we use these 100 queries as our workload in the experiments over csv and fits data .", "the weight of a query is given by its relative frequency .", "furthermore , we extract a subset of the 32 most popular queries and generate a second workload .", "the maximum number of attributes referenced in both workloads is 74 .", "we create the workload for the tweets data synthetically since we can not find a real workload that accesses more than a dozen of attributes .", "the number of attributes in a query is sampled from a normal distribution centered at 20 and having a standard deviation of 20 .", "the attributes in a query are randomly selected out of all the attributes in the schema or , alternatively , out of a subset of the attributes .", "the smaller the subset , the more attributes are not accessed in any query .", "the same weight is assigned to all the queries in the workload .      in this set of experiments ,", "we evaluate the sensitivity of the proposed heuristic with respect to the parameters of the problem , specifically , the number of queries in the workload and the storage budget .", "we study the impact each stage in the heuristic has on the overall accuracy .", "we measure the error incurred by the heuristic with respect to the optimal solution computed by cplex and the decrease in execution time .", "we also compare against several top - down vertical partitioning algorithms .", "we use the sdss data and workload in our evaluation .", "we consider the following vertical partitioning algorithms in our comparison : agrawal  @xcite , navathe  @xcite , and chu  @xcite . the _ agrawal algorithm _", "@xcite is a pruning - based algorithm in which all the possible column groups are generated based on the attribute co - occurrence in the query workload . for each column group ,", "an interestingness measure is computed . since there is an exponential number of such column groups , only the `` interesting '' ones", "are considered as possible partitions .", "a column group is interesting if the interestingness measure , i.e. , cg - cost , is larger than a specified threshold . the interesting column groups are further ranked based on another measure , i.e. , vp - confidence , which quantifies the frequency with which the entire column group is referenced in queries .", "the attributes to load are determined by selecting column groups in the order given by vp - confidence , as long as the storage budget is not filled .", "while many strategies can be envisioned , our implementation is greedy .", "it chooses those attributes in a column group that are not already loaded and that minimize the objective function , one - at - a - time .", "the agrawal algorithm has exponential complexity @xmath133 since this is the number of potential column groups .", "this can be reduced by selecting the cg - cost threshold intelligently . however , this results in a corresponding accuracy decrease .", "trojan layouts  @xcite are a newer version of the agrawal algorithm in which a different interestingness measure is defined and the selection of the partitions from the interesting column groups is done using an optimal exponential algorithm . since the authors admit that `` finding the right trojan layouts for scientific data sets ( having hundreds of attributes ) , like sdss , becomes a difficult task to achieve ''  @xcite , we use the original agrawal algorithm in our implementation .    the _ navathe algorithm _", "@xcite starts with an affinity matrix that quantifies the frequency with which any pair of two attributes appear together in a query .", "the main step of the algorithm consists in finding a permutation of the rows and columns that groups attributes that co - occur together in queries .", "this extends upon the affinity of two attributes to a larger number of attributes .", "while finding the optimal permutation is exponential in the number of attributes , a quadratic greedy algorithm that starts with two random attributes and then chooses the best attribute to add and the best position , one - at - a - time , is given .", "these are computed based on a benefit function that is independent of the objective .", "the attributes are ordered on the benefit function in the resulting matrix .", "the final step of the algorithm consists in finding a split point along the attribute axis that generates two partitions with minimum objective function value across the query workload .", "an additional condition that we have to consider in our implementation is the storage budget  we find the optimal partition that also fits in the available storage space .    the _ chu algorithm _  @xcite considers only those partitions supported by at least one query in the workload , i.e. , a column group can be a partition only if it is accessed entirely by a query .", "moreover , a column group supported by a query is never split into smaller sub - parts .", "the algorithm enumerates all the column groups supported by any number of queries in the workload  from a single query", "to all the queries  and chooses the partition that minimizes the objective function .", "the remaining attributes  not supported by the query  form the second partition .", "this algorithm is exponential in the number of queries in the workload @xmath134 .", "the solution proposed in  @xcite is to limit the number number of query combinations to a relatively small constant , e.g. , 5 . in our implementation", ", we let the algorithm run for a limited amount of time , e.g. , one hour , and report the best result at that time  if the algorithm has not finished by that time .    *", "heuristic stage analysis .", "* figure  [ fig : combined_compare ] and  [ fig : error ] depict the impact each stage in the heuristic  query * coverage * and attribute usage * frequency *  has on the accuracy , when taken separately and together , i.e. , * heuristic*. we measure both the absolute value ( figure  [ fig : combined_compare ] ) and the relative error with respect to the optimal value ( figure  [ fig : error ] ) .", "we depict these values as a function of the storage budget , given as the number of attributes that can be loaded .", "we use the 32 queries workload .", "as expected , when the budget increases , the objective decreases . in terms of relative error , though , the heuristic is more accurate at the extremes  small budget or large budget . when the budget is medium , the error is the highest", "the reason for this behavior is that , at the extremes , the number of choices for loading is considerably smaller and the heuristic finds a good enough solution .", "when the storage budget is medium , there are many loading choices and the heuristic makes only local optimal decisions that do not necessarily add - up to a good global solution .", "the two - stage heuristic has better accuracy than each stage taken separately .", "this is more clear in the case of the difficult problems with medium budget . between the two separate stages ,", "none of them is dominating the other in all the cases .", "this proves that our integrated heuristic is the right choice since it always improves upon the best stage taken individually .    *", "serial heuristic accuracy .", "* figure  [ fig : sequential_100_obj ] depicts the accuracy as a function of the storage budget for several algorithms in the case of serial raw data processing .", "the workload composed of 100 queries is used . out of the heuristic algorithms , the proposed heuristic is the most accurate .", "as already mentioned , the largest error is incurred when the budget is medium . between the vertical partitioning algorithms , the query - level granularity algorithm  @xcite", "is the most accurate .", "the other two algorithms  @xcite do not improve as the storage budget increases .", "this is because they are attribute - level algorithms that are not optimized for covering queries .", "* serial heuristic execution time .", "* figure  [ fig : sequential_100_time ] depicts the execution time for the same scenario as in figure  [ fig : sequential_100_obj ] .", "it is clear that the proposed heuristic is always the fastest , even by three orders of magnitude in the best case .", "surprisingly , calculating the exact solution using cplex is faster than all the vertical partitioning algorithms almost in all the cases .", "if an algorithm does not finish after one hour , we stop it and take the best solution at that moment .", "this is the case for chu  @xcite and agrawal  @xcite .", "however , the solution returned by chu is accurate  a known fact from the original paper .", "* pipelined heuristic accuracy . *", "the objective function value for pipelined processing over fits data is depicted in figure  [ fig : pipeline_100_obj ] . the same 100 query workload is used .", "the only difference compared to the serial case is that cplex can not find the optimal solution in less than one hour .", "however , it manages to find a good - enough solution in most cases .", "the proposed heuristic achieves the best accuracy for all the storage budgets .", "* pipelined heuristic execution time . *", "the proposed heuristic is the only solution that achieves sub - second execution time for all the storage budgets ( figure  [ fig : pipeline_100_time ] ) .", "cplex finishes execution in the alloted time only when the budget is large .", "the number of variables and constraints in the pipeline mip formulation increase the search space beyond what the cplex algorithms can handle .", "we provide a series of case studies over different data formats in order to validate that the raw data processing architecture depicted in figure  [ fig : scanraw ] is general and the mip models corresponding to this architecture fit reality .", "we use the implementation of the architecture in the scanraw operator  @xcite as a baseline .", "for a given workload and loading plan , we measure the cumulative execution time after each query and compare the result with the estimation computed by the mip formulation .", "if the two match , this is a good indication that the mip formulation models reality accurately .", "the csv format maps directly to the raw data processing architecture . in order to apply the mip formulation ,", "the value of the parameters has to be calibrated for a given system and a given input file .", "the time to tokenize @xmath7 and parse @xmath8 an attribute are the only parameters that require discussion .", "this can be done by executing the two stages on a sample of the data and measuring the average value of the parameter for each attribute .", "as long as accurate estimates are obtained , the model will be accurate .", "figure  [ fig : csv ] confirms this on the sdss workload of 32 queries . in this case", ", there is a perfect match between the model and the scanraw execution .", "[ fig : csv ]      since fits is a binary format , there is no extraction phase , i.e. , tokenizing and parsing , in the architecture .", "moreover , data can be read directly in the processing representation , as long as the file access library provides such a functionality .", "cfitsio  the library we use in our implementation", " can read a range of values of an attribute in a pre - allocated memory buffer .", "however , we observed experimentally that , in order to access any attribute , there is a high startup time .", "essentially , the entire data are read in order to extract the attribute .", "the additional time is linear in the number of attributes .", "[ fig : fits ]    based on these observations  that may be specific to cfitsio ", "the following parameters have to be calibrated : the time to read the raw data corresponds to the startup time ; an extraction time proportional with the number of attributes in the query is the equivalent of @xmath8 .", "@xmath7 is set to zero .", "although pipelining is an option for fits data , due to the specifics of the cfitsio library , it is impossible to apply it .", "the result for the sdss data confirms that the model is a good fit for fits data since there is almost complete overlap in figure  [ fig : fits ] .      at first sight", ", it seems impossible to map json data on the raw data processing architecture and the mip model .", "looking deeper , we observe that json data processing is even simpler than csv processing .", "this is because every object is fully - tokenized and parsed in an internal map data structure , independent of the requested attributes .", "at least this is how the jsoncpp library works .", "+    once the map is built , it can be queried for any key in the schema . for schemas with a reduced number of hierarchical levels  the case for tweets ", "there is no difference in query time across levels . essentially , the query time is proportional only with the number of requested keys , independent of their existence or not . based on these observations", ", we set the model parameters as follows .", "@xmath7 is set to the average time to build the map divided by the maximum number of attributes in the schema .", "@xmath8 is set to the map data structure query time .", "since @xmath7 is a constant , the pipelined mip formulation applies to the json format .", "the results in figure  [ fig : json ] confirm the accuracy of the model over a workload of 32 queries executed in scanraw .     [", "fig : json ]      the experimental evaluation provides answers to each of the questions raised at the beginning of the section .", "the two - stage heuristic improves over each of the component parts .", "it is not clear which of the query coverage and attribute usage frequency is more accurate .", "using them together guarantees the best results .", "the proposed heuristic comes close to the optimal solution whenever the storage budget is either small or large .", "when many choices are available  the case for a medium budget  the accuracy decreases , but remains superior to the accuracy of the other vertical partitioning methods . in terms of execution time ,", "the proposed heuristic is the clear winner  by as much as three orders of magnitude .", "surprisingly , cplex outperforms the other heuristics in the serial case .", "this is not necessarily unexpected , given that these algorithms have been introduced more than two decades ago .", "the case studies confirm the applicability of the mip formulation model to several raw data formats .", "the mip model fits the reality almost perfectly both for serial and pipelined raw data processing ."], ["two lines of research are most relevant to the work presented in this paper  raw data processing and vertical partitioning as a physical database design technique .", "our contribution is to integrate workload information in raw data processing and model the problem as vertical partitioning optimization . to the best of our knowledge ,", "this is the first paper to consider the problem of optimal vertical partitioning for raw data processing with partial loading .    *", "raw data processing .", "* several methods have been proposed for processing raw data within a database engine .", "the vast majority of them bring enhancements to the external table functionality , already supported by several major database servers  @xcite .", "a common factor across many of these methods is that they do not consider loading converted data inside the database . at most ,", "data are cached in memory on a query - by - query basis .", "this is the approach taken in nodb  @xcite , data vaults  @xcite , sds / q  @xcite , raw  @xcite , and impala  @xcite .", "even when loading is an option , for example in adaptive partial loading  @xcite , invisible loading  @xcite , and scanraw  @xcite , the workload is not taken into account and the storage budget is unlimited . the decision on what to load is local to every query , thus , prone to be acutely sub - optimal over the entire workload .", "the heuristic developed in this paper requires workload knowledge and aims to identify the optimal data to load such that the execution time of the entire workload is minimized . as in standard database processing", ", loading is executed offline , before query execution . however , the decision on what data to load is intelligent and the time spent on loading is limited by the allocated storage budget .", "notice that the heuristic is applicable both to secondary storage - based loading as well as to one - time in - memory caching without subsequent replacement .    * vertical partitioning . * vertical partitioning has a long - standing history as a physical database design strategy , dating back to the 1970 s .", "many types of solutions have been proposed over the years , ranging from integer programming formulations to top - down and bottom - up heuristics that operate at the granularity of a query or of an attribute .", "a comparative analysis of several vertical partitioning algorithms is presented in  @xcite .", "the serial mip formulation for raw data processing is inspired from the formulations for vertical partitioning given in  @xcite . while both are non - linear , none of these formulations considers pipeline processing .", "we prove that even the linear mip formulation is np - hard .", "the scale of the previous results for solving mip optimizations have to be taken with a grain of salt , given the extensive enhancements to integer programming solvers over the past two decades . as explained in section  [ sec : heuristic : comparison - vertical ] , the proposed heuristic combines ideas from several classes of vertical partitioning algorithms , adapting their optimal behavior to raw data processing with partial loading .", "the top - down transaction - level algorithm given in  @xcite is the closest to the query coverage stage .", "while query coverage is a greedy algorithm , @xcite employs exhaustive search to find the solution .", "as the experimental results show , this is time - consuming .", "other top - down heuristics  @xcite consider the interaction between attributes across the queries in the workload .", "the partitioning is guided by a quantitative parameter that measures the strength of the interaction . in  @xcite", ", only the interaction between pairs of attributes is considered .", "the attribute usage frequency phase of the proposed heuristic treats each attribute individually , but only after query coverage is executed .", "the objective in  @xcite is to find a set of vertical partitions that are subsequently evaluated for index creation .", "since we select a single partitioning scheme , the process is less time - consuming .", "finally , the difference between the proposed heuristic and bottom - up algorithms  @xcite is that the latter can not guarantee that only two partitions are generated at the end .", "this is a requirement for raw data processing with partial loading .", "all these algorithms are offline .", "they are executed only once , before query processing , over a known workload .", "online vertical partitioning algorithms form a separate class . in  @xcite", ", the entire workload is known in advance . however , the order of the queries is fixed and the vertical partitioning evolves .", "another series of algorithms  @xcite operates over an unknown workload , given one query at a time .", "their goal is to gather evidence from the past workload in order to determine the optimal vertical partitioning at each query ."], ["in this paper , we study the problem of workload - driven raw data processing with partial loading .", "we model loading as binary vertical partitioning with full replication . based on this equivalence ,", "we provide a linear mixed integer programming optimization formulation that we prove to be np - hard and inapproximable .", "we design a two - stage heuristic that combines the concepts of query coverage and attribute usage frequency .", "the heuristic comes within close range of the optimal solution in a fraction of the time .", "we extend the optimization formulation and the heuristic to a restricted type of pipelined raw data processing . in the pipelined scenario ,", "data access and extraction are executed concurrently .", "we evaluate the performance of the heuristic and the accuracy of the optimization formulation over three real data formats ", "csv , fits , and json  processed with a state - of - the - art pipelined operator for raw data processing .", "the results confirm the superior performance of the proposed heuristic over related vertical partitioning algorithms and the accuracy of the formulation in capturing the execution details of a real operator .", "following the steps of database physical design , we envision several avenues to extend the proposed research in the future .", "we can move from the offline loading setting to online loading , where query processing and loading are intertwined .", "we can assume that the workload is known beforehand or it is given one query at a time .", "we can drop the strict requirement of atomic attribute loading and allow for portions  horizontal partitions  of an attribute to be loaded .", "finally , we can consider a multi - query processing environment in which raw data access and attribute extraction can be shared across several queries ."]]}
{"article_id": "1702.06159", "article_text": ["mobile devices such as smartphones and wearable devices are increasingly gaining popularity as platforms for collecting and sharing sensor data .", "these devices , which are often equipped with sensors such as accelerometer , orientation sensor , magnetometer , camera , microphone , gps and so on , are being used by mobile sensing systems to make sophisticated inferences about users .", "these inferences have in turn enabled an entire ecosystem of context - aware apps such as behavior - based authentication @xcite , fitness monitoring based on activity tracking  @xcite , speech translation  @xcite and traffic / environmental monitoring  @xcite .", "while shared data has enabled context - aware apps to provide utility to the users , the same data can also be used by an adversary to make inferences that are possibly sensitive to the user such as speaker identity  @xcite , keystroke detection  @xcite , location tracking  @xcite , device placement  @xcite , onscreen taps recognition  @xcite , onset of stress  @xcite and detection of emotional state  @xcite .", "therefore , there exist fundamentally conflicting requirements between protecting privacy of the sensitive information contained in mobile sensor data and preserving utility of the same data for authorized inferences .", "we consider the following case studies that further illustrate this privacy - utility tradeoff :    1 .", "case study 1 : a user shares accelerometer data with an authentication app that performs keyless validation of user identity ( _ useful inference _ )  @xcite .", "but the same data can also be used to infer the activity mode of the user  if she is walking , or standing still , or moving up or down the stairs ( _ sensitive inferences _ )  @xcite  which in turn may lead to more serious inferences regarding living habits and health conditions .", "case study 2 : a user shares location and accelerometer data with a real - time life - logging app that helps with travel planning ( _ useful inferences _ )  @xcite .", "however , the same data can also be maliciously used to extract sequences of the user s entered text ( e.g. , passwords or pins ) ( _ sensitive inference _ )  @xcite .", "case study 3 : a user shares speech data with a voice - based search app that translates speech to text for web searches ( _ useful inference _ )  @xcite .", "but the same data can also be used to recognize the user s identity ( _ sensitive inference _ )  @xcite , compromising her privacy .", "recently , several sensor privacy protection mechanisms ( sppms ) have been proposed  @xcite .", "however , most existing sppms only provide binary access control over the sensor data that requires users to choose between sharing or blocking a sensor  limiting their applicability and adoption  @xcite .", "a limited set of sppms that do focus on obfuscating sensor data , however , lack provable privacy guarantees  @xcite .    in this paper", ", we propose a new framework called deeprotect that mediates privacy - preserving access to mobile sensor data .", "deeprotect enables a new permission model where instead of blocking or allowing sensors , users specify their utility and privacy preferences in terms of inferences that can be derived from mobile sensor data .", "the overall information flow of deeprotect is summarized in figure  [ fig : motivation ] .    in deeprotect", ", users can specify as input a set of useful / authorized inferences denoted by @xmath1 ( e.g. , behavior - based authentication , speech - to - text translation ) .", "all other inferences ( possibly unknown ) are considered sensitive by default .", "alternatively , a subset of sensitive inferences can be specified by the users , which is denoted by @xmath2 ( e.g. , detection of the text entered in keyboard , speaker identity recognition ) .", "these useful and sensitive inferences represent functions that map the mobile sensor data to their corresponding inference results ( i.e. , labels ) , which is the information we want to share ( for useful inferences ) or protect ( for sensitive inferences ) .", "outgoing sensor data is initially intercepted by deeprotect , which obfuscates the data before sharing such that @xmath3 can be derived accurately by the app and @xmath4 is kept private .    * approach overview *", ": we now illustrate the working of deeprotect by using case study 3 as an example .", "figure  [ venn_diagram ] shows the venn diagram of information measures in deeprotect . to protect users privacy", ", it is required that we should not directly share the raw sensor data @xmath5 ( the entire white square shown in figure  [ venn_diagram]1 ) with the untrusted third - party apps . in case", "study 3 , @xmath5 corresponds to the microphone data .", "our objective is to enable @xmath3 ( speech - to - text translation ) with high accuracy while protecting against @xmath4 ( speaker identity recognition ) . the key idea in deeprotect is to first ignore the information that is orthogonal to authorized ( useful ) inference information . in this case , we only retain information that enable @xmath3 and ignore the remaining information @xmath6 ( shown in yellow in figure  [ venn_diagram]2 ) . in the next step ,", "we perturb those information that are correlated to both the useful and sensitive inferences , @xmath7 ( shown in blue in figure  [ venn_diagram]3 ) , to satisfy provable privacy guarantees .", "this helps in selectively hindering the sensitive inferences without severely degrading the useful inferences .    for arbitrary useful and sensitive inferences , the three - stage data obfuscation pipeline used in deeprotect", "is illustrated in figure  [ fig : pipeline ] . in the first stage", ", we extract features from raw sensor data and perform _ data minimization _", "@xcite to guarantee that only features that are relevant to the authorized ( useful ) inferences are retained for further processing .", "specifically , deeprotect modifies an autoencoder network  @xcite to _ automatically _ explore the inherent structural characteristics of the sensor data , and learn a sparse feature representation of the data specific to the useful inferences . at this point ,", "some of the extracted features , although highly specific to the useful inferences , might still have some correlation with the sensitive inferences . in the next step of the pipeline , to provide provable privacy guarantees for a single user s mobile sensor stream , we exploit _ local differential privacy _", "@xcite to obfuscate the extracted features .", "note that differential privacy  @xcite is typically applied in multi - user settings , and local differential privacy can be applied in single - user settings .", "specifically , we develop a computationally efficient mechanism that perturbs the sparse features , using either the conventional local differential privacy framework  @xcite or our relaxed notion of local differential privacy framework , depending on users specification of sensitive inferences .", "finally , we reconstruct obfuscated sensor data from the perturbed features such that the existing interfaces for mobile apps can continue to work without any change .", "* usage mode : * deeprotect supports two usage modes , depending on how sensitive inferences are specified .    _ usage mode 1 : _ only the useful inference set @xmath3 is provided and the sensitive inference set @xmath8 is the default set of ` all possible inferences ' . under this mode ,", "the user is interested in rigorous privacy guarantees with respect to the entire set ( possibly unknown ) of inferences .", "deeprotect first implements deep learning based feature extraction method to raw sensor data and then applies local differential privacy to these extracted features .", "_ usage mode 2 : _ both the useful and the sensitive inference sets are provided . under this scenario ,", "the user is interested in protecting only a specific set of sensitive inferences ( and not all possible inferences ) . to provide provable privacy guarantees", ", we develop a relaxed variant of local differential privacy and use it to perturb the extracted features in a fine - grained manner .    our theoretical analysis ( section  [ sec_thory ] ) and", "experimental results on real - world datasets ( section  [ sec_evaluation ] ) show that both usage modes of our approach significantly outperform previous state - of - the - art with up to 8x improvement .", "* contributions : * we now summarize our contributions under three main thrusts :    1 .   _ provable privacy guarantees in single - user settings : _ we support rigorous local differential privacy guarantees for a single user s mobile sensor data", "however , this rigorous privacy guarantee can require significant addition of noise impacting application utility .", "thus , we propose a novel relaxed variant of local differential privacy for providing inference - based access control over the mobile sensor data .", "our proposed privacy metric satisfies composition properties , and can achieve improved utility by focusing on protecting a specific set of inferences .", "_ novel perturbation mechanisms for enhanced privacy - utility tradeoffs : _ we propose an effective perturbation mechanism which consists of two key techniques : deep learning based data minimization and feature obfuscation based data perturbation .", "our first technique automatically extracts features relevant to the useful / authorized inferences by extending the deep learning models for dimensionality reduction , which to our best knowledge is the first such attempt .", "for the second technique , we propose effective feature perturbation methods , that achieve both conventional and relaxed notions of local differential privacy guarantees . through rigorous theoretical analysis , we demonstrate the advantage of our approach over the state - of - the - art obfuscation mechanisms .", "implementation and evaluation : _ we implement our system using real - world datasets and mobile apps ( for all the case studies discussed previously ) . for our first feature learning step , we demonstrate that our modified deep learning network outperforms the state - of - the - art feature extraction approaches with up to @xmath9 improvement . for our end - to - end approach combining both feature extraction and perturbation ,", "we demonstrate the advantage of our technique over the state - of - the - art obfuscation mechanisms with up to @xmath0 improvement .", "we also implement experiments to show the effectiveness of our method in defending against inference attacks leveraging mobile sensor data .", "we will provide an open source software framework for all the experiments carried out in our study for independent verification and extensions of our results .", "mockdroid  @xcite is a modified version of the android operating system ( os ) with ability to ` mock ' a given resource requested by an app .", "this approach allows users to revoke access to particular resources at run - time , encouraging users to consider the trade - off between functionality and privacy .", "appfence  @xcite offers two approaches for protecting sensitive data from untrusted apps : shadowing and blocking sensitive data from being exfiltrated off the device .", "identidroid  @xcite is also a customized android os providing different identifications and privileges , to limit the uniqueness of device and user information .", "the above systems still rely on the user to determine the sensitive sensors and block , mock , or shadow them .", "ipshield  @xcite , provides users the ability to specify their privacy preferences in terms of inferences , however , they only generate binary privacy policies of _ allow _ and _ deny _ for individual sensors", ". such binary access control over the sensor data may seriously affect the functionality of various apps .", "deeprotect on the other hand provides the much needed automatic translation from higher - level privacy specifications in terms of inferences to obfuscation of sensor data .", "another important limitation of the state - of - the - art sppms is that they are often heuristic in nature and fail to provide rigorous privacy guarantees for inference - based access control over sensors in mobile devices .", "for instance , ipshield  @xcite does provide users with ability to configure obfuscation policies for raw sensor data ( through addition of random noise ) , but does not quantify any privacy guarantees for those policies .", "boxify  @xcite provides app sandboxing for untrusted apps and can be used to run an untrusted app in an isolated environment with minimum access privileges .", "easedroid  @xcite uses semi - supervised learning to perform automatic policy analysis and refinement .", "this could be used to compliment deeprotect by automatically learning the set of inferences that need to be protected and ones that need to be released from access patterns and audit logs .", "privacy - preserving mobile data aggregation has been studied in  @xcite .", "however , they assume the presence of a multi - user database whereas we aim to protect inference - based privacy for a single user s sensor stream .", "we surveyed more than @xmath10 research papers published in relevant conferences and journals over the past @xmath11 years to form a knowledge repository of inferences ( see table  [ tableknowledge ] in the appendix ) that can be made using a combination of sensors accessed by mobile apps . this table forms the universe of possible inferences over which the set of useful and sensitive inferences in deeprotect are defined .      to the best of our knowledge", ", deeprotect is the first system that provides a novel inference - based access control in mobile sensing applications .", "furthermore , deeprotect 1 ) provides provable privacy guarantees including both conventional local differential privacy and our relaxed variant of local differential privacy for sensitive sensor data ; 2 ) presents an effective mechanism consisting of two key techniques : deep learning based data minimization and feature obfuscation based data perturbation ; 3 ) outperforms the state - of - the - art methods in multiple mobile sensing datasets and applications .", "[ sec_threat ] we assume that deeprotect together with the underlying mobile os , sensors , and system services form the trusted domain . our adversary is an untrusted app provider , who publishes apps in the marketplace for advertised useful inferences .", "the app accesses sensors including ones to which the user has provided explicit permission , and also others for which no permission is required .", "the app would then send out the collected sensor data to the adversary .", "our goal is to ensure that the data shared with the app can only be used for deriving the authorized ( useful ) inferences and not any sensitive inferences .", "we aim to limit / bound the adversary s inference of the user s sensitive information via access to the obfuscated sensor data while achieving two rigorous privacy properties : ( a ) first , we aim to provide rigorous local differential privacy guarantees  @xcite , that limit an adversary against all possible inferences ( corresponding to usage mode 1 in section  [ sec : intro ] ) ; and ( b ) second , we aim to provide a novel relaxed variant of local differential privacy that limits an adversary against a specified set of sensitive inferences ( corresponding to usage mode 2 in section  [ sec : intro ] ) .", "furthermore , we adapt the popular laplace perturbation mechanism  @xcite to construct effective perturbation approaches , in order to provide both conventional and relaxed notions of local differential privacy guarantees for protecting mobile sensor data ( in sections  [ sec : usage1 ] ,  [ sec : usage2 ] ) .", "it is also interesting to note that a threat model similar to usage mode 2 ( i.e. , protecting against specified inferences ) has been explored in pufferfish privacy  @xcite and blowfish privacy  @xcite .", "for example , the set of sensitive inferences computed over sensor data in deeprotect is the set of potential secrets defined in pufferfish and blowfish privacy .", "blowfish privacy has recently been adopted by the u.s .", "census bureau demonstrating its usefulness as an enabler for practical deployment .", "differential privacy ( dp )  @xcite is a rigorous mathematical framework that prevents an attacker from inferring the presence or absence of a particular record in a statistical database .", "dp randomizes the query results , computed over the _ multi - user _ database , to ensure that the risk to an individual record s privacy does not increase substantially ( bounded by a function of the privacy budget @xmath12 ) as a result of participating in the database . local differential privacy ( ldp )  @xcite , as an adaption of dp ,", "is defined under the setting where the user does not trust anyone ( not even the central data collector ) .", "local privacy dates back to warner  @xcite , who proposed the randomized response method to provide plausible deniability for individuals responding to sensitive surveys . in our setting", ", we therefore apply ldp to protect a _ single _", "user s mobile sensor data while satisfying rigorous privacy guarantees .", "similar applications have also been explored in google s rappor system  @xcite .", "specifically , @xmath12-ldp is defined as follows :    [ dpdef ] _ ( @xmath12-ldp )  @xcite _ a randomized algorithm @xmath13 provides @xmath12-ldp if for any two databases @xmath14 and for any output set @xmath15 , @xmath16 where @xmath17 ( resp.@xmath18 ) is the output of @xmath13 on input @xmath19 ( resp.@xmath20 ) and @xmath12 is the privacy budget .", "smaller value of @xmath12 corresponds to a higher privacy level .    )", ".,title=\"fig:\",width=259,height=124 ] .      for a mobile sensing system , consisting of @xmath21 sensors across @xmath22 timestamps ,", "the temporal mobile sensor matrix @xmath23 is a real matrix where @xmath24 records the sensing data of the @xmath25-th sensor at the @xmath26-th timestamp . to fully explore the temporal characteristics of the data , we follow common practice  @xcite and partition it into a series of segments according to a user - specified time window size @xmath27 as shown in figure  [ twodata ] . for the @xmath28-th window", ", we stack the corresponding columns within it to form a column vector which is denoted by @xmath29 . the temporal mobile sensor matrix can thus be reformulated as @xmath30 $ ] , where @xmath31 .", "we therefore apply ldp to a single user s mobile sensor data which is segmented according to a user - specified window size . in this way", ", we can take the temporal dynamics of users mobile sensor data into consideration , while providing rigorous privacy guarantees .    from definition  [ dpdef ]", ", we observe that @xmath12-ldp has the same mathematical formulation as @xmath12-dp , except that the neighboring databases in ldp are _ any two possible databases without the constraint of one tuple s difference as in dp  @xcite_. thus , when apply ldp to mobile sensing applications , the neighboring databases represent any two segmented sensor data that differ in _ any possible sensor recording at any timestamp within the same window_. a mechanism that provides rigorous ldp can defend against any inference attacks over the obfuscated data , according to the post - processing invariant property  @xcite .", "therefore , we can provide rigorous ldp for users sensor data to defend against all possible inferences over the data ( corresponding to usage mode 1 in section  [ sec : intro ] ) .      * the need for a relaxed variant : * although we can provide rigorous ldp and defend against all inferences computed over the obfuscated sensor data ( in usage mode 1 ) , in practice , it may suffice to protect a specific subset of sensitive inferences instead of focusing on all possible inferences .", "therefore , we further consider a scenario where the user aims to defend against a _ specific subset of sensitive inferences _ over the sensor data ( corresponding to usage mode 2 ) . under this threat model , there are novel opportunities to strategically add noise and enhance utility .", "we thus relax the definition of ldp to provide provable privacy guarantees for a specific subset of sensitive inferences but significantly improve the utility performance ( will discuss in sections  [ sec_thory ] , [ sec_evaluation ] ) .", "pufferfish  @xcite and blowfish  @xcite privacy frameworks have similarly motivated the threat model of protecting against a specific set of sensitive computations over the data and have been deployed in real - world settings .", "inspired by these frameworks and definition  [ dpdef ] , we define our relaxed neighboring databases as follows :    [ neigh_data ] let us represent a set of sensitive inferences as @xmath32 , and let us represent an orthogonal function of @xmath33 as @xmath34 .", "thus , @xmath35 , where @xmath36 and @xmath37 is the orthogonal component of @xmath32 consisting of all the functions that are orthogonal to @xmath32 .", "two databases @xmath38 are relaxed neighboring databases , iff .", "@xmath39 , @xmath40 for any function @xmath34 .", "we illustrate the relaxed neighboring databases above in figure  [ nei ] . for concrete interpretation of the definition", ", we refer back to case study 3 in section  [ sec : intro ] , where the useful inference is _ speech - to - text translation _ and the sensitive inference is _ speaker identity recognition_. intuitively , both the useful and sensitive inferences rely on a limited number of features extracted from the sensor data .", "the commonly used features for speech - to - text translation are mel - frequency cepstral coefficients ( mfcc ) , spectrogram and n - gram  @xcite , while the popular features for speaker identity recognition are mfcc , spectrogram and the voice biometrics of the user ( such as her pitch , accent , etc . )", "according to definition  [ neigh_data ] , the relaxed neighboring databases there refer to two sensor data streams that correspond to different speakers ( i.e. , different values of mfcc , spectrogram and the voice biometrics ) and have the same values of n - gram ( the orthogonal component ) .", "therefore , the relaxed neighboring databases in definition  [ neigh_data ] focuses on protecting the privacy of sensitive inferences only ( whose values differ in the two databases ) , and not the component of the data that is orthogonal to sensitive inferences ( whose values remain the same in the two databases ) .    , which have different sensitive inference values whilst the same values for the orthogonal inferences.,title=\"fig:\",width=259,height=67 ] .    *", "remark on orthogonal functions : * we further illustrate the relaxed neighboring databases in a three - dimensional space ( our definition can be generally applied to any space ) in figure  [ illu ] , where there are two functions @xmath41 that are orthogonal with the sensitive inference function @xmath33 , i.e. , @xmath42 . according to definition  [ neigh_data ] , we have the following properties for the two relaxed neighboring databases @xmath43 : 1 ) they have the same projection on the direction corresponding to the orthogonal functions , i.e. , @xmath44 ; 2 ) they have different projection on the direction corresponding to the sensitive inference function , i.e. , @xmath45 .    inspired by the relaxed neighboring databases in definition  [ neigh_data ] ,", "we now formulate a relaxed variant of local differential privacy ( rldp ) which is also an instantiation of the pufferfish and blowfish privacy frameworks .", "in fact , we can observe that our set of sensitive inferences computed over sensor data is the set of potential secrets defined in pufferfish privacy ( definition 3.4 in @xcite ) and blowfish privacy ( definition 4.2 in @xcite ) .", "[ ipdef]_(@xmath12-rldp ) _ for a set of sensitive inference functions @xmath46 and a privacy budget @xmath12 , a randomized algorithm @xmath13 satisfies @xmath12-rldp for protecting @xmath46 , if for any two relaxed neighboring databases @xmath38 defined in definition  [ neigh_data ] and any output set @xmath47 , @xmath48 smaller values of @xmath12 correspond to higher privacy level .", "to achieve @xmath12-rldp for protecting sensitive information , it is required that the shared data @xmath49 conditioned on any two different sensitive information @xmath50 ( where @xmath39 in definition  [ neigh_data ] ) , is statistically indistinguishable from each other . this in turn guarantees that an adversary gains negligible information about the true sensitive information upon observing the shared data .", "we re - iterate that unlike traditional ldp frameworks , its relaxed variant focuses only on the privacy of a specific subset of sensitive inferences .", "a perturbation mechanism developed to satisfy rldp guarantees would thus result in better utility performance than the ldp mechanisms ( will discuss in sections  [ sec_thory ] , [ sec_evaluation ] ) .     in definition  [ neigh_data ] .", "@xmath41 are orthogonal functions of the sensitive inference function @xmath33.,title=\"fig:\",width=240,height=115 ] .     + .    *", "( sequential composition theorem)*[seq ] let randomized algorithm @xmath51 ( @xmath52 ) each provide @xmath53-rldp under the sensitive inference functions @xmath33 .", "the sequence of these algorithms @xmath54 provides @xmath55-rldp .    *", "( parallel composition theorem)*[paral ] let randomized algorithms @xmath51 ( @xmath52 ) provide @xmath53-rldp under the sensitive inference functions @xmath33 and @xmath56 be arbitrary disjoint data set .", "the sequence of these randomized algorithm @xmath57 provides @xmath58-rldp .", "in this section , we design effective perturbation mechanisms to achieve @xmath12-ldp and @xmath12-rldp ( corresponding to the two usage modes in section  [ sec : intro ] ) for protecting mobile sensor data in a single - user setting .", "our key insight is to exploit the structural characteristics of mobile sensor data to enhance privacy - utility tradeoffs . before proposing our privacy mechanisms", ", we first introduce a baseline approach which directly generalizes the traditional dp perturbation mechanisms to achieve @xmath12-ldp .", "to achieve @xmath12-dp , the laplace perturbation mechanism ( lpm )  @xcite applies noise drawn from a suitable laplace distribution to perturb the query results .", "more formally , for a query function @xmath59 , lpm computes and outputs @xmath60 , where @xmath61 is the parameter of the laplacian noise and @xmath62 is the global sensitivity of @xmath59  @xcite .", "when applying ldp ( recall definition  [ dpdef ] ) on segmented mobile sensor data @xmath63 , the neighboring databases @xmath64 may differ in all their possible tuples ( i.e. , all sensor recordings across all timestamps within the same window ) , while the neighboring databases in traditional dp frameworks only differ in one tuple . according to the _ composition theorem _ of dp  @xcite", ", we propose a baseline approach to achieve @xmath12-ldp , which inserts a laplacian noise to each temporal sensor data point with the same parameter of @xmath65   is estimated from our collected data serving as a local sensitivity .", "] , where @xmath66 is the dimension of each segmented sensor data @xmath63 ( we refer interested readers to @xcite for similar perturbation mechanisms ) .", "the baseline approach achieves @xmath12-ldp guarantees and the corresponding proof is deferred to the appendix to improve readability .", "note that the baseline approach introduces noise that is linear in the number of temporal sensor recordings within the time window .", "this increases the magnitude of noise that needs to be added and thus degrades the usability of the data .", "therefore , a better alternative to protect sensor data is to build a compact , privacy - preserving synopsis from the data by exploiting its structural characteristics .", "data minimization  @xcite is a fundamental legal instrument that protects privacy by limiting the collection of personal data to the minimum extent necessary for attaining legitimate goals . in our work , we enforce the principle of data minimization and retain only the minimum number of features necessary to enable the authorized inferences .", "these extracted features may still be correlated with sensitive inferences ; thus we incorporate local differential privacy to obfuscate the extracted features for protecting against sensitive inferences .    in this section ,", "we propose deeprotect ( recall its pipeline in figure  [ fig : pipeline ] ) which consists of two key steps of 1 ) first extracting features based on the data minimization principle and 2 ) then perturbing these features to achieve both conventional and relaxed notions of ldp guarantees ( corresponding to the two usage modes in section  [ sec : intro ] respectively ) .", "we will demonstrate the significant advantages of our mechanism over the existing privacy methods theoretically as well as using multiple real - world case studies on real datasets .", "mobile sensor data is usually high - dimensional in nature , which typically exhibits both structure and redundancy , allowing minimization  @xcite .", "this lays the foundation for the first technique in our deeprotect system : deep learning based data minimization .", "* existing autoencoder models : * deep learning models  @xcite learn multi - layer transformations from the input data to the output representations , which is more powerful for feature extraction than hand - crafted shallow models . among the building blocks of these models ,", "autoencoders @xcite automatically extract features in an unsupervised manner by minimizing the reconstruction error between the input and its reconstructed output .", "a single - layer autoencoder is shown in figure  [ auto ] .", "the encoder function @xmath67 maps the input data @xmath63 to the hidden units ( features ) according to @xmath68 , where @xmath69 is typically a sigmoid function , @xmath70 is a weight matrix and @xmath71 is a bias vector .", "the decoder function @xmath72 maps these features back to the original input space according to @xmath73 , where @xmath74 is usually the same form as that in the encoder , @xmath75 is a weight matrix and @xmath76 a bias vector . considering the inherent structure of the mobile sensor data  @xcite , we aim to learn an effective feature space on which the mobile sensor data would have a succinct representation , and the corresponding objective function is as follows .     .", "@xmath77    where @xmath78 is the reconstruction error between the input data @xmath63 and its reconstructed output @xmath79 ( detailed mathematical formulation for @xmath80 is deferred to the appendix to improve readability ) .", "existing autoencoders thus aim to minimize @xmath81 with respect to @xmath82  @xcite .", "* constructing data minimization model : * using a nonlinear encoding function an autoencoder can typically extract better features than previous linear transformation methods  @xcite .", "but , the features learnt are not specific to the useful ( authorized ) inferences . to handle this", ", we explicitly modify the autoencoder models in eq .", "[ cost ] by incorporating the useful inferences and other associated constraints as follows .    * incorporating useful inferences : * the objective for our data minimization mechanism is to maximize the utility performance with the minimum amount of information , therefore it is important to combine the useful inferences with the autoencoder models to automatically extract features in a supervised manner .", "_ to the best of our knowledge , this is the first work to modify the deep learning models through incorporating the authorized inferences . _", "specifically , we incorporate the minimization of cost function corresponding to the useful inferences to the objective function of existing autoencoders in eq .", "+ we analyze the cost function for each inference using case study 1 in section  [ sec : intro ] , where behavior - based authentication was considered as a useful inference and activity mode detection was deemed sensitive .", "both the useful and sensitive inferences can be mathematically transformed into a classification problem , which can be addressed by machine learning techniques .", "for instance , by leveraging the popular _ ridge regression _", "technique  @xcite , we can learn an optimal classifier as follows .", "@xmath83 where @xmath84 represents the cost function for the useful and sensitive inferences respectively . for behavior - based authentication ( _ useful inference _ ) , the label @xmath85 where @xmath86 represents the legitimate user and @xmath87 represents the adversary .", "similarly , for activity mode detection ( _ sensitive inference _ ) , @xmath88 where the labels @xmath89 and @xmath90 represent _ walking _ , _ standing still _ and _ moving up or down the stairs _ respectively .", "the optimal classifier @xmath91 learned in eq .", "[ krr_obj ] can be utilized to label the newly - coming mobile sensor data for behavior - based authentication or activity mode detection .", "the cost function in eq .", "[ krr_obj ] has been popularly used in machine learning community whilst other general models in  @xcite can also be explored as potential cost function to reflect the prediction accuracy of the inferences .", "note that our analysis are not restricted to these settings and can be utilized for arbitrary inference - based mobile applications .    * incorporating orthogonality : * in addition", ", we also aim to learn orthogonal features so that we can deal with each feature independently in our feature perturbation mechanism for achieving enhanced utility performance .", "therefore , we add another constraint as @xmath92 to the objective function to guarantee the _ orthogonality _ of the features ( similar intuition has also been utilized in  @xcite ) .    * our new autoencoder model : * incorporating the two constraints ( useful inferences and orthogonality ) into eq .", "[ cost ] and combining with eq .", "[ krr_obj ] , we construct the objective function for our data minimization method as @xmath93 where @xmath94 controls the trade - off between the reconstruction loss and the utility penalty , and @xmath92 represents the orthogonality constraint .", "note that the last two terms in eq .", "[ final_obj ] correspond to the cost function in ridge regression ( recall eq .", "[ krr_obj ] ) .", "although we only consider the cost function of ridge regression to optimize the data minimization process , our algorithm can be generalized to multiple machine learning techniques such as support vector machine  @xcite , naive bayesian  @xcite and random forests  @xcite in a straightforward manner .", "* model learning and stacking : * to minimize @xmath95 in eq .", "[ final_obj ] with respect to @xmath96 , we explore the stochastic gradient descent ( sgd ) technique , which has been shown to perform fairly well in practice  @xcite .", "furthermore , we explore multiple hidden layers to stack multiple model units , in order to generate even more compact and higher - level semantic features resulting in better data minimization . to improve readability , we defer all the details about model learning and stacking into the appendix , based on which we summarize our deep learning based data minimization mechanism in algorithm  [ alg1 ] .", "ldp considers the worst - case adversary which can rigorously protect _ against all possible inferences _ computed over the data ( recall definition  [ dpdef ] ) . in the absence of a user - specified set of sensitive inferences , or", "otherwise if the user chooses to operate under the ldp guarantees ( corresponding to usage mode 1 in section  [ sec : intro ] ) , we develop our perturbation mechanism through perturbing the features learnt from the deep learning based data minimization step in section  [ sec_feature ] . formally , to achieve @xmath12-ldp , deeprotect under usage mode 1 inserts an laplacian noise with parameter @xmath97 to each previously learned feature , where @xmath98 represent the dimension of the segmented sensor data @xmath63 and the features @xmath99 extracted from @xmath63 , respectively .", ", the number of features , i.e. , @xmath100 , needed to be perturbed is much smaller than the number of raw sampling points , i.e. , @xmath66 , in the baseline approach . ]", "deeprotect mechanism under usage mode 1 is summarized in algorithm  [ alg3 ] , which satisfies rigorous @xmath12-ldp as will be discussed in theorem  [ proofprivacy ] .", "our mechanism is different from the baseline approach in section  [ sec_baseline ] because we add laplacian noise after the application of the deep learning based data minimization step , while the baseline approach directly adds laplacian noise to the raw sensor data without the deep learning mechanism .", "after perturbing these features , we reconstruct the perturbed sensor data according to the decoder function @xmath72 in autoencoder ( recall eq .  [ cost ] ) .", "we will show that our mechanism significantly outperforms the baseline approach ( in sections  [ sec_thory ] , [ sec_evaluation ] ) .", "note that our privacy objective is also different from that in  @xcite since they aim to protect each user s training data in the deep learning training stage under the multi - user settings while in contrast we aim to protect the privacy of mobile sensor data stream in single - user settings .     + perturbed sensor data @xmath101 ; + * for each * @xmath102 +  1 .", "extract features @xmath99 from @xmath63 by data minimiza- +   -tion mechanism in algorithm  [ alg1 ] ; +  2 .", "obtain perturbed features @xmath103 by inserting noise +  of @xmath104 ; +  3 .", "reconstruct perturbed sensor data @xmath105 ;      if the user has specified a subset of sensitive inferences ( corresponding to usage mode 2 in section  [ sec : intro ] ) , we develop our perturbation mechanism ( summarized in algorithm  [ alg2 ] ) by first computing the relaxed sensitivity @xmath106 , and then inserting an laplacian noise with parameter @xmath107 to each previously learned feature , to satisfy @xmath12-rldp .", "the detailed process to compute the relaxed sensitivity is as follows .", "* computing relaxed sensitivity : * similar to _ sensitivity _ in dp  @xcite , our _ relaxed sensitivity _ can be computed as @xmath108 where @xmath109 are relaxed neighboring databases in definition  [ neigh_data ] , and the denominator is set to make _ relaxed sensitivity _ comparable with traditional _", "sensitivity _ computed over neighboring databases that differ in only one tuple .", "the sensitive inferences can be mathematically transformed into a classification problem , which can be addressed by machine learning techniques .", "the ridge regression classifier in eq .", "[ krr_obj ] is a linear function computed over the segmented sensor data @xmath63 , i.e. , @xmath110 .", "we explain the computation of relaxed sensitivity using this formulation , though our analysis can be generally applied to non - linear situations using kernel - based techniques  @xcite .", "we apply the _ gram - schmidt orthogonalization _ technique @xcite to a matrix @xmath111 $ ] ( where @xmath112 is an _ identity _ matrix and thus @xmath111 $ ] is full - rank ) , in order to obtain orthogonal vectors of @xmath113 as @xmath114 .", "based on that , we form an orthogonal matrix @xmath115 $ ] .", "any function @xmath34 that is orthogonal of the sensitive inference function @xmath32 ( recall definition  [ neigh_data ] ) can be represented by a linear combination of @xmath114 .", "for the two relaxed neighboring databases @xmath64 , we have @xmath116 , according to definition  [ neigh_data ] .", "therefore , we know that @xmath117^t=[\\gamma , 0,0,\\cdots]^t$ ] , i.e. , @xmath118^t$ ] , and the value of @xmath119 is restricted by the range of @xmath120 . since we consider the _ identity _ query ( as we publish the sanitized sensor data to mobile apps ) , we obtain the constraint of @xmath119 as @xmath121^t\\|_1=\\|{\\bm{x}}_{t1}-{\\bm{x}}_{t2}\\|_1\\le\\dim({\\bm{x}}_t)\\delta q$ ] .", "therefore , we can compute the relaxed sensitivity as follows .", "@xmath122^t\\|_1\\le \\dim({\\bm{x}}_t)\\delta q}}\\frac{\\|{\\bm{s}}^{-1}\\cdot[\\gamma,0,0,\\cdots]^t\\|_1}{\\dim({\\bm{x}}_t)}\\ ] ] it is worth noting that @xmath123 due to the constraint of relaxed neighboring databases for achieving the same orthogonal inference values ( recall definition  [ neigh_data ] ) .", "therefore , comparing to usage mode 1 , deeprotect under usage mode 2 would add less noise to the features extracted in the data minimization step thus achieving better utility .", "+ perturbed sensor data @xmath101 ; + * for each * @xmath102 +  1 . extract features @xmath99 from @xmath63 by data minimiza- +   -tion mechanism in algorithm  [ alg1 ] ; +  2 .", "compute relaxed sensitivity @xmath106 in eq .", "[ relaxed_sensitivity0 ] ; +  3 .", "obtain perturbed features @xmath103 by inserting noise +  of @xmath124 ; +  4 .", "reconstruct perturbed sensor data @xmath105 ;    our perturbation mechanisms summarized in algorithms  [ alg3 ] ,  [ alg2 ] satisfy @xmath12-ldp , @xmath12-rldp respectively , according to the following theorems . to improve readability , we defer the corresponding proofs to the appendix .", "[ proofprivacy ] deeprotect under usage mode 1 ( corresponding to algorithm  [ alg3 ] ) satisfies @xmath12-ldp .", "[ proofprivacy2 ] deeprotect under usage mode 2 ( corresponding to algorithm  [ alg2 ] ) satisfies @xmath12-rldp .      for a perturbation algorithm @xmath125 ,", "let us denote @xmath126 $ ] as the expected error in the release of data @xmath5 , where @xmath127 $ ] is the expectation taken over the randomness of @xmath128 .", "we quantify the utility advantage of deeprotect over the baseline approach in the following theorems  [ the_error ] , [ the_error2 ] ( corresponding to the two usage modes ) , and the detailed proofs are deferred to the appendix .", "[ the_error ] for deeprotect under usage mode 1 ( corresponding to algorithm  [ alg3 ] ) , the expected error @xmath129 is lower than that of the baseline approach in section  [ sec_baseline ] by a factor of @xmath130 , where @xmath99 is the feature set extracted from the segmented sensor data @xmath63 by using our deep learning based data minimization approach .", "[ the_error2 ] for deeprotect under usage mode 2 ( corresponding to algorithm  [ alg2 ] ) , the expected error @xmath129 is lower than that of the baseline approach in section  [ sec_baseline ] by a factor of @xmath131 , where @xmath132 are the sensitivity and the relaxed sensitivity corresponding to the query function @xmath59 , respectively .    therefore , we can see that the utility advantage of our deep learning based data minimization step in section  [ sec_feature ] is @xmath130 and of our relaxed variant of local differential privacy in section  [ sec : usage2 ] is @xmath133 .", "in this section , we describe our methodology for collecting sensor data from mobile devices , and the experimental setup ( including system parameters ) for evaluation .      in our experiments", ", we collected data using a google nexus 5 ( with 2.3ghz , krait 400 processor , 16 gb internal storage and 2 gb ram on android 4.4 ) and a moto360 smartwatch ( with omap 3 processor , 4 gb internal storage , 512 mb ram on android wear os ) . on the smartphone", ", data was captured from the _ accelerometer _ , _ gyroscope _ , _ orientation _ , _ ambient light _ , _ proximity _ , and _", "sensors . on the smartwatch , the _ accelerometer _ and _ gyroscope _ sensors were recorded ( recall figure  [ twodata ] ) .", "the sampling rate was fixed at @xmath134 hz .", "@xmath135 graduate students ( @xmath136 males and @xmath137 females ) in our university were invited to take our smartphone and smartwatch for two weeks and use them in the same way that they would use their personal devices in their daily lives .    to obtain the ground - truth information for performance evaluation", ", we ask the users to record labels for both the _ useful _ and _ sensitive inferences_. the labelled training data is grouped under two different categories : mode - detection data and identity - recognition data .", "users perform tasks such as walking , enunciating digits or specific alphabets , and the corresponding data segments are then labelled as per the tasks . the mode - detection data , correspond to labelled user activities ( e.g. , accelerometer data segments are marked with labels such as `` walking '' ) , and speech - to - text translation labels ( where audio segments are labelled with the corresponding spoken digit or alphabet ) .", "the identity - recognition data is used for authentication and speaker identity recognition experiments .", "the labelled data is generated by associating the identity of a user as label to a mobile device , on first use .", "we provide provable privacy guarantees for temporal mobile sensor data ( discussed above ) , which is segmented according to the parameter of time window size @xmath138 . for the deep learning based data minimization step ( in section  [ sec_feature ] )", ", we use 10-fold cross validation to generate the training data and testing data , where @xmath139 of our collected data is used as training data and the remaining @xmath140 is used as testing data .", "we repeated this process for @xmath141 iterations and reported the averaged results . in our experiments , we used stacked autoencoders ( @xmath142 in algorithm  [ alg1 ] ) with two hidden layers comprising of 15 and 7 units respectively . we will show that an autoencoder with only two - hidden layers was able to extract better features than the state - of - the - art techniques .", "the reduced number of layers ( and units ) in the autoencoder allowed us to train the model using small amount ( 2 weeks ) of labelled data from the user ( note that we are protecting the sensitive inferences for a single user ) .", "we implemented all the three case studies ( tradeoff between authentication and activity recognition , tradeoff between transportation detection and text recognition , tradeoff between speech translation and speaker identification ) discussed in section  [ sec : intro ] on our real - world dataset using the system parameters discussed above .", "in this section , we experimentally demonstrate the effectiveness of deeprotect using multiple real - world datasets and applications .", "we first show the advantage of our deep learning based data minimization step ( in section  [ sec_feature ] ) over existing feature extraction approaches with up to 2x improvement .", "next , we show the advantage of our end - to - end approaches combining both feature extraction and perturbation ( in sections  [ sec : usage1 ] ,  [ sec : usage2 ] ) over the baseline approach with up to 8x improvement .      we experimentally evaluate our deep learning based data minimization mechanism in section  [ sec_feature ] , using the dataset collected for case study 1 ( recall section  [ sec : intro ] ) , where the useful inference is the _ behavior - based authentication_. to show the advantage of our method , we further compare it with the state - of - the - art feature extraction approaches .", "the discrete _ fourier _ transform ( dft ) and discrete cosine transform ( dct ) are two basic transformation techniques in the signal processing community , and the haar basis forms the fundamental wavelet transformation for time - frequency signal analysis", ". the principal component analysis ( pca ) technique @xcite can also be utilized to reveal hidden structure in the mobile sensor data .", "furthermore , we also consider blind compressive sensing ( bcs ) as a typical dictionary learning method for comparison  @xcite .    * higher accuracy", "* : figure  [ feature_utility ] shows the utility - preserving performance under different feature extraction methods .", "note that @xmath143-axis is in log scale and @xmath144-axis is the accuracy for behavior - based authentication in case study 1 representing the ratio of correctly authenticated users .", "note that we use three - dimensional _", "accelerometer _ measurement for behavior - based authentication and set the window size as @xmath138 ( recall section  [ para ] ) , therefore the dimension of each segmented sensor data is @xmath145 .    from figure  [ feature_utility ]", ", we can see that 1 ) more features would be beneficial for improving the utility performance since the combination of multiple features would be more accurate and expressive to represent the input data ; 2 ) our method achieves higher accuracy than the state - of - the - art approaches with up to @xmath9 improvement .", "@xmath146 features are enough for our method to provide good utility performance with @xmath147 accuracy , while the accuracy by using all the @xmath148 features is @xmath149 . in comparison ,", "the maximum accuracy for the baseline approaches is only @xmath150 by using @xmath146 features ; 3 ) our data minimization mechanism significantly benefits from the automatic learning process which explicitly incorporates the useful inference information into deep learning models .", "based on our analysis above , we constrain the dimension of features extracted by our deep learning based data minimization method to @xmath146 , whose corresponding accuracy is higher than @xmath151 ( @xmath152 ) of the accuracy achieved by using all the features .", "therefore , we represent each of the @xmath148-dimensional input sensor data @xmath153 with a feature set @xmath99 consisting of only @xmath146 features .    * higher informativeness * : we further leverage _ informativeness _ in  @xcite as an important metric to evaluate the information - theoretic relationship between each individual feature and the useful inference information", ". the _ informativeness _", "@xmath154 of the @xmath25-th feature @xmath155 , measures the relative mutual information between the feature and the label of the utility function @xmath156 , and is computed as @xmath157 is the random variable taking the @xmath25-th value in each @xmath158 . ] . under the setting of case study 1", ", @xmath159 represents the legitimate user and the adversary , respectively .", "@xmath160 is the entropy of the variable @xmath156 and @xmath161 is the mutual information between the random variables of @xmath155 and @xmath156 . for each feature", "@xmath155 , this measure of _ informativeness _ takes a value between @xmath162 and @xmath86 , where @xmath162 means that the feature contains no information about the utility label @xmath156 and @xmath86 means that the feature can completely determine the utility label @xmath156 .", "recall that we set the dimension of features extracted by our method to @xmath146 ( see figure  [ feature_utility ] ) .", "we further evaluate their corresponding _ informativeness _ in figure  [ informativeness ] .", "we can observe that the features learned by our method have much higher _ informativeness _ with up to @xmath9 improvement over previous works .", "therefore , our proposed method captures more expressive , higher - quality features than the existing state - of - the - art approaches .          to demonstrate the effectiveness of our end - to - end perturbation mechanisms combining both feature extraction and perturbation", ", we again consider the _ behavior - based authentication _ as useful inference and the _ activity mode detection _ as sensitive inference ( recall case study 1 in section  [ sec : intro ] ) .", "we first compare the baseline method with our mechanism under usage mode 1 ( corresponding to algorithm  [ alg3 ] ) , since they achieve the same level of privacy guarantees for preventing all possible sensitive inferences .", "then , we compare the baseline method with our mechanism under usage mode 2 ( corresponding to algorithm  [ alg2 ] ) , to verify the effectiveness of our rldp for protecting a specific subset of sensitive inferences .    * utility advantage under both usage modes : * figure  [ utiltity_epsilon ] shows the utility performance computed over the obfuscated sensor data generated by deeprotect .", "we can make the following important observations using figure  [ acc_epsilon ] : 1 ) deeprotect achieves considerable advantage over the baseline approach with up to @xmath0 improvement in utility .", "this validates the effectiveness of deeprotect that not only provides rigorous privacy guarantees for protecting sensitive inferences , but also retains the utility of the perturbed data ; 2 ) deeprotect achieves better utility performance under usage mode 2 ( which only considers specific sensitive inferences ) than that under usage mode 1 ( which considers the entire set of sensitive inferences ) ; 3 ) as expected , at higher values of @xmath12 , there is an improvement in utility but at the cost of degradation in privacy ; 4 ) even at moderate value of @xmath163 which is a typical privacy budget in ldp ( similar values can also be found in google s rappor system  @xcite ) , the authentication accuracy using deeprotect ( under usage mode 2 ) is close to the noise - free level ( @xmath149 in figure  [ feature_utility ] ) .", "this observation further validates the effectiveness of our mechanisms .", "note that the neighboring databases in ldp / rldp may differ in all their possible tuples ( instead of differing in only one tuple as in dp ) .", "thus , a proper privacy budget in ldp / rldp for balancing utility and privacy is usually higher than that of dp .", "+    to investigate the effectiveness of deeprotect on real - world mobile applications , we plot the trade - off between the accuracy of making useful inferences versus the accuracy of making sensitive inferences for all the three case studies ( corresponding to usage mode 2 and thus algorithm  [ alg2 ] is applied ) , as shown in figure  [ balance123 ] . * utility - privacy tradeoffs for inference - based access control : * in figure  [ balance1 ]", ", we observe that 1 ) deeprotect achieves good inference performance for behavior - based authentication ( useful inference ) , while significantly deteriorating the activity mode detection ( sensitive inference ) . for privacy budget @xmath163 which is a typical privacy budget in ldp ( similar values have also been used in google s rappor system  @xcite ) , the accuracy for inferring the useful information ( authentication ) is larger than @xmath164 while the accuracy for inferring sensitive information ( activity modes ) drops to roughly @xmath165 which is equivalent to random guessing users data in our experiments , the accuracy for random guessing is @xmath166 . ] .", "therefore , deeprotect can achieve practical privacy while only degrading utility performance by @xmath167 .", "this provides an effective guide for users to choose a proper value of @xmath12 for real world applications ; 2 ) higher levels of perturbation would degrade the inference performance for both the useful and sensitive information ; 3 ) deeprotect is effective for defending against sensitive inference attacks computed over sensor data .", "these observations demonstrate that deeprotect works well in practice and returns an acceptable utility performance while satisfying provable privacy guarantees .", "similarly , for the other two case studies , the accuracy of the sensitive inferences degrades at a much faster rate than that of the useful inferences when more noise is added ( corresponding to a smaller privacy budget @xmath12 ) , which further validates the effectiveness of deeprotect . for case study 2 , in figure  [ balance2 ] , when @xmath168 , deeprotect achieves good performance for transportation detection ( useful inference ) , while significantly degrading the identification of entered text ( sensitive inference ) .", "for case study 3 , in figure  [ balance3 ] , when @xmath169 , deeprotect achieves good performance for speech translation ( useful inference ) , while preventing the recognition of speaker identity ( sensitive inference ) .", "depending on the mode the user chooses to operate in deeprotect can provide either the rigorous ldp guarantee that protects all sensitive inferences ( usage mode 1 ) or our rldp that protects a user - chosen subset of inferences ( usage mode 2 ) . while rldp provides weaker privacy guarantees than ldp , it comes with the advantage of stronger utility properties . when applying ldp / rldp on mobile sensor data , the neighboring databases may differ in all their possible tuples ( i.e. , all sensor recordings across all timestamps within the same window ) , while the neighboring databases in traditional dp only differ in one tuple .", "therefore , a proper privacy budget in ldp / rldp for balancing privacy and utility is usually higher than that of dp  @xcite .", "previous work in @xcite proposed to ignore several records in the database to bound sensitivity in dp for providing better privacy - utility tradeoffs .", "our approach of using deep learning techniques to extract authorized features is a conceptual improvement over these methods .", "while deeprotect has been presented in the context of privacy - aware data sharing on mobile phones , the techniques developed are not restricted to the specific setting , and can be applied to other scenarios beyond mobile phones .", "we will make deeprotect tool available to public as open source software .", "although our privacy guarantee is limited to segmented sensor data , we do take the temporal dynamics existing in mobile sensor stream into consideration according to a user - specified parameter of window size .", "similar to any dp - oriented metrics , our privacy guarantees also compose securely , i.e. , retain privacy guarantees even in the presence of multiple independent releases ( recall theorems  [ seq ] ,  [ paral ] ) . while there is a non - trivial utility cost incurred by our mechanisms , deeprotect ( under both usage modes )", "significantly outperforms the baseline approach .", "in addition , our utility performance can be further improved by 1 ) exploiting fine - grained correlation among sensors ( e.g. , among the three dimensions of accelerometer in case study 1 ) according to @xcite ; 2 ) training customized models specific to each individual user that installs deeprotect in phone ; and 3 ) training models based on a larger amount of users data as in @xcite .", "our perturbation mechanism can also be easily generalized to consider correlation across time windows , according to the composition properties of our privacy metrics ( refer to @xcite and theorems  [ seq],[paral ] ) .", "an important component of our perturbation mechanism is the computation of the feature sensitivity in algorithms  [ alg3],[alg2 ] .", "ways to accurately compute the sensitivity for arbitrary mobile applications , and deal with its possible underestimation would be a challenge we would like to address in the future .", "we are working towards an implementation of the auto - encoder on the android platform using one of the deep learning packages on android  @xcite .", "the training itself could be done offline and then transferred on the phone to perform data minimization in real time  @xcite .", "our off - line training process requires a small amount of labeled data , for example , in our experiments , we only required 20 users data collected for 2 weeks .    any dp - oriented mechanism that adds noise to the data ( or corresponding query ) can risk the problem of breaking the integrity constraints of these sensors . in practice , since our framework targets simple sensors such as accelerometer , gyroscope , etc .", ", our decoding process is unlikely to break the integrity constraints for these sensors .", "therefore , apps can continue to use the perturbed sensor data generated by deeprotect without any change to their existing interface .", "in this paper , we propose deeprotect , a general privacy - preserving framework for inference - based access control of time - series sensor data on mobile devices .", "deeprotect not only supports conventional ldp guarantees , but also provides a novel relaxed variant of ldp .", "we further propose effective perturbation mechanisms consisting of two key steps : 1 ) we uniquely explore deep learning techniques to realize data minimization and only retain features relevant to the useful ( authorized ) inferences .", "this prevents the leakage of any information that is orthogonal to the useful inferences ; and 2 ) to further enhance the privacy of sensitive inferences , we perturb the extracted features , using an effective obfuscation mechanism that ensures both conventional and relaxed ldp guarantees while simultaneously maintaining the utility of the data . finally , for reasons of compatibility with existing third - party apps , we reconstruct the sensor data , from the noisy features before sharing . through theoretical analysis and extensive experiments over multiple real - world datasets ,", "we demonstrate that compared to the state - of - the - art research , deeprotect significantly improves the accuracy for supporting mobile sensing applications while providing provable privacy guarantees .", "this work has been partly supported by faculty awards from google , intel and nsf .", "changchang liu is partly supported by ibm phd fellowship .", "we are very thankful to brendan mcmahan , keith bonawitz , daniel ramage , nina taft from google , and richard chow from intel for useful feedback and discussions .", "* sequential composition theorem : * for @xmath170 and @xmath171 , let @xmath172 and @xmath173 . for any sequence @xmath174 of outcomes @xmath175 ,", "the probability of output @xmath174 from the sequence of @xmath176 is @xmath177 . applying the definition of rldp for each @xmath51", ", we have @xmath178 .    * parallel composition theorem : * for @xmath170 and @xmath171 , let @xmath179 and @xmath180 . for any sequence @xmath174 of outcomes @xmath175 , the probability of output @xmath174 from the sequence of @xmath176 is @xmath177 . applying the definition of rldp for each @xmath51 , we have @xmath181 .", "* proof for theorem  [ the_error ] : * first , we derive the variance of a randomized algorithm @xmath13 for our deeprotect under usage mode 1 as @xmath189 .", "then , we compute the expected error as @xmath190 \\le \\mathbb{e}[\\|{a}({\\bm{x}}_t)-\\mathbb{e}[{a}({\\bm{x}}_t)]\\|_1]+\\mathbb{e}[\\|{a}({\\bm{x}}_t)-{\\bm{x}}_t\\|_1 ] = re\\_{error}({a}({\\bm{x}}_t))+\\sqrt{\\mathbb{e}[\\|{a}({\\bm{x}}_t)-{\\bm{x}}_t\\|_2 ^ 2 ] } = re\\_{error}({a}({\\bm{x}}_t))+\\sqrt{var({a}({\\bm{x}}_t))}$ ] .", "note that our deep learning based data minimization mechanism would result in a negligible reconstruction error , which makes it fairly applicable in many practical scenarios .", "it is likely that the reconstruction error @xmath191 is much lower than the perturbation error caused by adding noise .", "therefore , we approximate the utility performance of deeprotect as @xmath192 ( similar results can also be found in  @xcite ) . similarly , we evaluate the expected error for the baseline approach as @xmath193=\\mathbb{e}[\\|(lap(\\delta q/\\epsilon))\\|_1]=\\delta q/\\epsilon$ ] . comparing the utility performance for both methods , we know that deeprotect under usage mode 1 reduces the expected error of the baseline approach with a factor of @xmath194 .", "* proof for theorem  [ the_error2 ] : * first , we derive the variance of a randomized algorithm @xmath13 for our deeprotect under usage mode 2 as @xmath190 \\le \\le \\mathbb{e}[\\|{a}({\\bm{x}}_t)-\\mathbb{e}[{a}({\\bm{x}}_t)]\\|_1]+\\mathbb{e}[\\|{a}({\\bm{x}}_t)-{\\bm{x}}_t\\|_1 ] = re\\_{error}({a}({\\bm{x}}_t))+\\sqrt{\\mathbb{e}[\\|{a}({\\bm{x}}_t)-{\\bm{x}}_t\\|_2 ^ 2 ] } = re\\_{error}({a}({\\bm{x}}_t))+\\sqrt{var({a}({\\bm{x}}_t))}$ ] .", "similarly , we have @xmath195 . comparing the utility performance for both methods , we know that deeprotect under usage mode 2 reduces the expected error of the baseline approach with a factor of @xmath196 .", "[ [ model - learning - and - stacking - in - deep - learning - based - data - minimization ] ] model learning and stacking in deep learning based data minimization ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    * existing autoencoder models : * the mathematical formulations of existing autoencoder models  @xcite ( expanding eq .", "[ cost ] ) is @xmath197 where the encoder function @xmath67 maps the input data @xmath198 to the hidden units ( features ) @xmath199 according to @xmath68 , and the decoder function @xmath72 maps the outputs of the hidden units ( features ) back to the original input space according to @xmath73 .", "similar to most existing deep learning methods  @xcite , we choose the sigmoid function for both the encoding activation function @xmath69 and the decoding activation function @xmath74 , and we focus on the tied weights case in which @xmath200 ( where @xmath201 is the transpose of @xmath202 ) .", "@xmath203 is a loss function , typically the square loss @xmath204 .", "the second term in eq .", "[ cost2 ] is a weight decay term that helps prevent overfitting  @xcite . the third term in eq .", "[ cost2 ] is a sparsity constraint @xmath205 with a pre - determined sparsity parameter @xmath206 , where @xmath207 is the average activation of hidden unit @xmath25 and @xmath208 is the kl divergence between two bernoulli random variables with mean @xmath206 and @xmath209 respectively .", "* model learning : * the objective for our data minimization mechanism is to minimize @xmath95 in eq .  [ final_obj ] with respect to @xmath96 .", "we explore the stochastic gradient descent ( sgd ) technique to solve this optimization problem , which has been shown to perform fairly well in practice  @xcite .", "note that we use sgd to only solve the convex optimization problem in eq .", "[ final_obj ] ( without considering the non - convex constraint of @xmath213 ) . to train our neural network ,", "we first initialize each parameter @xmath96 to a small random value near zero , and then apply sgd for iterative optimization . before computing the exact gradient for the objective function in eq .", "[ final_obj ] with respect to each variable , we first perform the feedforward pass and evaluate @xmath214 .", "next , we compute the error terms @xmath215 as @xmath216 . finally , the gradient descents as partial derivatives can be computed as @xmath217 .", "_ update @xmath70 to satisfy orthogonality constraint : _ after obtaining @xmath70 from sgd , we need to modify @xmath70 to satisfy the orthogonality constraint of @xmath213 in eq .", "[ final_obj ] .", "this constraint is difficult to solve since it is non - convex and of high computational complexity .", "our method to overcome this constraint follows the rigorous analysis in @xcite by adopting a simple operation , @xmath218 , which sets the singular values of @xmath70 to be all ones .", "the above operation is conducted after every sgd step .", "the overall process for our deep learning based data minimization method is shown in algorithm  [ alg1 ] .    * model stacking : * although algorithm  [ alg1 ] is effective for solving eq .", "[ final_obj ] , the learnt result heavily relies on the seeds , used to initialize the optimization process .", "therefore , we use multiple hidden layers to stack the model in order to achieve more stable performance . in other words ,", "our data minimization mechanism can also be used to build a deep network through model stacking . for the first layer in the deep learning model", ", we find the optimal layer by minimizing the objective function in eq .", "[ final_obj ] .", "the representations learned by the first layer are then used as the input of the second layer , and so on so forth . using a stacked deep auto - encoder", ", we can learn stable and finer - grained features to better represent the raw sensor data ."], "abstract_text": ["<S> personal sensory data is used by context - aware mobile applications to provide utility . </S>", "<S> however , the same data can be used by an adversary to make sensitive inferences about a user thereby violating her privacy . </S>", "<S> we present deeprotect , a framework that enables a novel form of access control that we refer to as the _ inference - based access control _ </S>", "<S> , in which mobile apps with access to sensor data are limited ( provably ) in their ability to make inferences about user s sensitive data and behavior . </S>", "<S> deeprotect adopts a two - layered privacy strategy . </S>", "<S> first , it leverages novel deep learning techniques to perform data minimization and limits the amount of information being shared ; the learning network is used to derive a compact representation of sensor data consisting only of features relevant to authorized utility - providing inferences . </S>", "<S> second , deeprotect obfuscates the previously learnt features , thereby providing an additional layer of protection against sensitive inferences ; our approach can provide both conventional and relaxed notions of local differential privacy , depending on how sensitive inferences are specified . through theoretical analysis and extensive experiments using real - world apps and datasets , </S>", "<S> we demonstrate that when compared to existing approaches deeprotect provides provable privacy guarantees with up to @xmath0 improvement in utility . finally , deeprotect shares obfuscated but raw sensor data reconstructed from the perturbed features , thus requiring no changes to the existing app interfaces . </S>"], "labels": null, "section_names": ["introduction", "related work", "threat model and system model", "privacy metrics for protecting mobile sensor data in single-user settings", "perturbation mechanisms", "theoretical analysis", "experimental setup", "evaluation", "discussions and limitations", "conclusions", "acknowledgement", "appendix"], "sections": [["mobile devices such as smartphones and wearable devices are increasingly gaining popularity as platforms for collecting and sharing sensor data .", "these devices , which are often equipped with sensors such as accelerometer , orientation sensor , magnetometer , camera , microphone , gps and so on , are being used by mobile sensing systems to make sophisticated inferences about users .", "these inferences have in turn enabled an entire ecosystem of context - aware apps such as behavior - based authentication @xcite , fitness monitoring based on activity tracking  @xcite , speech translation  @xcite and traffic / environmental monitoring  @xcite .", "while shared data has enabled context - aware apps to provide utility to the users , the same data can also be used by an adversary to make inferences that are possibly sensitive to the user such as speaker identity  @xcite , keystroke detection  @xcite , location tracking  @xcite , device placement  @xcite , onscreen taps recognition  @xcite , onset of stress  @xcite and detection of emotional state  @xcite .", "therefore , there exist fundamentally conflicting requirements between protecting privacy of the sensitive information contained in mobile sensor data and preserving utility of the same data for authorized inferences .", "we consider the following case studies that further illustrate this privacy - utility tradeoff :    1 .", "case study 1 : a user shares accelerometer data with an authentication app that performs keyless validation of user identity ( _ useful inference _ )  @xcite .", "but the same data can also be used to infer the activity mode of the user  if she is walking , or standing still , or moving up or down the stairs ( _ sensitive inferences _ )  @xcite  which in turn may lead to more serious inferences regarding living habits and health conditions .", "case study 2 : a user shares location and accelerometer data with a real - time life - logging app that helps with travel planning ( _ useful inferences _ )  @xcite .", "however , the same data can also be maliciously used to extract sequences of the user s entered text ( e.g. , passwords or pins ) ( _ sensitive inference _ )  @xcite .", "case study 3 : a user shares speech data with a voice - based search app that translates speech to text for web searches ( _ useful inference _ )  @xcite .", "but the same data can also be used to recognize the user s identity ( _ sensitive inference _ )  @xcite , compromising her privacy .", "recently , several sensor privacy protection mechanisms ( sppms ) have been proposed  @xcite .", "however , most existing sppms only provide binary access control over the sensor data that requires users to choose between sharing or blocking a sensor  limiting their applicability and adoption  @xcite .", "a limited set of sppms that do focus on obfuscating sensor data , however , lack provable privacy guarantees  @xcite .    in this paper", ", we propose a new framework called deeprotect that mediates privacy - preserving access to mobile sensor data .", "deeprotect enables a new permission model where instead of blocking or allowing sensors , users specify their utility and privacy preferences in terms of inferences that can be derived from mobile sensor data .", "the overall information flow of deeprotect is summarized in figure  [ fig : motivation ] .    in deeprotect", ", users can specify as input a set of useful / authorized inferences denoted by @xmath1 ( e.g. , behavior - based authentication , speech - to - text translation ) .", "all other inferences ( possibly unknown ) are considered sensitive by default .", "alternatively , a subset of sensitive inferences can be specified by the users , which is denoted by @xmath2 ( e.g. , detection of the text entered in keyboard , speaker identity recognition ) .", "these useful and sensitive inferences represent functions that map the mobile sensor data to their corresponding inference results ( i.e. , labels ) , which is the information we want to share ( for useful inferences ) or protect ( for sensitive inferences ) .", "outgoing sensor data is initially intercepted by deeprotect , which obfuscates the data before sharing such that @xmath3 can be derived accurately by the app and @xmath4 is kept private .    * approach overview *", ": we now illustrate the working of deeprotect by using case study 3 as an example .", "figure  [ venn_diagram ] shows the venn diagram of information measures in deeprotect . to protect users privacy", ", it is required that we should not directly share the raw sensor data @xmath5 ( the entire white square shown in figure  [ venn_diagram]1 ) with the untrusted third - party apps . in case", "study 3 , @xmath5 corresponds to the microphone data .", "our objective is to enable @xmath3 ( speech - to - text translation ) with high accuracy while protecting against @xmath4 ( speaker identity recognition ) . the key idea in deeprotect is to first ignore the information that is orthogonal to authorized ( useful ) inference information . in this case , we only retain information that enable @xmath3 and ignore the remaining information @xmath6 ( shown in yellow in figure  [ venn_diagram]2 ) . in the next step ,", "we perturb those information that are correlated to both the useful and sensitive inferences , @xmath7 ( shown in blue in figure  [ venn_diagram]3 ) , to satisfy provable privacy guarantees .", "this helps in selectively hindering the sensitive inferences without severely degrading the useful inferences .    for arbitrary useful and sensitive inferences , the three - stage data obfuscation pipeline used in deeprotect", "is illustrated in figure  [ fig : pipeline ] . in the first stage", ", we extract features from raw sensor data and perform _ data minimization _", "@xcite to guarantee that only features that are relevant to the authorized ( useful ) inferences are retained for further processing .", "specifically , deeprotect modifies an autoencoder network  @xcite to _ automatically _ explore the inherent structural characteristics of the sensor data , and learn a sparse feature representation of the data specific to the useful inferences . at this point ,", "some of the extracted features , although highly specific to the useful inferences , might still have some correlation with the sensitive inferences . in the next step of the pipeline , to provide provable privacy guarantees for a single user s mobile sensor stream , we exploit _ local differential privacy _", "@xcite to obfuscate the extracted features .", "note that differential privacy  @xcite is typically applied in multi - user settings , and local differential privacy can be applied in single - user settings .", "specifically , we develop a computationally efficient mechanism that perturbs the sparse features , using either the conventional local differential privacy framework  @xcite or our relaxed notion of local differential privacy framework , depending on users specification of sensitive inferences .", "finally , we reconstruct obfuscated sensor data from the perturbed features such that the existing interfaces for mobile apps can continue to work without any change .", "* usage mode : * deeprotect supports two usage modes , depending on how sensitive inferences are specified .    _ usage mode 1 : _ only the useful inference set @xmath3 is provided and the sensitive inference set @xmath8 is the default set of ` all possible inferences ' . under this mode ,", "the user is interested in rigorous privacy guarantees with respect to the entire set ( possibly unknown ) of inferences .", "deeprotect first implements deep learning based feature extraction method to raw sensor data and then applies local differential privacy to these extracted features .", "_ usage mode 2 : _ both the useful and the sensitive inference sets are provided . under this scenario ,", "the user is interested in protecting only a specific set of sensitive inferences ( and not all possible inferences ) . to provide provable privacy guarantees", ", we develop a relaxed variant of local differential privacy and use it to perturb the extracted features in a fine - grained manner .    our theoretical analysis ( section  [ sec_thory ] ) and", "experimental results on real - world datasets ( section  [ sec_evaluation ] ) show that both usage modes of our approach significantly outperform previous state - of - the - art with up to 8x improvement .", "* contributions : * we now summarize our contributions under three main thrusts :    1 .   _ provable privacy guarantees in single - user settings : _ we support rigorous local differential privacy guarantees for a single user s mobile sensor data", "however , this rigorous privacy guarantee can require significant addition of noise impacting application utility .", "thus , we propose a novel relaxed variant of local differential privacy for providing inference - based access control over the mobile sensor data .", "our proposed privacy metric satisfies composition properties , and can achieve improved utility by focusing on protecting a specific set of inferences .", "_ novel perturbation mechanisms for enhanced privacy - utility tradeoffs : _ we propose an effective perturbation mechanism which consists of two key techniques : deep learning based data minimization and feature obfuscation based data perturbation .", "our first technique automatically extracts features relevant to the useful / authorized inferences by extending the deep learning models for dimensionality reduction , which to our best knowledge is the first such attempt .", "for the second technique , we propose effective feature perturbation methods , that achieve both conventional and relaxed notions of local differential privacy guarantees . through rigorous theoretical analysis , we demonstrate the advantage of our approach over the state - of - the - art obfuscation mechanisms .", "implementation and evaluation : _ we implement our system using real - world datasets and mobile apps ( for all the case studies discussed previously ) . for our first feature learning step , we demonstrate that our modified deep learning network outperforms the state - of - the - art feature extraction approaches with up to @xmath9 improvement . for our end - to - end approach combining both feature extraction and perturbation ,", "we demonstrate the advantage of our technique over the state - of - the - art obfuscation mechanisms with up to @xmath0 improvement .", "we also implement experiments to show the effectiveness of our method in defending against inference attacks leveraging mobile sensor data .", "we will provide an open source software framework for all the experiments carried out in our study for independent verification and extensions of our results ."], ["mockdroid  @xcite is a modified version of the android operating system ( os ) with ability to ` mock ' a given resource requested by an app .", "this approach allows users to revoke access to particular resources at run - time , encouraging users to consider the trade - off between functionality and privacy .", "appfence  @xcite offers two approaches for protecting sensitive data from untrusted apps : shadowing and blocking sensitive data from being exfiltrated off the device .", "identidroid  @xcite is also a customized android os providing different identifications and privileges , to limit the uniqueness of device and user information .", "the above systems still rely on the user to determine the sensitive sensors and block , mock , or shadow them .", "ipshield  @xcite , provides users the ability to specify their privacy preferences in terms of inferences , however , they only generate binary privacy policies of _ allow _ and _ deny _ for individual sensors", ". such binary access control over the sensor data may seriously affect the functionality of various apps .", "deeprotect on the other hand provides the much needed automatic translation from higher - level privacy specifications in terms of inferences to obfuscation of sensor data .", "another important limitation of the state - of - the - art sppms is that they are often heuristic in nature and fail to provide rigorous privacy guarantees for inference - based access control over sensors in mobile devices .", "for instance , ipshield  @xcite does provide users with ability to configure obfuscation policies for raw sensor data ( through addition of random noise ) , but does not quantify any privacy guarantees for those policies .", "boxify  @xcite provides app sandboxing for untrusted apps and can be used to run an untrusted app in an isolated environment with minimum access privileges .", "easedroid  @xcite uses semi - supervised learning to perform automatic policy analysis and refinement .", "this could be used to compliment deeprotect by automatically learning the set of inferences that need to be protected and ones that need to be released from access patterns and audit logs .", "privacy - preserving mobile data aggregation has been studied in  @xcite .", "however , they assume the presence of a multi - user database whereas we aim to protect inference - based privacy for a single user s sensor stream .", "we surveyed more than @xmath10 research papers published in relevant conferences and journals over the past @xmath11 years to form a knowledge repository of inferences ( see table  [ tableknowledge ] in the appendix ) that can be made using a combination of sensors accessed by mobile apps . this table forms the universe of possible inferences over which the set of useful and sensitive inferences in deeprotect are defined .      to the best of our knowledge", ", deeprotect is the first system that provides a novel inference - based access control in mobile sensing applications .", "furthermore , deeprotect 1 ) provides provable privacy guarantees including both conventional local differential privacy and our relaxed variant of local differential privacy for sensitive sensor data ; 2 ) presents an effective mechanism consisting of two key techniques : deep learning based data minimization and feature obfuscation based data perturbation ; 3 ) outperforms the state - of - the - art methods in multiple mobile sensing datasets and applications ."], ["[ sec_threat ] we assume that deeprotect together with the underlying mobile os , sensors , and system services form the trusted domain . our adversary is an untrusted app provider , who publishes apps in the marketplace for advertised useful inferences .", "the app accesses sensors including ones to which the user has provided explicit permission , and also others for which no permission is required .", "the app would then send out the collected sensor data to the adversary .", "our goal is to ensure that the data shared with the app can only be used for deriving the authorized ( useful ) inferences and not any sensitive inferences .", "we aim to limit / bound the adversary s inference of the user s sensitive information via access to the obfuscated sensor data while achieving two rigorous privacy properties : ( a ) first , we aim to provide rigorous local differential privacy guarantees  @xcite , that limit an adversary against all possible inferences ( corresponding to usage mode 1 in section  [ sec : intro ] ) ; and ( b ) second , we aim to provide a novel relaxed variant of local differential privacy that limits an adversary against a specified set of sensitive inferences ( corresponding to usage mode 2 in section  [ sec : intro ] ) .", "furthermore , we adapt the popular laplace perturbation mechanism  @xcite to construct effective perturbation approaches , in order to provide both conventional and relaxed notions of local differential privacy guarantees for protecting mobile sensor data ( in sections  [ sec : usage1 ] ,  [ sec : usage2 ] ) .", "it is also interesting to note that a threat model similar to usage mode 2 ( i.e. , protecting against specified inferences ) has been explored in pufferfish privacy  @xcite and blowfish privacy  @xcite .", "for example , the set of sensitive inferences computed over sensor data in deeprotect is the set of potential secrets defined in pufferfish and blowfish privacy .", "blowfish privacy has recently been adopted by the u.s .", "census bureau demonstrating its usefulness as an enabler for practical deployment ."], ["differential privacy ( dp )  @xcite is a rigorous mathematical framework that prevents an attacker from inferring the presence or absence of a particular record in a statistical database .", "dp randomizes the query results , computed over the _ multi - user _ database , to ensure that the risk to an individual record s privacy does not increase substantially ( bounded by a function of the privacy budget @xmath12 ) as a result of participating in the database . local differential privacy ( ldp )  @xcite , as an adaption of dp ,", "is defined under the setting where the user does not trust anyone ( not even the central data collector ) .", "local privacy dates back to warner  @xcite , who proposed the randomized response method to provide plausible deniability for individuals responding to sensitive surveys . in our setting", ", we therefore apply ldp to protect a _ single _", "user s mobile sensor data while satisfying rigorous privacy guarantees .", "similar applications have also been explored in google s rappor system  @xcite .", "specifically , @xmath12-ldp is defined as follows :    [ dpdef ] _ ( @xmath12-ldp )  @xcite _ a randomized algorithm @xmath13 provides @xmath12-ldp if for any two databases @xmath14 and for any output set @xmath15 , @xmath16 where @xmath17 ( resp.@xmath18 ) is the output of @xmath13 on input @xmath19 ( resp.@xmath20 ) and @xmath12 is the privacy budget .", "smaller value of @xmath12 corresponds to a higher privacy level .    )", ".,title=\"fig:\",width=259,height=124 ] .      for a mobile sensing system , consisting of @xmath21 sensors across @xmath22 timestamps ,", "the temporal mobile sensor matrix @xmath23 is a real matrix where @xmath24 records the sensing data of the @xmath25-th sensor at the @xmath26-th timestamp . to fully explore the temporal characteristics of the data , we follow common practice  @xcite and partition it into a series of segments according to a user - specified time window size @xmath27 as shown in figure  [ twodata ] . for the @xmath28-th window", ", we stack the corresponding columns within it to form a column vector which is denoted by @xmath29 . the temporal mobile sensor matrix can thus be reformulated as @xmath30 $ ] , where @xmath31 .", "we therefore apply ldp to a single user s mobile sensor data which is segmented according to a user - specified window size . in this way", ", we can take the temporal dynamics of users mobile sensor data into consideration , while providing rigorous privacy guarantees .    from definition  [ dpdef ]", ", we observe that @xmath12-ldp has the same mathematical formulation as @xmath12-dp , except that the neighboring databases in ldp are _ any two possible databases without the constraint of one tuple s difference as in dp  @xcite_. thus , when apply ldp to mobile sensing applications , the neighboring databases represent any two segmented sensor data that differ in _ any possible sensor recording at any timestamp within the same window_. a mechanism that provides rigorous ldp can defend against any inference attacks over the obfuscated data , according to the post - processing invariant property  @xcite .", "therefore , we can provide rigorous ldp for users sensor data to defend against all possible inferences over the data ( corresponding to usage mode 1 in section  [ sec : intro ] ) .      * the need for a relaxed variant : * although we can provide rigorous ldp and defend against all inferences computed over the obfuscated sensor data ( in usage mode 1 ) , in practice , it may suffice to protect a specific subset of sensitive inferences instead of focusing on all possible inferences .", "therefore , we further consider a scenario where the user aims to defend against a _ specific subset of sensitive inferences _ over the sensor data ( corresponding to usage mode 2 ) . under this threat model , there are novel opportunities to strategically add noise and enhance utility .", "we thus relax the definition of ldp to provide provable privacy guarantees for a specific subset of sensitive inferences but significantly improve the utility performance ( will discuss in sections  [ sec_thory ] , [ sec_evaluation ] ) .", "pufferfish  @xcite and blowfish  @xcite privacy frameworks have similarly motivated the threat model of protecting against a specific set of sensitive computations over the data and have been deployed in real - world settings .", "inspired by these frameworks and definition  [ dpdef ] , we define our relaxed neighboring databases as follows :    [ neigh_data ] let us represent a set of sensitive inferences as @xmath32 , and let us represent an orthogonal function of @xmath33 as @xmath34 .", "thus , @xmath35 , where @xmath36 and @xmath37 is the orthogonal component of @xmath32 consisting of all the functions that are orthogonal to @xmath32 .", "two databases @xmath38 are relaxed neighboring databases , iff .", "@xmath39 , @xmath40 for any function @xmath34 .", "we illustrate the relaxed neighboring databases above in figure  [ nei ] . for concrete interpretation of the definition", ", we refer back to case study 3 in section  [ sec : intro ] , where the useful inference is _ speech - to - text translation _ and the sensitive inference is _ speaker identity recognition_. intuitively , both the useful and sensitive inferences rely on a limited number of features extracted from the sensor data .", "the commonly used features for speech - to - text translation are mel - frequency cepstral coefficients ( mfcc ) , spectrogram and n - gram  @xcite , while the popular features for speaker identity recognition are mfcc , spectrogram and the voice biometrics of the user ( such as her pitch , accent , etc . )", "according to definition  [ neigh_data ] , the relaxed neighboring databases there refer to two sensor data streams that correspond to different speakers ( i.e. , different values of mfcc , spectrogram and the voice biometrics ) and have the same values of n - gram ( the orthogonal component ) .", "therefore , the relaxed neighboring databases in definition  [ neigh_data ] focuses on protecting the privacy of sensitive inferences only ( whose values differ in the two databases ) , and not the component of the data that is orthogonal to sensitive inferences ( whose values remain the same in the two databases ) .    , which have different sensitive inference values whilst the same values for the orthogonal inferences.,title=\"fig:\",width=259,height=67 ] .    *", "remark on orthogonal functions : * we further illustrate the relaxed neighboring databases in a three - dimensional space ( our definition can be generally applied to any space ) in figure  [ illu ] , where there are two functions @xmath41 that are orthogonal with the sensitive inference function @xmath33 , i.e. , @xmath42 . according to definition  [ neigh_data ] , we have the following properties for the two relaxed neighboring databases @xmath43 : 1 ) they have the same projection on the direction corresponding to the orthogonal functions , i.e. , @xmath44 ; 2 ) they have different projection on the direction corresponding to the sensitive inference function , i.e. , @xmath45 .    inspired by the relaxed neighboring databases in definition  [ neigh_data ] ,", "we now formulate a relaxed variant of local differential privacy ( rldp ) which is also an instantiation of the pufferfish and blowfish privacy frameworks .", "in fact , we can observe that our set of sensitive inferences computed over sensor data is the set of potential secrets defined in pufferfish privacy ( definition 3.4 in @xcite ) and blowfish privacy ( definition 4.2 in @xcite ) .", "[ ipdef]_(@xmath12-rldp ) _ for a set of sensitive inference functions @xmath46 and a privacy budget @xmath12 , a randomized algorithm @xmath13 satisfies @xmath12-rldp for protecting @xmath46 , if for any two relaxed neighboring databases @xmath38 defined in definition  [ neigh_data ] and any output set @xmath47 , @xmath48 smaller values of @xmath12 correspond to higher privacy level .", "to achieve @xmath12-rldp for protecting sensitive information , it is required that the shared data @xmath49 conditioned on any two different sensitive information @xmath50 ( where @xmath39 in definition  [ neigh_data ] ) , is statistically indistinguishable from each other . this in turn guarantees that an adversary gains negligible information about the true sensitive information upon observing the shared data .", "we re - iterate that unlike traditional ldp frameworks , its relaxed variant focuses only on the privacy of a specific subset of sensitive inferences .", "a perturbation mechanism developed to satisfy rldp guarantees would thus result in better utility performance than the ldp mechanisms ( will discuss in sections  [ sec_thory ] , [ sec_evaluation ] ) .     in definition  [ neigh_data ] .", "@xmath41 are orthogonal functions of the sensitive inference function @xmath33.,title=\"fig:\",width=240,height=115 ] .     + .    *", "( sequential composition theorem)*[seq ] let randomized algorithm @xmath51 ( @xmath52 ) each provide @xmath53-rldp under the sensitive inference functions @xmath33 .", "the sequence of these algorithms @xmath54 provides @xmath55-rldp .    *", "( parallel composition theorem)*[paral ] let randomized algorithms @xmath51 ( @xmath52 ) provide @xmath53-rldp under the sensitive inference functions @xmath33 and @xmath56 be arbitrary disjoint data set .", "the sequence of these randomized algorithm @xmath57 provides @xmath58-rldp ."], ["in this section , we design effective perturbation mechanisms to achieve @xmath12-ldp and @xmath12-rldp ( corresponding to the two usage modes in section  [ sec : intro ] ) for protecting mobile sensor data in a single - user setting .", "our key insight is to exploit the structural characteristics of mobile sensor data to enhance privacy - utility tradeoffs . before proposing our privacy mechanisms", ", we first introduce a baseline approach which directly generalizes the traditional dp perturbation mechanisms to achieve @xmath12-ldp .", "to achieve @xmath12-dp , the laplace perturbation mechanism ( lpm )  @xcite applies noise drawn from a suitable laplace distribution to perturb the query results .", "more formally , for a query function @xmath59 , lpm computes and outputs @xmath60 , where @xmath61 is the parameter of the laplacian noise and @xmath62 is the global sensitivity of @xmath59  @xcite .", "when applying ldp ( recall definition  [ dpdef ] ) on segmented mobile sensor data @xmath63 , the neighboring databases @xmath64 may differ in all their possible tuples ( i.e. , all sensor recordings across all timestamps within the same window ) , while the neighboring databases in traditional dp frameworks only differ in one tuple . according to the _ composition theorem _ of dp  @xcite", ", we propose a baseline approach to achieve @xmath12-ldp , which inserts a laplacian noise to each temporal sensor data point with the same parameter of @xmath65   is estimated from our collected data serving as a local sensitivity .", "] , where @xmath66 is the dimension of each segmented sensor data @xmath63 ( we refer interested readers to @xcite for similar perturbation mechanisms ) .", "the baseline approach achieves @xmath12-ldp guarantees and the corresponding proof is deferred to the appendix to improve readability .", "note that the baseline approach introduces noise that is linear in the number of temporal sensor recordings within the time window .", "this increases the magnitude of noise that needs to be added and thus degrades the usability of the data .", "therefore , a better alternative to protect sensor data is to build a compact , privacy - preserving synopsis from the data by exploiting its structural characteristics .", "data minimization  @xcite is a fundamental legal instrument that protects privacy by limiting the collection of personal data to the minimum extent necessary for attaining legitimate goals . in our work , we enforce the principle of data minimization and retain only the minimum number of features necessary to enable the authorized inferences .", "these extracted features may still be correlated with sensitive inferences ; thus we incorporate local differential privacy to obfuscate the extracted features for protecting against sensitive inferences .    in this section ,", "we propose deeprotect ( recall its pipeline in figure  [ fig : pipeline ] ) which consists of two key steps of 1 ) first extracting features based on the data minimization principle and 2 ) then perturbing these features to achieve both conventional and relaxed notions of ldp guarantees ( corresponding to the two usage modes in section  [ sec : intro ] respectively ) .", "we will demonstrate the significant advantages of our mechanism over the existing privacy methods theoretically as well as using multiple real - world case studies on real datasets .", "mobile sensor data is usually high - dimensional in nature , which typically exhibits both structure and redundancy , allowing minimization  @xcite .", "this lays the foundation for the first technique in our deeprotect system : deep learning based data minimization .", "* existing autoencoder models : * deep learning models  @xcite learn multi - layer transformations from the input data to the output representations , which is more powerful for feature extraction than hand - crafted shallow models . among the building blocks of these models ,", "autoencoders @xcite automatically extract features in an unsupervised manner by minimizing the reconstruction error between the input and its reconstructed output .", "a single - layer autoencoder is shown in figure  [ auto ] .", "the encoder function @xmath67 maps the input data @xmath63 to the hidden units ( features ) according to @xmath68 , where @xmath69 is typically a sigmoid function , @xmath70 is a weight matrix and @xmath71 is a bias vector .", "the decoder function @xmath72 maps these features back to the original input space according to @xmath73 , where @xmath74 is usually the same form as that in the encoder , @xmath75 is a weight matrix and @xmath76 a bias vector . considering the inherent structure of the mobile sensor data  @xcite , we aim to learn an effective feature space on which the mobile sensor data would have a succinct representation , and the corresponding objective function is as follows .     .", "@xmath77    where @xmath78 is the reconstruction error between the input data @xmath63 and its reconstructed output @xmath79 ( detailed mathematical formulation for @xmath80 is deferred to the appendix to improve readability ) .", "existing autoencoders thus aim to minimize @xmath81 with respect to @xmath82  @xcite .", "* constructing data minimization model : * using a nonlinear encoding function an autoencoder can typically extract better features than previous linear transformation methods  @xcite .", "but , the features learnt are not specific to the useful ( authorized ) inferences . to handle this", ", we explicitly modify the autoencoder models in eq .", "[ cost ] by incorporating the useful inferences and other associated constraints as follows .    * incorporating useful inferences : * the objective for our data minimization mechanism is to maximize the utility performance with the minimum amount of information , therefore it is important to combine the useful inferences with the autoencoder models to automatically extract features in a supervised manner .", "_ to the best of our knowledge , this is the first work to modify the deep learning models through incorporating the authorized inferences . _", "specifically , we incorporate the minimization of cost function corresponding to the useful inferences to the objective function of existing autoencoders in eq .", "+ we analyze the cost function for each inference using case study 1 in section  [ sec : intro ] , where behavior - based authentication was considered as a useful inference and activity mode detection was deemed sensitive .", "both the useful and sensitive inferences can be mathematically transformed into a classification problem , which can be addressed by machine learning techniques .", "for instance , by leveraging the popular _ ridge regression _", "technique  @xcite , we can learn an optimal classifier as follows .", "@xmath83 where @xmath84 represents the cost function for the useful and sensitive inferences respectively . for behavior - based authentication ( _ useful inference _ ) , the label @xmath85 where @xmath86 represents the legitimate user and @xmath87 represents the adversary .", "similarly , for activity mode detection ( _ sensitive inference _ ) , @xmath88 where the labels @xmath89 and @xmath90 represent _ walking _ , _ standing still _ and _ moving up or down the stairs _ respectively .", "the optimal classifier @xmath91 learned in eq .", "[ krr_obj ] can be utilized to label the newly - coming mobile sensor data for behavior - based authentication or activity mode detection .", "the cost function in eq .", "[ krr_obj ] has been popularly used in machine learning community whilst other general models in  @xcite can also be explored as potential cost function to reflect the prediction accuracy of the inferences .", "note that our analysis are not restricted to these settings and can be utilized for arbitrary inference - based mobile applications .    * incorporating orthogonality : * in addition", ", we also aim to learn orthogonal features so that we can deal with each feature independently in our feature perturbation mechanism for achieving enhanced utility performance .", "therefore , we add another constraint as @xmath92 to the objective function to guarantee the _ orthogonality _ of the features ( similar intuition has also been utilized in  @xcite ) .    * our new autoencoder model : * incorporating the two constraints ( useful inferences and orthogonality ) into eq .", "[ cost ] and combining with eq .", "[ krr_obj ] , we construct the objective function for our data minimization method as @xmath93 where @xmath94 controls the trade - off between the reconstruction loss and the utility penalty , and @xmath92 represents the orthogonality constraint .", "note that the last two terms in eq .", "[ final_obj ] correspond to the cost function in ridge regression ( recall eq .", "[ krr_obj ] ) .", "although we only consider the cost function of ridge regression to optimize the data minimization process , our algorithm can be generalized to multiple machine learning techniques such as support vector machine  @xcite , naive bayesian  @xcite and random forests  @xcite in a straightforward manner .", "* model learning and stacking : * to minimize @xmath95 in eq .", "[ final_obj ] with respect to @xmath96 , we explore the stochastic gradient descent ( sgd ) technique , which has been shown to perform fairly well in practice  @xcite .", "furthermore , we explore multiple hidden layers to stack multiple model units , in order to generate even more compact and higher - level semantic features resulting in better data minimization . to improve readability , we defer all the details about model learning and stacking into the appendix , based on which we summarize our deep learning based data minimization mechanism in algorithm  [ alg1 ] .", "ldp considers the worst - case adversary which can rigorously protect _ against all possible inferences _ computed over the data ( recall definition  [ dpdef ] ) . in the absence of a user - specified set of sensitive inferences , or", "otherwise if the user chooses to operate under the ldp guarantees ( corresponding to usage mode 1 in section  [ sec : intro ] ) , we develop our perturbation mechanism through perturbing the features learnt from the deep learning based data minimization step in section  [ sec_feature ] . formally , to achieve @xmath12-ldp , deeprotect under usage mode 1 inserts an laplacian noise with parameter @xmath97 to each previously learned feature , where @xmath98 represent the dimension of the segmented sensor data @xmath63 and the features @xmath99 extracted from @xmath63 , respectively .", ", the number of features , i.e. , @xmath100 , needed to be perturbed is much smaller than the number of raw sampling points , i.e. , @xmath66 , in the baseline approach . ]", "deeprotect mechanism under usage mode 1 is summarized in algorithm  [ alg3 ] , which satisfies rigorous @xmath12-ldp as will be discussed in theorem  [ proofprivacy ] .", "our mechanism is different from the baseline approach in section  [ sec_baseline ] because we add laplacian noise after the application of the deep learning based data minimization step , while the baseline approach directly adds laplacian noise to the raw sensor data without the deep learning mechanism .", "after perturbing these features , we reconstruct the perturbed sensor data according to the decoder function @xmath72 in autoencoder ( recall eq .  [ cost ] ) .", "we will show that our mechanism significantly outperforms the baseline approach ( in sections  [ sec_thory ] , [ sec_evaluation ] ) .", "note that our privacy objective is also different from that in  @xcite since they aim to protect each user s training data in the deep learning training stage under the multi - user settings while in contrast we aim to protect the privacy of mobile sensor data stream in single - user settings .     + perturbed sensor data @xmath101 ; + * for each * @xmath102 +  1 .", "extract features @xmath99 from @xmath63 by data minimiza- +   -tion mechanism in algorithm  [ alg1 ] ; +  2 .", "obtain perturbed features @xmath103 by inserting noise +  of @xmath104 ; +  3 .", "reconstruct perturbed sensor data @xmath105 ;      if the user has specified a subset of sensitive inferences ( corresponding to usage mode 2 in section  [ sec : intro ] ) , we develop our perturbation mechanism ( summarized in algorithm  [ alg2 ] ) by first computing the relaxed sensitivity @xmath106 , and then inserting an laplacian noise with parameter @xmath107 to each previously learned feature , to satisfy @xmath12-rldp .", "the detailed process to compute the relaxed sensitivity is as follows .", "* computing relaxed sensitivity : * similar to _ sensitivity _ in dp  @xcite , our _ relaxed sensitivity _ can be computed as @xmath108 where @xmath109 are relaxed neighboring databases in definition  [ neigh_data ] , and the denominator is set to make _ relaxed sensitivity _ comparable with traditional _", "sensitivity _ computed over neighboring databases that differ in only one tuple .", "the sensitive inferences can be mathematically transformed into a classification problem , which can be addressed by machine learning techniques .", "the ridge regression classifier in eq .", "[ krr_obj ] is a linear function computed over the segmented sensor data @xmath63 , i.e. , @xmath110 .", "we explain the computation of relaxed sensitivity using this formulation , though our analysis can be generally applied to non - linear situations using kernel - based techniques  @xcite .", "we apply the _ gram - schmidt orthogonalization _ technique @xcite to a matrix @xmath111 $ ] ( where @xmath112 is an _ identity _ matrix and thus @xmath111 $ ] is full - rank ) , in order to obtain orthogonal vectors of @xmath113 as @xmath114 .", "based on that , we form an orthogonal matrix @xmath115 $ ] .", "any function @xmath34 that is orthogonal of the sensitive inference function @xmath32 ( recall definition  [ neigh_data ] ) can be represented by a linear combination of @xmath114 .", "for the two relaxed neighboring databases @xmath64 , we have @xmath116 , according to definition  [ neigh_data ] .", "therefore , we know that @xmath117^t=[\\gamma , 0,0,\\cdots]^t$ ] , i.e. , @xmath118^t$ ] , and the value of @xmath119 is restricted by the range of @xmath120 . since we consider the _ identity _ query ( as we publish the sanitized sensor data to mobile apps ) , we obtain the constraint of @xmath119 as @xmath121^t\\|_1=\\|{\\bm{x}}_{t1}-{\\bm{x}}_{t2}\\|_1\\le\\dim({\\bm{x}}_t)\\delta q$ ] .", "therefore , we can compute the relaxed sensitivity as follows .", "@xmath122^t\\|_1\\le \\dim({\\bm{x}}_t)\\delta q}}\\frac{\\|{\\bm{s}}^{-1}\\cdot[\\gamma,0,0,\\cdots]^t\\|_1}{\\dim({\\bm{x}}_t)}\\ ] ] it is worth noting that @xmath123 due to the constraint of relaxed neighboring databases for achieving the same orthogonal inference values ( recall definition  [ neigh_data ] ) .", "therefore , comparing to usage mode 1 , deeprotect under usage mode 2 would add less noise to the features extracted in the data minimization step thus achieving better utility ."], ["+ perturbed sensor data @xmath101 ; + * for each * @xmath102 +  1 . extract features @xmath99 from @xmath63 by data minimiza- +   -tion mechanism in algorithm  [ alg1 ] ; +  2 .", "compute relaxed sensitivity @xmath106 in eq .", "[ relaxed_sensitivity0 ] ; +  3 .", "obtain perturbed features @xmath103 by inserting noise +  of @xmath124 ; +  4 .", "reconstruct perturbed sensor data @xmath105 ;    our perturbation mechanisms summarized in algorithms  [ alg3 ] ,  [ alg2 ] satisfy @xmath12-ldp , @xmath12-rldp respectively , according to the following theorems . to improve readability , we defer the corresponding proofs to the appendix .", "[ proofprivacy ] deeprotect under usage mode 1 ( corresponding to algorithm  [ alg3 ] ) satisfies @xmath12-ldp .", "[ proofprivacy2 ] deeprotect under usage mode 2 ( corresponding to algorithm  [ alg2 ] ) satisfies @xmath12-rldp .      for a perturbation algorithm @xmath125 ,", "let us denote @xmath126 $ ] as the expected error in the release of data @xmath5 , where @xmath127 $ ] is the expectation taken over the randomness of @xmath128 .", "we quantify the utility advantage of deeprotect over the baseline approach in the following theorems  [ the_error ] , [ the_error2 ] ( corresponding to the two usage modes ) , and the detailed proofs are deferred to the appendix .", "[ the_error ] for deeprotect under usage mode 1 ( corresponding to algorithm  [ alg3 ] ) , the expected error @xmath129 is lower than that of the baseline approach in section  [ sec_baseline ] by a factor of @xmath130 , where @xmath99 is the feature set extracted from the segmented sensor data @xmath63 by using our deep learning based data minimization approach .", "[ the_error2 ] for deeprotect under usage mode 2 ( corresponding to algorithm  [ alg2 ] ) , the expected error @xmath129 is lower than that of the baseline approach in section  [ sec_baseline ] by a factor of @xmath131 , where @xmath132 are the sensitivity and the relaxed sensitivity corresponding to the query function @xmath59 , respectively .    therefore , we can see that the utility advantage of our deep learning based data minimization step in section  [ sec_feature ] is @xmath130 and of our relaxed variant of local differential privacy in section  [ sec : usage2 ] is @xmath133 ."], ["in this section , we describe our methodology for collecting sensor data from mobile devices , and the experimental setup ( including system parameters ) for evaluation .      in our experiments", ", we collected data using a google nexus 5 ( with 2.3ghz , krait 400 processor , 16 gb internal storage and 2 gb ram on android 4.4 ) and a moto360 smartwatch ( with omap 3 processor , 4 gb internal storage , 512 mb ram on android wear os ) . on the smartphone", ", data was captured from the _ accelerometer _ , _ gyroscope _ , _ orientation _ , _ ambient light _ , _ proximity _ , and _", "sensors . on the smartwatch , the _ accelerometer _ and _ gyroscope _ sensors were recorded ( recall figure  [ twodata ] ) .", "the sampling rate was fixed at @xmath134 hz .", "@xmath135 graduate students ( @xmath136 males and @xmath137 females ) in our university were invited to take our smartphone and smartwatch for two weeks and use them in the same way that they would use their personal devices in their daily lives .    to obtain the ground - truth information for performance evaluation", ", we ask the users to record labels for both the _ useful _ and _ sensitive inferences_. the labelled training data is grouped under two different categories : mode - detection data and identity - recognition data .", "users perform tasks such as walking , enunciating digits or specific alphabets , and the corresponding data segments are then labelled as per the tasks . the mode - detection data , correspond to labelled user activities ( e.g. , accelerometer data segments are marked with labels such as `` walking '' ) , and speech - to - text translation labels ( where audio segments are labelled with the corresponding spoken digit or alphabet ) .", "the identity - recognition data is used for authentication and speaker identity recognition experiments .", "the labelled data is generated by associating the identity of a user as label to a mobile device , on first use .", "we provide provable privacy guarantees for temporal mobile sensor data ( discussed above ) , which is segmented according to the parameter of time window size @xmath138 . for the deep learning based data minimization step ( in section  [ sec_feature ] )", ", we use 10-fold cross validation to generate the training data and testing data , where @xmath139 of our collected data is used as training data and the remaining @xmath140 is used as testing data .", "we repeated this process for @xmath141 iterations and reported the averaged results . in our experiments , we used stacked autoencoders ( @xmath142 in algorithm  [ alg1 ] ) with two hidden layers comprising of 15 and 7 units respectively . we will show that an autoencoder with only two - hidden layers was able to extract better features than the state - of - the - art techniques .", "the reduced number of layers ( and units ) in the autoencoder allowed us to train the model using small amount ( 2 weeks ) of labelled data from the user ( note that we are protecting the sensitive inferences for a single user ) .", "we implemented all the three case studies ( tradeoff between authentication and activity recognition , tradeoff between transportation detection and text recognition , tradeoff between speech translation and speaker identification ) discussed in section  [ sec : intro ] on our real - world dataset using the system parameters discussed above ."], ["in this section , we experimentally demonstrate the effectiveness of deeprotect using multiple real - world datasets and applications .", "we first show the advantage of our deep learning based data minimization step ( in section  [ sec_feature ] ) over existing feature extraction approaches with up to 2x improvement .", "next , we show the advantage of our end - to - end approaches combining both feature extraction and perturbation ( in sections  [ sec : usage1 ] ,  [ sec : usage2 ] ) over the baseline approach with up to 8x improvement .      we experimentally evaluate our deep learning based data minimization mechanism in section  [ sec_feature ] , using the dataset collected for case study 1 ( recall section  [ sec : intro ] ) , where the useful inference is the _ behavior - based authentication_. to show the advantage of our method , we further compare it with the state - of - the - art feature extraction approaches .", "the discrete _ fourier _ transform ( dft ) and discrete cosine transform ( dct ) are two basic transformation techniques in the signal processing community , and the haar basis forms the fundamental wavelet transformation for time - frequency signal analysis", ". the principal component analysis ( pca ) technique @xcite can also be utilized to reveal hidden structure in the mobile sensor data .", "furthermore , we also consider blind compressive sensing ( bcs ) as a typical dictionary learning method for comparison  @xcite .    * higher accuracy", "* : figure  [ feature_utility ] shows the utility - preserving performance under different feature extraction methods .", "note that @xmath143-axis is in log scale and @xmath144-axis is the accuracy for behavior - based authentication in case study 1 representing the ratio of correctly authenticated users .", "note that we use three - dimensional _", "accelerometer _ measurement for behavior - based authentication and set the window size as @xmath138 ( recall section  [ para ] ) , therefore the dimension of each segmented sensor data is @xmath145 .    from figure  [ feature_utility ]", ", we can see that 1 ) more features would be beneficial for improving the utility performance since the combination of multiple features would be more accurate and expressive to represent the input data ; 2 ) our method achieves higher accuracy than the state - of - the - art approaches with up to @xmath9 improvement .", "@xmath146 features are enough for our method to provide good utility performance with @xmath147 accuracy , while the accuracy by using all the @xmath148 features is @xmath149 . in comparison ,", "the maximum accuracy for the baseline approaches is only @xmath150 by using @xmath146 features ; 3 ) our data minimization mechanism significantly benefits from the automatic learning process which explicitly incorporates the useful inference information into deep learning models .", "based on our analysis above , we constrain the dimension of features extracted by our deep learning based data minimization method to @xmath146 , whose corresponding accuracy is higher than @xmath151 ( @xmath152 ) of the accuracy achieved by using all the features .", "therefore , we represent each of the @xmath148-dimensional input sensor data @xmath153 with a feature set @xmath99 consisting of only @xmath146 features .    * higher informativeness * : we further leverage _ informativeness _ in  @xcite as an important metric to evaluate the information - theoretic relationship between each individual feature and the useful inference information", ". the _ informativeness _", "@xmath154 of the @xmath25-th feature @xmath155 , measures the relative mutual information between the feature and the label of the utility function @xmath156 , and is computed as @xmath157 is the random variable taking the @xmath25-th value in each @xmath158 . ] . under the setting of case study 1", ", @xmath159 represents the legitimate user and the adversary , respectively .", "@xmath160 is the entropy of the variable @xmath156 and @xmath161 is the mutual information between the random variables of @xmath155 and @xmath156 . for each feature", "@xmath155 , this measure of _ informativeness _ takes a value between @xmath162 and @xmath86 , where @xmath162 means that the feature contains no information about the utility label @xmath156 and @xmath86 means that the feature can completely determine the utility label @xmath156 .", "recall that we set the dimension of features extracted by our method to @xmath146 ( see figure  [ feature_utility ] ) .", "we further evaluate their corresponding _ informativeness _ in figure  [ informativeness ] .", "we can observe that the features learned by our method have much higher _ informativeness _ with up to @xmath9 improvement over previous works .", "therefore , our proposed method captures more expressive , higher - quality features than the existing state - of - the - art approaches .          to demonstrate the effectiveness of our end - to - end perturbation mechanisms combining both feature extraction and perturbation", ", we again consider the _ behavior - based authentication _ as useful inference and the _ activity mode detection _ as sensitive inference ( recall case study 1 in section  [ sec : intro ] ) .", "we first compare the baseline method with our mechanism under usage mode 1 ( corresponding to algorithm  [ alg3 ] ) , since they achieve the same level of privacy guarantees for preventing all possible sensitive inferences .", "then , we compare the baseline method with our mechanism under usage mode 2 ( corresponding to algorithm  [ alg2 ] ) , to verify the effectiveness of our rldp for protecting a specific subset of sensitive inferences .    * utility advantage under both usage modes : * figure  [ utiltity_epsilon ] shows the utility performance computed over the obfuscated sensor data generated by deeprotect .", "we can make the following important observations using figure  [ acc_epsilon ] : 1 ) deeprotect achieves considerable advantage over the baseline approach with up to @xmath0 improvement in utility .", "this validates the effectiveness of deeprotect that not only provides rigorous privacy guarantees for protecting sensitive inferences , but also retains the utility of the perturbed data ; 2 ) deeprotect achieves better utility performance under usage mode 2 ( which only considers specific sensitive inferences ) than that under usage mode 1 ( which considers the entire set of sensitive inferences ) ; 3 ) as expected , at higher values of @xmath12 , there is an improvement in utility but at the cost of degradation in privacy ; 4 ) even at moderate value of @xmath163 which is a typical privacy budget in ldp ( similar values can also be found in google s rappor system  @xcite ) , the authentication accuracy using deeprotect ( under usage mode 2 ) is close to the noise - free level ( @xmath149 in figure  [ feature_utility ] ) .", "this observation further validates the effectiveness of our mechanisms .", "note that the neighboring databases in ldp / rldp may differ in all their possible tuples ( instead of differing in only one tuple as in dp ) .", "thus , a proper privacy budget in ldp / rldp for balancing utility and privacy is usually higher than that of dp .", "+    to investigate the effectiveness of deeprotect on real - world mobile applications , we plot the trade - off between the accuracy of making useful inferences versus the accuracy of making sensitive inferences for all the three case studies ( corresponding to usage mode 2 and thus algorithm  [ alg2 ] is applied ) , as shown in figure  [ balance123 ] . * utility - privacy tradeoffs for inference - based access control : * in figure  [ balance1 ]", ", we observe that 1 ) deeprotect achieves good inference performance for behavior - based authentication ( useful inference ) , while significantly deteriorating the activity mode detection ( sensitive inference ) . for privacy budget @xmath163 which is a typical privacy budget in ldp ( similar values have also been used in google s rappor system  @xcite ) , the accuracy for inferring the useful information ( authentication ) is larger than @xmath164 while the accuracy for inferring sensitive information ( activity modes ) drops to roughly @xmath165 which is equivalent to random guessing users data in our experiments , the accuracy for random guessing is @xmath166 . ] .", "therefore , deeprotect can achieve practical privacy while only degrading utility performance by @xmath167 .", "this provides an effective guide for users to choose a proper value of @xmath12 for real world applications ; 2 ) higher levels of perturbation would degrade the inference performance for both the useful and sensitive information ; 3 ) deeprotect is effective for defending against sensitive inference attacks computed over sensor data .", "these observations demonstrate that deeprotect works well in practice and returns an acceptable utility performance while satisfying provable privacy guarantees .", "similarly , for the other two case studies , the accuracy of the sensitive inferences degrades at a much faster rate than that of the useful inferences when more noise is added ( corresponding to a smaller privacy budget @xmath12 ) , which further validates the effectiveness of deeprotect . for case study 2 , in figure  [ balance2 ] , when @xmath168 , deeprotect achieves good performance for transportation detection ( useful inference ) , while significantly degrading the identification of entered text ( sensitive inference ) .", "for case study 3 , in figure  [ balance3 ] , when @xmath169 , deeprotect achieves good performance for speech translation ( useful inference ) , while preventing the recognition of speaker identity ( sensitive inference ) ."], ["depending on the mode the user chooses to operate in deeprotect can provide either the rigorous ldp guarantee that protects all sensitive inferences ( usage mode 1 ) or our rldp that protects a user - chosen subset of inferences ( usage mode 2 ) . while rldp provides weaker privacy guarantees than ldp , it comes with the advantage of stronger utility properties . when applying ldp / rldp on mobile sensor data , the neighboring databases may differ in all their possible tuples ( i.e. , all sensor recordings across all timestamps within the same window ) , while the neighboring databases in traditional dp only differ in one tuple .", "therefore , a proper privacy budget in ldp / rldp for balancing privacy and utility is usually higher than that of dp  @xcite .", "previous work in @xcite proposed to ignore several records in the database to bound sensitivity in dp for providing better privacy - utility tradeoffs .", "our approach of using deep learning techniques to extract authorized features is a conceptual improvement over these methods .", "while deeprotect has been presented in the context of privacy - aware data sharing on mobile phones , the techniques developed are not restricted to the specific setting , and can be applied to other scenarios beyond mobile phones .", "we will make deeprotect tool available to public as open source software .", "although our privacy guarantee is limited to segmented sensor data , we do take the temporal dynamics existing in mobile sensor stream into consideration according to a user - specified parameter of window size .", "similar to any dp - oriented metrics , our privacy guarantees also compose securely , i.e. , retain privacy guarantees even in the presence of multiple independent releases ( recall theorems  [ seq ] ,  [ paral ] ) . while there is a non - trivial utility cost incurred by our mechanisms , deeprotect ( under both usage modes )", "significantly outperforms the baseline approach .", "in addition , our utility performance can be further improved by 1 ) exploiting fine - grained correlation among sensors ( e.g. , among the three dimensions of accelerometer in case study 1 ) according to @xcite ; 2 ) training customized models specific to each individual user that installs deeprotect in phone ; and 3 ) training models based on a larger amount of users data as in @xcite .", "our perturbation mechanism can also be easily generalized to consider correlation across time windows , according to the composition properties of our privacy metrics ( refer to @xcite and theorems  [ seq],[paral ] ) .", "an important component of our perturbation mechanism is the computation of the feature sensitivity in algorithms  [ alg3],[alg2 ] .", "ways to accurately compute the sensitivity for arbitrary mobile applications , and deal with its possible underestimation would be a challenge we would like to address in the future .", "we are working towards an implementation of the auto - encoder on the android platform using one of the deep learning packages on android  @xcite .", "the training itself could be done offline and then transferred on the phone to perform data minimization in real time  @xcite .", "our off - line training process requires a small amount of labeled data , for example , in our experiments , we only required 20 users data collected for 2 weeks .    any dp - oriented mechanism that adds noise to the data ( or corresponding query ) can risk the problem of breaking the integrity constraints of these sensors . in practice , since our framework targets simple sensors such as accelerometer , gyroscope , etc .", ", our decoding process is unlikely to break the integrity constraints for these sensors .", "therefore , apps can continue to use the perturbed sensor data generated by deeprotect without any change to their existing interface ."], ["in this paper , we propose deeprotect , a general privacy - preserving framework for inference - based access control of time - series sensor data on mobile devices .", "deeprotect not only supports conventional ldp guarantees , but also provides a novel relaxed variant of ldp .", "we further propose effective perturbation mechanisms consisting of two key steps : 1 ) we uniquely explore deep learning techniques to realize data minimization and only retain features relevant to the useful ( authorized ) inferences .", "this prevents the leakage of any information that is orthogonal to the useful inferences ; and 2 ) to further enhance the privacy of sensitive inferences , we perturb the extracted features , using an effective obfuscation mechanism that ensures both conventional and relaxed ldp guarantees while simultaneously maintaining the utility of the data . finally , for reasons of compatibility with existing third - party apps , we reconstruct the sensor data , from the noisy features before sharing . through theoretical analysis and extensive experiments over multiple real - world datasets ,", "we demonstrate that compared to the state - of - the - art research , deeprotect significantly improves the accuracy for supporting mobile sensing applications while providing provable privacy guarantees ."], ["this work has been partly supported by faculty awards from google , intel and nsf .", "changchang liu is partly supported by ibm phd fellowship .", "we are very thankful to brendan mcmahan , keith bonawitz , daniel ramage , nina taft from google , and richard chow from intel for useful feedback and discussions ."], ["* sequential composition theorem : * for @xmath170 and @xmath171 , let @xmath172 and @xmath173 . for any sequence @xmath174 of outcomes @xmath175 ,", "the probability of output @xmath174 from the sequence of @xmath176 is @xmath177 . applying the definition of rldp for each @xmath51", ", we have @xmath178 .    * parallel composition theorem : * for @xmath170 and @xmath171 , let @xmath179 and @xmath180 . for any sequence @xmath174 of outcomes @xmath175 , the probability of output @xmath174 from the sequence of @xmath176 is @xmath177 . applying the definition of rldp for each @xmath51 , we have @xmath181 .", "* proof for theorem  [ the_error ] : * first , we derive the variance of a randomized algorithm @xmath13 for our deeprotect under usage mode 1 as @xmath189 .", "then , we compute the expected error as @xmath190 \\le \\mathbb{e}[\\|{a}({\\bm{x}}_t)-\\mathbb{e}[{a}({\\bm{x}}_t)]\\|_1]+\\mathbb{e}[\\|{a}({\\bm{x}}_t)-{\\bm{x}}_t\\|_1 ] = re\\_{error}({a}({\\bm{x}}_t))+\\sqrt{\\mathbb{e}[\\|{a}({\\bm{x}}_t)-{\\bm{x}}_t\\|_2 ^ 2 ] } = re\\_{error}({a}({\\bm{x}}_t))+\\sqrt{var({a}({\\bm{x}}_t))}$ ] .", "note that our deep learning based data minimization mechanism would result in a negligible reconstruction error , which makes it fairly applicable in many practical scenarios .", "it is likely that the reconstruction error @xmath191 is much lower than the perturbation error caused by adding noise .", "therefore , we approximate the utility performance of deeprotect as @xmath192 ( similar results can also be found in  @xcite ) . similarly , we evaluate the expected error for the baseline approach as @xmath193=\\mathbb{e}[\\|(lap(\\delta q/\\epsilon))\\|_1]=\\delta q/\\epsilon$ ] . comparing the utility performance for both methods , we know that deeprotect under usage mode 1 reduces the expected error of the baseline approach with a factor of @xmath194 .", "* proof for theorem  [ the_error2 ] : * first , we derive the variance of a randomized algorithm @xmath13 for our deeprotect under usage mode 2 as @xmath190 \\le \\le \\mathbb{e}[\\|{a}({\\bm{x}}_t)-\\mathbb{e}[{a}({\\bm{x}}_t)]\\|_1]+\\mathbb{e}[\\|{a}({\\bm{x}}_t)-{\\bm{x}}_t\\|_1 ] = re\\_{error}({a}({\\bm{x}}_t))+\\sqrt{\\mathbb{e}[\\|{a}({\\bm{x}}_t)-{\\bm{x}}_t\\|_2 ^ 2 ] } = re\\_{error}({a}({\\bm{x}}_t))+\\sqrt{var({a}({\\bm{x}}_t))}$ ] .", "similarly , we have @xmath195 . comparing the utility performance for both methods , we know that deeprotect under usage mode 2 reduces the expected error of the baseline approach with a factor of @xmath196 .", "[ [ model - learning - and - stacking - in - deep - learning - based - data - minimization ] ] model learning and stacking in deep learning based data minimization ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    * existing autoencoder models : * the mathematical formulations of existing autoencoder models  @xcite ( expanding eq .", "[ cost ] ) is @xmath197 where the encoder function @xmath67 maps the input data @xmath198 to the hidden units ( features ) @xmath199 according to @xmath68 , and the decoder function @xmath72 maps the outputs of the hidden units ( features ) back to the original input space according to @xmath73 .", "similar to most existing deep learning methods  @xcite , we choose the sigmoid function for both the encoding activation function @xmath69 and the decoding activation function @xmath74 , and we focus on the tied weights case in which @xmath200 ( where @xmath201 is the transpose of @xmath202 ) .", "@xmath203 is a loss function , typically the square loss @xmath204 .", "the second term in eq .", "[ cost2 ] is a weight decay term that helps prevent overfitting  @xcite . the third term in eq .", "[ cost2 ] is a sparsity constraint @xmath205 with a pre - determined sparsity parameter @xmath206 , where @xmath207 is the average activation of hidden unit @xmath25 and @xmath208 is the kl divergence between two bernoulli random variables with mean @xmath206 and @xmath209 respectively .", "* model learning : * the objective for our data minimization mechanism is to minimize @xmath95 in eq .  [ final_obj ] with respect to @xmath96 .", "we explore the stochastic gradient descent ( sgd ) technique to solve this optimization problem , which has been shown to perform fairly well in practice  @xcite .", "note that we use sgd to only solve the convex optimization problem in eq .", "[ final_obj ] ( without considering the non - convex constraint of @xmath213 ) . to train our neural network ,", "we first initialize each parameter @xmath96 to a small random value near zero , and then apply sgd for iterative optimization . before computing the exact gradient for the objective function in eq .", "[ final_obj ] with respect to each variable , we first perform the feedforward pass and evaluate @xmath214 .", "next , we compute the error terms @xmath215 as @xmath216 . finally , the gradient descents as partial derivatives can be computed as @xmath217 .", "_ update @xmath70 to satisfy orthogonality constraint : _ after obtaining @xmath70 from sgd , we need to modify @xmath70 to satisfy the orthogonality constraint of @xmath213 in eq .", "[ final_obj ] .", "this constraint is difficult to solve since it is non - convex and of high computational complexity .", "our method to overcome this constraint follows the rigorous analysis in @xcite by adopting a simple operation , @xmath218 , which sets the singular values of @xmath70 to be all ones .", "the above operation is conducted after every sgd step .", "the overall process for our deep learning based data minimization method is shown in algorithm  [ alg1 ] .    * model stacking : * although algorithm  [ alg1 ] is effective for solving eq .", "[ final_obj ] , the learnt result heavily relies on the seeds , used to initialize the optimization process .", "therefore , we use multiple hidden layers to stack the model in order to achieve more stable performance . in other words ,", "our data minimization mechanism can also be used to build a deep network through model stacking . for the first layer in the deep learning model", ", we find the optimal layer by minimizing the objective function in eq .", "[ final_obj ] .", "the representations learned by the first layer are then used as the input of the second layer , and so on so forth . using a stacked deep auto - encoder", ", we can learn stable and finer - grained features to better represent the raw sensor data ."]]}
{"article_id": "1611.07649", "article_text": ["the architectures for big data systems rely on parallel execution techniques like mapreduce @xcite for fast processing . with the growing popularity of real - time data processing in big data environments ,", "there is a pressing need to re - imagine the traditional computing techniques .", "for example , data locality in popular big data system distributions like hadoop @xcite and spark @xcite is redefined as bringing compute to data instead of the traditional approach of the moving the data that needs to get processed .", "this trend of re - inventing the traditional methods do not necessarily transform to the security needs of big data .", "the security features implemented in big data systems are still based on traditional methods for systems based on general purpose machines .", "user authentication , multi - level data access control and logging are typically used for security in big data @xcite .", "data encryption is slowly being adopted in the big data field , but it is limited by big data properties like volume and velocity . as we covered in our previous work @xcite , big data security is premature and there is a lot of scope for improvement in this area .", "for instance , the current security standards for big data systems assume system - level consistency which is not necessarily true always .", "we demonstrated in our previous work @xcite that big data platforms can be affected by insider attacks . in this work ,", "we concentrate on detecting process - level intrusions within big data systems", ".    intrusion detection systems ( ids ) can identify malicious use based on their knowledge of possible threats or by learning from the behavior of programs .", "knowledge - based ids usually search a program for known threat signatures that are stored in a database . with new and zero - day attacks emerging regularly , it is impractical to have a pre - populated database of all possible threats . even if it is assumed to have such a database , maintaining it would require a lot of resources and running search queries against it would be expensive .", "behavior based ids tries to model , analyze and compare application behavior to identify anomalies .", "this technique needs more resources and is more complex than signature - based ids but it is more effective in a dynamically changing threat environment .", "behavior based ids generally use statistics and rules to detect anomalies .", "figure [ fig_tax ] gives a taxonomy of the different types of ids .", "in today s internet age , a distributed implementation of ids is needed for which aggregation , communication and cooperation are key factors of success .", "distributed ids gives centralized control and detects behavioral patterns even in large networks but it has to be employed at multiple levels : host , network and data @xcite .", "hence , using big data in general - purpose distributed ids implementations is recommended for faster processing . in this work ,", "we concentrate on ids that can be used for security within big data systems .", "ids within a big data system favors anamoly - based ids when compared to knowledge - based ids because of the naturally large and ever increasing scope of threats .    using control - flow graphs for logic level intrusion detection", "is a commonly known idea @xcite .", "for example , control - flow integrity @xcite is a security mechanism that can identify misuse of application logic bugs , like buffer - overflow attacks .", "though cfgs are generally sparse graphs , they can grow very big in size .", "hence , it is important to design ids techniques that can work with a reduced representation of cfgs . a minimum spanning tree ( mst ) contains all vertices and only some paths of its source graph and the number of msts for sparse graphs is generally less .", "hence , a set of msts extracted from a cfg can be used for ids that detects program level anomalies .    in this paper", ", we propose a control - flow based intrusion detection technique for big data systems .", "the proposed technique checks for program level anomalies in big data applications by analyzing and comparing the control - flow behavior of all processes running inside a big data system .", "the proposed intrusion detection technique is divided into two parts .", "first , the control - flow of each process running on a data node in the big data cluster is locally analyzed .", "this is done by extracting a set of msts from the instruction level cfg of a compiled program .", "the extracted set of msts are hashed and stored in an array called the _ program signature_. then , the stored program signature is encrypted and shared with other replica nodes that run the same program . in the second step ,", "the received encrypted program signature is decrypted and matched with the local version to check for coherence . matching two program signatures", "involves finding a perfect match for every mst in a signature within the set of msts of the other .", "the result of the matching step is then shared with replica nodes for consensus .", "our technique is designed to be simple , scalable and efficient in identifying both control - flow and brute - force attacks .", "the rest of this paper is organized as follows .", "section ii gives some background about big data systems , control - flow graphs and ids .", "the various related works are also discussed here .", "section iii explains the proposed intrusion detection technique in detail .", "experimental setup and results are thoroughly discussed in section iv .", "finally , section v gives the conclusion and future work .", "in this section , background about the three topics - big data systems , control - flow graphs and intrusion detection is provided .", "the related works are briefly outlined here .", "big data systems are data driven and their work can be classified into 2 major tasks - writing user data to the disk for storage and ; reading stored data when user requests for it .", "typically , this data is quantified in units called _", "blocks_. for fast and fault - tolerant service , big data systems rely on replication of data blocks which in turn demands data consistency .", "big data systems can not afford to have read or write service - level inconsistency .", "the motivation for this work comes from a weak assumption in the big data community that the services used by a big data system to maintain data consistency are never attacked .", "it is our knowledge that this problem has not been widely addressed before .    to propose an ids for big data services ,", "it is important to understand how the services work . for this", ", we picked 2 popular big data services - reads and writes .", "when a client ( or user ) wants to write a block , the namenode picks _ n _ data nodes from the big data cluster to complete this task where _ n _ is the replication factor of the cluster .", "first the namenode checks if the datanodes are ready .", "it sends a ready request to datanode1 which when ready , forwards that request to datanode2 and so on .", "when the namenode knows that all _", "n _ datanodes are ready , it asks the client to start writing .", "the client only writes to datanode1 which is subsequently written on to datanode2 , datanode3 and so on . in case of any failure ,", "namenode orders a new datanode to maintain block replicas .", "when the client wants to read a block , namenode gives the client a list of all datanodes that have the block and the client picks first datanode .", "if there is a problem reading from datanode1 , the client request gets forwarded to the next datanode that has a copy of the same block .      0.45        0.45        0.2     0.2     a control - flow graph ( cfg ) is a directed graph representation of a program and usually a sparse graph .", "cfgs include all possible control paths in a program .", "this makes cfg a great tool to obtain control - flow behavior of its process .", "vertices in a cfg give the level of detail , such as instruction - level or basic block level , that can not be further divided .", "edges in cfg represent control jumps and are classified into two types - forward and backward .", "branch instructions , function calls , conditional and unconditional jumps account for forward edges .", "virtual calls and indirect function calls are also considered as forward edges but their destinations are difficult to determine .", "loops and returns generally account for backward edges .", "the integrity among duplicate processes that run on replica nodes of a big data system can be verified with the information available in a cfg @xcite .", "similarity check between program logic of two programs can be performed by comparing their cfgs for isomorphism .", "there are many ways to check for such graph isomorphism @xcite but analyzing the similarity of two processes by conducting cfg level graph isomorphism is hard and time consuming .", "graph isomorphism is a complex problem , sometimes known to be np - complete as well @xcite . to reduce the complexity of graph algorithms", ", cfgs can be reduced to trees or subgraphs before performing any coherence or integrity checks @xcite .", "a cfg can be converted to a tree using methods such as depth - first traversal .", "several tree structures like dominator tree , minimumm spanning tree ( mst ) , minimumm spanning arborescence ( msa ) can be extracted form cfgs @xcite . for this work ,", "mst and msa can be used interchangeably .", "cfgs can be broken into subgraphs using methods like k sub - graph matching and graph coloring .", "some popular methods for graph reduction and graph comparison that can be found in the literature are given below ( assume graphs to have n vertices and m edges ) :    * _ based on edit distance _ : using smith - waterman algorithm with levenshtein distance to identify similarity between two graphs represented as strings @xcite .", "the time complexity is o(nm ) . * _ based on traversal _ :", "( a ) a preorder traversal of a graph g where each node is processed before its descendants .", "( b ) a reverse postorder in a dag gives a topological order of the nodes @xcite .", "* _ based on dominator trees _ : a data structure built using depth first search or using the method proposed by tarjan in @xcite .", "tarjan s method has a time complexity of o((n+m)log(n+m ) ) . *", "_ based on reachability _ : transitive reduction of a sparse graph to another graph with fewer edges but same transitive closure @xcite .", "the time complexity is o(nm ) .    in this work", ", we chose to reduce a cfg to a set of msts because cfgs are generally sparse graphs and hence the size of the set of msts will be finite and small .", "edmond s algorithm can be used to extract msts from a digraph @xcite . since", "an mst contains all vertices of its graph , there will be no loss in the program instruction data .", "depending on the connectedness of the graph , the edge count will defer between the cfg and mst representation of a program .", "figure [ fig_ex1 ] shows transformation of a line of java code to basic blocks of bytecode to cfg to set of msas .", "vertices b1 , b2 , b3 , b4 are the basic blocks formed from java bytecode .", "there exists an o(m + n log n ) time algorithm to compute a min - cost arborescence @xcite .", "alternately , another approach for converting a cfg to mst using union find is used by popular compilers like llvm and gcc for security purposes .", "one known disadvantage of using cfgs and msts for security is that dynamic link library calls can not be verified .", "traditionally , ids checks for known malware in programs by performing signature matching on a threat database @xcite .", "signature match using exact string matching is limited in its scope .", "this is because variants of same attack will have different signatures .", "recently , methods to detect new malwares using statistical machine learning have been proposed .", "static analysis using cfg is another efficient way to detect intrusions but it is very complex @xcite . converting a cfg to a string and implementing string matching is another way to deal with this problem but the solution will not be polynomial . also , cfg at basic block level can have basic block variants that look different but perform the same function . to deal with these shortcomings ,", "many approximate matching techniques have been proposed . tracing applications to get", "their cfg is another approach that is used in applications like xtrace , pivottrace etc @xcite . in case of big data systems ,", "data nodes usually have the same processor architecture .", "hence it can be assumed that there will be no variants when the cfg is constructed at byte - level .", "it is then sufficient to verify similarity among the cfgs of two processes to confirm coherence in the nodes of a big data system .    1        1", "in this section , we describe our proposed two - step intrusion detection technique for big data systems .", "the first step involves capturing the control - flow of a process running on a datanode of the big data system .", "the second step involves process - level similarity check followed by consensus among replica datanodes .      in this work", ", we emphasize on process level intrusion detection by observing coherence in the behavior of duplicate processes running on replica datanodes of a distributed big data system . to capture the program behavior , the first step is to identify a representation of the program that has the information we need and filters out all other data .", "we call this representation as the program signature .", "since our goal is to identify intrusions from control - flow mismatch , our program signatures should contain all possible control flow information of a program .", "compiled source code of a program is generally used to generate static cfg .", "since most big data frameworks use a virtual machine ( like jvm ) , an instruction level cfg in this context is generated from java byte code . in this work , disassembled object code ( doc ) from java byte code", "is used as input to generate the cfg at instruction level .", "it is important for the program signature to contain only the information that is necessary .", "hence , every cfg is converted into a set of msts that are later used to generate the program signature . in this work", ", we propose the idea of representing a program by a set of msts / msas that can be extracted from a byte - level cfg using edmonds algorithm .", "this set of msts that are extracted from a cfg are further filtered to only the set of edge - disjoint msts .", "there are many versions proposed for edmonds algorithm @xcite and for this work we used a version from networkx graph library @xcite that generates edge disjoint spanning trees from the root vertex of a given digraph .", "once a minimal representation of the logic in a program is obtained in the form of an msa , it is converted into a string by listing the node list first followed by edge list , which is in accordance to the dot format representation .", "the length of a mst string in dot format is dependent on program size . to make the comparison step faster", ", we convert the variable length mst strings of a program to fixed length strings using hashing .", "the extracted set of edge - disjoint msts are hashed using popular hashing algorithms like sha or md5 to generate a set of fixed - length hash strings . since a sparse graph like", "cfg can have multiple msas , the program signature can be a single hash string or a set of hash strings .", "having all possible msas in the program signature makes the graph similarity check more reliable . in the end , a _ program signature _ is a set of fixed - length strings .", "program signatures are encrypted before being shared with replica datanodes for tighter security .", "the private key for encryption is generated from a harcoded master key if we use secure hardware like the one proposed in our previous work @xcite .", "every datanode in a big data system runs the proposed _ profiling method _ for every running process and it includes all the steps involved in converting the compiled binary of a program to its program signature . a pictorial representation of the steps in profiling method is given in figure [ fig_algo ] .", "replication property of big data systems opens scope for new methods of implementing application logic level ids techniques .", "process similarity check among duplicate nodes of the cluster helps in checking for coherence among the replica datanodes while performing a write or read operation . when a process is scheduled to run on a datanode that hosts the primary copy of a data , a signature for that process is created by the _ profiling method _", "( step 1 ) of our proposed ids technique and that signature string is shared with all replica datanodes . in the _ matching method _", "( step 2 ) , these signatures received from other datanodes are decrypted and matched with the local versions of the same process . the results are shared with all other replica datanodes for consensus . for secure communication among datanodes", ", we intend to use the same secure communication protocol that was proposed in our previous work @xcite .", "the most important part of the matching method is to check for similarity ( or dissimilarity ) between two program signatures .", "generally , graph similarity check can be performed by checking node similarity and edge similarity .", "the following points are considered while comparing msts to check for similarity among programs :    * msts are sparse graphs obtained from byte - level cfgs . hence , checking for path sensitivity is not exponential . *", "all edges are assumed to have the same weight of 1 . *", "the total number of msts for a cfg is limited ( by cayley s formula @xcite ) .", "* by edmond s theorem , a graph which is k - connected always has k edge - disjoint arborescences . *", "two msts are a perfect match if their node sets and edge sets match exactly . *", "if edge set of one mst is a subset of the edge set of another mst , the source graphs of these msts are not similar . *", "two graphs are similar if for every mst in one graph there exists a perfect match in the set of msts of the other graph .", "* hashing algorithms like sha1 or md5 are quick and efficient .    based on the points listed above", ", the following method is developed for graph similarity check .", "let us consider 2 control - flow graphs g1 and g2 .", "let @xmath0 represent g1 where n1 is the node set of the graph g1 and e1 is the edge set of the graph .", "similarly , @xmath1 represents g2 where n2 is the node set of the graph g1 and e2 is the edge set of the graph . after employing a variation of edmonds algorithm on these cfgs ( such as finding all edge - disjoint msts ) , lets us assume that m1 @xmath2 $ ] is the set of mst / msa for g1 and m2 @xmath3 $ ] is the set of mst / msa for g2 . in order to check for similarity in both graphs g1 and g2 , we check if there is a perfect match in m2 for all msts in m1 . in order to simplify the match function , we propose using a hash function on m1 and m2 that creates a unique hash for every mst .", "let h1 be a set of hashes generated from m1 and h2 be the set of hashes from m2 .", "if any hash in h1 does not exist in h2 , we deduce that the graphs are not equal .", "in this section , the experimental setup and experiments used for testing the proposed technique are provided . the results and some analysis", "are also provided .", "an amazon ec2 @xcite m4.xlarge instance running ubuntu 14.04 is used to generate msts ( and their hashes ) from cfgs using sagemath .", "the proposed technique was implemented and tested on an amazon ec2 big data cluster of 5 t2.micro nodes - 1 master node , 1 secondary master node and 3 datanodes with a replication factor of 3 .", "the list of softwares used in conducting our experiments are :    * * sagemath * @xcite is a free open - source mathematics software system for mathematical calculations .", "* * graphml * @xcite is a popular graph representation format which can used to represent both cfg and mst . * * graphviz * @xcite is open source graph visualization software that takes input in dot format and makes diagrams in useful formats . * * networkx * @xcite is a python language software package that provides graph algorithms like edmonds and vf2 . * * control - flow graph factory * @xcite is a software that generates cfgs from java bytecode ( class file ) and exports them to graphml or dot formats .", "0.48| c | c | l | * e.no*&*name*&*description * + 1 & wordmean & a map / reduce program that counts the average length of the words in the input files . + 2 & pentomino & a map / reduce tile laying program to find solutions to pentomino problems .", "+ 3 & distbbp & a map / reduce program that uses a bbp type formula to compute the exact bits of pi .", "+ 4 & aggregatewordcount & an aggregate based map / reduce program that counts the words in the input files .", "+ 5 & secondarysort & an example defining a secondary sort to the reduce .", "+ 6 & aggregatewordhist & an aggregate based map / reduce program that computes the histogram of the words in the input files .", "+ 7 & randomwriter & a map / reduce program that writes 10 gb of random data per node .", "+ 8 & teravalidate & check the results of the terasort .", "+ 9 & qmc & a map / reduce program that estimates the value of pi using a quasi - monte carlo ( qmc ) method .", "+ 10 & wordstandarddeviation & a map / reduce program that counts the standard deviation of the length of the words in the input files .", "+ 11 & wordmedian & a map / reduce program that counts the median length of the words in the input files .", "+ 12 & bbp & a map / reduce program that uses bailey borwein plouffe to compute the exact digits of pi .", "+ 13 & teragen & generate data for the terasort .", "+ 14 & sudoku & a sudoku solver .", "+ 15 & wordcount & a map / reduce program that counts the words in the input files .", "+ 16 & multifilewc & a job that counts words from several files .", "+      the proposed intrusion detection technique was tested using 16 hadoop map - reduce examples that can be found in all hadoop distributions .", "these examples cover a wide range of big data applications as listed in table [ table_examples_hadoop ] .", "the class files of these examples are readily available in the hadoop distributions .", "first , control - flow graph factory @xcite was used to generate control flow graphs from the class files .", "these graphs are stored in graphml format and given as input to a simple sagemath @xcite script that uses networkx library @xcite and computes the edge - disjoint msas and hashes them using md5 .", "a c++ application was used to implement encryption and secure communication needed for the proposed ids technique .", "the implementation was based on framework from @xcite .", "the hashes are fixed length strings and so we restrained to using a basic numeric key based left / right shift for encryption / decryption of messages . since there are no benchmarks for some of these examples , we executed them with minimum input requirements .", "* example * & * profiling method * & * cfg to msa set * & * hashing * & * matching method * & * avg hash match * & * consensus * & * proposed * & * exec time * & * % time * +    1 & wordmean & 0.0216 & 0.0216 & 7.89e-05 & 0.0190 & 0.0002 & 0.0187 & 0.0407 & 6.988 & 0.58% + 2 & pentomino & 0.0288 & 0.0288 & 8.70e-05 & 0.0196 & 0.0013 & 0.0182 & 0.0485 & 4.914 & 0.99% + 3 & distbbp * & 0.0567 & 0.0567 & 6.29e-05 & 0.0150 & 0.0019 & 0.0130 & 0.0718 & 28.58 & 0.25% + 4 & aggregatewordcount & 0.0070 & 0.007 & 5.70e-05 & 0.0145 & 0.0002 & 0.0143 & 0.0215 & 19.002 & 0.11% + 5 & secondarysort * & 0.0199 & 0.0199 & 5.10e-05 & 0.0072 & 0.0018 & 0.0054 & 0.0272 & 11.657 & 0.23% + 6 & aggregatewordhist & 0.0066 & 0.0066 & 4.20e-05 & 0.0135 & 0.0012 & 0.0122 & 0.0201 & 18.024 & 0.11% + 7 & randomwriter & 0.2561 & 0.2561 & 8.58e-05 & 0.0217 & 0.0025 & 0.0191 & 0.2779 & 29.111 & 0.95% + 8 & teravalidate & 0.0181 & 0.0181 & 5.20e-05 & 0.0169 & 0.0001 & 0.0168 & 0.0351 & 5.958 & 0.59% + 9 & qmc * & 0.0238 & 0.0238 & 7.39e-05 & 0.0202 & 0.0015 & 0.0186 & 0.0440 & 11.657 & 0.38% + 10 & wordstandarddeviation & 0.0193 & 0.0193 & 7.89e-05 & 0.0098 & 0.0021 & 0.0076 & 0.0292 & 7.112 & 0.41% + 11 & wordmedian & 0.0312 & 0.0312 & 6.20e-05 & 0.0208 & 0.0020 & 0.0187 & 0.0520 & 7.028 & 0.73% + 12 & bbp & 0.0415 & 0.0415 & 9.08e-05 & 0.0118 & 0.0003 & 0.0115 & 0.0534 & 6.865 & 0.78% + 13 & teragen & 0.0169 & 0.0169 & 5.51e-05 & 0.0131 & 0.0023 & 0.0108 & 0.0301 & 4.905 & 0.61% + 14 & sudoku * & 0.0177 & 0.0177 & 5.60e-05 & 0.0156 & 0.0006 & 0.0150 & 0.0334 & 11.657 & 0.29% + 15 & wordcount & 0.3672 & 0.3672 & 6.99e-05 & 0.0221 & 0.0023 & 0.0197 & 0.3893 & 7.034 & 5.54% + 16 & multifilewc & 0.0159 & 0.0159 & 5.20e-05 & 0.0118 & 0.0001 & 0.0116 & 0.0277 & 5.963 & 0.47% + & 0.0593 & 0.0592 & 6.59e-05 & 0.0158 & 0.0013 & 0.0144 & 0.07516 & 11.657 & 0.81% +      table [ table_hadoop_values ] , figures [ fig_r1 ] and [ fig_r2 ] show the results of our experiments . figure [ fig_r1 ] shows the comparison between the time taken to run the hadoop map - reduce examples on a big data cluster and the time taken to run the proposed intrusion detection technique .", "the execution times for some examples ( represented by * in table [ table_hadoop_values ] ) are inconsistent among multiple runs .", "we can notice from table [ table_hadoop_values ] that only 0.81% of time taken to execute an example is needed to analyze it for intrusion detection .", "the time needed to run the proposed detection technique includes ( a ) time taken to create cfg for the main method from the class file ; ( b ) time taken to extract mst set from cfg ; ( c ) time taken to hash the msts and encrypt them and ; ( d ) time taken to check for similarity among duplicate processes by comparing the program signatures . all of these values can be found in table [ table_hadoop_values ] .", "the last row of this table gives the average values .", "it can be noticed from figure [ fig_r2 ] that the time required by the proposed technique is influenced by the profiling method trying to extract msas from cfg , particularly when there are more than one msas for a cfg . though the matching method performance is directly proportional to the square of the size of the number of edge - disjoint msas in a cfg i.e.  @xmath4 worst case complexity", ", we observed that it is rare to have more than a couple of edge - disjoint msas in a cfg because of the sparse nature of cfg .", "0.47        0.47", "in this paper , we introduced a novel approach to detect program level intrusions in big data systems with help of control flow analysis .", "the main idea is to use the replication property of big data systems and check for coherence in program behavior among replica datanodes .", "behavior of a program is modeled by extracting a msa set representation of its cfg .", "similarity check among duplicate programs is performed by a complete matching among hashed sets of msas .", "experiments were conducted on real - world hadoop map - reduce examples and it is observed that the proposed technique takes only 0.8% of execution time to identify intrusions .", "the naturally sparse nature of cfgs helps in achieving this low overhead . for future work", ", we would like to explore graph string matching and compare the proposed matching method ( step2 ) with other graph isomorphism techniques .    1 dean , jeffrey , and sanjay ghemawat .", "`` mapreduce : simplified data processing on large clusters . ''", "communications of the acm 51.1 ( 2008 ) : 107 - 113 .", "white , tom .", "`` hadoop : the definitive guide . ''", "oreilly media , inc . , 2012 .", "zaharia , matei , et al .", "`` spark : cluster computing with working sets . ''", "proceedings of the 2nd usenix conference on hot topics in cloud computing . 2010 .", "omalley , owen .", "`` integrating kerberos into apache hadoop . ''", "kerberos conference .", "aditham , santosh , and nagarajan ranganathan .", "`` a novel framework for mitigating insider attacks in big data systems . '' big data ( big data ) , 2015 ieee international conference on .", "ieee , 2015 .", "tan , zhiyuan , et al .", "`` enhancing big data security with collaborative intrusion detection . ''", "cloud computing , ieee 1.3 ( 2014 ) : 27 - 33 .", "bruschi , danilo , lorenzo martignoni , and mattia monga .", "`` detecting self - mutating malware using control - flow graph matching . ''", "detection of intrusions and malware & vulnerability assessment .", "springer berlin heidelberg , 2006 .", "129 - 143 .", "nagarajan , vijay , et al .", "`` matching control flow of program versions . '' software maintenance , 2007 .", "icsm 2007 .", "ieee international conference on .", "ieee , 2007 .", "dullien , thomas , and rolf rolles .", "`` graph - based comparison of executable objects ( english version ) . ''", "sstic 5 ( 2005 ) : 1 - 3 .", "abadi , martn , et al .", "`` control - flow integrity principles , implementations , and applications . ''", "acm transactions on information and system security ( tissec ) 13.1 ( 2009 ) : 4 .", "amighi , afshin , et al .", "`` provably correct control flow graphs from java bytecode programs with exceptions . ''", "international journal on software tools for technology transfer ( 2015 ) : 1 - 32 .", "gold , robert .", "`` reductions of control flow graphs . ''", "world academy of science , engineering and technology , international journal of computer , electrical , automation , control and information engineering 8.3 ( 2014 ) : 417 - 424 .", "gabow , harold n. , et al .", "`` efficient algorithms for finding minimum spanning trees in undirected and directed graphs . ''", "combinatorica 6.2 ( 1986 ) : 109 - 122 .", "uno , takeaki .", "an algorithm for enumerating all directed spanning trees in a directed graph .", "springer berlin heidelberg , 1996 .", "j. edmonds , optimum branchings , j. res .", "standards 71b ( 1967 ) , 233240 .", "bunke , horst .", "`` on a relation between graph edit distance and maximum common subgraph . '' pattern recognition letters 18.8 ( 1997 ) : 689 - 694 .", "sharir , micha .", "`` a strong - connectivity algorithm and its applications in data flow analysis . ''", "computers & mathematics with applications 7.1 ( 1981 ) : 67 - 72 .", "georgiadis , loukas , robert endre tarjan , and renato fonseca f. werneck .", "`` finding dominators in practice . ''", "j. graph algorithms appl .", "10.1 ( 2006 ) : 69 - 94 .", "tarjan , robert e. , and mihalis yannakakis .", "`` simple linear - time algorithms to test chordality of graphs , test acyclicity of hypergraphs , and selectively reduce acyclic hypergraphs . ''", "siam journal on computing 13.3 ( 1984 ) : 566 - 579 .", "pathan , al - sakib khan , ed .", "the state of the art in intrusion prevention and detection .", "crc press , 2014 .", "wagner , david , and drew dean .", "`` intrusion detection via static analysis . '' security and privacy , 2001 .", "s&p 2001 . proceedings .", "2001 ieee symposium on .", "ieee , 2001 .", "wang , william .", "end - to - end tracing in hdfs .", "carnegie mellon university pittsburgh , pa , 2011 .", "mace , jonathan , ryan roelke , and rodrigo fonseca .", "`` pivot tracing : dynamic causal monitoring for distributed systems . ''", "proceedings of the 25th symposium on operating systems principles .", "acm , 2015 .", "koutra , danai , et al .", "algorithms for graph similarity and subgraph matching . technical report of carnegie - mellon - university , 2011 .", "cordella , luigi p. , et al .", "`` a ( sub ) graph isomorphism algorithm for matching large graphs . '' pattern analysis and machine intelligence , ieee transactions on 26.10 ( 2004 ) : 1367 - 1372 .", "shor , peter w. `` a new proof of cayley s formula for counting labeled trees . ''", "journal of combinatorial theory , series a 71.1 ( 1995 ) : 154 - 158 .", "amazon , e. c. `` amazon elastic compute cloud ( amazon ec2 ) . ''", "amazon elastic compute cloud ( amazon ec2 ) ( 2010 ) .", "sage mathematics software ( version 4.0 ) , the sage developers , 2016 , http://www.sagemath.org .", "brandes , ulrik et al .", "`` graph markup language ( graphml ) . ''", "crc ( 2013 ) .", "emden r. gansner and stephen c. north .", "`` an open graph visualization system and its applications to software engineering . '' software - practice and experience 30.11 ( 2000 ) : 1203 - 1233 .", "aric a. hagberg , daniel a. schult and pieter j. swart , `` exploring network structure , dynamics , and function using networkx '' , in proceedings of the 7th python in science conference ( scipy2008 ) , gel varoquaux , travis vaught , and jarrod millman ( eds ) , ( pasadena , ca usa ) , pp . 1115 , aug 2008 alekseev , sergej ,", "peter palaga , and sebastian reschke .", "`` bytecode visualizer . ''", "control flow graph factory .", "n.p . , 2008"], "abstract_text": ["<S> _ security and distributed infrastructure are two of the most common requirements for big data software . </S>", "<S> but the security features of the big data platforms are still premature . </S>", "<S> it is critical to identify , modify , test and execute some of the existing security mechanisms before using them in the big data world . in this paper , we propose a novel intrusion detection technique that understands and works according to the needs of big data systems . </S>", "<S> our proposed technique identifies program level anomalies using two methods - a * profiling method * that models application behavior by creating process signatures from control - flow graphs ; and a * matching method * that checks for coherence among the replica nodes of a big data system by matching the process signatures . </S>", "<S> the profiling method creates a process signature by reducing the control - flow graph of a process to a set of minimum spanning trees and then creates a hash of that set . the matching method first checks for similarity in process behavior by matching the received process signature with the local signature and then shares the result with all replica datanodes for consensus . </S>", "<S> experimental results show only 0.8% overhead due to the proposed technique when tested on the hadoop map - reduce examples in real - time . _    big data ; intrusion detection ; control - flow graph ; </S>"], "labels": null, "section_names": ["introduction", "background and related work", "proposed technique", "experimental results", "conclusion and future work"], "sections": [["the architectures for big data systems rely on parallel execution techniques like mapreduce @xcite for fast processing . with the growing popularity of real - time data processing in big data environments ,", "there is a pressing need to re - imagine the traditional computing techniques .", "for example , data locality in popular big data system distributions like hadoop @xcite and spark @xcite is redefined as bringing compute to data instead of the traditional approach of the moving the data that needs to get processed .", "this trend of re - inventing the traditional methods do not necessarily transform to the security needs of big data .", "the security features implemented in big data systems are still based on traditional methods for systems based on general purpose machines .", "user authentication , multi - level data access control and logging are typically used for security in big data @xcite .", "data encryption is slowly being adopted in the big data field , but it is limited by big data properties like volume and velocity . as we covered in our previous work @xcite , big data security is premature and there is a lot of scope for improvement in this area .", "for instance , the current security standards for big data systems assume system - level consistency which is not necessarily true always .", "we demonstrated in our previous work @xcite that big data platforms can be affected by insider attacks . in this work ,", "we concentrate on detecting process - level intrusions within big data systems", ".    intrusion detection systems ( ids ) can identify malicious use based on their knowledge of possible threats or by learning from the behavior of programs .", "knowledge - based ids usually search a program for known threat signatures that are stored in a database . with new and zero - day attacks emerging regularly , it is impractical to have a pre - populated database of all possible threats . even if it is assumed to have such a database , maintaining it would require a lot of resources and running search queries against it would be expensive .", "behavior based ids tries to model , analyze and compare application behavior to identify anomalies .", "this technique needs more resources and is more complex than signature - based ids but it is more effective in a dynamically changing threat environment .", "behavior based ids generally use statistics and rules to detect anomalies .", "figure [ fig_tax ] gives a taxonomy of the different types of ids .", "in today s internet age , a distributed implementation of ids is needed for which aggregation , communication and cooperation are key factors of success .", "distributed ids gives centralized control and detects behavioral patterns even in large networks but it has to be employed at multiple levels : host , network and data @xcite .", "hence , using big data in general - purpose distributed ids implementations is recommended for faster processing . in this work ,", "we concentrate on ids that can be used for security within big data systems .", "ids within a big data system favors anamoly - based ids when compared to knowledge - based ids because of the naturally large and ever increasing scope of threats .    using control - flow graphs for logic level intrusion detection", "is a commonly known idea @xcite .", "for example , control - flow integrity @xcite is a security mechanism that can identify misuse of application logic bugs , like buffer - overflow attacks .", "though cfgs are generally sparse graphs , they can grow very big in size .", "hence , it is important to design ids techniques that can work with a reduced representation of cfgs . a minimum spanning tree ( mst ) contains all vertices and only some paths of its source graph and the number of msts for sparse graphs is generally less .", "hence , a set of msts extracted from a cfg can be used for ids that detects program level anomalies .    in this paper", ", we propose a control - flow based intrusion detection technique for big data systems .", "the proposed technique checks for program level anomalies in big data applications by analyzing and comparing the control - flow behavior of all processes running inside a big data system .", "the proposed intrusion detection technique is divided into two parts .", "first , the control - flow of each process running on a data node in the big data cluster is locally analyzed .", "this is done by extracting a set of msts from the instruction level cfg of a compiled program .", "the extracted set of msts are hashed and stored in an array called the _ program signature_. then , the stored program signature is encrypted and shared with other replica nodes that run the same program . in the second step ,", "the received encrypted program signature is decrypted and matched with the local version to check for coherence . matching two program signatures", "involves finding a perfect match for every mst in a signature within the set of msts of the other .", "the result of the matching step is then shared with replica nodes for consensus .", "our technique is designed to be simple , scalable and efficient in identifying both control - flow and brute - force attacks .", "the rest of this paper is organized as follows .", "section ii gives some background about big data systems , control - flow graphs and ids .", "the various related works are also discussed here .", "section iii explains the proposed intrusion detection technique in detail .", "experimental setup and results are thoroughly discussed in section iv .", "finally , section v gives the conclusion and future work ."], ["in this section , background about the three topics - big data systems , control - flow graphs and intrusion detection is provided .", "the related works are briefly outlined here .", "big data systems are data driven and their work can be classified into 2 major tasks - writing user data to the disk for storage and ; reading stored data when user requests for it .", "typically , this data is quantified in units called _", "blocks_. for fast and fault - tolerant service , big data systems rely on replication of data blocks which in turn demands data consistency .", "big data systems can not afford to have read or write service - level inconsistency .", "the motivation for this work comes from a weak assumption in the big data community that the services used by a big data system to maintain data consistency are never attacked .", "it is our knowledge that this problem has not been widely addressed before .    to propose an ids for big data services ,", "it is important to understand how the services work . for this", ", we picked 2 popular big data services - reads and writes .", "when a client ( or user ) wants to write a block , the namenode picks _ n _ data nodes from the big data cluster to complete this task where _ n _ is the replication factor of the cluster .", "first the namenode checks if the datanodes are ready .", "it sends a ready request to datanode1 which when ready , forwards that request to datanode2 and so on .", "when the namenode knows that all _", "n _ datanodes are ready , it asks the client to start writing .", "the client only writes to datanode1 which is subsequently written on to datanode2 , datanode3 and so on . in case of any failure ,", "namenode orders a new datanode to maintain block replicas .", "when the client wants to read a block , namenode gives the client a list of all datanodes that have the block and the client picks first datanode .", "if there is a problem reading from datanode1 , the client request gets forwarded to the next datanode that has a copy of the same block .      0.45        0.45        0.2     0.2     a control - flow graph ( cfg ) is a directed graph representation of a program and usually a sparse graph .", "cfgs include all possible control paths in a program .", "this makes cfg a great tool to obtain control - flow behavior of its process .", "vertices in a cfg give the level of detail , such as instruction - level or basic block level , that can not be further divided .", "edges in cfg represent control jumps and are classified into two types - forward and backward .", "branch instructions , function calls , conditional and unconditional jumps account for forward edges .", "virtual calls and indirect function calls are also considered as forward edges but their destinations are difficult to determine .", "loops and returns generally account for backward edges .", "the integrity among duplicate processes that run on replica nodes of a big data system can be verified with the information available in a cfg @xcite .", "similarity check between program logic of two programs can be performed by comparing their cfgs for isomorphism .", "there are many ways to check for such graph isomorphism @xcite but analyzing the similarity of two processes by conducting cfg level graph isomorphism is hard and time consuming .", "graph isomorphism is a complex problem , sometimes known to be np - complete as well @xcite . to reduce the complexity of graph algorithms", ", cfgs can be reduced to trees or subgraphs before performing any coherence or integrity checks @xcite .", "a cfg can be converted to a tree using methods such as depth - first traversal .", "several tree structures like dominator tree , minimumm spanning tree ( mst ) , minimumm spanning arborescence ( msa ) can be extracted form cfgs @xcite . for this work ,", "mst and msa can be used interchangeably .", "cfgs can be broken into subgraphs using methods like k sub - graph matching and graph coloring .", "some popular methods for graph reduction and graph comparison that can be found in the literature are given below ( assume graphs to have n vertices and m edges ) :    * _ based on edit distance _ : using smith - waterman algorithm with levenshtein distance to identify similarity between two graphs represented as strings @xcite .", "the time complexity is o(nm ) . * _ based on traversal _ :", "( a ) a preorder traversal of a graph g where each node is processed before its descendants .", "( b ) a reverse postorder in a dag gives a topological order of the nodes @xcite .", "* _ based on dominator trees _ : a data structure built using depth first search or using the method proposed by tarjan in @xcite .", "tarjan s method has a time complexity of o((n+m)log(n+m ) ) . *", "_ based on reachability _ : transitive reduction of a sparse graph to another graph with fewer edges but same transitive closure @xcite .", "the time complexity is o(nm ) .    in this work", ", we chose to reduce a cfg to a set of msts because cfgs are generally sparse graphs and hence the size of the set of msts will be finite and small .", "edmond s algorithm can be used to extract msts from a digraph @xcite . since", "an mst contains all vertices of its graph , there will be no loss in the program instruction data .", "depending on the connectedness of the graph , the edge count will defer between the cfg and mst representation of a program .", "figure [ fig_ex1 ] shows transformation of a line of java code to basic blocks of bytecode to cfg to set of msas .", "vertices b1 , b2 , b3 , b4 are the basic blocks formed from java bytecode .", "there exists an o(m + n log n ) time algorithm to compute a min - cost arborescence @xcite .", "alternately , another approach for converting a cfg to mst using union find is used by popular compilers like llvm and gcc for security purposes .", "one known disadvantage of using cfgs and msts for security is that dynamic link library calls can not be verified .", "traditionally , ids checks for known malware in programs by performing signature matching on a threat database @xcite .", "signature match using exact string matching is limited in its scope .", "this is because variants of same attack will have different signatures .", "recently , methods to detect new malwares using statistical machine learning have been proposed .", "static analysis using cfg is another efficient way to detect intrusions but it is very complex @xcite . converting a cfg to a string and implementing string matching is another way to deal with this problem but the solution will not be polynomial . also , cfg at basic block level can have basic block variants that look different but perform the same function . to deal with these shortcomings ,", "many approximate matching techniques have been proposed . tracing applications to get", "their cfg is another approach that is used in applications like xtrace , pivottrace etc @xcite . in case of big data systems ,", "data nodes usually have the same processor architecture .", "hence it can be assumed that there will be no variants when the cfg is constructed at byte - level .", "it is then sufficient to verify similarity among the cfgs of two processes to confirm coherence in the nodes of a big data system .    1        1"], ["in this section , we describe our proposed two - step intrusion detection technique for big data systems .", "the first step involves capturing the control - flow of a process running on a datanode of the big data system .", "the second step involves process - level similarity check followed by consensus among replica datanodes .      in this work", ", we emphasize on process level intrusion detection by observing coherence in the behavior of duplicate processes running on replica datanodes of a distributed big data system . to capture the program behavior , the first step is to identify a representation of the program that has the information we need and filters out all other data .", "we call this representation as the program signature .", "since our goal is to identify intrusions from control - flow mismatch , our program signatures should contain all possible control flow information of a program .", "compiled source code of a program is generally used to generate static cfg .", "since most big data frameworks use a virtual machine ( like jvm ) , an instruction level cfg in this context is generated from java byte code . in this work , disassembled object code ( doc ) from java byte code", "is used as input to generate the cfg at instruction level .", "it is important for the program signature to contain only the information that is necessary .", "hence , every cfg is converted into a set of msts that are later used to generate the program signature . in this work", ", we propose the idea of representing a program by a set of msts / msas that can be extracted from a byte - level cfg using edmonds algorithm .", "this set of msts that are extracted from a cfg are further filtered to only the set of edge - disjoint msts .", "there are many versions proposed for edmonds algorithm @xcite and for this work we used a version from networkx graph library @xcite that generates edge disjoint spanning trees from the root vertex of a given digraph .", "once a minimal representation of the logic in a program is obtained in the form of an msa , it is converted into a string by listing the node list first followed by edge list , which is in accordance to the dot format representation .", "the length of a mst string in dot format is dependent on program size . to make the comparison step faster", ", we convert the variable length mst strings of a program to fixed length strings using hashing .", "the extracted set of edge - disjoint msts are hashed using popular hashing algorithms like sha or md5 to generate a set of fixed - length hash strings . since a sparse graph like", "cfg can have multiple msas , the program signature can be a single hash string or a set of hash strings .", "having all possible msas in the program signature makes the graph similarity check more reliable . in the end , a _ program signature _ is a set of fixed - length strings .", "program signatures are encrypted before being shared with replica datanodes for tighter security .", "the private key for encryption is generated from a harcoded master key if we use secure hardware like the one proposed in our previous work @xcite .", "every datanode in a big data system runs the proposed _ profiling method _ for every running process and it includes all the steps involved in converting the compiled binary of a program to its program signature . a pictorial representation of the steps in profiling method is given in figure [ fig_algo ] .", "replication property of big data systems opens scope for new methods of implementing application logic level ids techniques .", "process similarity check among duplicate nodes of the cluster helps in checking for coherence among the replica datanodes while performing a write or read operation . when a process is scheduled to run on a datanode that hosts the primary copy of a data , a signature for that process is created by the _ profiling method _", "( step 1 ) of our proposed ids technique and that signature string is shared with all replica datanodes . in the _ matching method _", "( step 2 ) , these signatures received from other datanodes are decrypted and matched with the local versions of the same process . the results are shared with all other replica datanodes for consensus . for secure communication among datanodes", ", we intend to use the same secure communication protocol that was proposed in our previous work @xcite .", "the most important part of the matching method is to check for similarity ( or dissimilarity ) between two program signatures .", "generally , graph similarity check can be performed by checking node similarity and edge similarity .", "the following points are considered while comparing msts to check for similarity among programs :    * msts are sparse graphs obtained from byte - level cfgs . hence , checking for path sensitivity is not exponential . *", "all edges are assumed to have the same weight of 1 . *", "the total number of msts for a cfg is limited ( by cayley s formula @xcite ) .", "* by edmond s theorem , a graph which is k - connected always has k edge - disjoint arborescences . *", "two msts are a perfect match if their node sets and edge sets match exactly . *", "if edge set of one mst is a subset of the edge set of another mst , the source graphs of these msts are not similar . *", "two graphs are similar if for every mst in one graph there exists a perfect match in the set of msts of the other graph .", "* hashing algorithms like sha1 or md5 are quick and efficient .    based on the points listed above", ", the following method is developed for graph similarity check .", "let us consider 2 control - flow graphs g1 and g2 .", "let @xmath0 represent g1 where n1 is the node set of the graph g1 and e1 is the edge set of the graph .", "similarly , @xmath1 represents g2 where n2 is the node set of the graph g1 and e2 is the edge set of the graph . after employing a variation of edmonds algorithm on these cfgs ( such as finding all edge - disjoint msts ) , lets us assume that m1 @xmath2 $ ] is the set of mst / msa for g1 and m2 @xmath3 $ ] is the set of mst / msa for g2 . in order to check for similarity in both graphs g1 and g2 , we check if there is a perfect match in m2 for all msts in m1 . in order to simplify the match function , we propose using a hash function on m1 and m2 that creates a unique hash for every mst .", "let h1 be a set of hashes generated from m1 and h2 be the set of hashes from m2 .", "if any hash in h1 does not exist in h2 , we deduce that the graphs are not equal ."], ["in this section , the experimental setup and experiments used for testing the proposed technique are provided . the results and some analysis", "are also provided .", "an amazon ec2 @xcite m4.xlarge instance running ubuntu 14.04 is used to generate msts ( and their hashes ) from cfgs using sagemath .", "the proposed technique was implemented and tested on an amazon ec2 big data cluster of 5 t2.micro nodes - 1 master node , 1 secondary master node and 3 datanodes with a replication factor of 3 .", "the list of softwares used in conducting our experiments are :    * * sagemath * @xcite is a free open - source mathematics software system for mathematical calculations .", "* * graphml * @xcite is a popular graph representation format which can used to represent both cfg and mst . * * graphviz * @xcite is open source graph visualization software that takes input in dot format and makes diagrams in useful formats . * * networkx * @xcite is a python language software package that provides graph algorithms like edmonds and vf2 . * * control - flow graph factory * @xcite is a software that generates cfgs from java bytecode ( class file ) and exports them to graphml or dot formats .", "0.48| c | c | l | * e.no*&*name*&*description * + 1 & wordmean & a map / reduce program that counts the average length of the words in the input files . + 2 & pentomino & a map / reduce tile laying program to find solutions to pentomino problems .", "+ 3 & distbbp & a map / reduce program that uses a bbp type formula to compute the exact bits of pi .", "+ 4 & aggregatewordcount & an aggregate based map / reduce program that counts the words in the input files .", "+ 5 & secondarysort & an example defining a secondary sort to the reduce .", "+ 6 & aggregatewordhist & an aggregate based map / reduce program that computes the histogram of the words in the input files .", "+ 7 & randomwriter & a map / reduce program that writes 10 gb of random data per node .", "+ 8 & teravalidate & check the results of the terasort .", "+ 9 & qmc & a map / reduce program that estimates the value of pi using a quasi - monte carlo ( qmc ) method .", "+ 10 & wordstandarddeviation & a map / reduce program that counts the standard deviation of the length of the words in the input files .", "+ 11 & wordmedian & a map / reduce program that counts the median length of the words in the input files .", "+ 12 & bbp & a map / reduce program that uses bailey borwein plouffe to compute the exact digits of pi .", "+ 13 & teragen & generate data for the terasort .", "+ 14 & sudoku & a sudoku solver .", "+ 15 & wordcount & a map / reduce program that counts the words in the input files .", "+ 16 & multifilewc & a job that counts words from several files .", "+      the proposed intrusion detection technique was tested using 16 hadoop map - reduce examples that can be found in all hadoop distributions .", "these examples cover a wide range of big data applications as listed in table [ table_examples_hadoop ] .", "the class files of these examples are readily available in the hadoop distributions .", "first , control - flow graph factory @xcite was used to generate control flow graphs from the class files .", "these graphs are stored in graphml format and given as input to a simple sagemath @xcite script that uses networkx library @xcite and computes the edge - disjoint msas and hashes them using md5 .", "a c++ application was used to implement encryption and secure communication needed for the proposed ids technique .", "the implementation was based on framework from @xcite .", "the hashes are fixed length strings and so we restrained to using a basic numeric key based left / right shift for encryption / decryption of messages . since there are no benchmarks for some of these examples , we executed them with minimum input requirements .", "* example * & * profiling method * & * cfg to msa set * & * hashing * & * matching method * & * avg hash match * & * consensus * & * proposed * & * exec time * & * % time * +    1 & wordmean & 0.0216 & 0.0216 & 7.89e-05 & 0.0190 & 0.0002 & 0.0187 & 0.0407 & 6.988 & 0.58% + 2 & pentomino & 0.0288 & 0.0288 & 8.70e-05 & 0.0196 & 0.0013 & 0.0182 & 0.0485 & 4.914 & 0.99% + 3 & distbbp * & 0.0567 & 0.0567 & 6.29e-05 & 0.0150 & 0.0019 & 0.0130 & 0.0718 & 28.58 & 0.25% + 4 & aggregatewordcount & 0.0070 & 0.007 & 5.70e-05 & 0.0145 & 0.0002 & 0.0143 & 0.0215 & 19.002 & 0.11% + 5 & secondarysort * & 0.0199 & 0.0199 & 5.10e-05 & 0.0072 & 0.0018 & 0.0054 & 0.0272 & 11.657 & 0.23% + 6 & aggregatewordhist & 0.0066 & 0.0066 & 4.20e-05 & 0.0135 & 0.0012 & 0.0122 & 0.0201 & 18.024 & 0.11% + 7 & randomwriter & 0.2561 & 0.2561 & 8.58e-05 & 0.0217 & 0.0025 & 0.0191 & 0.2779 & 29.111 & 0.95% + 8 & teravalidate & 0.0181 & 0.0181 & 5.20e-05 & 0.0169 & 0.0001 & 0.0168 & 0.0351 & 5.958 & 0.59% + 9 & qmc * & 0.0238 & 0.0238 & 7.39e-05 & 0.0202 & 0.0015 & 0.0186 & 0.0440 & 11.657 & 0.38% + 10 & wordstandarddeviation & 0.0193 & 0.0193 & 7.89e-05 & 0.0098 & 0.0021 & 0.0076 & 0.0292 & 7.112 & 0.41% + 11 & wordmedian & 0.0312 & 0.0312 & 6.20e-05 & 0.0208 & 0.0020 & 0.0187 & 0.0520 & 7.028 & 0.73% + 12 & bbp & 0.0415 & 0.0415 & 9.08e-05 & 0.0118 & 0.0003 & 0.0115 & 0.0534 & 6.865 & 0.78% + 13 & teragen & 0.0169 & 0.0169 & 5.51e-05 & 0.0131 & 0.0023 & 0.0108 & 0.0301 & 4.905 & 0.61% + 14 & sudoku * & 0.0177 & 0.0177 & 5.60e-05 & 0.0156 & 0.0006 & 0.0150 & 0.0334 & 11.657 & 0.29% + 15 & wordcount & 0.3672 & 0.3672 & 6.99e-05 & 0.0221 & 0.0023 & 0.0197 & 0.3893 & 7.034 & 5.54% + 16 & multifilewc & 0.0159 & 0.0159 & 5.20e-05 & 0.0118 & 0.0001 & 0.0116 & 0.0277 & 5.963 & 0.47% + & 0.0593 & 0.0592 & 6.59e-05 & 0.0158 & 0.0013 & 0.0144 & 0.07516 & 11.657 & 0.81% +      table [ table_hadoop_values ] , figures [ fig_r1 ] and [ fig_r2 ] show the results of our experiments . figure [ fig_r1 ] shows the comparison between the time taken to run the hadoop map - reduce examples on a big data cluster and the time taken to run the proposed intrusion detection technique .", "the execution times for some examples ( represented by * in table [ table_hadoop_values ] ) are inconsistent among multiple runs .", "we can notice from table [ table_hadoop_values ] that only 0.81% of time taken to execute an example is needed to analyze it for intrusion detection .", "the time needed to run the proposed detection technique includes ( a ) time taken to create cfg for the main method from the class file ; ( b ) time taken to extract mst set from cfg ; ( c ) time taken to hash the msts and encrypt them and ; ( d ) time taken to check for similarity among duplicate processes by comparing the program signatures . all of these values can be found in table [ table_hadoop_values ] .", "the last row of this table gives the average values .", "it can be noticed from figure [ fig_r2 ] that the time required by the proposed technique is influenced by the profiling method trying to extract msas from cfg , particularly when there are more than one msas for a cfg . though the matching method performance is directly proportional to the square of the size of the number of edge - disjoint msas in a cfg i.e.  @xmath4 worst case complexity", ", we observed that it is rare to have more than a couple of edge - disjoint msas in a cfg because of the sparse nature of cfg .", "0.47        0.47"], ["in this paper , we introduced a novel approach to detect program level intrusions in big data systems with help of control flow analysis .", "the main idea is to use the replication property of big data systems and check for coherence in program behavior among replica datanodes .", "behavior of a program is modeled by extracting a msa set representation of its cfg .", "similarity check among duplicate programs is performed by a complete matching among hashed sets of msas .", "experiments were conducted on real - world hadoop map - reduce examples and it is observed that the proposed technique takes only 0.8% of execution time to identify intrusions .", "the naturally sparse nature of cfgs helps in achieving this low overhead . for future work", ", we would like to explore graph string matching and compare the proposed matching method ( step2 ) with other graph isomorphism techniques .    1 dean , jeffrey , and sanjay ghemawat .", "`` mapreduce : simplified data processing on large clusters . ''", "communications of the acm 51.1 ( 2008 ) : 107 - 113 .", "white , tom .", "`` hadoop : the definitive guide . ''", "oreilly media , inc . , 2012 .", "zaharia , matei , et al .", "`` spark : cluster computing with working sets . ''", "proceedings of the 2nd usenix conference on hot topics in cloud computing . 2010 .", "omalley , owen .", "`` integrating kerberos into apache hadoop . ''", "kerberos conference .", "aditham , santosh , and nagarajan ranganathan .", "`` a novel framework for mitigating insider attacks in big data systems . '' big data ( big data ) , 2015 ieee international conference on .", "ieee , 2015 .", "tan , zhiyuan , et al .", "`` enhancing big data security with collaborative intrusion detection . ''", "cloud computing , ieee 1.3 ( 2014 ) : 27 - 33 .", "bruschi , danilo , lorenzo martignoni , and mattia monga .", "`` detecting self - mutating malware using control - flow graph matching . ''", "detection of intrusions and malware & vulnerability assessment .", "springer berlin heidelberg , 2006 .", "129 - 143 .", "nagarajan , vijay , et al .", "`` matching control flow of program versions . '' software maintenance , 2007 .", "icsm 2007 .", "ieee international conference on .", "ieee , 2007 .", "dullien , thomas , and rolf rolles .", "`` graph - based comparison of executable objects ( english version ) . ''", "sstic 5 ( 2005 ) : 1 - 3 .", "abadi , martn , et al .", "`` control - flow integrity principles , implementations , and applications . ''", "acm transactions on information and system security ( tissec ) 13.1 ( 2009 ) : 4 .", "amighi , afshin , et al .", "`` provably correct control flow graphs from java bytecode programs with exceptions . ''", "international journal on software tools for technology transfer ( 2015 ) : 1 - 32 .", "gold , robert .", "`` reductions of control flow graphs . ''", "world academy of science , engineering and technology , international journal of computer , electrical , automation , control and information engineering 8.3 ( 2014 ) : 417 - 424 .", "gabow , harold n. , et al .", "`` efficient algorithms for finding minimum spanning trees in undirected and directed graphs . ''", "combinatorica 6.2 ( 1986 ) : 109 - 122 .", "uno , takeaki .", "an algorithm for enumerating all directed spanning trees in a directed graph .", "springer berlin heidelberg , 1996 .", "j. edmonds , optimum branchings , j. res .", "standards 71b ( 1967 ) , 233240 .", "bunke , horst .", "`` on a relation between graph edit distance and maximum common subgraph . '' pattern recognition letters 18.8 ( 1997 ) : 689 - 694 .", "sharir , micha .", "`` a strong - connectivity algorithm and its applications in data flow analysis . ''", "computers & mathematics with applications 7.1 ( 1981 ) : 67 - 72 .", "georgiadis , loukas , robert endre tarjan , and renato fonseca f. werneck .", "`` finding dominators in practice . ''", "j. graph algorithms appl .", "10.1 ( 2006 ) : 69 - 94 .", "tarjan , robert e. , and mihalis yannakakis .", "`` simple linear - time algorithms to test chordality of graphs , test acyclicity of hypergraphs , and selectively reduce acyclic hypergraphs . ''", "siam journal on computing 13.3 ( 1984 ) : 566 - 579 .", "pathan , al - sakib khan , ed .", "the state of the art in intrusion prevention and detection .", "crc press , 2014 .", "wagner , david , and drew dean .", "`` intrusion detection via static analysis . '' security and privacy , 2001 .", "s&p 2001 . proceedings .", "2001 ieee symposium on .", "ieee , 2001 .", "wang , william .", "end - to - end tracing in hdfs .", "carnegie mellon university pittsburgh , pa , 2011 .", "mace , jonathan , ryan roelke , and rodrigo fonseca .", "`` pivot tracing : dynamic causal monitoring for distributed systems . ''", "proceedings of the 25th symposium on operating systems principles .", "acm , 2015 .", "koutra , danai , et al .", "algorithms for graph similarity and subgraph matching . technical report of carnegie - mellon - university , 2011 .", "cordella , luigi p. , et al .", "`` a ( sub ) graph isomorphism algorithm for matching large graphs . '' pattern analysis and machine intelligence , ieee transactions on 26.10 ( 2004 ) : 1367 - 1372 .", "shor , peter w. `` a new proof of cayley s formula for counting labeled trees . ''", "journal of combinatorial theory , series a 71.1 ( 1995 ) : 154 - 158 .", "amazon , e. c. `` amazon elastic compute cloud ( amazon ec2 ) . ''", "amazon elastic compute cloud ( amazon ec2 ) ( 2010 ) .", "sage mathematics software ( version 4.0 ) , the sage developers , 2016 , http://www.sagemath.org .", "brandes , ulrik et al .", "`` graph markup language ( graphml ) . ''", "crc ( 2013 ) .", "emden r. gansner and stephen c. north .", "`` an open graph visualization system and its applications to software engineering . '' software - practice and experience 30.11 ( 2000 ) : 1203 - 1233 .", "aric a. hagberg , daniel a. schult and pieter j. swart , `` exploring network structure , dynamics , and function using networkx '' , in proceedings of the 7th python in science conference ( scipy2008 ) , gel varoquaux , travis vaught , and jarrod millman ( eds ) , ( pasadena , ca usa ) , pp . 1115 , aug 2008 alekseev , sergej ,", "peter palaga , and sebastian reschke .", "`` bytecode visualizer . ''", "control flow graph factory .", "n.p . , 2008"]]}
{"article_id": "1203.4732", "article_text": ["the relational data base model , introduced by codd in  @xcite , has been particularly successful since it is a mathematically elegant model well suited to describe almost all `` real world '' situations .", "since the query languages associated to such model ( the _ relational algebra _ and the _ relational calculus _ ) have a formal and simple definition , an interesting field of research is to study the expressive power of such language .", "codd  @xcite has proved that the relational algebra is equivalent to the _ relational calculus _ , in the sense that both query languages can compute the same set of relations .    a breakthrough in this field  @xcite has been a syntactic characterization of the set of relations that can be computed in a give data base .", "these results , also known as _ bp - completeness _ , are based on the principle of data independency from the physical representation : the information that can be extracted from the data base is completely determined at the logical level of such data base .", "this fact can be stated in a simple way : a relation @xmath0 can be computed from a data base @xmath1 if and only if all permutations over the elements of @xmath1 which preserve @xmath1 ( that is , all permutations that produce a data base isomorphic to @xmath1 ) , also preserve @xmath0 .", "an interesting interpretation of this property is that only the information given by the structure of the data can be used to differentiate data values ; consequently , a query is expressible if and only if it does not add any additional differentiation to the one initially available  @xcite .", "this idea can be rephrased by stating that the result of a query is invariant w.r.t .", "permutations of indistinguishable values ; such a permutation was captured with the notion of automorphism in  @xcite .", "while the @xmath2-criterion is a natural requirement , it refers to properties of relations in a given data base instead of queries as a whole .", "we recall that a query is an expression of the query language that can be applied to different data bases leading to possibly different results .", "thus it has been extended to a property of queries as partial functions from data bases to data bases , which is known nowadays as _", "@xcite : it has been recognized as the capability of the calculus to preserve isomorphisms between data bases , rather than automorphisms .", "genericity is a common requirement for query languages and it is traditionally related to the _ data independence _", "principle that assumes that the data base is constructed over an abstract domain which is independent from the internal representation of data .", "subsequent research has shown that this approach to the analysis of the expressiveness of a query language has certain shortcomings @xcite , mainly when new data models , such as the object - based model , are introduced .", "other notions have been proposed to analyze properties of queries in some new models  @xcite pointing out the importance of extending genericity to be used in more complex models . in  @xcite languages", "are classified w.r.t .", "the degree of the use of the equality predicate , by analyzing the invariance property of queries under different mappings ( not necessarily isomorphisms ) over the data domain , which are compatible with the relational structure of the data base .", "subsequent advances in data base theory have led to different models that take into account the limitations of the relational model when it comes to describe complex situations .", "most of such models have been introduced in the graph - based or object - oriented frameworks , but usually their mathematical foundations do not allow a complete study of the expressive power of the query languages introduced .", "in fact , to our knowledge , the only exception is the graph - based model good @xcite .    in this paper", "we introduce a different syntactic characterization of queries computable in a data base .", "our characterization relies upon the notion of partitions of the domain , where each partition represents a level of _ undifferentiation _ among objects , values or vertices .", "notice that an automorphism also can represent a certain level of undifferentiation .", "initially we will exploit such notion to give two new characterizations of relations expressible in a relational data base .", "subsequently , we will show how to apply the new framework to analyze a simple graph - based model , hence proving that our characterization can be useful in comparing the expressive power of different data languages .    following the approach of  @xcite , the data models studied in this paper are domain - preserving , that is , it is not possible to create new vertices or values , but only to query an existing data base . in our framework ,", "a binary relation over sets of data values is defined , denoted by @xmath3 , which relates those sets of values that can not be differentiated . from the relation", "@xmath3 we build some sets of partitions that _ respect _", "@xmath3 , that is , all classes in a partition are preserved by @xmath3 .", "we prove that expressiveness of a query language can be stated as the conservation of some of those partitions , where the exact set of partitions that must be preserved depends on the data model .", "the expressibility results we obtain have the following form : _ given a data base @xmath1 , let @xmath4 be a relation or a graph over the domain set of @xmath1 .", "then @xmath4 can be expressed in @xmath1 if and only if @xmath5 _ , where @xmath6 and @xmath7 are two sets of partitions which depend on the model under consideration .", "all sets considered in this paper are assumed to be finite and nonempty .", "given a set @xmath8 , a _ relation _ @xmath0 over @xmath8 is a subset of the cartesian product @xmath9 ( @xmath10 times ) for some fixed integer @xmath11 , that is a set of tuples of length @xmath10 , where all components of a tuple are elements of @xmath8 .", "the number @xmath12 is called the _ order _ or _ arity _ of the relation . given a set @xmath13 of relations over @xmath8 , the pair @xmath14 is called a _", "relational database _ ; in this setting , @xmath8 is the _ domain _ of the database , and @xmath15 is the set of relations of the database .", "given a relation @xmath16 of a database @xmath14 , we denote with @xmath17 the _ data domain _ of @xmath0 , that is the subset of the elements of the database domain @xmath8 that are in at least one tuple of @xmath0 . the notion of data domain", "is easily extended to the set @xmath15 of relations as the set union of relations data domains : @xmath18 . without loss of generality", ", we can assume that @xmath19 for every considered database @xmath14 .", "this seemingly trivial requirement is indeed very important , as it will become evident after theorem  [ teo : paredaens2 ] , therefore we will omit the universe set unless it is necessary to avoid any ambiguities .", "just as in  @xcite , when referring to a relational database , we use the _ relational algebra _ as a query language . in relational algebra", "two binary operators ( union and product ) and three unary operators ( projection , equality restriction and inequality restriction ) are given . in the following definition", "all relations are defined over the same database domain @xmath8 .", "[ def : relational - algebra ] let @xmath0 and @xmath4 be two relations with the same arity ; the _ union _ of @xmath0 and @xmath4 , denoted by @xmath20 , is simply the set  theoretical union of the two sets of tuples .", "given two relations @xmath0 and @xmath4 ( not necessarily with the same arity ) , the ( cartesian ) _ product _ of @xmath0 and @xmath4 , denoted by @xmath21 , is the set of all possible concatenations of a tuple of @xmath0 with a tuple of @xmath4 : @xmath22 .", "the abbreviation @xmath23 is used to express the relation @xmath24 ( @xmath25 times ) .", "let @xmath26 be the arity of a relation @xmath0 , @xmath27 a positive integer and @xmath28 @xmath29 a function .", "the _ projection _ of @xmath0 over @xmath30 , denoted by @xmath31 , is the relation : @xmath32 .", "now , let @xmath33 and @xmath34 be two integers such that @xmath35 , where @xmath26 is the arity of a relation @xmath0 .", "the _ equality restriction _ of @xmath0 on @xmath33 and @xmath34 is the relation , denoted by @xmath36 , that is obtained by taking from @xmath0 all the tuples for which the @xmath33-th and the @xmath34-th components are equal : @xmath37 .", "analogously , the _ inequality restriction _ of @xmath0 on @xmath33 and @xmath34 , denoted by @xmath38 , is the relation obtained by taking from @xmath0 all the tuples for which the @xmath33-th and the @xmath34-th components are different : @xmath39 .", "the five operations just described are sufficient to generate the operations of intersection , difference , join and division , usually assumed as primitives in codd s relational algebra ; a proof of this fact can be found , for example , in  @xcite .    given a relational data base @xmath40 , we will denote by @xmath41 the relation which is the result of applying the expression ( of the relational algebra ) e to the data base @xmath1 .", "moreover a relation @xmath4 over @xmath8 is told to be _ expressible _ from @xmath15 if there exists an expression @xmath42 whose operands are all relations in @xmath15 , and such that @xmath41 is equal to @xmath4 . following  @xcite ,", "we denote with @xmath43 ( basic information contained in the set of relations @xmath15 ) the set of relations that can be expressed from @xmath15 .", "as observed in  @xcite , @xmath43 is the set of the answers to all possible queries that can be asked to a relational datmabase that contains the relations @xmath15 . in  @xcite", ", paredaens gives a characterization of the class @xmath43 based upon appropriate automorphisms , that is permutations of the elements of the database domain .", "let @xmath0 be a relation of order @xmath26 over a set @xmath8 .", "as in  @xcite , an _ automorphism _ is a bijective function ( that is , a permutation ) on @xmath8 .", "we say that the automorphism @xmath44 _ respects _ the relation @xmath0 or , equivalently , that @xmath45 is @xmath0-_compatible _ if , for each tuple @xmath46 , @xmath47 .", "the compatibility of an automorphism @xmath44 with respect to a relation @xmath0 can be naturally extended to a set @xmath15 of relations in the following way : @xmath45 _ respects _ the relations in @xmath15 or , equivalently , @xmath45 is @xmath15-_compatible _ if @xmath45 is @xmath0-compatible for each relation @xmath0 in @xmath15 .", "notice that the set of automorphisms @xmath15-_compatible _ , is a group of elements , a binary associative operation on @xmath48 , and an identity element @xmath49 , such that the operation is closed and invertible in @xmath48 ] where the operation is the composition of functions and the identity is the identity function ( i.e. the function defined as @xmath51 ) . as in  @xcite , we denote with @xmath52 the set of all the automorphisms @xmath44 which are @xmath15-compatible ; with a small abuse of notation , if @xmath53 , we will usually write @xmath54 instead of @xmath55 . it will be very useful to consider the following representation of @xmath52 .", "let @xmath14 be a relational database , with @xmath56 , @xmath57 , and let @xmath58 be the set of @xmath15-compatible automorphisms .", "the following relation of arity @xmath59 : @xmath60 is called the _ cogroup  relation _ of @xmath14 .    as we can see , each row ( tuple ) of the relation @xmath61 represents one of the @xmath15-compatible automorphisms . since we do not associate any particular meaning to the elements of the domain @xmath8 , if @xmath62 we can assume , without loss of generality , @xmath63 .", "we can also assume that the first tuple of @xmath61 represents the identity function on @xmath8 ( which is always present in @xmath52 , since it is compatible with every nonempty set of relations ) ; as a consequence , it can always be assumed that the first row of @xmath61 is the tuple @xmath64 .", "let @xmath14 be a relational data base , with :    * @xmath65 * @xmath66 , with : @xmath67    it is easily verified that : @xmath68 @xmath69 if we look at the @xmath15-compatible automorphisms as permutations over @xmath8 , we can express @xmath52 as follows : @xmath70 [ ex : klein - group ]    it is not difficult to see that , for a given database @xmath14 , the set @xmath52 of @xmath15-compatible automorphisms is indeed a group with respect to function composition , with the identity function over @xmath8 as unitary element .", "in fact , the identity over @xmath8 is always in @xmath52 , the inverse of an @xmath15-compatible automorphism is still an @xmath15-compatible automorphism , and the composition between two @xmath15-compatible automorphisms is again an @xmath15-compatible automorphism . since we can always assume @xmath71", ", we can think of @xmath52 as a finite permutation group over the set @xmath72 , that is a subgroup of the symmetric group @xmath73 .", "in this paper we investigate the relation between expressive power and partitions of the database domain .", "more precisely , we investigate the possibility to characterize the expressive power of relational and graph - based databases via one or more theorems abiding to the following meta theorem .", "[ thm : scheme1 ] let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8", ". then @xmath74 , where @xmath75 and @xmath76 are sets of partitions over @xmath8 , built from the sets @xmath15 and @xmath77 of relations respectively .", "the relevance of the main result in  @xcite is that it is the first syntactic characterization of the relations that can be obtained from a given database @xmath14 when the relational algebra is used as a query language .", "more precisely , in  @xcite the following theorem is proved .", "let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8", ". then @xmath78 and @xmath79 .", "[ teo : paredaens ]    basically , paredaens has been able to point out the fundamental relation between expressiveness in a database and the set of automorphisms in the relational model .", "such result has been successively extended in  @xcite to define in a formal way the notion of _ genericity _ , that is computable queries @xcite have to be invariant with respect to the isomorphisms between databases .", "we can restate theorem  [ teo : paredaens ] in a form that will be more convenient for our purposes .", "let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8", ". then @xmath80 .", "[ teo : paredaens2 ]    first of all , we show that @xmath81 . proving that @xmath82 is trivial as @xmath83 .", "the latter stems from the fact that the relations which are expressible from @xmath15 are those obtained from @xmath84 simply ignoring the relation @xmath4 .", "let now be @xmath85 and @xmath86 .", "if the expression that gives @xmath87 from @xmath84 does contain some occurrence of the relation @xmath4 , it is sufficient to replace such occurrence with the expression that gives @xmath4 from @xmath15 to conclude that @xmath88 , and thus @xmath89 .", "it is immediate to notice that @xmath90 implies @xmath91 .    since we have established that @xmath81 , the two databases @xmath14 and @xmath92 are _ basic information equivalent _  that is , every relation of the first database can be obtained from the relations of the second database and vice versa  if and only if @xmath4 is expressible from @xmath14 .", "a direct consequence of theorem  [ teo : paredaens ] is that two databases @xmath93 and @xmath94 are basic information equivalent if and only if @xmath95 ( which are assumed to be both equal to @xmath8 ) and @xmath96 ; thus , we can conclude that @xmath80 as stated .", "we observe that , given our assumption that @xmath97 , in theorem [ teo : paredaens2 ] we can get rid of the inclusion between the domains , since it is implicit from the fact that @xmath4 is a relation over @xmath8 . on the other hand , we _ can not _ ignore the inclusion condition if we suppose that @xmath98 , since in such a situation it is not difficult to show two relations @xmath0 and @xmath4 such that @xmath99 but @xmath100 .", "a notion that seems tightly related to the expressiveness of relations in a database is that of _ indistinguishability _ between elements of the domain . intuitively , the idea is that the elements of a subset of the domain of a given database are indistinguishable if and only if no query to the database is able to divide the set in two parts , one made of the elements that occur in the relation resulting from the query and the other made of the elements that do not occur in the relation .", "in such a situation , we say that the set of indistinguishable elements can not be _ separated _ by any of the queries that can be presented to the database .", "thus , a relation resulting from a query to the database can only contain all or none of the elements of a non - separable set .    theorem  [ thm : scheme1 ] defines the general framework we propose to investigate the expressive power of query languages . in this framework", "different notions of expressible queries can be studied by considering different sets of partitions . for a given database @xmath14 , we say that a set @xmath75 of partitions of @xmath8 is a set of _ valid partitions _ if and only if it satisfies theorem  [ thm : scheme1 ] . by the results in  @xcite", ", it seems to us quite natural to define the following sets of valid partitions , namely the _ orbit partitions _ and the _ cycle partitions _ ; indeed later we will be able to prove that , in the context of theorem  [ thm : scheme1 ] , they are equivalent to the characterization of relations obtainable in a relational data base of  @xcite .", "let @xmath14 be a relational database , and let @xmath101 , @xmath102 be a partition of @xmath8 .", "@xmath103 is an _ orbit partition _ of @xmath8 with respect to @xmath15 if both the following conditions hold :    1 .   for each relation", "@xmath16 and for each class @xmath104 , @xmath105 or @xmath106 ; 2 .", "for each class @xmath107 and for each pair @xmath108 of elements of @xmath109 there exists an automorphism @xmath110 such that @xmath111 , and @xmath112 for every class @xmath113 .    we denote with @xmath114 the set of all orbit partitions of the given database @xmath14 .", "[ def : orbit - partition ]    let @xmath14 be a relational database , and let @xmath101 , @xmath102 be a partition of @xmath8 .", "@xmath103 is a _ cycle partition _ of @xmath8 with respect to @xmath15 if both the following conditions hold :    1 .   for each relation", "@xmath16 and for each class @xmath104 , @xmath105 or @xmath106 ; 2 .", "there exists an automorphism @xmath110 such that for each class @xmath107 and for each pair @xmath108 of elements of @xmath109 there exists an integer @xmath59 such that @xmath115 and @xmath112 for every class @xmath113 .", "we denote with @xmath116 the set of all cycle partitions of the given database @xmath14 .", "[ def : cycle - partition ]    as already stated for @xmath52 , if @xmath0 is a relation we will write @xmath117 and @xmath118 instead of @xmath119 and @xmath120 respectively .", "the following theorem is an alternative formulation of the main result of  @xcite ( the equivalence of the two formulation follows from theorem [ teo : paredaens2 ] ) which is more useful for our purposes .", "let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8 .", "then @xmath121 .", "let @xmath14 be a relational database , and let @xmath52 and @xmath61 be respectively the group of @xmath15-compatible automorphisms and the cogroup - relation of @xmath15 .", "a useful fact proved in  @xcite is that the cogroup - relation is expressible from @xmath15 , that is @xmath122 .", "using this fact , we are able to prove the following theorem .    @xmath123 .", "[ teo : autr = cgr ]    since @xmath122 , by theorem  [ teo : paredaens ] we can conclude that @xmath124 . now , let @xmath125 ; as we have already observed , @xmath126 is a permutation of the set @xmath97 , as well as of the tuples that compose the relation @xmath61 . thus , for each tuple @xmath127 , we have that @xmath128 . in particular , by letting @xmath59 be the cardinality of @xmath8 , we have : @xmath129 thus , the elements of @xmath8 are mapped by @xmath126 in such a way that the result is a row of the cogroup - relation ; so we can conclude that @xmath110 .    a direct consequence of theorem  [ teo : autr = cgr ] is that not only @xmath122 , as established by paredaens , but also @xmath130 for every relation @xmath16 , since @xmath131 and @xmath132 . as a corollary of theorem  [ teo : autr = cgr ] , if we are interested to study the expressive power of a given relational database @xmath14 then we can work as well on the database @xmath133 , which has only one relation and , moreover , such relation is an explicit representation of the finite permutation group @xmath52 .", "we now turn our attention to the structure of @xmath114 and @xmath116 .", "first of all we observe that , thanks to theorem  [ teo : autr = cgr ] , we can get rid of item 1 in definitions  [ def : orbit - partition ] and [ def : cycle - partition ] since , by considering the database @xmath134 , there is only one relation and , for such relation , it holds @xmath135 for each @xmath107 .    to characterize the sets of cycle and orbit partitions we need to recall some notions from basic abstract algebra .", "let @xmath136 be a set and @xmath137 a group .", "action of @xmath48 on @xmath136 _ is a map @xmath138 such that    1 .   @xmath139 ; 2 .", "@xmath140    in group theory it is customary to omit the operators symbols from expressions when confusion does not arise ; so , the expression in item 2 above is usually written as : @xmath141 .", "let @xmath48 be a group acting on a set @xmath136 .", "for @xmath142 , let @xmath143 if and only if there exists @xmath144 such that @xmath145 .", "it is not difficult to see that @xmath146 is an equivalence relation on @xmath136 , and thus it induces a partition @xmath103 on @xmath136 .", "the classes of @xmath103 are called the _ orbits _ in @xmath136 under @xmath48 . if @xmath147 , the class containing @xmath148  denoted by @xmath149", " is called the _ orbit of _ @xmath148 under @xmath48 . in other words , @xmath150 .", "it is not difficult to see that the partition induced by the orbits of @xmath52 on @xmath8 satisfies definition  [ def : orbit - partition ] .", "in fact , every automorphism @xmath110 maps each orbit @xmath151 into itself and , given a pair @xmath108 of elements of @xmath8 , there exists an automorphism that maps @xmath152 to @xmath153 if and only if @xmath152 and @xmath153 are in the same orbit .", "moreover , if @xmath154 is a subgroup of a group @xmath48 acting on the set @xmath136 , then every orbit @xmath155 is a subset of the orbit @xmath149 ; more precisely , it is not difficult to prove that the orbits induced by @xmath154 are a _ refinement _ of the orbits induced by @xmath48 . since each partition induced by the orbits of every subgroup of @xmath52 satisfies definition  [ def : orbit - partition ] , we have that @xmath114 contains the set of those partitions .", "vice versa , let @xmath156 .", "it is not difficult to see that the set of automorphisms @xmath110 that map each class of @xmath103 into itself and that map each element of a class to an element of the same class forms a subgroup of @xmath52 ; moreover , the orbit partition induced by such a subgroup is just @xmath103 . as a consequence ,", "@xmath114 is a subset of the set of partitions induced by all the subgroups of @xmath52 ; since also the converse inclusion holds , the two sets indeed coincide .", "let @xmath48 be a group acting on the set @xmath136 , and let @xmath157 .", "for @xmath142 , let @xmath143 if and only if there exists an integer @xmath59 such that @xmath158 , where @xmath159 is the application of @xmath160 for @xmath59 times .", "it is not difficult to see that @xmath146 is an equivalence relation on @xmath136 , and thus it induces a partition @xmath103 on @xmath136 .", "the classes of @xmath103 are called the _ cycles _ of @xmath160 on @xmath136 .", "analogously to what said about orbits , it is not difficult to see that the partitions induced by the cycles of the automorphisms of @xmath52 satisfy definition  [ def : cycle - partition ] .", "we observe that , while an orbit partition is induced by a subgroup of @xmath52 , a cycle partition is induced by an automorphism , that is by an element of @xmath52 .", "the class @xmath116 is thus the set of cycle partitions obtained by considering every element of @xmath52 .", "let @xmath48 be a group acting on the set @xmath136 and let @xmath160 be a permutation in @xmath48 .", "then the _ orbits of the _ ( cyclic ) _ group _ @xmath161 generated by @xmath160 are the cycles of @xmath160 .", "since @xmath161 is a subgroup of @xmath48 , we have immediately that every cycle partition of @xmath48 is also an orbit partition of @xmath48 , that is , @xmath162 .", "example  [ ex : klein - group ] can be used to show that the converse does not generally hold : not every orbit partition is also a cycle partition .", "in fact we have : @xmath163    as noted above , theorem  [ teo : autr = cgr ] allows us to deal only with cogroup - relations instead of sets of arbitrary relations .", "the same can be done when working with cycle and orbit partitions : since cycles and orbits that form the partitions in @xmath116 and @xmath114 are completely determined from the elements and the subgroups of @xmath52 respectively , by theorem  [ teo : autr = cgr ] we can conclude that @xmath164 and @xmath165 .", "it is possible to show that both the set @xmath116 of cycle partitions and the set @xmath114 of orbit partitions of a given database @xmath14 constitute a partially ordered set ( poset ) with respect to the binary relation @xmath166 , where @xmath167 iff each class of @xmath168 is contained in some class of @xmath169 , where @xmath168 and @xmath169 are two partitions in @xmath75 , @xmath75 is equal to @xmath116 or @xmath114 . in fact , it is not difficult to see that @xmath166 is reflexive , antisymmetric and transitive : that is , @xmath166 is an order relation over both @xmath116 and @xmath114 .", "one notably difference between the posets @xmath170 and @xmath171 is that the first has always a maximum element , corresponding to the orbits of the entire @xmath52 , while the second may not have a maximum element , as shown above referring to example [ ex : klein - group ] , where @xmath52 is the so called klein group . instead", ", both the posets have a minimum element , corresponding to the cycles ( equal to the orbits ) induced by the identity element of @xmath52 : that is , the trivial partition , where each class is a singleton .    in order to prove our main results we need some definitions and some well known properties of finite groups . here", "we just recall the notion of _ stabilizer _ ; we address the reader to an introductory book on abstract algebra , such as @xcite , for the notion of coset and its properties .", "let @xmath48 be a group acting on a set @xmath136 , and let @xmath147 .", "the subgroup @xmath172 of @xmath48 defined as @xmath173 is called the _ stabilizer _ of @xmath148 in @xmath48 .", "it is not difficult to see that if @xmath48 is a group which acts on the set @xmath136 , and @xmath147 , then the stabilizer @xmath172 of @xmath148 can be considered as a group which acts on the set @xmath174 .", "the following are two well known results in group theory : lagrange s theorem  which correlates the cardinality of a given group @xmath48 and the cardinality of a given subgroup @xmath154 of @xmath48 with the number of left cosets of @xmath48 with respect to @xmath154  and a theorem which expresses the cardinality of the orbit of @xmath48 containing @xmath148 as the number of left cosets of @xmath48 with respect to the stabilizer @xmath172 .", "let @xmath48 be a finite group , and let @xmath154 be a subgroup of @xmath48", ". then @xmath175 , where @xmath176 is the number of left cosets of @xmath48 with respect to @xmath154 , and is usually called the _ index of @xmath154 in @xmath48_.    let @xmath48 be a finite group acting on a set @xmath136 , and let @xmath147", ". then @xmath177 , that is there exists a one - to - one correspondence between the elements of the orbit @xmath149 of @xmath148 under @xmath48 and the left cosets of the stabilizer @xmath172 in @xmath48 .", "[ teo : card - orbit ]    we are now able to prove the following theorem .", "let @xmath48 be a subgroup of the symmetric group @xmath73 , and let @xmath154 be a subgroup of @xmath48", ". if the orbit partitions of @xmath48 and @xmath154 are the same , then @xmath178 .", "[ teo : orbit - partitions ]    we prove the assertion by induction on @xmath59 .", "for @xmath179 the theorem can be proved by direct inspection of the subgroups of @xmath73 .", "now , let us suppose that the theorem is true for @xmath180 , and let us show that it holds also for @xmath59 .", "we first observe that since the orbit partitions of @xmath48 and @xmath154 are the same , then also the orbits @xmath181 and @xmath182 of the element @xmath59 with respect to @xmath48 and @xmath154 are the same . now ,", "if we take all the partitions having @xmath183 as a class , we get the orbit partitions induced by the stabilizers @xmath184 and @xmath185 of the element @xmath59 with respect to @xmath48 and @xmath154 .", "these orbit partitions are equal and thus , by induction hypothesis , @xmath186 . by lagrange", "s theorem , we can express the cardinalities of @xmath48 and @xmath154 with respect to the cardinalities of their stabilizers as @xmath187 and @xmath188 .", "where @xmath189 and @xmath190 are the indices , respectively , of the stabilizer @xmath184 in @xmath48 and of the stabilizer @xmath185 in @xmath154 . by theorem  [ teo : card - orbit ] , we can infer that @xmath191 and @xmath192 .", "since @xmath193 and @xmath194 , we can conclude that @xmath48 and @xmath154 have the same order , and thus @xmath195 .", "theorem  [ teo : orbit - partitions ] allows us to show that the orbit partitions of a given database satisfy theorem scheme ii ; in fact , the following theorem provides a first characterization of expressible queries in relational databases alternative to the one originally given by paredaens .    let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8", ". then @xmath196 [ teo : scheme - with - orbits ]    if @xmath197 , since the orbit partitions are completely determined from the subgroups of @xmath52 , we obtain that @xmath198 .    for the converse ,", "we observe that @xmath52 is a subgroup of the symmetric group @xmath73 , and @xmath199 is a subgroup of @xmath52 . by hypothesis , the orbit partitions of @xmath52 and @xmath199 are equal and thus , by theorem  [ teo : orbit - partitions ] , @xmath197", ".    a second characterization of expressible queries in relational databases can be obtained by considering cycle partitions instead of orbit partitions .", "we need the following lemma .", "let @xmath48 be a subgroup of the symmetric group @xmath73 , and let @xmath154 be a subgroup of @xmath48 .", "if the cycle partitions of @xmath48 and @xmath154 are the same , then also the orbits of @xmath48 and @xmath154 are the same , that is @xmath200 for every @xmath201 .", "[ lem : orbits - from - cycles ]    since the orbit @xmath149 is the set of elements of @xmath72 which are reachable from @xmath148 through some element @xmath160 of @xmath48 , while a cycle containing @xmath148 is the set of elements which are reachable from @xmath148 through one element @xmath160 of @xmath48 , one method to build @xmath149 from the cycle partitions of @xmath48 is given by algorithm  [ alg:1 ] .", "result @xmath202 @xmath203    algorithm  [ alg:1 ] computes the least subset @xmath204 of @xmath72 which contains @xmath148 and such that , for every cycle partition @xmath103 of @xmath48 , @xmath204 is the union of some cycles in @xmath103 ; it is not difficult to see that @xmath204 is , indeed , the orbit @xmath149 .", "since the cycle partitions of @xmath48 and @xmath154 are the same by hypothesis , the orbits computed by the algorithm above will be the same for @xmath48 and @xmath154 , for every choice of @xmath205 .", "we are now ready to prove the following theorem .", "let @xmath48 be a subgroup of the symmetric group @xmath73 , and let @xmath154 be a subgroup of @xmath48", ". if the cycle partitions of @xmath48 and @xmath154 are the same , then @xmath178 .", "[ teo : cycle - partitions ]    by lemma  [ lem : orbits - from - cycles ] , the orbits of @xmath48 and @xmath154 are the same . thus we can prove the theorem by the same argument used for theorem [ teo : orbit - partitions ] .", "a direct consequence of theorem  [ teo : cycle - partitions ] is that the cycle partitions of a given database satisfy theorem schema ii ; thus , the following theorem provides a second characterization of expressible queries in relational databases alternative to the one originally given by paredaens .", "the proof is analogous to the one given for theorem [ teo : scheme - with - orbits ] .", "let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8 .", "then : @xmath206 [ teo : scheme - with - cycles ]    a final observation is due about theorems  [ teo : scheme - with - orbits ] and [ teo : scheme - with - cycles ] . even though there is a strong resemblance between our meta theorem  [ thm : scheme1 ] and theorem  [ teo : paredaens2 ] , our results _", "can not _ be expressed neither in the form @xmath207 and @xmath208 nor in the form @xmath209 and @xmath79 , as shown in the next example .", "let @xmath210 and @xmath211 be two relational databases , with :    * @xmath212 * r = + [ cols=\"^,^,^,^,^ \" , ]     notice that @xmath54 is the cyclic group generated by the permutation @xmath213 , while @xmath214 is the cyclic group generated by the permutation @xmath215 : @xmath216    from @xmath54 and @xmath214 we can easily obtain @xmath217 .", "clearly , @xmath4 is not expressible from @xmath0 , since we have @xmath218 but @xmath219 ; on the other hand , @xmath220 and @xmath221 , and @xmath222 and @xmath221 .", "the fact that @xmath4 is not expressible from @xmath0 can be correctly determined through orbit partitions or through cycle partitions by observing that : @xmath223 or @xmath224 .", "in this section we study a simple graph - based model where two labeled graphs are used to model data bases .", "a data base consists of two distinct layers : a _ schema _ layer and a _ structure _ layer ; the objects can be found in the latter , while the former describe the data organization .", "each layer is a labeled weakly - connected directed graph , moreover there exists a function that maps a schema into a structure : such function will be called an _ extension_. both vertices and edges of the graphs are labeled , and we can assume that the sets of edge labels and vertex labels , as well as schema labels and structure labels , are disjoint .", "an example of data base is represented in figures  [ fig : schema ] ,  [ fig : structure ] , from which it is easy to note how the schema and the structure are closely related , the following definitions only formalize the intuitive idea .            a _", "schema graph _ , in short _ schema _ , is a triple @xmath225 , where @xmath226 is an oriented , weakly - connected graph , and @xmath227 , @xmath228 are respectively the injective functions that maps each node ( resp .", "edge ) to its label .", "[ def_struttura ] a _ structure _ is a triple @xmath229 , with @xmath4 a colored oriented graph @xmath230 , where @xmath231 is the set of _ nodes _ of the structure , @xmath232 is the set of _ edges _ , @xmath233 , @xmath234 are respectively the injective functions that maps each node ( resp .", "edge ) to its label , and @xmath235 , is a labeling of the edges over the finite alphabet @xmath236 , called _ coloring _ of the structure .    in the following", ", we will use the set @xmath237 of colors that allows to specify that a link between object instances in @xmath238 is actual or not . in the example of fig .", "[ fig : structure ] , only the links labeled true are represented , and the presence ( or the abscence ) of links labeled false does not change the data stored in the data base . in fig .  [", "fig : false ] is represented a part of the structure , where false links are represented with dotted arrows .", "the schema and the structure must be strongly correlated ; in fact there must exist a function , called _ extension _ ( denoted by @xmath239 ) , mapping the schema into the structure . in order to have a sound definition of extension some restrictions", "must be enforced , as pointed out in the following definition , where @xmath240 stands for the family of all nonempty subsets of @xmath241 .", "informally @xmath239 maps each vertex of the schema into some vertices of the structure and each edge of the schema into some edges of the structure .", "[ def_f1 ] let @xmath242 be a schema and @xmath243 a structure , where @xmath244 .", "then @xmath238 is an _ extensional structure _ of @xmath245 if there is a function ( the _ extension _ ) from @xmath245 to @xmath238 , @xmath246 , such that : [ condizioni_rel ]    1 .", "@xmath247 is a partition @xmath248 of the set @xmath249 , 2", ".   for every @xmath250 , the pair @xmath251 iff @xmath252 ;    notice that the first point of the definition of extension implies that the function @xmath253 is well defined . in the following ,", "if @xmath238 is the extensional structure of @xmath245 , then we write @xmath254 and we will simply say that @xmath238 is a structure of @xmath245", ". given two vertices @xmath255 and @xmath256 of the schema , connected with a link @xmath257 then in the structure there must exist all links @xmath258 for @xmath259 , @xmath260 .", "such requirement justifies the introduction of a labeling ( and especially of a true - false labeling ) in order to have a reasonable graph - based model .", "[ basedati ] a _ data base _", "@xmath261 is a pair @xmath262 , where @xmath245 is a schema and @xmath238 is an extensional structure of @xmath245 .", "the schema describes the conceptual organization of the data , while the data content or instantiation of the data base is given by the extensional structure associated to the schema .", "it is not hard to notice that , given a schema , there is a one - to - one correspondence between structures and extension functions , therefore we will sometimes use the pair @xmath263 as a data base .", "some preliminary definitions are required for introducing our query language .", "given a partial function @xmath264 ( i.e. a function where each element of @xmath241 can be associated to one or none of the elements of @xmath261 ) , by @xmath265 we denote the domain of @xmath266 , that is the set of elements @xmath267 such that @xmath268 is defined .", "let @xmath269 be two partial functions from the set @xmath241 to the set @xmath270 . then _", "@xmath266 is a restriction of @xmath160 _ , denoted by @xmath271 , if @xmath272 and for every @xmath273 , @xmath274 .", "moreover by @xmath275 we denote the set obtained as union of all images of elements in @xmath276 : formally @xmath277 .", "[ morfi ] let @xmath278 be a data base .", "an _ instance _ of @xmath261 is a restriction @xmath266 of @xmath239 such that @xmath265 induces a weakly - connected subgraph of @xmath245 .", "the following notations will be used in the rest of the paper .", "the set @xmath279 is the set of all instances of @xmath261 .", "let @xmath280 be a subset of @xmath279 , then by @xmath281 , we mean the set of nodes of the structure of @xmath261 that is the union of all images of instances in @xmath282 , while @xmath283 is the union of all domains of instances in @xmath282 .", "an element in @xmath284 is called a _ value _ , while an element in @xmath285 is called a _", "name_. then the image of a name @xmath286 is the subset @xmath241 of @xmath284 such that @xmath287 . for a value @xmath288 ,", "the _ inverse image of @xmath289 _ , denoted by @xmath290 , is the name of @xmath285 that is mapped by @xmath239 , to a set containing the element @xmath289 .", "similarly , given a set @xmath241 of values , the _ inverse image of @xmath241 _ is the set @xmath291 which is union of all inverse images of the values in @xmath241 .", "our graph data model is proposed as a domain - preserving data base , along the same lines as other papers where the expressiveness of the relational algebra is studied  @xcite , and it gives a formal embedding for languages used for the retrieval of graph - structured information  @xcite .", "the requirement that we are dealing with domain - preserving data bases reflects in the query language : in fact we have no operation for creating new elements or modifying the schema graph , and all operations must preserve the schema and the original structure .    the main consequence of the assumption that our model is domain preserving consists in the fact that we will deal with a schema which is mapped to an instance through an extensional mapping .", "therefore there is a complete equivalence between subgraphs of the structure and restrictions of the extensional mapping .", "we are now able to introduce the operations of our graph algebra : according to our reasoning above we can describe the operation as over partial functions whenever it allows a simpler formulation .", "[ sovrapposizione ] let @xmath292 be a data base and let @xmath293 .", "the _ addition _ of @xmath294 and @xmath295 , denoted as @xmath296 , is the following function over domain @xmath297 .", "the operation is defined only if @xmath298 : @xmath299    [ prodotto ] let @xmath292 be a data base , and let @xmath300 be two functions in @xmath279 .", "the _ product _ of @xmath294 and @xmath295 , denoted as @xmath301 is the instance in @xmath279 defined as follows :    @xmath302 the product is defined only if @xmath303 induces a weakly - connected subgraph of @xmath245 .", "[ proiezione ] let @xmath292 be a data base .", "let @xmath266 be a function in @xmath279 and let @xmath241 be a subset of the domain of @xmath266 , such that @xmath241 induces in @xmath245 a weakly - connected subgraph .", "the _ projection of @xmath266 on @xmath241 _ , denoted as @xmath304 , is the instance defined as follows :    @xmath305    [ differenza ] let @xmath292 be a data base .", "let @xmath300 be two functions in @xmath279 over the same domain @xmath241 .", "the _ difference of @xmath294 by @xmath295 _", ", denoted as @xmath306 is the following instance :    @xmath307    the difference is defined only if @xmath308 induces a weakly - connected subgraph of @xmath245 .", "since the coloring of the edges encodes the fact that a relation between two objects is actual or not , it is natural that the query language has some tools for exploiting such coloring . in our model", "we will need to extract instances where `` similar '' edges are the same color .", "the definition of _ selector _ is the first step in such direction .", "let @xmath245 be the schema of a data base @xmath261 .", "then a _ selector of @xmath245 _ is a pair @xmath309 consisting of a weakly - connected subgraph @xmath48 of @xmath245 and a coloring @xmath310 of the edges of @xmath48 .    querying for a selector in a data base returns all subgraphs of the structure that are isomorphic to the selector : each such subgraph is indeed called a _", "simple instance_. moreover , it is natural to define an operation of selection that allows to obtain instances which are compatible with a coloring of the schema over the alphabet @xmath236 .", "this is the last operation of our algebra .", "[ simple ] let @xmath263 be a data base , where @xmath311 .", "let @xmath312 be a selector of @xmath245 , where @xmath313 .", "then a _ simple instance induced by the selector @xmath314 _ is a restriction @xmath266 of @xmath239 such that @xmath315 , @xmath316 for each @xmath317 and @xmath318 for each @xmath319 .", "notice that all simple instaces have the same domain .", "[ selezione ] let @xmath292 be a data base .", "let @xmath266 be a function in @xmath279 and @xmath314 a selector .", "let @xmath320 be the set of all simple instances induced by @xmath321 that are also subinstances of @xmath266 .", "the _ selection of @xmath322 by @xmath314 _", ", denoted as @xmath323 , is @xmath324 .", "given a set @xmath280 of instances , our first aim is to give a characterization of all instances that can be obtained with a query that uses only the information contained in the instances in @xmath280 , or equivalently by an expression of the algebra that has only instances in @xmath280 as operands . in such direction", "the main result of this section is that expressiveness in our graph algebra is equivalent to the conservation of a certain partition .", "it is natural to associate a notion of _ undistinguishability _ to a partition , where all elements in a set of the partition are deemed undistinguishable .", "we share the goals of  @xcite , but we have introduced in this paper a new framework , that is we are looking for a notion of expressiveness that is coherent with our meta theorem . just as the notion of _ automorphisms _ , introduced in  @xcite for relations , gives a global description of the logical dependencies among data that must be preserved when querying the data base , in our model a partition ( or an equivalence relation ) will represent all such logical dependencies .", "the equivalence partition over elements of the structure that we will study is called _ stability _ and is denoted by @xmath325 ( where @xmath326 is an instance ) .", "the simplest possible form of undifferentiation ( called _ 0-stability _ ) is based on the idea that we are able to distinguish images of different vertices of the instance and vertices of the extensional structure belonging to different functions of @xmath326 .", "such notion basically consists of using expressions in our algebra that do not contain any selection .", "[ def : split ] let @xmath241 be a subset of the image of a set @xmath282 of instances .", "then @xmath241 is _ split _ by @xmath282 iff there is a function @xmath327 in @xmath282 such that @xmath328 and @xmath329 .", "we are now able to introduce formally the definition of 0-stability , as follows :    [ 0-stable ] let @xmath282 be a set of instances over a data base @xmath262 , and let @xmath241 be a subset of @xmath281 . then @xmath241 is _", "0-stable _ w.r.t .", "@xmath280 , if the two following conditions hold :    1 .", "@xmath330 , where @xmath331 , and @xmath332 is a name of the schema .", "2 .   for each function @xmath333", ", then @xmath241 and @xmath334 are disjoint or one is contained into the other one .    a more refined notion of undifferentiation is called _", "@xmath335-stability _ ; informally a set @xmath241 is 1-stable w.r.t . @xmath261", "if @xmath241 is 0-stable and @xmath261 is not able to distinguish two vertices of @xmath241 with edges outgoing from @xmath261 and ingoing in @xmath241 ( or outgoing from @xmath241 and ingoing in @xmath261 ) .", "notice that 1-stability is a binary relation over subsets of @xmath284 , while 0-stability is a unary relation .", "the formal definition is :    [ un_passo ] let @xmath282 be a set of instances over a data base @xmath262 and let @xmath241 and @xmath261 be two disjoint subsets @xmath281 . then @xmath241 is _", "1-stable _ w.r.t .", "@xmath261 and @xmath280 , denoted as @xmath336 if the following conditions are verified :    1 .", "@xmath241 is 0-stable w.r.t .", "@xmath282 ; 2", ".   for each edge @xmath337 of @xmath338 , with @xmath339 and for each @xmath340 there exists @xmath341 such that @xmath342 ; 3 .   for each edge @xmath343 of @xmath338 , with @xmath339 and for each @xmath340 there", "exists @xmath341 such that @xmath344 .", "informally 1-stability means that whenever there is an colored edge ( say a red edge ) from a vertex of @xmath241 to a vertex of @xmath261 , then all vertices of @xmath241 have a red edge ingoing in @xmath261 . in other words", "if we assume that @xmath261 is undistinguishable , then also @xmath241 is undistinguishable , by any single - edge path .", "the notion of 1-stability can be further generalized , but first we need a new definition .", "[ cammino ] let @xmath345 be a labeled graph .", "then a _ colored path _ in @xmath48 is a pair @xmath346 where @xmath347 , and for every @xmath348 , @xmath349 belongs to @xmath231 and @xmath350 is an edge of @xmath48 such that @xmath351 or @xmath352 .", "moreover @xmath353 is the sequence @xmath354 where @xmath355 is the color of the edge @xmath350 in @xmath48 .", "notice that the definition of path used in the paper is different from the one that can be usually found in a graph theory textbook , as arcs can also be in the reverse direction .", "the _ length _ of a path is the number of edges it contains .", "let @xmath356 be a data base and let @xmath346 be a colored path of @xmath338 , with @xmath357 .", "then the _ path schema _ of @xmath346 is the pair @xmath358 , with @xmath359 , where for every @xmath360 , @xmath361 and @xmath362 .", "[ path - dependencies ] let @xmath363 be two nodes of the @xmath280-structure , and let @xmath364 be a subset of @xmath281 , then the _ path dependencies _ from @xmath148 to @xmath289 in @xmath364 , denoted as @xmath365 is the set of path schemata of all paths of the @xmath280-structure that are starting in @xmath148 and ending in @xmath289 and entirely contained in @xmath364 .    informally given @xmath366 , their path dependencies is obtained by removing all vertices not in @xmath364 , then computing all possible paths from @xmath148 to @xmath289 , and finally computing the respective path schemata .", "the next step is to generalize 1-stability to @xmath25-stability , that is taking into account length-@xmath25 paths instead of simple edges ( that is length-1 paths ) .", "[ k_passi ] let @xmath282 be a set of instances over a data base @xmath262 , let @xmath25 be an integer larger than one , and let @xmath241 and @xmath261 be two disjoint subsets of @xmath281 .", "then @xmath241 is _", "@xmath25-stable _ w.r.t .", "@xmath261 and @xmath280 , denoted as @xmath367 if the following conditions are verified :    1 .", "@xmath241 is 0-stable w.r.t .", "@xmath282 ; 2 .", "@xmath241 is @xmath368-stable w.r.t . @xmath261 and @xmath280 ; 3", ".   for each @xmath369 there exists @xmath341 such that @xmath370 .", "the main idea is that when @xmath367 then if @xmath261 is undistinguishable also @xmath241 is undistinguishable when only paths no longer than @xmath25 are taken into account .", "our main definition follows :    [ rel_collettiva ] let @xmath280 be a set of instances , and let @xmath371 be two disjoint subsets of @xmath281 . then @xmath241 is _ stable w.r.t . @xmath261 _ in @xmath280 , denoted as @xmath372 , if @xmath373 for all @xmath374 .    by def .", "[ rel_collettiva ] , it is immediate to verify the following properties of stability :    [ prop1_rel ] let @xmath280 be a set of instances , and let @xmath375 , then :    1 .   if @xmath376 and @xmath377 , then @xmath372 and @xmath378 , 2 .", "if @xmath379 , @xmath372 and @xmath378 , then @xmath380 , 3 .", "if @xmath379 , @xmath381 , @xmath382 and @xmath383 , then @xmath384 .", "stability is a relation between disjoint subsets of the domain .", "the definition of expressiveness in the query language that we want to obtain is based on partitions , and now we are able to introduce the class of partitions we are interested into .", "a partition @xmath385 of nodes of the structure is called _ valid _ if and only if for each set @xmath241 of the partition and every set @xmath261 that is a union of sets of @xmath385 , then @xmath261 can not differentiate @xmath241 .", "[ partizione_accettabile ] let @xmath280 be a set of instances .", "a partition @xmath386 of @xmath387 is _ valid _ if for every @xmath388 , then @xmath389 .", "given a set @xmath282 of instances , then there may be various valid partitions of @xmath281 , and at least one valid partition always exists ( the partition where each vertex of the extentional structure is a set ) .", "some valid partitions are more representative of the actual undifferentiation , in fact we will assume as a measure of the undifferentiation induced by @xmath280 the coarsest valid partition , which we will call _ canonical partition _ and denote as @xmath390 .", "we can show that the definition of canonical partition is well - formed .", "every set @xmath280 of instances has a unique canonical partition @xmath390 .", "clearly the partition of @xmath281 into singletons is a valid partition , so there exists at least one canonical partition .", "now assume to the contrary that there exist two coarsest valid partitions @xmath391 and @xmath392 .", "let @xmath393 and @xmath394 be the equivalence relations induced by the partitions @xmath391 and @xmath392 , respectively .", "let @xmath395 be the transitive closure of the relation @xmath0 defined as follows : @xmath396 if and only if @xmath148 and @xmath289 are in the same set of @xmath391 or @xmath392 .", "we can prove that the partition @xmath385 induced by @xmath395 is a valid one of index strictly less than @xmath25 . by construction of @xmath395", "each set of @xmath385 is a union of sets in @xmath391 and also a union of sets in @xmath392 , moreover each set in @xmath385 is contained in the image of a single name ( since each set must be 0-stable ) .", "notice that @xmath397 iff @xmath398 .", "let @xmath399 be a set of @xmath385 , and let @xmath400 be a class of @xmath391 contained in @xmath399 .", "since @xmath391 is a valid partition , and @xmath399 is a union of disjoint sets of @xmath401 , by lemmata  [ rel_collettiva ] ,  [ prop1_rel ] , we have that @xmath402 and @xmath403 .", "let @xmath399 , @xmath404 be two sets of @xmath385 , with @xmath405 and @xmath406 .", "we have already proved that @xmath407 and @xmath408 , applying again lemmata  [ rel_collettiva ] ,  [ prop1_rel ] and noting that @xmath406 we obtain @xmath409 , by the generality of @xmath399 and @xmath404 the partition @xmath385 is valid .", "moreover @xmath385 is coarser than @xmath401 , which is a contradiction .", "in this section we will prove our main result regarding the expressiveness of the graph - based query language by showing that a function can be computed if and only if adding such function does not change the canonical partition . in the following", "we will assume that the union of the images of all functions in @xmath326 is exactly the universe set ; such assumption does not violate the generality since otherwise we would simply have some sets of the canonical partition whose union consists of exactly those elements of the universe set that are not in any function in @xmath326 .", "[ graal - expressivess ] let @xmath410 be the set of functions that are a result of an expression of the graph algebra where only functions of @xmath280 are operand .", "then @xmath411 if and only if the canonical partition induced by @xmath280 is equal to the canonical partition induced by @xmath412 , that is @xmath413 .", "the following two properties , that are consequences of def .  [ partizione_accettabile ] , will be useful to prove the main result of the paper .", "[ f_1 ] let @xmath280 be a set of instances and let @xmath385 be a valid partition .", "then the image of every instance @xmath414 is the union of sets of @xmath385 .", "we prove the lemma by induction on the number @xmath59 of operations of the expression for @xmath415 . if @xmath416 , then @xmath415 is a function in @xmath417 .", "since all sets of a valid partition are @xmath418-stable , no set of a valid partition has both an element in @xmath419 and an element not in @xmath419 , therefore the union of all sets of @xmath385 that are contained in @xmath419 is contained in @xmath419 . to prove that such containment is not strict ( i.e. such union is equal to @xmath419 ) it is sufficient to note that all elements of @xmath419 belong to some set of @xmath385 .", "assume now that @xmath420 and @xmath415 is obtained by the application of an operation to two expressions @xmath421 and @xmath422 in @xmath410 , or one expression for a selection . clearly , by inductive hypothesis", "the images of @xmath421 and @xmath422 are obtained as the union of some sets of @xmath385 .", "it is immediate to verify the lemma for the case that @xmath423 , @xmath424 , @xmath425 and @xmath426 .", "finally , assume that @xmath427 , where @xmath321 is a selector .", "by definition of the selection , then @xmath419 is the union of the images of all simple instances induced by @xmath321 .", "assume to the contrary that there exists a set @xmath241 of the valid partition such that @xmath428 and @xmath429 .", "now , let @xmath430 and @xmath431 .", "hence , by construction of selection , @xmath289 can not be in the image of any simple instance induced by @xmath321 , while @xmath148 is contained in the image of a simple instance induced by @xmath321 . by inductive hypothesis @xmath432", "is union of sets of @xmath385 , moreover since @xmath241 is a set of @xmath385 , also @xmath433 is union of sets of the valid partition , implying that @xmath434 , it follows that , for each @xmath435 , @xmath436 for some @xmath437 .", "this implies that there is a simple instance induced by @xmath321 that has in its image @xmath289 , which is a contradiction with the above assumption .", "consequently , the image of @xmath415 must be union of sets of @xmath385", ".    we will prove that an alternative characterization of canonical partition is as the partition induced by the equivalence relation @xmath438 between elements of @xmath439 , where @xmath440 if and only if for every instance @xmath441 , @xmath442 . in the following of the paper", "let @xmath443 denote the partition induced by the equivalence relation @xmath438 .", "successively we will prove that a function @xmath322 belongs to @xmath410 if and only if @xmath275 can be obtained as union of sets of @xmath443 , completing the proof of our main result , in two steps .", "first we will prove that a function @xmath266 belongs to @xmath444 if and only if @xmath445 is union of sets in @xmath443 , then we will prove that @xmath446 .", "the following proposition is an immediate consequence of the definitions of @xmath440 and of projection .", "[ unique1 ]", "let @xmath447 such that @xmath440 .", "then both @xmath148 and @xmath289 belong to the set @xmath448 for some name @xmath449 .", "[ unique ] let @xmath450 be the partition induced by a set @xmath280 of instances . then , the inverse image of every set of the partition consists of a single vertex .", "[ minimo_ottenibile ] let @xmath280 be a set of instances over @xmath261 , let @xmath443 be the partition induced by @xmath282 , and let @xmath451 .", "then there exists an instance @xmath441 such that @xmath452 .", "let @xmath453 be the set of functions @xmath454 such that @xmath455 . by cor .", "[ unique ] all functions in @xmath453 have the same domain , therefore the expression @xmath456 is well - formed ; by construction @xmath457 . by definition of @xmath443", "all functions @xmath458 are such that @xmath459 or @xmath460 , therefore all functions @xmath458 whose image intersect @xmath241 are such that @xmath455 , which in turn implies that are also in @xmath453 .", "hence @xmath461 , for otherwise there would be an element of @xmath241 not belonging to the image of any function in @xmath326 .", "[ uni2 ] let @xmath326 be a set of instances over @xmath261 , let @xmath450 be the partition induced by @xmath282 and let @xmath241 be a union of sets in @xmath443 , such that the inverse image of @xmath241 induces a weakly - connected subgraph of the schema , then @xmath241 is the image of an instance @xmath441 .", "let @xmath462 be the sets of @xmath443 whose union is @xmath241 , and notice that , by lemma  [ minimo_ottenibile ] , it is possible to associate to each set @xmath463 the instance @xmath464 whose image is @xmath463 , moreover for each such @xmath465 , @xmath466 . for each vertex @xmath148 in the inverse image of @xmath241 we can construct the function @xmath467 as @xmath468 . then let @xmath469 ; it is immediate to note that @xmath461 .    an immediate consequence of lemmata  [ f_1 ] and  [ uni2 ] is the following :    [ canonical = obtainable ]", "let @xmath326 be a set of instances over @xmath261 and let @xmath333 .", "then @xmath470 if and only if @xmath445 is union of sets in @xmath443 and the inverse image of @xmath445 induces a weakly - connected subgraph of the schema .    with lemma  [ uni2 ]", "we have proved that all interesting unions of sets of the partition @xmath450 can be obtained with an expression of the graph algebra where all operands are taken from @xmath280 , therefore @xmath450 conveys all expressibility information . but @xmath450 is defined on the set @xmath410 , we still need to correlate the definition of canonical partition with that of @xmath450 .", "[ pbi_accettabile ] let @xmath280 be a set of instances .", "then @xmath450 is a valid partition of @xmath280 .", "let @xmath371 be two disjoint sets where @xmath471 and @xmath261 is union of sets in @xmath443 , we will prove that @xmath372 .", "first of all we will show that @xmath241 is 0-stable .", "remember that , by definition of @xmath443 , for each @xmath470 either @xmath472 or @xmath473 .", "since @xmath474 contains @xmath326 , it is immediate to not that @xmath241 is 0-stable . in the following", "let @xmath10 be the single - vertex inverse image of @xmath241 .", "if the inverse image of @xmath475 does not induce a weakly - connected subgraph of the instance , then 0-stability of @xmath241 suffices to prove that @xmath372 , therefore assume that the inverse image of @xmath475 induces a weakly - connected subgraph of the instance .", "let us assume that @xmath372 does not hold , then we will get a contradiction . without loss of generality we can assume that @xmath261 is a minimum set for which @xmath372 does not hold .", "the new assumption implies that @xmath25-stability does not hold for some @xmath25 .", "it follows that there are two elements @xmath476 and an element @xmath477 such that @xmath478 is not contained in @xmath479 , for every @xmath480 .", "now , by lemma  [ uni2 ] , there is an instance @xmath414 such that @xmath481 .", "let @xmath482 and @xmath483 and let @xmath480 such that @xmath484 . let us consider the function @xmath485", ". by construction @xmath148 belongs to @xmath486 , but @xmath289 does not ; since @xmath487 we have found a function in @xmath474 containing @xmath148 but not @xmath289 , contradicting the assumption that @xmath488 .", "[ pd = pbi ] let @xmath280 be a set of instances . then @xmath489 .    by lemma  [ pbi_accettabile ] , @xmath490 must be a valid partition of @xmath280 . by lemma  [ minimo_ottenibile ] ,", "every set @xmath491 is obtained as the image of some instance @xmath441 . clearly , since @xmath441 , by lemma  [ f_1 ] the image @xmath241 of @xmath322 is the union of sets of the canonical partition of @xmath280 .", "hence @xmath492 .", "[ uguale1 ] let @xmath280 be a set of instances", ". then an instance @xmath415 belongs to @xmath474 if and only if @xmath493 .    clearly by lemma  [ pd = pbi ] it suffices to show that @xmath414 if and only if @xmath494 , moreover it is immediate to note that , by construction of @xmath443 , if @xmath414 then @xmath494 .", "assume now that @xmath494 .", "by lemma  [ pbi_accettabile ] , @xmath495 must be a valid partition . by lemma  [ f_1 ] , and exploiting the assumption that @xmath496 , for each @xmath497 , @xmath498 must be the union of some sets in @xmath499 , that is @xmath500 , for some sets @xmath501 .", "but by lemma  [ minimo_ottenibile ] , for each set @xmath502 there is an instance @xmath503 such that @xmath502 is the image of @xmath504 .", "consequently , @xmath505 , and hence @xmath506 , which proves that @xmath414 as required .", "theorem  [ uguale1 ] and lemma  [ pd = pbi ] lead to our main result .", "[ cor : uguale1 ] let @xmath280 be a set of instances", ". then an instance @xmath415 belongs to @xmath474 if and only if @xmath507 .", "we have introduced the idea that partitions of the domain set can be used for characterizing the set of relations or graphs that can be extracted in a data base in the relational or in a graph - based model .", "by formally proving those expressiveness results we have effectively given a new framework for the analysis of data base query languages .", "s.  abiteboul , c.  beeri , m.  gyssens , and d.  van gucht .", "an introduction to the completeness of languages for complex objects and nested relations . in _ nested relations and complex objects _ ,", "volume 361 of _ lncs _ , pages 117138 , 1989 .", "m.  andries and j.  paredaens . a language for generic graph transformations . in _", "graph - theoretic conecpts in computer science , 17th international workshop , wg91 _ , volume 570 of _ lncs _ , pages 170183 , berlin , 1991 .", "springer - verlag .    f.  bancilhon . on the completeness of query languages for relational data bases . in _", "proceedings , 7th symp . on mathematical foundations of computer science _ ,", "volume  64 of _ lncs _ , pages 112123 , berlin , 1978 .", "springer - verlag .                    j.  van den bussche , d.  van gucht , m.  andries , and m.  gyssens . on the completeness of object - creating query languages . in _", "33rd annual symposium on foundations of computer science .", "proceedings _ , pages 372379 , 1992 ."], "abstract_text": ["<S> in this extended abstract we provide a unifying framework that can be used to characterize and compare the expressive power of query languages for different data base models . </S>", "<S> the framework is based upon the new idea of _ valid _ partition , that is a partition of the elements of a given data base , where each class of the partition is composed by elements that can not be separated ( distinguished ) according to some level of information contained in the data base . </S>", "<S> we describe two applications of this new framework , first by deriving a new syntactic characterization of the expressive power of relational algebra which is equivalent to the one given by paredaens , and subsequently by studying the expressive power of a simple graph - based data model .    </S>", "<S> [ section ] [ section ] [ section ] [ section ] [ section ] [ section ] [ section ] [ section ] [ section ] [ thm]proposition [ thm]corollary [ section ] </S>"], "labels": null, "section_names": ["introduction", "preliminaries", "expressiveness in relational databases", "expressiveness in graph-based data bases", "stability", "expressiveness", "conclusions"], "sections": [["the relational data base model , introduced by codd in  @xcite , has been particularly successful since it is a mathematically elegant model well suited to describe almost all `` real world '' situations .", "since the query languages associated to such model ( the _ relational algebra _ and the _ relational calculus _ ) have a formal and simple definition , an interesting field of research is to study the expressive power of such language .", "codd  @xcite has proved that the relational algebra is equivalent to the _ relational calculus _ , in the sense that both query languages can compute the same set of relations .    a breakthrough in this field  @xcite has been a syntactic characterization of the set of relations that can be computed in a give data base .", "these results , also known as _ bp - completeness _ , are based on the principle of data independency from the physical representation : the information that can be extracted from the data base is completely determined at the logical level of such data base .", "this fact can be stated in a simple way : a relation @xmath0 can be computed from a data base @xmath1 if and only if all permutations over the elements of @xmath1 which preserve @xmath1 ( that is , all permutations that produce a data base isomorphic to @xmath1 ) , also preserve @xmath0 .", "an interesting interpretation of this property is that only the information given by the structure of the data can be used to differentiate data values ; consequently , a query is expressible if and only if it does not add any additional differentiation to the one initially available  @xcite .", "this idea can be rephrased by stating that the result of a query is invariant w.r.t .", "permutations of indistinguishable values ; such a permutation was captured with the notion of automorphism in  @xcite .", "while the @xmath2-criterion is a natural requirement , it refers to properties of relations in a given data base instead of queries as a whole .", "we recall that a query is an expression of the query language that can be applied to different data bases leading to possibly different results .", "thus it has been extended to a property of queries as partial functions from data bases to data bases , which is known nowadays as _", "@xcite : it has been recognized as the capability of the calculus to preserve isomorphisms between data bases , rather than automorphisms .", "genericity is a common requirement for query languages and it is traditionally related to the _ data independence _", "principle that assumes that the data base is constructed over an abstract domain which is independent from the internal representation of data .", "subsequent research has shown that this approach to the analysis of the expressiveness of a query language has certain shortcomings @xcite , mainly when new data models , such as the object - based model , are introduced .", "other notions have been proposed to analyze properties of queries in some new models  @xcite pointing out the importance of extending genericity to be used in more complex models . in  @xcite languages", "are classified w.r.t .", "the degree of the use of the equality predicate , by analyzing the invariance property of queries under different mappings ( not necessarily isomorphisms ) over the data domain , which are compatible with the relational structure of the data base .", "subsequent advances in data base theory have led to different models that take into account the limitations of the relational model when it comes to describe complex situations .", "most of such models have been introduced in the graph - based or object - oriented frameworks , but usually their mathematical foundations do not allow a complete study of the expressive power of the query languages introduced .", "in fact , to our knowledge , the only exception is the graph - based model good @xcite .    in this paper", "we introduce a different syntactic characterization of queries computable in a data base .", "our characterization relies upon the notion of partitions of the domain , where each partition represents a level of _ undifferentiation _ among objects , values or vertices .", "notice that an automorphism also can represent a certain level of undifferentiation .", "initially we will exploit such notion to give two new characterizations of relations expressible in a relational data base .", "subsequently , we will show how to apply the new framework to analyze a simple graph - based model , hence proving that our characterization can be useful in comparing the expressive power of different data languages .    following the approach of  @xcite , the data models studied in this paper are domain - preserving , that is , it is not possible to create new vertices or values , but only to query an existing data base . in our framework ,", "a binary relation over sets of data values is defined , denoted by @xmath3 , which relates those sets of values that can not be differentiated . from the relation", "@xmath3 we build some sets of partitions that _ respect _", "@xmath3 , that is , all classes in a partition are preserved by @xmath3 .", "we prove that expressiveness of a query language can be stated as the conservation of some of those partitions , where the exact set of partitions that must be preserved depends on the data model .", "the expressibility results we obtain have the following form : _ given a data base @xmath1 , let @xmath4 be a relation or a graph over the domain set of @xmath1 .", "then @xmath4 can be expressed in @xmath1 if and only if @xmath5 _ , where @xmath6 and @xmath7 are two sets of partitions which depend on the model under consideration ."], ["all sets considered in this paper are assumed to be finite and nonempty .", "given a set @xmath8 , a _ relation _ @xmath0 over @xmath8 is a subset of the cartesian product @xmath9 ( @xmath10 times ) for some fixed integer @xmath11 , that is a set of tuples of length @xmath10 , where all components of a tuple are elements of @xmath8 .", "the number @xmath12 is called the _ order _ or _ arity _ of the relation . given a set @xmath13 of relations over @xmath8 , the pair @xmath14 is called a _", "relational database _ ; in this setting , @xmath8 is the _ domain _ of the database , and @xmath15 is the set of relations of the database .", "given a relation @xmath16 of a database @xmath14 , we denote with @xmath17 the _ data domain _ of @xmath0 , that is the subset of the elements of the database domain @xmath8 that are in at least one tuple of @xmath0 . the notion of data domain", "is easily extended to the set @xmath15 of relations as the set union of relations data domains : @xmath18 . without loss of generality", ", we can assume that @xmath19 for every considered database @xmath14 .", "this seemingly trivial requirement is indeed very important , as it will become evident after theorem  [ teo : paredaens2 ] , therefore we will omit the universe set unless it is necessary to avoid any ambiguities .", "just as in  @xcite , when referring to a relational database , we use the _ relational algebra _ as a query language . in relational algebra", "two binary operators ( union and product ) and three unary operators ( projection , equality restriction and inequality restriction ) are given . in the following definition", "all relations are defined over the same database domain @xmath8 .", "[ def : relational - algebra ] let @xmath0 and @xmath4 be two relations with the same arity ; the _ union _ of @xmath0 and @xmath4 , denoted by @xmath20 , is simply the set  theoretical union of the two sets of tuples .", "given two relations @xmath0 and @xmath4 ( not necessarily with the same arity ) , the ( cartesian ) _ product _ of @xmath0 and @xmath4 , denoted by @xmath21 , is the set of all possible concatenations of a tuple of @xmath0 with a tuple of @xmath4 : @xmath22 .", "the abbreviation @xmath23 is used to express the relation @xmath24 ( @xmath25 times ) .", "let @xmath26 be the arity of a relation @xmath0 , @xmath27 a positive integer and @xmath28 @xmath29 a function .", "the _ projection _ of @xmath0 over @xmath30 , denoted by @xmath31 , is the relation : @xmath32 .", "now , let @xmath33 and @xmath34 be two integers such that @xmath35 , where @xmath26 is the arity of a relation @xmath0 .", "the _ equality restriction _ of @xmath0 on @xmath33 and @xmath34 is the relation , denoted by @xmath36 , that is obtained by taking from @xmath0 all the tuples for which the @xmath33-th and the @xmath34-th components are equal : @xmath37 .", "analogously , the _ inequality restriction _ of @xmath0 on @xmath33 and @xmath34 , denoted by @xmath38 , is the relation obtained by taking from @xmath0 all the tuples for which the @xmath33-th and the @xmath34-th components are different : @xmath39 .", "the five operations just described are sufficient to generate the operations of intersection , difference , join and division , usually assumed as primitives in codd s relational algebra ; a proof of this fact can be found , for example , in  @xcite .    given a relational data base @xmath40 , we will denote by @xmath41 the relation which is the result of applying the expression ( of the relational algebra ) e to the data base @xmath1 .", "moreover a relation @xmath4 over @xmath8 is told to be _ expressible _ from @xmath15 if there exists an expression @xmath42 whose operands are all relations in @xmath15 , and such that @xmath41 is equal to @xmath4 . following  @xcite ,", "we denote with @xmath43 ( basic information contained in the set of relations @xmath15 ) the set of relations that can be expressed from @xmath15 .", "as observed in  @xcite , @xmath43 is the set of the answers to all possible queries that can be asked to a relational datmabase that contains the relations @xmath15 . in  @xcite", ", paredaens gives a characterization of the class @xmath43 based upon appropriate automorphisms , that is permutations of the elements of the database domain .", "let @xmath0 be a relation of order @xmath26 over a set @xmath8 .", "as in  @xcite , an _ automorphism _ is a bijective function ( that is , a permutation ) on @xmath8 .", "we say that the automorphism @xmath44 _ respects _ the relation @xmath0 or , equivalently , that @xmath45 is @xmath0-_compatible _ if , for each tuple @xmath46 , @xmath47 .", "the compatibility of an automorphism @xmath44 with respect to a relation @xmath0 can be naturally extended to a set @xmath15 of relations in the following way : @xmath45 _ respects _ the relations in @xmath15 or , equivalently , @xmath45 is @xmath15-_compatible _ if @xmath45 is @xmath0-compatible for each relation @xmath0 in @xmath15 .", "notice that the set of automorphisms @xmath15-_compatible _ , is a group of elements , a binary associative operation on @xmath48 , and an identity element @xmath49 , such that the operation is closed and invertible in @xmath48 ] where the operation is the composition of functions and the identity is the identity function ( i.e. the function defined as @xmath51 ) . as in  @xcite , we denote with @xmath52 the set of all the automorphisms @xmath44 which are @xmath15-compatible ; with a small abuse of notation , if @xmath53 , we will usually write @xmath54 instead of @xmath55 . it will be very useful to consider the following representation of @xmath52 .", "let @xmath14 be a relational database , with @xmath56 , @xmath57 , and let @xmath58 be the set of @xmath15-compatible automorphisms .", "the following relation of arity @xmath59 : @xmath60 is called the _ cogroup  relation _ of @xmath14 .    as we can see , each row ( tuple ) of the relation @xmath61 represents one of the @xmath15-compatible automorphisms . since we do not associate any particular meaning to the elements of the domain @xmath8 , if @xmath62 we can assume , without loss of generality , @xmath63 .", "we can also assume that the first tuple of @xmath61 represents the identity function on @xmath8 ( which is always present in @xmath52 , since it is compatible with every nonempty set of relations ) ; as a consequence , it can always be assumed that the first row of @xmath61 is the tuple @xmath64 .", "let @xmath14 be a relational data base , with :    * @xmath65 * @xmath66 , with : @xmath67    it is easily verified that : @xmath68 @xmath69 if we look at the @xmath15-compatible automorphisms as permutations over @xmath8 , we can express @xmath52 as follows : @xmath70 [ ex : klein - group ]    it is not difficult to see that , for a given database @xmath14 , the set @xmath52 of @xmath15-compatible automorphisms is indeed a group with respect to function composition , with the identity function over @xmath8 as unitary element .", "in fact , the identity over @xmath8 is always in @xmath52 , the inverse of an @xmath15-compatible automorphism is still an @xmath15-compatible automorphism , and the composition between two @xmath15-compatible automorphisms is again an @xmath15-compatible automorphism . since we can always assume @xmath71", ", we can think of @xmath52 as a finite permutation group over the set @xmath72 , that is a subgroup of the symmetric group @xmath73 .", "in this paper we investigate the relation between expressive power and partitions of the database domain .", "more precisely , we investigate the possibility to characterize the expressive power of relational and graph - based databases via one or more theorems abiding to the following meta theorem .", "[ thm : scheme1 ] let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8", ". then @xmath74 , where @xmath75 and @xmath76 are sets of partitions over @xmath8 , built from the sets @xmath15 and @xmath77 of relations respectively ."], ["the relevance of the main result in  @xcite is that it is the first syntactic characterization of the relations that can be obtained from a given database @xmath14 when the relational algebra is used as a query language .", "more precisely , in  @xcite the following theorem is proved .", "let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8", ". then @xmath78 and @xmath79 .", "[ teo : paredaens ]    basically , paredaens has been able to point out the fundamental relation between expressiveness in a database and the set of automorphisms in the relational model .", "such result has been successively extended in  @xcite to define in a formal way the notion of _ genericity _ , that is computable queries @xcite have to be invariant with respect to the isomorphisms between databases .", "we can restate theorem  [ teo : paredaens ] in a form that will be more convenient for our purposes .", "let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8", ". then @xmath80 .", "[ teo : paredaens2 ]    first of all , we show that @xmath81 . proving that @xmath82 is trivial as @xmath83 .", "the latter stems from the fact that the relations which are expressible from @xmath15 are those obtained from @xmath84 simply ignoring the relation @xmath4 .", "let now be @xmath85 and @xmath86 .", "if the expression that gives @xmath87 from @xmath84 does contain some occurrence of the relation @xmath4 , it is sufficient to replace such occurrence with the expression that gives @xmath4 from @xmath15 to conclude that @xmath88 , and thus @xmath89 .", "it is immediate to notice that @xmath90 implies @xmath91 .    since we have established that @xmath81 , the two databases @xmath14 and @xmath92 are _ basic information equivalent _  that is , every relation of the first database can be obtained from the relations of the second database and vice versa  if and only if @xmath4 is expressible from @xmath14 .", "a direct consequence of theorem  [ teo : paredaens ] is that two databases @xmath93 and @xmath94 are basic information equivalent if and only if @xmath95 ( which are assumed to be both equal to @xmath8 ) and @xmath96 ; thus , we can conclude that @xmath80 as stated .", "we observe that , given our assumption that @xmath97 , in theorem [ teo : paredaens2 ] we can get rid of the inclusion between the domains , since it is implicit from the fact that @xmath4 is a relation over @xmath8 . on the other hand , we _ can not _ ignore the inclusion condition if we suppose that @xmath98 , since in such a situation it is not difficult to show two relations @xmath0 and @xmath4 such that @xmath99 but @xmath100 .", "a notion that seems tightly related to the expressiveness of relations in a database is that of _ indistinguishability _ between elements of the domain . intuitively , the idea is that the elements of a subset of the domain of a given database are indistinguishable if and only if no query to the database is able to divide the set in two parts , one made of the elements that occur in the relation resulting from the query and the other made of the elements that do not occur in the relation .", "in such a situation , we say that the set of indistinguishable elements can not be _ separated _ by any of the queries that can be presented to the database .", "thus , a relation resulting from a query to the database can only contain all or none of the elements of a non - separable set .    theorem  [ thm : scheme1 ] defines the general framework we propose to investigate the expressive power of query languages . in this framework", "different notions of expressible queries can be studied by considering different sets of partitions . for a given database @xmath14 , we say that a set @xmath75 of partitions of @xmath8 is a set of _ valid partitions _ if and only if it satisfies theorem  [ thm : scheme1 ] . by the results in  @xcite", ", it seems to us quite natural to define the following sets of valid partitions , namely the _ orbit partitions _ and the _ cycle partitions _ ; indeed later we will be able to prove that , in the context of theorem  [ thm : scheme1 ] , they are equivalent to the characterization of relations obtainable in a relational data base of  @xcite .", "let @xmath14 be a relational database , and let @xmath101 , @xmath102 be a partition of @xmath8 .", "@xmath103 is an _ orbit partition _ of @xmath8 with respect to @xmath15 if both the following conditions hold :    1 .   for each relation", "@xmath16 and for each class @xmath104 , @xmath105 or @xmath106 ; 2 .", "for each class @xmath107 and for each pair @xmath108 of elements of @xmath109 there exists an automorphism @xmath110 such that @xmath111 , and @xmath112 for every class @xmath113 .    we denote with @xmath114 the set of all orbit partitions of the given database @xmath14 .", "[ def : orbit - partition ]    let @xmath14 be a relational database , and let @xmath101 , @xmath102 be a partition of @xmath8 .", "@xmath103 is a _ cycle partition _ of @xmath8 with respect to @xmath15 if both the following conditions hold :    1 .   for each relation", "@xmath16 and for each class @xmath104 , @xmath105 or @xmath106 ; 2 .", "there exists an automorphism @xmath110 such that for each class @xmath107 and for each pair @xmath108 of elements of @xmath109 there exists an integer @xmath59 such that @xmath115 and @xmath112 for every class @xmath113 .", "we denote with @xmath116 the set of all cycle partitions of the given database @xmath14 .", "[ def : cycle - partition ]    as already stated for @xmath52 , if @xmath0 is a relation we will write @xmath117 and @xmath118 instead of @xmath119 and @xmath120 respectively .", "the following theorem is an alternative formulation of the main result of  @xcite ( the equivalence of the two formulation follows from theorem [ teo : paredaens2 ] ) which is more useful for our purposes .", "let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8 .", "then @xmath121 .", "let @xmath14 be a relational database , and let @xmath52 and @xmath61 be respectively the group of @xmath15-compatible automorphisms and the cogroup - relation of @xmath15 .", "a useful fact proved in  @xcite is that the cogroup - relation is expressible from @xmath15 , that is @xmath122 .", "using this fact , we are able to prove the following theorem .    @xmath123 .", "[ teo : autr = cgr ]    since @xmath122 , by theorem  [ teo : paredaens ] we can conclude that @xmath124 . now , let @xmath125 ; as we have already observed , @xmath126 is a permutation of the set @xmath97 , as well as of the tuples that compose the relation @xmath61 . thus , for each tuple @xmath127 , we have that @xmath128 . in particular , by letting @xmath59 be the cardinality of @xmath8 , we have : @xmath129 thus , the elements of @xmath8 are mapped by @xmath126 in such a way that the result is a row of the cogroup - relation ; so we can conclude that @xmath110 .    a direct consequence of theorem  [ teo : autr = cgr ] is that not only @xmath122 , as established by paredaens , but also @xmath130 for every relation @xmath16 , since @xmath131 and @xmath132 . as a corollary of theorem  [ teo : autr = cgr ] , if we are interested to study the expressive power of a given relational database @xmath14 then we can work as well on the database @xmath133 , which has only one relation and , moreover , such relation is an explicit representation of the finite permutation group @xmath52 .", "we now turn our attention to the structure of @xmath114 and @xmath116 .", "first of all we observe that , thanks to theorem  [ teo : autr = cgr ] , we can get rid of item 1 in definitions  [ def : orbit - partition ] and [ def : cycle - partition ] since , by considering the database @xmath134 , there is only one relation and , for such relation , it holds @xmath135 for each @xmath107 .    to characterize the sets of cycle and orbit partitions we need to recall some notions from basic abstract algebra .", "let @xmath136 be a set and @xmath137 a group .", "action of @xmath48 on @xmath136 _ is a map @xmath138 such that    1 .   @xmath139 ; 2 .", "@xmath140    in group theory it is customary to omit the operators symbols from expressions when confusion does not arise ; so , the expression in item 2 above is usually written as : @xmath141 .", "let @xmath48 be a group acting on a set @xmath136 .", "for @xmath142 , let @xmath143 if and only if there exists @xmath144 such that @xmath145 .", "it is not difficult to see that @xmath146 is an equivalence relation on @xmath136 , and thus it induces a partition @xmath103 on @xmath136 .", "the classes of @xmath103 are called the _ orbits _ in @xmath136 under @xmath48 . if @xmath147 , the class containing @xmath148  denoted by @xmath149", " is called the _ orbit of _ @xmath148 under @xmath48 . in other words , @xmath150 .", "it is not difficult to see that the partition induced by the orbits of @xmath52 on @xmath8 satisfies definition  [ def : orbit - partition ] .", "in fact , every automorphism @xmath110 maps each orbit @xmath151 into itself and , given a pair @xmath108 of elements of @xmath8 , there exists an automorphism that maps @xmath152 to @xmath153 if and only if @xmath152 and @xmath153 are in the same orbit .", "moreover , if @xmath154 is a subgroup of a group @xmath48 acting on the set @xmath136 , then every orbit @xmath155 is a subset of the orbit @xmath149 ; more precisely , it is not difficult to prove that the orbits induced by @xmath154 are a _ refinement _ of the orbits induced by @xmath48 . since each partition induced by the orbits of every subgroup of @xmath52 satisfies definition  [ def : orbit - partition ] , we have that @xmath114 contains the set of those partitions .", "vice versa , let @xmath156 .", "it is not difficult to see that the set of automorphisms @xmath110 that map each class of @xmath103 into itself and that map each element of a class to an element of the same class forms a subgroup of @xmath52 ; moreover , the orbit partition induced by such a subgroup is just @xmath103 . as a consequence ,", "@xmath114 is a subset of the set of partitions induced by all the subgroups of @xmath52 ; since also the converse inclusion holds , the two sets indeed coincide .", "let @xmath48 be a group acting on the set @xmath136 , and let @xmath157 .", "for @xmath142 , let @xmath143 if and only if there exists an integer @xmath59 such that @xmath158 , where @xmath159 is the application of @xmath160 for @xmath59 times .", "it is not difficult to see that @xmath146 is an equivalence relation on @xmath136 , and thus it induces a partition @xmath103 on @xmath136 .", "the classes of @xmath103 are called the _ cycles _ of @xmath160 on @xmath136 .", "analogously to what said about orbits , it is not difficult to see that the partitions induced by the cycles of the automorphisms of @xmath52 satisfy definition  [ def : cycle - partition ] .", "we observe that , while an orbit partition is induced by a subgroup of @xmath52 , a cycle partition is induced by an automorphism , that is by an element of @xmath52 .", "the class @xmath116 is thus the set of cycle partitions obtained by considering every element of @xmath52 .", "let @xmath48 be a group acting on the set @xmath136 and let @xmath160 be a permutation in @xmath48 .", "then the _ orbits of the _ ( cyclic ) _ group _ @xmath161 generated by @xmath160 are the cycles of @xmath160 .", "since @xmath161 is a subgroup of @xmath48 , we have immediately that every cycle partition of @xmath48 is also an orbit partition of @xmath48 , that is , @xmath162 .", "example  [ ex : klein - group ] can be used to show that the converse does not generally hold : not every orbit partition is also a cycle partition .", "in fact we have : @xmath163    as noted above , theorem  [ teo : autr = cgr ] allows us to deal only with cogroup - relations instead of sets of arbitrary relations .", "the same can be done when working with cycle and orbit partitions : since cycles and orbits that form the partitions in @xmath116 and @xmath114 are completely determined from the elements and the subgroups of @xmath52 respectively , by theorem  [ teo : autr = cgr ] we can conclude that @xmath164 and @xmath165 .", "it is possible to show that both the set @xmath116 of cycle partitions and the set @xmath114 of orbit partitions of a given database @xmath14 constitute a partially ordered set ( poset ) with respect to the binary relation @xmath166 , where @xmath167 iff each class of @xmath168 is contained in some class of @xmath169 , where @xmath168 and @xmath169 are two partitions in @xmath75 , @xmath75 is equal to @xmath116 or @xmath114 . in fact , it is not difficult to see that @xmath166 is reflexive , antisymmetric and transitive : that is , @xmath166 is an order relation over both @xmath116 and @xmath114 .", "one notably difference between the posets @xmath170 and @xmath171 is that the first has always a maximum element , corresponding to the orbits of the entire @xmath52 , while the second may not have a maximum element , as shown above referring to example [ ex : klein - group ] , where @xmath52 is the so called klein group . instead", ", both the posets have a minimum element , corresponding to the cycles ( equal to the orbits ) induced by the identity element of @xmath52 : that is , the trivial partition , where each class is a singleton .    in order to prove our main results we need some definitions and some well known properties of finite groups . here", "we just recall the notion of _ stabilizer _ ; we address the reader to an introductory book on abstract algebra , such as @xcite , for the notion of coset and its properties .", "let @xmath48 be a group acting on a set @xmath136 , and let @xmath147 .", "the subgroup @xmath172 of @xmath48 defined as @xmath173 is called the _ stabilizer _ of @xmath148 in @xmath48 .", "it is not difficult to see that if @xmath48 is a group which acts on the set @xmath136 , and @xmath147 , then the stabilizer @xmath172 of @xmath148 can be considered as a group which acts on the set @xmath174 .", "the following are two well known results in group theory : lagrange s theorem  which correlates the cardinality of a given group @xmath48 and the cardinality of a given subgroup @xmath154 of @xmath48 with the number of left cosets of @xmath48 with respect to @xmath154  and a theorem which expresses the cardinality of the orbit of @xmath48 containing @xmath148 as the number of left cosets of @xmath48 with respect to the stabilizer @xmath172 .", "let @xmath48 be a finite group , and let @xmath154 be a subgroup of @xmath48", ". then @xmath175 , where @xmath176 is the number of left cosets of @xmath48 with respect to @xmath154 , and is usually called the _ index of @xmath154 in @xmath48_.    let @xmath48 be a finite group acting on a set @xmath136 , and let @xmath147", ". then @xmath177 , that is there exists a one - to - one correspondence between the elements of the orbit @xmath149 of @xmath148 under @xmath48 and the left cosets of the stabilizer @xmath172 in @xmath48 .", "[ teo : card - orbit ]    we are now able to prove the following theorem .", "let @xmath48 be a subgroup of the symmetric group @xmath73 , and let @xmath154 be a subgroup of @xmath48", ". if the orbit partitions of @xmath48 and @xmath154 are the same , then @xmath178 .", "[ teo : orbit - partitions ]    we prove the assertion by induction on @xmath59 .", "for @xmath179 the theorem can be proved by direct inspection of the subgroups of @xmath73 .", "now , let us suppose that the theorem is true for @xmath180 , and let us show that it holds also for @xmath59 .", "we first observe that since the orbit partitions of @xmath48 and @xmath154 are the same , then also the orbits @xmath181 and @xmath182 of the element @xmath59 with respect to @xmath48 and @xmath154 are the same . now ,", "if we take all the partitions having @xmath183 as a class , we get the orbit partitions induced by the stabilizers @xmath184 and @xmath185 of the element @xmath59 with respect to @xmath48 and @xmath154 .", "these orbit partitions are equal and thus , by induction hypothesis , @xmath186 . by lagrange", "s theorem , we can express the cardinalities of @xmath48 and @xmath154 with respect to the cardinalities of their stabilizers as @xmath187 and @xmath188 .", "where @xmath189 and @xmath190 are the indices , respectively , of the stabilizer @xmath184 in @xmath48 and of the stabilizer @xmath185 in @xmath154 . by theorem  [ teo : card - orbit ] , we can infer that @xmath191 and @xmath192 .", "since @xmath193 and @xmath194 , we can conclude that @xmath48 and @xmath154 have the same order , and thus @xmath195 .", "theorem  [ teo : orbit - partitions ] allows us to show that the orbit partitions of a given database satisfy theorem scheme ii ; in fact , the following theorem provides a first characterization of expressible queries in relational databases alternative to the one originally given by paredaens .    let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8", ". then @xmath196 [ teo : scheme - with - orbits ]    if @xmath197 , since the orbit partitions are completely determined from the subgroups of @xmath52 , we obtain that @xmath198 .    for the converse ,", "we observe that @xmath52 is a subgroup of the symmetric group @xmath73 , and @xmath199 is a subgroup of @xmath52 . by hypothesis , the orbit partitions of @xmath52 and @xmath199 are equal and thus , by theorem  [ teo : orbit - partitions ] , @xmath197", ".    a second characterization of expressible queries in relational databases can be obtained by considering cycle partitions instead of orbit partitions .", "we need the following lemma .", "let @xmath48 be a subgroup of the symmetric group @xmath73 , and let @xmath154 be a subgroup of @xmath48 .", "if the cycle partitions of @xmath48 and @xmath154 are the same , then also the orbits of @xmath48 and @xmath154 are the same , that is @xmath200 for every @xmath201 .", "[ lem : orbits - from - cycles ]    since the orbit @xmath149 is the set of elements of @xmath72 which are reachable from @xmath148 through some element @xmath160 of @xmath48 , while a cycle containing @xmath148 is the set of elements which are reachable from @xmath148 through one element @xmath160 of @xmath48 , one method to build @xmath149 from the cycle partitions of @xmath48 is given by algorithm  [ alg:1 ] .", "result @xmath202 @xmath203    algorithm  [ alg:1 ] computes the least subset @xmath204 of @xmath72 which contains @xmath148 and such that , for every cycle partition @xmath103 of @xmath48 , @xmath204 is the union of some cycles in @xmath103 ; it is not difficult to see that @xmath204 is , indeed , the orbit @xmath149 .", "since the cycle partitions of @xmath48 and @xmath154 are the same by hypothesis , the orbits computed by the algorithm above will be the same for @xmath48 and @xmath154 , for every choice of @xmath205 .", "we are now ready to prove the following theorem .", "let @xmath48 be a subgroup of the symmetric group @xmath73 , and let @xmath154 be a subgroup of @xmath48", ". if the cycle partitions of @xmath48 and @xmath154 are the same , then @xmath178 .", "[ teo : cycle - partitions ]    by lemma  [ lem : orbits - from - cycles ] , the orbits of @xmath48 and @xmath154 are the same . thus we can prove the theorem by the same argument used for theorem [ teo : orbit - partitions ] .", "a direct consequence of theorem  [ teo : cycle - partitions ] is that the cycle partitions of a given database satisfy theorem schema ii ; thus , the following theorem provides a second characterization of expressible queries in relational databases alternative to the one originally given by paredaens .", "the proof is analogous to the one given for theorem [ teo : scheme - with - orbits ] .", "let @xmath14 be a relational database , and let @xmath4 be a relation over @xmath8 .", "then : @xmath206 [ teo : scheme - with - cycles ]    a final observation is due about theorems  [ teo : scheme - with - orbits ] and [ teo : scheme - with - cycles ] . even though there is a strong resemblance between our meta theorem  [ thm : scheme1 ] and theorem  [ teo : paredaens2 ] , our results _", "can not _ be expressed neither in the form @xmath207 and @xmath208 nor in the form @xmath209 and @xmath79 , as shown in the next example .", "let @xmath210 and @xmath211 be two relational databases , with :    * @xmath212 * r = + [ cols=\"^,^,^,^,^ \" , ]     notice that @xmath54 is the cyclic group generated by the permutation @xmath213 , while @xmath214 is the cyclic group generated by the permutation @xmath215 : @xmath216    from @xmath54 and @xmath214 we can easily obtain @xmath217 .", "clearly , @xmath4 is not expressible from @xmath0 , since we have @xmath218 but @xmath219 ; on the other hand , @xmath220 and @xmath221 , and @xmath222 and @xmath221 .", "the fact that @xmath4 is not expressible from @xmath0 can be correctly determined through orbit partitions or through cycle partitions by observing that : @xmath223 or @xmath224 ."], ["in this section we study a simple graph - based model where two labeled graphs are used to model data bases .", "a data base consists of two distinct layers : a _ schema _ layer and a _ structure _ layer ; the objects can be found in the latter , while the former describe the data organization .", "each layer is a labeled weakly - connected directed graph , moreover there exists a function that maps a schema into a structure : such function will be called an _ extension_. both vertices and edges of the graphs are labeled , and we can assume that the sets of edge labels and vertex labels , as well as schema labels and structure labels , are disjoint .", "an example of data base is represented in figures  [ fig : schema ] ,  [ fig : structure ] , from which it is easy to note how the schema and the structure are closely related , the following definitions only formalize the intuitive idea .            a _", "schema graph _ , in short _ schema _ , is a triple @xmath225 , where @xmath226 is an oriented , weakly - connected graph , and @xmath227 , @xmath228 are respectively the injective functions that maps each node ( resp .", "edge ) to its label .", "[ def_struttura ] a _ structure _ is a triple @xmath229 , with @xmath4 a colored oriented graph @xmath230 , where @xmath231 is the set of _ nodes _ of the structure , @xmath232 is the set of _ edges _ , @xmath233 , @xmath234 are respectively the injective functions that maps each node ( resp .", "edge ) to its label , and @xmath235 , is a labeling of the edges over the finite alphabet @xmath236 , called _ coloring _ of the structure .    in the following", ", we will use the set @xmath237 of colors that allows to specify that a link between object instances in @xmath238 is actual or not . in the example of fig .", "[ fig : structure ] , only the links labeled true are represented , and the presence ( or the abscence ) of links labeled false does not change the data stored in the data base . in fig .  [", "fig : false ] is represented a part of the structure , where false links are represented with dotted arrows .", "the schema and the structure must be strongly correlated ; in fact there must exist a function , called _ extension _ ( denoted by @xmath239 ) , mapping the schema into the structure . in order to have a sound definition of extension some restrictions", "must be enforced , as pointed out in the following definition , where @xmath240 stands for the family of all nonempty subsets of @xmath241 .", "informally @xmath239 maps each vertex of the schema into some vertices of the structure and each edge of the schema into some edges of the structure .", "[ def_f1 ] let @xmath242 be a schema and @xmath243 a structure , where @xmath244 .", "then @xmath238 is an _ extensional structure _ of @xmath245 if there is a function ( the _ extension _ ) from @xmath245 to @xmath238 , @xmath246 , such that : [ condizioni_rel ]    1 .", "@xmath247 is a partition @xmath248 of the set @xmath249 , 2", ".   for every @xmath250 , the pair @xmath251 iff @xmath252 ;    notice that the first point of the definition of extension implies that the function @xmath253 is well defined . in the following ,", "if @xmath238 is the extensional structure of @xmath245 , then we write @xmath254 and we will simply say that @xmath238 is a structure of @xmath245", ". given two vertices @xmath255 and @xmath256 of the schema , connected with a link @xmath257 then in the structure there must exist all links @xmath258 for @xmath259 , @xmath260 .", "such requirement justifies the introduction of a labeling ( and especially of a true - false labeling ) in order to have a reasonable graph - based model .", "[ basedati ] a _ data base _", "@xmath261 is a pair @xmath262 , where @xmath245 is a schema and @xmath238 is an extensional structure of @xmath245 .", "the schema describes the conceptual organization of the data , while the data content or instantiation of the data base is given by the extensional structure associated to the schema .", "it is not hard to notice that , given a schema , there is a one - to - one correspondence between structures and extension functions , therefore we will sometimes use the pair @xmath263 as a data base .", "some preliminary definitions are required for introducing our query language .", "given a partial function @xmath264 ( i.e. a function where each element of @xmath241 can be associated to one or none of the elements of @xmath261 ) , by @xmath265 we denote the domain of @xmath266 , that is the set of elements @xmath267 such that @xmath268 is defined .", "let @xmath269 be two partial functions from the set @xmath241 to the set @xmath270 . then _", "@xmath266 is a restriction of @xmath160 _ , denoted by @xmath271 , if @xmath272 and for every @xmath273 , @xmath274 .", "moreover by @xmath275 we denote the set obtained as union of all images of elements in @xmath276 : formally @xmath277 .", "[ morfi ] let @xmath278 be a data base .", "an _ instance _ of @xmath261 is a restriction @xmath266 of @xmath239 such that @xmath265 induces a weakly - connected subgraph of @xmath245 .", "the following notations will be used in the rest of the paper .", "the set @xmath279 is the set of all instances of @xmath261 .", "let @xmath280 be a subset of @xmath279 , then by @xmath281 , we mean the set of nodes of the structure of @xmath261 that is the union of all images of instances in @xmath282 , while @xmath283 is the union of all domains of instances in @xmath282 .", "an element in @xmath284 is called a _ value _ , while an element in @xmath285 is called a _", "name_. then the image of a name @xmath286 is the subset @xmath241 of @xmath284 such that @xmath287 . for a value @xmath288 ,", "the _ inverse image of @xmath289 _ , denoted by @xmath290 , is the name of @xmath285 that is mapped by @xmath239 , to a set containing the element @xmath289 .", "similarly , given a set @xmath241 of values , the _ inverse image of @xmath241 _ is the set @xmath291 which is union of all inverse images of the values in @xmath241 .", "our graph data model is proposed as a domain - preserving data base , along the same lines as other papers where the expressiveness of the relational algebra is studied  @xcite , and it gives a formal embedding for languages used for the retrieval of graph - structured information  @xcite .", "the requirement that we are dealing with domain - preserving data bases reflects in the query language : in fact we have no operation for creating new elements or modifying the schema graph , and all operations must preserve the schema and the original structure .    the main consequence of the assumption that our model is domain preserving consists in the fact that we will deal with a schema which is mapped to an instance through an extensional mapping .", "therefore there is a complete equivalence between subgraphs of the structure and restrictions of the extensional mapping .", "we are now able to introduce the operations of our graph algebra : according to our reasoning above we can describe the operation as over partial functions whenever it allows a simpler formulation .", "[ sovrapposizione ] let @xmath292 be a data base and let @xmath293 .", "the _ addition _ of @xmath294 and @xmath295 , denoted as @xmath296 , is the following function over domain @xmath297 .", "the operation is defined only if @xmath298 : @xmath299    [ prodotto ] let @xmath292 be a data base , and let @xmath300 be two functions in @xmath279 .", "the _ product _ of @xmath294 and @xmath295 , denoted as @xmath301 is the instance in @xmath279 defined as follows :    @xmath302 the product is defined only if @xmath303 induces a weakly - connected subgraph of @xmath245 .", "[ proiezione ] let @xmath292 be a data base .", "let @xmath266 be a function in @xmath279 and let @xmath241 be a subset of the domain of @xmath266 , such that @xmath241 induces in @xmath245 a weakly - connected subgraph .", "the _ projection of @xmath266 on @xmath241 _ , denoted as @xmath304 , is the instance defined as follows :    @xmath305    [ differenza ] let @xmath292 be a data base .", "let @xmath300 be two functions in @xmath279 over the same domain @xmath241 .", "the _ difference of @xmath294 by @xmath295 _", ", denoted as @xmath306 is the following instance :    @xmath307    the difference is defined only if @xmath308 induces a weakly - connected subgraph of @xmath245 .", "since the coloring of the edges encodes the fact that a relation between two objects is actual or not , it is natural that the query language has some tools for exploiting such coloring . in our model", "we will need to extract instances where `` similar '' edges are the same color .", "the definition of _ selector _ is the first step in such direction .", "let @xmath245 be the schema of a data base @xmath261 .", "then a _ selector of @xmath245 _ is a pair @xmath309 consisting of a weakly - connected subgraph @xmath48 of @xmath245 and a coloring @xmath310 of the edges of @xmath48 .    querying for a selector in a data base returns all subgraphs of the structure that are isomorphic to the selector : each such subgraph is indeed called a _", "simple instance_. moreover , it is natural to define an operation of selection that allows to obtain instances which are compatible with a coloring of the schema over the alphabet @xmath236 .", "this is the last operation of our algebra .", "[ simple ] let @xmath263 be a data base , where @xmath311 .", "let @xmath312 be a selector of @xmath245 , where @xmath313 .", "then a _ simple instance induced by the selector @xmath314 _ is a restriction @xmath266 of @xmath239 such that @xmath315 , @xmath316 for each @xmath317 and @xmath318 for each @xmath319 .", "notice that all simple instaces have the same domain .", "[ selezione ] let @xmath292 be a data base .", "let @xmath266 be a function in @xmath279 and @xmath314 a selector .", "let @xmath320 be the set of all simple instances induced by @xmath321 that are also subinstances of @xmath266 .", "the _ selection of @xmath322 by @xmath314 _", ", denoted as @xmath323 , is @xmath324 ."], ["given a set @xmath280 of instances , our first aim is to give a characterization of all instances that can be obtained with a query that uses only the information contained in the instances in @xmath280 , or equivalently by an expression of the algebra that has only instances in @xmath280 as operands . in such direction", "the main result of this section is that expressiveness in our graph algebra is equivalent to the conservation of a certain partition .", "it is natural to associate a notion of _ undistinguishability _ to a partition , where all elements in a set of the partition are deemed undistinguishable .", "we share the goals of  @xcite , but we have introduced in this paper a new framework , that is we are looking for a notion of expressiveness that is coherent with our meta theorem . just as the notion of _ automorphisms _ , introduced in  @xcite for relations , gives a global description of the logical dependencies among data that must be preserved when querying the data base , in our model a partition ( or an equivalence relation ) will represent all such logical dependencies .", "the equivalence partition over elements of the structure that we will study is called _ stability _ and is denoted by @xmath325 ( where @xmath326 is an instance ) .", "the simplest possible form of undifferentiation ( called _ 0-stability _ ) is based on the idea that we are able to distinguish images of different vertices of the instance and vertices of the extensional structure belonging to different functions of @xmath326 .", "such notion basically consists of using expressions in our algebra that do not contain any selection .", "[ def : split ] let @xmath241 be a subset of the image of a set @xmath282 of instances .", "then @xmath241 is _ split _ by @xmath282 iff there is a function @xmath327 in @xmath282 such that @xmath328 and @xmath329 .", "we are now able to introduce formally the definition of 0-stability , as follows :    [ 0-stable ] let @xmath282 be a set of instances over a data base @xmath262 , and let @xmath241 be a subset of @xmath281 . then @xmath241 is _", "0-stable _ w.r.t .", "@xmath280 , if the two following conditions hold :    1 .", "@xmath330 , where @xmath331 , and @xmath332 is a name of the schema .", "2 .   for each function @xmath333", ", then @xmath241 and @xmath334 are disjoint or one is contained into the other one .    a more refined notion of undifferentiation is called _", "@xmath335-stability _ ; informally a set @xmath241 is 1-stable w.r.t . @xmath261", "if @xmath241 is 0-stable and @xmath261 is not able to distinguish two vertices of @xmath241 with edges outgoing from @xmath261 and ingoing in @xmath241 ( or outgoing from @xmath241 and ingoing in @xmath261 ) .", "notice that 1-stability is a binary relation over subsets of @xmath284 , while 0-stability is a unary relation .", "the formal definition is :    [ un_passo ] let @xmath282 be a set of instances over a data base @xmath262 and let @xmath241 and @xmath261 be two disjoint subsets @xmath281 . then @xmath241 is _", "1-stable _ w.r.t .", "@xmath261 and @xmath280 , denoted as @xmath336 if the following conditions are verified :    1 .", "@xmath241 is 0-stable w.r.t .", "@xmath282 ; 2", ".   for each edge @xmath337 of @xmath338 , with @xmath339 and for each @xmath340 there exists @xmath341 such that @xmath342 ; 3 .   for each edge @xmath343 of @xmath338 , with @xmath339 and for each @xmath340 there", "exists @xmath341 such that @xmath344 .", "informally 1-stability means that whenever there is an colored edge ( say a red edge ) from a vertex of @xmath241 to a vertex of @xmath261 , then all vertices of @xmath241 have a red edge ingoing in @xmath261 . in other words", "if we assume that @xmath261 is undistinguishable , then also @xmath241 is undistinguishable , by any single - edge path .", "the notion of 1-stability can be further generalized , but first we need a new definition .", "[ cammino ] let @xmath345 be a labeled graph .", "then a _ colored path _ in @xmath48 is a pair @xmath346 where @xmath347 , and for every @xmath348 , @xmath349 belongs to @xmath231 and @xmath350 is an edge of @xmath48 such that @xmath351 or @xmath352 .", "moreover @xmath353 is the sequence @xmath354 where @xmath355 is the color of the edge @xmath350 in @xmath48 .", "notice that the definition of path used in the paper is different from the one that can be usually found in a graph theory textbook , as arcs can also be in the reverse direction .", "the _ length _ of a path is the number of edges it contains .", "let @xmath356 be a data base and let @xmath346 be a colored path of @xmath338 , with @xmath357 .", "then the _ path schema _ of @xmath346 is the pair @xmath358 , with @xmath359 , where for every @xmath360 , @xmath361 and @xmath362 .", "[ path - dependencies ] let @xmath363 be two nodes of the @xmath280-structure , and let @xmath364 be a subset of @xmath281 , then the _ path dependencies _ from @xmath148 to @xmath289 in @xmath364 , denoted as @xmath365 is the set of path schemata of all paths of the @xmath280-structure that are starting in @xmath148 and ending in @xmath289 and entirely contained in @xmath364 .    informally given @xmath366 , their path dependencies is obtained by removing all vertices not in @xmath364 , then computing all possible paths from @xmath148 to @xmath289 , and finally computing the respective path schemata .", "the next step is to generalize 1-stability to @xmath25-stability , that is taking into account length-@xmath25 paths instead of simple edges ( that is length-1 paths ) .", "[ k_passi ] let @xmath282 be a set of instances over a data base @xmath262 , let @xmath25 be an integer larger than one , and let @xmath241 and @xmath261 be two disjoint subsets of @xmath281 .", "then @xmath241 is _", "@xmath25-stable _ w.r.t .", "@xmath261 and @xmath280 , denoted as @xmath367 if the following conditions are verified :    1 .", "@xmath241 is 0-stable w.r.t .", "@xmath282 ; 2 .", "@xmath241 is @xmath368-stable w.r.t . @xmath261 and @xmath280 ; 3", ".   for each @xmath369 there exists @xmath341 such that @xmath370 .", "the main idea is that when @xmath367 then if @xmath261 is undistinguishable also @xmath241 is undistinguishable when only paths no longer than @xmath25 are taken into account .", "our main definition follows :    [ rel_collettiva ] let @xmath280 be a set of instances , and let @xmath371 be two disjoint subsets of @xmath281 . then @xmath241 is _ stable w.r.t . @xmath261 _ in @xmath280 , denoted as @xmath372 , if @xmath373 for all @xmath374 .    by def .", "[ rel_collettiva ] , it is immediate to verify the following properties of stability :    [ prop1_rel ] let @xmath280 be a set of instances , and let @xmath375 , then :    1 .   if @xmath376 and @xmath377 , then @xmath372 and @xmath378 , 2 .", "if @xmath379 , @xmath372 and @xmath378 , then @xmath380 , 3 .", "if @xmath379 , @xmath381 , @xmath382 and @xmath383 , then @xmath384 .", "stability is a relation between disjoint subsets of the domain .", "the definition of expressiveness in the query language that we want to obtain is based on partitions , and now we are able to introduce the class of partitions we are interested into .", "a partition @xmath385 of nodes of the structure is called _ valid _ if and only if for each set @xmath241 of the partition and every set @xmath261 that is a union of sets of @xmath385 , then @xmath261 can not differentiate @xmath241 .", "[ partizione_accettabile ] let @xmath280 be a set of instances .", "a partition @xmath386 of @xmath387 is _ valid _ if for every @xmath388 , then @xmath389 .", "given a set @xmath282 of instances , then there may be various valid partitions of @xmath281 , and at least one valid partition always exists ( the partition where each vertex of the extentional structure is a set ) .", "some valid partitions are more representative of the actual undifferentiation , in fact we will assume as a measure of the undifferentiation induced by @xmath280 the coarsest valid partition , which we will call _ canonical partition _ and denote as @xmath390 .", "we can show that the definition of canonical partition is well - formed .", "every set @xmath280 of instances has a unique canonical partition @xmath390 .", "clearly the partition of @xmath281 into singletons is a valid partition , so there exists at least one canonical partition .", "now assume to the contrary that there exist two coarsest valid partitions @xmath391 and @xmath392 .", "let @xmath393 and @xmath394 be the equivalence relations induced by the partitions @xmath391 and @xmath392 , respectively .", "let @xmath395 be the transitive closure of the relation @xmath0 defined as follows : @xmath396 if and only if @xmath148 and @xmath289 are in the same set of @xmath391 or @xmath392 .", "we can prove that the partition @xmath385 induced by @xmath395 is a valid one of index strictly less than @xmath25 . by construction of @xmath395", "each set of @xmath385 is a union of sets in @xmath391 and also a union of sets in @xmath392 , moreover each set in @xmath385 is contained in the image of a single name ( since each set must be 0-stable ) .", "notice that @xmath397 iff @xmath398 .", "let @xmath399 be a set of @xmath385 , and let @xmath400 be a class of @xmath391 contained in @xmath399 .", "since @xmath391 is a valid partition , and @xmath399 is a union of disjoint sets of @xmath401 , by lemmata  [ rel_collettiva ] ,  [ prop1_rel ] , we have that @xmath402 and @xmath403 .", "let @xmath399 , @xmath404 be two sets of @xmath385 , with @xmath405 and @xmath406 .", "we have already proved that @xmath407 and @xmath408 , applying again lemmata  [ rel_collettiva ] ,  [ prop1_rel ] and noting that @xmath406 we obtain @xmath409 , by the generality of @xmath399 and @xmath404 the partition @xmath385 is valid .", "moreover @xmath385 is coarser than @xmath401 , which is a contradiction ."], ["in this section we will prove our main result regarding the expressiveness of the graph - based query language by showing that a function can be computed if and only if adding such function does not change the canonical partition . in the following", "we will assume that the union of the images of all functions in @xmath326 is exactly the universe set ; such assumption does not violate the generality since otherwise we would simply have some sets of the canonical partition whose union consists of exactly those elements of the universe set that are not in any function in @xmath326 .", "[ graal - expressivess ] let @xmath410 be the set of functions that are a result of an expression of the graph algebra where only functions of @xmath280 are operand .", "then @xmath411 if and only if the canonical partition induced by @xmath280 is equal to the canonical partition induced by @xmath412 , that is @xmath413 .", "the following two properties , that are consequences of def .  [ partizione_accettabile ] , will be useful to prove the main result of the paper .", "[ f_1 ] let @xmath280 be a set of instances and let @xmath385 be a valid partition .", "then the image of every instance @xmath414 is the union of sets of @xmath385 .", "we prove the lemma by induction on the number @xmath59 of operations of the expression for @xmath415 . if @xmath416 , then @xmath415 is a function in @xmath417 .", "since all sets of a valid partition are @xmath418-stable , no set of a valid partition has both an element in @xmath419 and an element not in @xmath419 , therefore the union of all sets of @xmath385 that are contained in @xmath419 is contained in @xmath419 . to prove that such containment is not strict ( i.e. such union is equal to @xmath419 ) it is sufficient to note that all elements of @xmath419 belong to some set of @xmath385 .", "assume now that @xmath420 and @xmath415 is obtained by the application of an operation to two expressions @xmath421 and @xmath422 in @xmath410 , or one expression for a selection . clearly , by inductive hypothesis", "the images of @xmath421 and @xmath422 are obtained as the union of some sets of @xmath385 .", "it is immediate to verify the lemma for the case that @xmath423 , @xmath424 , @xmath425 and @xmath426 .", "finally , assume that @xmath427 , where @xmath321 is a selector .", "by definition of the selection , then @xmath419 is the union of the images of all simple instances induced by @xmath321 .", "assume to the contrary that there exists a set @xmath241 of the valid partition such that @xmath428 and @xmath429 .", "now , let @xmath430 and @xmath431 .", "hence , by construction of selection , @xmath289 can not be in the image of any simple instance induced by @xmath321 , while @xmath148 is contained in the image of a simple instance induced by @xmath321 . by inductive hypothesis @xmath432", "is union of sets of @xmath385 , moreover since @xmath241 is a set of @xmath385 , also @xmath433 is union of sets of the valid partition , implying that @xmath434 , it follows that , for each @xmath435 , @xmath436 for some @xmath437 .", "this implies that there is a simple instance induced by @xmath321 that has in its image @xmath289 , which is a contradiction with the above assumption .", "consequently , the image of @xmath415 must be union of sets of @xmath385", ".    we will prove that an alternative characterization of canonical partition is as the partition induced by the equivalence relation @xmath438 between elements of @xmath439 , where @xmath440 if and only if for every instance @xmath441 , @xmath442 . in the following of the paper", "let @xmath443 denote the partition induced by the equivalence relation @xmath438 .", "successively we will prove that a function @xmath322 belongs to @xmath410 if and only if @xmath275 can be obtained as union of sets of @xmath443 , completing the proof of our main result , in two steps .", "first we will prove that a function @xmath266 belongs to @xmath444 if and only if @xmath445 is union of sets in @xmath443 , then we will prove that @xmath446 .", "the following proposition is an immediate consequence of the definitions of @xmath440 and of projection .", "[ unique1 ]", "let @xmath447 such that @xmath440 .", "then both @xmath148 and @xmath289 belong to the set @xmath448 for some name @xmath449 .", "[ unique ] let @xmath450 be the partition induced by a set @xmath280 of instances . then , the inverse image of every set of the partition consists of a single vertex .", "[ minimo_ottenibile ] let @xmath280 be a set of instances over @xmath261 , let @xmath443 be the partition induced by @xmath282 , and let @xmath451 .", "then there exists an instance @xmath441 such that @xmath452 .", "let @xmath453 be the set of functions @xmath454 such that @xmath455 . by cor .", "[ unique ] all functions in @xmath453 have the same domain , therefore the expression @xmath456 is well - formed ; by construction @xmath457 . by definition of @xmath443", "all functions @xmath458 are such that @xmath459 or @xmath460 , therefore all functions @xmath458 whose image intersect @xmath241 are such that @xmath455 , which in turn implies that are also in @xmath453 .", "hence @xmath461 , for otherwise there would be an element of @xmath241 not belonging to the image of any function in @xmath326 .", "[ uni2 ] let @xmath326 be a set of instances over @xmath261 , let @xmath450 be the partition induced by @xmath282 and let @xmath241 be a union of sets in @xmath443 , such that the inverse image of @xmath241 induces a weakly - connected subgraph of the schema , then @xmath241 is the image of an instance @xmath441 .", "let @xmath462 be the sets of @xmath443 whose union is @xmath241 , and notice that , by lemma  [ minimo_ottenibile ] , it is possible to associate to each set @xmath463 the instance @xmath464 whose image is @xmath463 , moreover for each such @xmath465 , @xmath466 . for each vertex @xmath148 in the inverse image of @xmath241 we can construct the function @xmath467 as @xmath468 . then let @xmath469 ; it is immediate to note that @xmath461 .    an immediate consequence of lemmata  [ f_1 ] and  [ uni2 ] is the following :    [ canonical = obtainable ]", "let @xmath326 be a set of instances over @xmath261 and let @xmath333 .", "then @xmath470 if and only if @xmath445 is union of sets in @xmath443 and the inverse image of @xmath445 induces a weakly - connected subgraph of the schema .    with lemma  [ uni2 ]", "we have proved that all interesting unions of sets of the partition @xmath450 can be obtained with an expression of the graph algebra where all operands are taken from @xmath280 , therefore @xmath450 conveys all expressibility information . but @xmath450 is defined on the set @xmath410 , we still need to correlate the definition of canonical partition with that of @xmath450 .", "[ pbi_accettabile ] let @xmath280 be a set of instances .", "then @xmath450 is a valid partition of @xmath280 .", "let @xmath371 be two disjoint sets where @xmath471 and @xmath261 is union of sets in @xmath443 , we will prove that @xmath372 .", "first of all we will show that @xmath241 is 0-stable .", "remember that , by definition of @xmath443 , for each @xmath470 either @xmath472 or @xmath473 .", "since @xmath474 contains @xmath326 , it is immediate to not that @xmath241 is 0-stable . in the following", "let @xmath10 be the single - vertex inverse image of @xmath241 .", "if the inverse image of @xmath475 does not induce a weakly - connected subgraph of the instance , then 0-stability of @xmath241 suffices to prove that @xmath372 , therefore assume that the inverse image of @xmath475 induces a weakly - connected subgraph of the instance .", "let us assume that @xmath372 does not hold , then we will get a contradiction . without loss of generality we can assume that @xmath261 is a minimum set for which @xmath372 does not hold .", "the new assumption implies that @xmath25-stability does not hold for some @xmath25 .", "it follows that there are two elements @xmath476 and an element @xmath477 such that @xmath478 is not contained in @xmath479 , for every @xmath480 .", "now , by lemma  [ uni2 ] , there is an instance @xmath414 such that @xmath481 .", "let @xmath482 and @xmath483 and let @xmath480 such that @xmath484 . let us consider the function @xmath485", ". by construction @xmath148 belongs to @xmath486 , but @xmath289 does not ; since @xmath487 we have found a function in @xmath474 containing @xmath148 but not @xmath289 , contradicting the assumption that @xmath488 .", "[ pd = pbi ] let @xmath280 be a set of instances . then @xmath489 .    by lemma  [ pbi_accettabile ] , @xmath490 must be a valid partition of @xmath280 . by lemma  [ minimo_ottenibile ] ,", "every set @xmath491 is obtained as the image of some instance @xmath441 . clearly , since @xmath441 , by lemma  [ f_1 ] the image @xmath241 of @xmath322 is the union of sets of the canonical partition of @xmath280 .", "hence @xmath492 .", "[ uguale1 ] let @xmath280 be a set of instances", ". then an instance @xmath415 belongs to @xmath474 if and only if @xmath493 .    clearly by lemma  [ pd = pbi ] it suffices to show that @xmath414 if and only if @xmath494 , moreover it is immediate to note that , by construction of @xmath443 , if @xmath414 then @xmath494 .", "assume now that @xmath494 .", "by lemma  [ pbi_accettabile ] , @xmath495 must be a valid partition . by lemma  [ f_1 ] , and exploiting the assumption that @xmath496 , for each @xmath497 , @xmath498 must be the union of some sets in @xmath499 , that is @xmath500 , for some sets @xmath501 .", "but by lemma  [ minimo_ottenibile ] , for each set @xmath502 there is an instance @xmath503 such that @xmath502 is the image of @xmath504 .", "consequently , @xmath505 , and hence @xmath506 , which proves that @xmath414 as required .", "theorem  [ uguale1 ] and lemma  [ pd = pbi ] lead to our main result .", "[ cor : uguale1 ] let @xmath280 be a set of instances", ". then an instance @xmath415 belongs to @xmath474 if and only if @xmath507 ."], ["we have introduced the idea that partitions of the domain set can be used for characterizing the set of relations or graphs that can be extracted in a data base in the relational or in a graph - based model .", "by formally proving those expressiveness results we have effectively given a new framework for the analysis of data base query languages .", "s.  abiteboul , c.  beeri , m.  gyssens , and d.  van gucht .", "an introduction to the completeness of languages for complex objects and nested relations . in _ nested relations and complex objects _ ,", "volume 361 of _ lncs _ , pages 117138 , 1989 .", "m.  andries and j.  paredaens . a language for generic graph transformations . in _", "graph - theoretic conecpts in computer science , 17th international workshop , wg91 _ , volume 570 of _ lncs _ , pages 170183 , berlin , 1991 .", "springer - verlag .    f.  bancilhon . on the completeness of query languages for relational data bases . in _", "proceedings , 7th symp . on mathematical foundations of computer science _ ,", "volume  64 of _ lncs _ , pages 112123 , berlin , 1978 .", "springer - verlag .                    j.  van den bussche , d.  van gucht , m.  andries , and m.  gyssens . on the completeness of object - creating query languages . in _", "33rd annual symposium on foundations of computer science .", "proceedings _ , pages 372379 , 1992 ."]]}
{"article_id": "1210.0340", "article_text": ["the maximum network flow problem is a well known fundamental problem in algorithms and optimization with plenty of important applications @xcite . it is known to be @xmath7-complete even in its integral version provided that the edge capacities are exponentially large in the size of the network @xcite .", "the minimum - cost flow problem is a well known important generalization of the maximum flow problem @xcite .", "the objective is to compute a maximum flow of minimum cost in a directed graph where each edge is assigned a cost . for a flow @xmath8 in a directed graph @xmath9", "the cost of @xmath8 is simply @xmath10    the prospects for designing a fast and processor efficient parallel algorithm , in particular an nc algorithm @xcite , for maximum integral flow or minimum - cost integral flow are small .", "the fastest known parallel implementations of general maximum flow and/or minimum - cost flow algorithms achieve solely a moderate speed up and still run in @xmath11 time , where @xmath12 is a positive constant , see @xcite .", "the situation changes when the edge capacities or the supply of flow as well as edge costs are substantially bounded .", "for example , if the edge capacities and edge costs are bounded by a polynomials in @xmath2 both problems admit rnc algorithms", ". then , the maximum integer flow problem admits even an @xmath6 algorithm @xcite while the minimum - cost integer flow problem admits an @xmath13 algorithm @xcite . at the heart of the aforementioned rnc solutions", "is the randomized method of detecting a perfect matching by randomly testing edmonds multi - variable polynomials for non - identity with zero @xcite .", "when the flow supply is relatively small , e.g. , logarithmic in the size of the network or a poly - logarithmic one , then just an nc implementation of the basic phase in the standard ford - fulkerson method @xcite yields an nc algorithm ( @xmath14 when the supply is logarithmic ) for maximum integer flow that can be extended to an nc algorithm for minimum - cost integer flow ( when edge costs are polynomially bounded ) .", "the number of processors used corresponds to that required by a shortest path computation . in this paper", ", we present a new approach to the minimum - cost integral flow problem for a small value @xmath0 of the flow .", "we directly associate a simple polynomial over a finite field with the corresponding problem of the existence of @xmath0 mutually vertex disjoint paths of bounded total length , connecting two sets of @xmath0 terminals in a directed graph . by using the idea of monomial cancellation", ", the latter problem reduces to testing the polynomial over a finite field of characteristic two for non - identity with zero .", "we combine the demillo - lipton - schwartz - zippel lemma @xcite on probabilistic verification of polynomial identities with parallel dynamic programming to perform the test efficiently in parallel .", "additionally , we use the isolation lemma to construct the minimum - cost flow @xcite .    in effect", ", we infer that a minimum - cost flow of value @xmath0 in a network with @xmath1 vertices , a sink and a source , integral edge capacities and positive integral edge costs polynomially bounded in @xmath1 can be found by a randomized pram , with errors of exponentially small probability in @xmath1 , running in @xmath3 time and using @xmath4 processors .", "thus , in particular , for the minimum - cost flow of value @xmath5 we obtain an @xmath6 algorithm", ".    * related work .", "* for the rnc algorithms for the related problem of minimum - cost perfect matching see @xcite . for the comparison of time and substantial processor complexities of prior rnc algorithms for the minimum - cost flow", "see page 7 in @xcite .", "the fastest of the reported algorithms is not an @xmath6 one even when the flow supply and thus the edge capacities are logarithmic in the size of the network .", "the idea of associating a polynomial over a finite field to the sought structure has been already used by edmonds to detect matching @xcite and then in several papers presenting rnc algorithms for perfect matching construction @xcite .", "it appears in several recent papers that also exploit the idea of monomial cancellation @xcite .", "* organization .", "* in the next section , we comment briefly on the basic notation and the model of parallel computation used in the paper . in section 3", ", we derive our fast randomized parallel method for detecting the existence of @xmath0 mutually vertex disjoint paths of bounded total length connecting two sets of @xmath0 terminals in a directed graph . in section 4 , we generalize the method to include edge costs which enables us to replace the total length bound with the total cost one . in section 5 ,", "we show a straightforward reduction of the minimum - cost integer flow problem parametrized by the flow value to the corresponding disjoint paths problem which enables us to derive our main result on detecting minimum - cost small flows in parallel .", "for a natural number @xmath2 we let @xmath15 $ ] denote the set of natural numbers in the interval @xmath16.$ ] the cardinality of a set @xmath17 will be denoted by @xmath18    we assume the standard definitions of _ flow _ and _ flow value _ in a network ( directed graph ) with integral edge capacities , a distinguished source vertex @xmath19 and a distinguished sink vertex @xmath20 ( e.g. , see @xcite ) .", "for the definitions of parallel random access machines ( pram ) , the classes nc and rnc and the corresponding notions of nc and rnc algorithms , the reader is referred to @xcite .", "the _ characteristic _ of a ring or a field is the minimum number of @xmath21 in a sum that yields @xmath22 a finite field with @xmath23 elements is often denoted by @xmath24", "it is well known that the maximum integral network flow problem with bounded edge capacities corresponds to a disjoint path problem ( cf .", "@xcite ) . in section 5", ", we provide an efficient parallel reduction of the minimum - cost integral flow problem parametrized by the flow value to a parametrized disjoint path problem .", "this section is devoted to a derivation of a fast randomized parallel method for the decision version of the parametrized path problem .", "let @xmath25 be a network in a form of a directed graph with @xmath1 vertices , among them a distinguished set @xmath26 of @xmath0 source vertices and a disjoint distinguished set @xmath27 of @xmath0 sink vertices .", "a _ walk _ in @xmath28 is a sequence of vertices @xmath29 of @xmath28 such that for @xmath30 @xmath31 @xmath32 is in @xmath33 @xmath34 are in @xmath35 @xmath36 is in @xmath37 the length of the walk is @xmath38 in other words , a walk is just a ( not necessarily simple ) path starting from a vertex in @xmath39 , having intermediate vertices in @xmath35 and ending at a vertex in @xmath37    a _ proper set @xmath40 of walks _ in @xmath28 is a set @xmath41 of @xmath0 walks of total length @xmath42 , each with a distinct start vertex in @xmath39 and a distinct end vertex in @xmath37 a _ signature _ of a proper set @xmath40 of walks is the pair @xmath43 that is smallest in lexicographic order such that the two walks that start at @xmath44 and @xmath45 respectively intersect , and the first intersection vertex of these two walks is the first intersection vertex of the walk starting from @xmath44 with any walk in @xmath46    note that walks in @xmath40 are pairwise vertex disjoint iff the signature of @xmath40 is not defined .", "we define the transformation @xmath47 on @xmath40 as follows .", "if @xmath40 has the signature @xmath43 then @xmath47 switches the suffix of the walk starting at @xmath44 with that of the walk starting at @xmath45 at the first intersection vertex of these two walks .", "1 . otherwise , if the signature of @xmath40 is not defined then @xmath47 is an identity on @xmath46    [ fig : paths ]     of walks and the companion proper set @xmath48 of walks .", ", height=188 ]    observe that if the signature of @xmath40 is defined then @xmath48 has the same signature as @xmath40 and @xmath49 the first observation is immediate . to show the second one it is sufficient to note that @xmath50 holds iff @xmath47 transforms the two walks which yield the signature of @xmath40 onto themselves .", "the latter is however impossible since they have different start vertices and different end vertices .", "note also that the walks in @xmath48 have the same total length as those in @xmath46    it follows that @xmath47 is an involution on sets of proper walks of total length @xmath51 i.e. , @xmath52 holds for any proper set @xmath40 of walks of total length @xmath53 .    for the network @xmath28 and @xmath54,$ ] let @xmath55 be the family of all proper sets of @xmath0 walks of total length @xmath56 in @xmath57 assign a distinct variable @xmath58 to each edge @xmath59 in @xmath57 for a walk @xmath60 let @xmath61 be the monomial , where @xmath58 has multiplicity equal to the number of occurrences of @xmath59 in @xmath62 next , let @xmath63 denote the polynomial @xmath64    [ lem : zero ] for the network @xmath28 and @xmath54,$ ] there is a proper set of @xmath0 mutually vertex - disjoint walks of total length @xmath56 in @xmath28 iff @xmath63 is not identical to zero over a field of characteristic two .", "@xmath55 can be partitioned into the family @xmath65 of sets @xmath40 of walks such that @xmath50 and the family @xmath66 of sets @xmath40 of walks such that @xmath49 the polynomial @xmath67 is identical to zero over a field of characteristic two since for each @xmath68 the monomials @xmath69 and @xmath70 contain equal multiplicities of the same variables and @xmath52 so @xmath40 and @xmath48 can be paired . on the other hand , since each set @xmath40 of walks in @xmath65 consist of mutually vertex - disjoint walks , the monomials @xmath69 in the polynomial @xmath71 are in one - to one correspondence with @xmath40 and thus are unique provided that the walks in @xmath40 are simple paths .", "now , it is sufficient to observe that mutually vertex - disjoint walks can be always trivially pruned to corresponding mutually vertex - disjoint simple paths .    to warm up", ", we prove the following lemma on sequential evaluation of @xmath72    [ lem : eval ] @xmath63 can be evaluated for a given assignment of values over a field @xmath73 of characteristic two in @xmath74 time .", "for @xmath75 @xmath76,$ ] we consider the family @xmath77 of all sets @xmath40 consisting of @xmath78 walks connecting @xmath78 distinct sources in @xmath79 with the @xmath78 distinct sinks in @xmath80 so that the total length of the walks is exactly @xmath81 next , we define the polynomial @xmath82 as @xmath83 note that @xmath84 and @xmath85    on the other hand , for @xmath86,$ ] @xmath87 and @xmath88 we consider the set @xmath89 of walks of length @xmath90 in @xmath28 that start at @xmath91 and end at @xmath92 let @xmath93 be the polynomial @xmath94    we have the following recurrence for a nonempty subset @xmath80 of @xmath95 and @xmath96:$ ]    @xmath97}q_{p - q}(b\\setminus \\ { y\\})q_q(x_{|b|},y).\\ ] ]    next , we have also the following recurrence for @xmath98 @xmath88 and @xmath99:$ ]    @xmath100    we have also @xmath101 if @xmath102 and otherwise @xmath103 consequently , we can evaluate all the polynomials @xmath104 by the second recurrence in @xmath105 time .    now , by using the first recurrence and setting @xmath106 to @xmath21 in the field , we can evaluate all the polynomials @xmath82 in the increasing order of the cardinalities of @xmath80 in @xmath107 time .    by @xmath84 and @xmath108", "we conclude that @xmath63 can be evaluated in @xmath74 time .", "note that @xmath109 let @xmath110 be the sink in @xmath80 with the largest index .", "we assume @xmath111 and if @xmath112 then @xmath113 now , we can evaluate @xmath114 and @xmath115 by the following recurrences    @xmath116    @xmath117    in @xmath118 time .", "we can partially parallelize the sequential evaluation of @xmath63 in order to obtain the following lemma .", "[ lem : pareval ] @xmath63 can be evaluated for a given assignment of values over a field @xmath73 of characteristic two in @xmath119 time by a crew pram using @xmath120 processors .", "we generalize the definition of the set @xmath121 and the corresponding polynomial @xmath104 to include arbitrary start vertex @xmath122 , requiring @xmath123 as previously .", "then , we can evaluate @xmath124 for @xmath122 and @xmath88 for @xmath99 $ ] by the following standard doubling recurrence for @xmath125    @xmath126    at the bottom of the recursion , we have @xmath101 if @xmath102 otherwise @xmath103 it follows that all @xmath104 for @xmath122 , @xmath88 and @xmath127 $ ] can be evaluated in a bottom - up manner in @xmath128 time by a crew pram using @xmath129 processors .", "recall the first recurrence from the proof of lemma [ lem : eval ] .", "when the polynomials @xmath130 for @xmath131 and @xmath132 are evaluated , we can evaluate in turn the polynomials @xmath82 , where @xmath75 @xmath96 $ ] in @xmath0 phases in the increasing order of the cardinalities of @xmath80 by this recurrence .", "it can be done in @xmath133 time by a crew pram using @xmath134 processors .    by @xmath135 @xmath108 and @xmath136", "we conclude that @xmath63 can be evaluated in @xmath119 time by a crew pram using @xmath120 processors .", "the following lemma on polynomial identities verification has been shown independently by demillo and lipton , schwartz , and zippel .", "[ lem : zip]@xcite let @xmath137 be a nonzero polynomial of degree @xmath138 over a field of size @xmath139 then , for @xmath140 @xmath141 ... , @xmath142 chosen independently and uniformly at random from the field , the probability that @xmath143 is not equal to zero is at least @xmath144    note that the polynomial @xmath63 is of degree @xmath53 not larger than @xmath145 we can use lemma [ lem : zip ] with a field @xmath146 of characteristic two to obtain a randomized test of the polynomial @xmath63 for not being identical to zero with one side errors . for sufficiently large constant @xmath147", "the one side errors are of probability not larger than a constant smaller than @xmath148 by performing @xmath149 such independent tests , the probability of one side errors can be decreased to exponentially small in @xmath1 one .    by lemma [ lem : pareval ] , the series of the tests can be performed in @xmath150 time by a pram using @xmath151 processors . by lemma [ lem : zero ] , these tests verify if there is a proper set of mutually vertex - disjoint walks of total length @xmath56 in the network @xmath28 .", "the latter in turn is equivalent to the existence of @xmath0 mutually vertex - disjoint paths of total length @xmath56 connecting @xmath39 with @xmath95 in @xmath28 by the definition of a proper set of walks in @xmath57 hence , observing that each walk can be trivially pruned to a simple directed path with the same endpoints , we obtain our main result .", "[ theo : pdec ] the problem of whether or not there is a set of @xmath0 mutually vertex - disjoint simple directed paths of total length @xmath56 connecting @xmath39 with @xmath95 in the network @xmath28 can be decided by a randomized crew pram , with one - sided errors of exponentially small probability in @xmath2 running in @xmath150 time and using @xmath151 processors .", "in this section , we shall consider a more general situation where there are a positive integer @xmath152 and a cost function @xmath153 assigning to each of the @xmath154 edges @xmath59 in the network @xmath28 a cost @xmath155.$ ] the cost of a walk or a path is simply the sum of the costs of the edges forming it ( the cost of an edge is counted the number of times it appears on the walk or path ) .", "we would like to detect a proper set of @xmath0 walks in @xmath28 that achieves the minimum cost .", "for this reason , we consider the following generalization of the polynomial @xmath156 for @xmath157,$ ] let @xmath158 be the set of all proper sets of walks in the edge - costed network @xmath28 that have total cost not greater than @xmath159 next , for a walk @xmath41 in @xmath160 as previously , let @xmath61 be the monomial which is the product of @xmath58 over the occurrences of edges @xmath59 on @xmath62 the polynomial @xmath161 is defined by @xmath162    by using the proof method of lemma [ lem : zero ] , we obtain the following counterpart of this lemma for @xmath161 .    [ lem : czero ] for the edge - costed network @xmath28 , there is a proper set of @xmath0 mutually vertex - disjoint walks of total cost @xmath163 in @xmath28 iff @xmath161 is not identical to zero over a field of characteristic two .", "next , we obtain the following counterpart of lemma [ lem : pareval ] for @xmath161 .", "[ lem : cpareval ] @xmath161 can be evaluated for a given assignment @xmath8 of values over a field @xmath73 of characteristic two in @xmath164 time by a pram using @xmath165 processors .", "the proof reduces to that of lemma [ lem : pareval ] .", "we replace each directed edge @xmath59 of cost @xmath155 $ ] in the network @xmath28 by a directed path of length @xmath166 introducing @xmath167 additional vertices . with each edge on such a path , we associate a variable .", "we assign @xmath168 to the variable associated with the first edge on the path replacing @xmath169 and just @xmath21 of the field to the variables associated with the remaining edges on the path .", "the resulting network @xmath170 is of size @xmath171 let @xmath172 be the family of all proper sets of @xmath0 walks of total cost @xmath163 in the network @xmath170 .", "we can evaluate the polynomial @xmath173 in parallel analogously as @xmath63 in the proof of lemma [ lem : pareval ] .", "it remains to observe that the value of @xmath161 under the assignment @xmath8 is equal to that of @xmath174 under the aforementioned assignment .", "now , we are ready to derive our main result in this section .", "the minimum cost of a set of @xmath0 mutually vertex - disjoint simple directed paths connecting @xmath39 with @xmath95 in the network @xmath28 with edge costs in @xmath175 $ ] can be computed by a randomized crew pram , with errors of exponentially small probability in @xmath2 running in @xmath164 time and using @xmath176 processors .", "the minimum cost of the sought set of vertex - disjoint paths is in @xmath177.$ ] hence , by lemma [ lem : czero ] , it is sufficient to test the polynomials @xmath161 for non - identity with zero for all @xmath178 $ ] in parallel . by applying lemmata [ lem : zip ] and [ lem : cpareval ] in a manner analogous to the proof of theorem [ theo : pdec ] , we conclude that it can be done by a randomized crew pram , with one - sided errors of exponentially small in @xmath1 probability , running in @xmath164 time and using + @xmath179 processors .", "a straightforward approach of extending our randomized parallel method for deciding if there is a proper set of @xmath0 mutually vertex - disjoint walks ( of a bounded total cost ) between two sets of vertices of cardinality @xmath0 to include the finding variant could be roughly as follows . in parallel , for each @xmath0-tuple of respective neighbors of the @xmath0 start vertices in @xmath33 replace the set of start vertices by the @xmath0-tuple and apply our method recursively to the resulting network", ". if the test is positive , the first edges on the walks are known , and we can iterate the method .", "the problem with this approach is that its recursive depth is proportional to the maximum length of a walk in the resulting set of mutually vertex - disjoint walks between @xmath39 and @xmath37    also , it is not clear how one could implement a straightforward divide - and - conquer approach of guessing intermediate vertices in order to find a set of @xmath0 mutually - vertex disjoint walks of a given cost efficiently in parallel .", "we need more advanced methods to obtain a very fast parallelization of the finding variant .", "we shall modify the edge cost in the network @xmath28 in order to use the so called _ isolation lemma _ in a manner analogous to the rnc method of finding a perfect matching given in @xcite .", "( the isolation lemma @xcite ) .", "let @xmath180 be a family of subsets of a set with @xmath23 elements and let @xmath181 be a non - negative integer .", "suppose that each element @xmath19 of the set is independently assigned a weight @xmath182 uniformly at random from @xmath183 $ ] , and the weight of a subset @xmath40 in @xmath180 is defined as @xmath184 then , the probability that there is a unique set in @xmath180 of minimum weight is at least @xmath185    [ cor : ciso ] for each of the @xmath154 edges @xmath59 in the network @xmath160 modify its cost @xmath166 to @xmath186 where the weight @xmath187 is drawn uniformly at random from @xmath183 $ ] .", "then , the probability that there is a unique minimum - cost set of mutually vertex - disjoint paths connecting @xmath39 with @xmath95 in the edge weighted network @xmath28 is at least @xmath188    to use the isolation lemma , let the underlying set to consist of all edges in the network @xmath57 next , note that a set of mutually vertex - disjoint paths connecting @xmath39 with @xmath95 achieving a minimum cost consists of simple paths and thus it can be identified with the set of edges on the paths .", "let @xmath7 be the family of all sets of mutually vertex - disjoint simple paths connecting @xmath39 with @xmath95 in the network @xmath57 by the setting of new costs @xmath189 solely those sets in @xmath7 that achieved the minimum cost , say @xmath190 under the original costs @xmath166 can achieve a minimum cost under the new costs @xmath191", "so , we can set @xmath180 to the aforementioned sub - family of @xmath7 , and define the weight of a set of @xmath0 paths in @xmath180 as the sum of the weights @xmath187 of the edges @xmath59 on the paths in this set in order to use the isolation lemma . by the isolation lemma", ", there is a unique set @xmath40 in @xmath180 that achieves the minimum weight @xmath192 with the probability at least @xmath188 the corollary follows since each set @xmath40 in @xmath180 has the cost @xmath193 equal to @xmath194    throughout the rest of this section , we shall assume that each of the @xmath154 edges @xmath59 in the network @xmath28 is assigned the cost @xmath195 as in corollary [ cor : ciso ] and that @xmath196.$ ]    suppose that we know the minimum cost of a set of @xmath0 mutually vertex - disjoint paths connecting @xmath39 with @xmath95 in the network @xmath28 with the edge costs indicated , and such a minimum - cost set is unique .", "then , it is sufficient to show that we can test quickly in parallel if the network @xmath28 with an arbitrary edge removed still contains a set of @xmath0 mutually vertex - disjoint paths connecting @xmath39 with @xmath95 that achieves the minimum cost . by performing the test for each edge of @xmath28 in parallel , we can determine the set of edges forming the unique minimum - cost set of @xmath0 mutually vertex - disjoint paths connecting @xmath39 with @xmath95 .    to carry out these tests ,", "we need to generalize the polynomial @xmath161 to a polynomial @xmath197 , where @xmath59 is an edge in @xmath28 and @xmath198 is a cost constraint from @xmath199=[cn^{o(1)}]$ ] .", "let @xmath200 be the family of all proper sets of @xmath0 walks in the network @xmath28 with the edge @xmath59 removed that have total at most @xmath159 ( in the total cost of a set of walks , we count the cost of an edge the number of times equal to the sum of the multiplicities of the edge in the walks . )    as in the definition of @xmath63 assign a distinct variable @xmath58 to each edge @xmath59 in @xmath160 and for a walk @xmath201 let @xmath61 be the monomial , where @xmath58 has multiplicity equal to the number of occurrences of @xmath59 in @xmath62 the polynomial @xmath197 is defined by @xmath202    by using the proof method of lemma [ lem : zero ] , we obtain the following counterpart of this lemma for @xmath197 .    [ lem : gzero ] for the edge - costed network @xmath28 with @xmath154 edges , edge @xmath169 and @xmath203,$ ] there is a proper set of @xmath0 mutually vertex - disjoint walks of total cost @xmath163 in the network @xmath28 with the edge @xmath59 removed iff @xmath197 is not identical to zero over a field of characteristic two .", "next , we obtain the counterpart of lemma [ lem : pareval ] for @xmath197 following the proof of lemma [ lem : cpareval ] .", "[ lem : gcpareval ] @xmath197 can be evaluated for a given assignment of values over a field @xmath73 of characteristic two in @xmath164 time by a pram using @xmath204 processors .", "now , we are ready to derive our main result in this section .", "[ theo : cpfind ] there is a randomized pram returning almost certainly ( i.e. , with probability at least @xmath205 , where @xmath206 ) a minimum - cost set of @xmath0 mutually vertex - disjoint paths connecting @xmath39 with @xmath95 in the network @xmath28 with the original edge costs in @xmath175 $ ] ( iff such a set exists ) in @xmath164 time using @xmath204 processors .", "we set @xmath181 to , say , @xmath207 and specify the new edge costs @xmath195 in the network @xmath28 drawing the weights @xmath187 uniformly at random from @xmath183 $ ] as in corollary [ cor : ciso ] .", "next , for each @xmath208=[cn^{o(1)}],$ ] we proceed in parallel as follows . for each edge @xmath59 of the network @xmath28", ", we test the polynomial @xmath197 for the non - identity with zero by using lemma [ lem : zip ] and lemma [ lem : gcpareval ] ( we can perform a linear in @xmath1 number of such tests in parallel in order to decrease the probability of the one - sided error to an exponentially small one ) .", "next , we verify if the edges that passed the test positively yield a set of @xmath0 mutually vertex - disjoint paths connecting @xmath39 with @xmath95 .", "for example , it can be done by checking for each endpoint of the edges outside @xmath209 if it is shared by exactly two of the edges , and then computing and examining the transitive closure of the graph induced by the edges ( see @xcite ) .", "if so , we save the resulting set of paths of total ( new ) cost @xmath210 by corollary [ cor : ciso ] , there is a @xmath211 $ ] for which the above procedure will find such a set of paths that achieves the minimum ( original ) cost with probability at least @xmath212", "the following lemma is a straightforward generalization of a folklore reduction of maximum integral flow to a corresponding disjoint connecting path problem ( for instance cf .", "@xcite ) to include minimum - cost integral flow . we shall call a flow proper , if it ships each flow unit along a simple path from the source to the sink .", "[ lem : red ] the problem of whether or not there is a proper integral flow of value @xmath0 and cost @xmath213 from a distinguished source vertex @xmath19 to a distinguished sink vertex @xmath20 in a directed network with @xmath1 vertices .", "integral edge capacities and edge costs in @xmath175 $ ] can be ( many - one ) reduced to that of whether or not there is set of @xmath0 mutually vertex - disjoint simple directed paths of total cost @xmath214 , where @xmath215 connecting two distinguished sets of @xmath0 vertices in a directed network on @xmath216 vertices in @xmath217 time by a crew pram using @xmath218 processors .", "let @xmath219 be the directed network with integral edge capacities , edges costs in @xmath175 $ ] and the distinguished source vertex @xmath19 and sink vertex @xmath220 since we are interested in a flow of value @xmath0 , we can assume w.l.o.g that all edge capacities do not exceed @xmath221    we form a directed network @xmath222 on the basis of the network @xmath223 as follows .", "let @xmath224 next , let @xmath225 be the set of edges in @xmath223 incoming into @xmath226 , and let @xmath227 be the set of edges in @xmath223 leaving @xmath228 for each @xmath229 and @xmath230,$ ] we create the vertex @xmath231 analogously , for each @xmath232 and @xmath233,$ ] we create the vertex @xmath234 furthermore , we direct an edge from each vertex @xmath235 to each vertex @xmath236 . to each such an edge , we assign the cost @xmath148 also , for each edge @xmath237 of @xmath238 we direct an edge from @xmath239 to @xmath240 for @xmath230.$ ] to each such an edge , we assign the cost @xmath241 see fig .", "[ fig : paths1 ]     and the corresponding part of the network @xmath242,height=188 ]    let @xmath243 be the set of vertices of the form @xmath244 , and let @xmath245 denote the set of vertices of the form @xmath246 create an additional set @xmath39 of @xmath0 vertices and from each vertex in @xmath39 direct an edge to each vertex in @xmath243 .", "symmetrically , create another additional set @xmath95 of @xmath0 vertices and from each vertex in @xmath245 direct an edge to each vertex in @xmath37    it is easy to observe that there is a proper integral flow of value @xmath0 and cost @xmath213 from @xmath19 to @xmath20 in the network @xmath223 iff there is a set of @xmath0 mutually vertex - disjoint simple paths of total cost @xmath214 connecting @xmath39 with @xmath95 in the network @xmath247 such that @xmath248    now it is sufficient to note that the construction of @xmath222 , @xmath39 and @xmath95 on the basis of @xmath223 easily implemented by a crew pram in @xmath217-time using @xmath218 processors , where @xmath1 is the number of vertices in @xmath249    by combining theorem [ theo : pdec ] with lemma [ lem : red ] , we obtain our first main result .", "the minimum cost of a flow of value @xmath0 in a network with @xmath1 vertices , a sink and a source , integral edge capacities and positive integral edge costs in @xmath175 $ ] can be found by a randomized pram , with errors of exponentially small probability in @xmath2 running in @xmath250 time and using @xmath204 processors .    by combining in turn theorem [ theo : cpfind ] with the finding variant of lemma [ lem : red ] using exactly the same reduction", ", we obtain our second main result .", "[ theo : ffind ] there is a randomized pram algorithm returning almost certainly a minimum - cost flow of value @xmath0 ( iff a flow of value @xmath0 exists ) in a network with @xmath1 vertices , a sink and a source , integral edge capacities and edge costs in @xmath251,$ ] in @xmath3 time using @xmath4 processors .", "the problem of finding a minimum - cost flow of value @xmath252 in a network with @xmath1 vertices , a sink and a source , and integral edge capacities bounded polynomially in @xmath1 admits an @xmath6 algorithm .", "we have resented a new approach to the minimum - cost integral flow problem . in particular , it yields an @xmath6 algorithm when the flow supply is ( at most ) logarithmic in the size of the network .", "i. koutis .", "faster algebraic algorithms for path and packing problems .", "35th annual international colloquium on automata , languages and programming ( icalp ) , lecture notes in computer science 5555 , pp .", "653 - 664 , 2009 .", "orlin and c. stein .", "parallel algorithms for the assignment and minimum - cost flow problems .", "operations research letters , * 14 * , pp .", "181 - 186 , 1993 .", "reif ( editor ) .", "synthesis of parallel algorithms .", "morgan - kauffman , 1993 ."], "abstract_text": ["<S> we present a new approach to the minimum - cost integral flow problem for small values of the flow . </S>", "<S> it reduces the problem to the tests of simple multi - variable polynomials over a finite field of characteristic two for non - identity with zero . in effect </S>", "<S> , we show that a minimum - cost flow of value @xmath0 in a network with @xmath1 vertices , a sink and a source , integral edge capacities and positive integral edge costs polynomially bounded in @xmath1 can be found by a randomized pram , with errors of exponentially small probability in @xmath2 running in @xmath3 time and using @xmath4 processors . </S>", "<S> thus , in particular , for the minimum - cost flow of value @xmath5 we obtain an @xmath6 algorithm . </S>"], "labels": null, "section_names": ["introduction", "terminology", "connecting vertex-disjoint paths", "vertex-disjoint connecting paths of bounded cost", "finding vertex-disjoint connecting paths", "minimum-cost logarithmic integral flow is in @xmath6", "final remarks"], "sections": [["the maximum network flow problem is a well known fundamental problem in algorithms and optimization with plenty of important applications @xcite . it is known to be @xmath7-complete even in its integral version provided that the edge capacities are exponentially large in the size of the network @xcite .", "the minimum - cost flow problem is a well known important generalization of the maximum flow problem @xcite .", "the objective is to compute a maximum flow of minimum cost in a directed graph where each edge is assigned a cost . for a flow @xmath8 in a directed graph @xmath9", "the cost of @xmath8 is simply @xmath10    the prospects for designing a fast and processor efficient parallel algorithm , in particular an nc algorithm @xcite , for maximum integral flow or minimum - cost integral flow are small .", "the fastest known parallel implementations of general maximum flow and/or minimum - cost flow algorithms achieve solely a moderate speed up and still run in @xmath11 time , where @xmath12 is a positive constant , see @xcite .", "the situation changes when the edge capacities or the supply of flow as well as edge costs are substantially bounded .", "for example , if the edge capacities and edge costs are bounded by a polynomials in @xmath2 both problems admit rnc algorithms", ". then , the maximum integer flow problem admits even an @xmath6 algorithm @xcite while the minimum - cost integer flow problem admits an @xmath13 algorithm @xcite . at the heart of the aforementioned rnc solutions", "is the randomized method of detecting a perfect matching by randomly testing edmonds multi - variable polynomials for non - identity with zero @xcite .", "when the flow supply is relatively small , e.g. , logarithmic in the size of the network or a poly - logarithmic one , then just an nc implementation of the basic phase in the standard ford - fulkerson method @xcite yields an nc algorithm ( @xmath14 when the supply is logarithmic ) for maximum integer flow that can be extended to an nc algorithm for minimum - cost integer flow ( when edge costs are polynomially bounded ) .", "the number of processors used corresponds to that required by a shortest path computation . in this paper", ", we present a new approach to the minimum - cost integral flow problem for a small value @xmath0 of the flow .", "we directly associate a simple polynomial over a finite field with the corresponding problem of the existence of @xmath0 mutually vertex disjoint paths of bounded total length , connecting two sets of @xmath0 terminals in a directed graph . by using the idea of monomial cancellation", ", the latter problem reduces to testing the polynomial over a finite field of characteristic two for non - identity with zero .", "we combine the demillo - lipton - schwartz - zippel lemma @xcite on probabilistic verification of polynomial identities with parallel dynamic programming to perform the test efficiently in parallel .", "additionally , we use the isolation lemma to construct the minimum - cost flow @xcite .    in effect", ", we infer that a minimum - cost flow of value @xmath0 in a network with @xmath1 vertices , a sink and a source , integral edge capacities and positive integral edge costs polynomially bounded in @xmath1 can be found by a randomized pram , with errors of exponentially small probability in @xmath1 , running in @xmath3 time and using @xmath4 processors .", "thus , in particular , for the minimum - cost flow of value @xmath5 we obtain an @xmath6 algorithm", ".    * related work .", "* for the rnc algorithms for the related problem of minimum - cost perfect matching see @xcite . for the comparison of time and substantial processor complexities of prior rnc algorithms for the minimum - cost flow", "see page 7 in @xcite .", "the fastest of the reported algorithms is not an @xmath6 one even when the flow supply and thus the edge capacities are logarithmic in the size of the network .", "the idea of associating a polynomial over a finite field to the sought structure has been already used by edmonds to detect matching @xcite and then in several papers presenting rnc algorithms for perfect matching construction @xcite .", "it appears in several recent papers that also exploit the idea of monomial cancellation @xcite .", "* organization .", "* in the next section , we comment briefly on the basic notation and the model of parallel computation used in the paper . in section 3", ", we derive our fast randomized parallel method for detecting the existence of @xmath0 mutually vertex disjoint paths of bounded total length connecting two sets of @xmath0 terminals in a directed graph . in section 4 , we generalize the method to include edge costs which enables us to replace the total length bound with the total cost one . in section 5 ,", "we show a straightforward reduction of the minimum - cost integer flow problem parametrized by the flow value to the corresponding disjoint paths problem which enables us to derive our main result on detecting minimum - cost small flows in parallel ."], ["for a natural number @xmath2 we let @xmath15 $ ] denote the set of natural numbers in the interval @xmath16.$ ] the cardinality of a set @xmath17 will be denoted by @xmath18    we assume the standard definitions of _ flow _ and _ flow value _ in a network ( directed graph ) with integral edge capacities , a distinguished source vertex @xmath19 and a distinguished sink vertex @xmath20 ( e.g. , see @xcite ) .", "for the definitions of parallel random access machines ( pram ) , the classes nc and rnc and the corresponding notions of nc and rnc algorithms , the reader is referred to @xcite .", "the _ characteristic _ of a ring or a field is the minimum number of @xmath21 in a sum that yields @xmath22 a finite field with @xmath23 elements is often denoted by @xmath24"], ["it is well known that the maximum integral network flow problem with bounded edge capacities corresponds to a disjoint path problem ( cf .", "@xcite ) . in section 5", ", we provide an efficient parallel reduction of the minimum - cost integral flow problem parametrized by the flow value to a parametrized disjoint path problem .", "this section is devoted to a derivation of a fast randomized parallel method for the decision version of the parametrized path problem .", "let @xmath25 be a network in a form of a directed graph with @xmath1 vertices , among them a distinguished set @xmath26 of @xmath0 source vertices and a disjoint distinguished set @xmath27 of @xmath0 sink vertices .", "a _ walk _ in @xmath28 is a sequence of vertices @xmath29 of @xmath28 such that for @xmath30 @xmath31 @xmath32 is in @xmath33 @xmath34 are in @xmath35 @xmath36 is in @xmath37 the length of the walk is @xmath38 in other words , a walk is just a ( not necessarily simple ) path starting from a vertex in @xmath39 , having intermediate vertices in @xmath35 and ending at a vertex in @xmath37    a _ proper set @xmath40 of walks _ in @xmath28 is a set @xmath41 of @xmath0 walks of total length @xmath42 , each with a distinct start vertex in @xmath39 and a distinct end vertex in @xmath37 a _ signature _ of a proper set @xmath40 of walks is the pair @xmath43 that is smallest in lexicographic order such that the two walks that start at @xmath44 and @xmath45 respectively intersect , and the first intersection vertex of these two walks is the first intersection vertex of the walk starting from @xmath44 with any walk in @xmath46    note that walks in @xmath40 are pairwise vertex disjoint iff the signature of @xmath40 is not defined .", "we define the transformation @xmath47 on @xmath40 as follows .", "if @xmath40 has the signature @xmath43 then @xmath47 switches the suffix of the walk starting at @xmath44 with that of the walk starting at @xmath45 at the first intersection vertex of these two walks .", "1 . otherwise , if the signature of @xmath40 is not defined then @xmath47 is an identity on @xmath46    [ fig : paths ]     of walks and the companion proper set @xmath48 of walks .", ", height=188 ]    observe that if the signature of @xmath40 is defined then @xmath48 has the same signature as @xmath40 and @xmath49 the first observation is immediate . to show the second one it is sufficient to note that @xmath50 holds iff @xmath47 transforms the two walks which yield the signature of @xmath40 onto themselves .", "the latter is however impossible since they have different start vertices and different end vertices .", "note also that the walks in @xmath48 have the same total length as those in @xmath46    it follows that @xmath47 is an involution on sets of proper walks of total length @xmath51 i.e. , @xmath52 holds for any proper set @xmath40 of walks of total length @xmath53 .    for the network @xmath28 and @xmath54,$ ] let @xmath55 be the family of all proper sets of @xmath0 walks of total length @xmath56 in @xmath57 assign a distinct variable @xmath58 to each edge @xmath59 in @xmath57 for a walk @xmath60 let @xmath61 be the monomial , where @xmath58 has multiplicity equal to the number of occurrences of @xmath59 in @xmath62 next , let @xmath63 denote the polynomial @xmath64    [ lem : zero ] for the network @xmath28 and @xmath54,$ ] there is a proper set of @xmath0 mutually vertex - disjoint walks of total length @xmath56 in @xmath28 iff @xmath63 is not identical to zero over a field of characteristic two .", "@xmath55 can be partitioned into the family @xmath65 of sets @xmath40 of walks such that @xmath50 and the family @xmath66 of sets @xmath40 of walks such that @xmath49 the polynomial @xmath67 is identical to zero over a field of characteristic two since for each @xmath68 the monomials @xmath69 and @xmath70 contain equal multiplicities of the same variables and @xmath52 so @xmath40 and @xmath48 can be paired . on the other hand , since each set @xmath40 of walks in @xmath65 consist of mutually vertex - disjoint walks , the monomials @xmath69 in the polynomial @xmath71 are in one - to one correspondence with @xmath40 and thus are unique provided that the walks in @xmath40 are simple paths .", "now , it is sufficient to observe that mutually vertex - disjoint walks can be always trivially pruned to corresponding mutually vertex - disjoint simple paths .    to warm up", ", we prove the following lemma on sequential evaluation of @xmath72    [ lem : eval ] @xmath63 can be evaluated for a given assignment of values over a field @xmath73 of characteristic two in @xmath74 time .", "for @xmath75 @xmath76,$ ] we consider the family @xmath77 of all sets @xmath40 consisting of @xmath78 walks connecting @xmath78 distinct sources in @xmath79 with the @xmath78 distinct sinks in @xmath80 so that the total length of the walks is exactly @xmath81 next , we define the polynomial @xmath82 as @xmath83 note that @xmath84 and @xmath85    on the other hand , for @xmath86,$ ] @xmath87 and @xmath88 we consider the set @xmath89 of walks of length @xmath90 in @xmath28 that start at @xmath91 and end at @xmath92 let @xmath93 be the polynomial @xmath94    we have the following recurrence for a nonempty subset @xmath80 of @xmath95 and @xmath96:$ ]    @xmath97}q_{p - q}(b\\setminus \\ { y\\})q_q(x_{|b|},y).\\ ] ]    next , we have also the following recurrence for @xmath98 @xmath88 and @xmath99:$ ]    @xmath100    we have also @xmath101 if @xmath102 and otherwise @xmath103 consequently , we can evaluate all the polynomials @xmath104 by the second recurrence in @xmath105 time .    now , by using the first recurrence and setting @xmath106 to @xmath21 in the field , we can evaluate all the polynomials @xmath82 in the increasing order of the cardinalities of @xmath80 in @xmath107 time .    by @xmath84 and @xmath108", "we conclude that @xmath63 can be evaluated in @xmath74 time .", "note that @xmath109 let @xmath110 be the sink in @xmath80 with the largest index .", "we assume @xmath111 and if @xmath112 then @xmath113 now , we can evaluate @xmath114 and @xmath115 by the following recurrences    @xmath116    @xmath117    in @xmath118 time .", "we can partially parallelize the sequential evaluation of @xmath63 in order to obtain the following lemma .", "[ lem : pareval ] @xmath63 can be evaluated for a given assignment of values over a field @xmath73 of characteristic two in @xmath119 time by a crew pram using @xmath120 processors .", "we generalize the definition of the set @xmath121 and the corresponding polynomial @xmath104 to include arbitrary start vertex @xmath122 , requiring @xmath123 as previously .", "then , we can evaluate @xmath124 for @xmath122 and @xmath88 for @xmath99 $ ] by the following standard doubling recurrence for @xmath125    @xmath126    at the bottom of the recursion , we have @xmath101 if @xmath102 otherwise @xmath103 it follows that all @xmath104 for @xmath122 , @xmath88 and @xmath127 $ ] can be evaluated in a bottom - up manner in @xmath128 time by a crew pram using @xmath129 processors .", "recall the first recurrence from the proof of lemma [ lem : eval ] .", "when the polynomials @xmath130 for @xmath131 and @xmath132 are evaluated , we can evaluate in turn the polynomials @xmath82 , where @xmath75 @xmath96 $ ] in @xmath0 phases in the increasing order of the cardinalities of @xmath80 by this recurrence .", "it can be done in @xmath133 time by a crew pram using @xmath134 processors .    by @xmath135 @xmath108 and @xmath136", "we conclude that @xmath63 can be evaluated in @xmath119 time by a crew pram using @xmath120 processors .", "the following lemma on polynomial identities verification has been shown independently by demillo and lipton , schwartz , and zippel .", "[ lem : zip]@xcite let @xmath137 be a nonzero polynomial of degree @xmath138 over a field of size @xmath139 then , for @xmath140 @xmath141 ... , @xmath142 chosen independently and uniformly at random from the field , the probability that @xmath143 is not equal to zero is at least @xmath144    note that the polynomial @xmath63 is of degree @xmath53 not larger than @xmath145 we can use lemma [ lem : zip ] with a field @xmath146 of characteristic two to obtain a randomized test of the polynomial @xmath63 for not being identical to zero with one side errors . for sufficiently large constant @xmath147", "the one side errors are of probability not larger than a constant smaller than @xmath148 by performing @xmath149 such independent tests , the probability of one side errors can be decreased to exponentially small in @xmath1 one .    by lemma [ lem : pareval ] , the series of the tests can be performed in @xmath150 time by a pram using @xmath151 processors . by lemma [ lem : zero ] , these tests verify if there is a proper set of mutually vertex - disjoint walks of total length @xmath56 in the network @xmath28 .", "the latter in turn is equivalent to the existence of @xmath0 mutually vertex - disjoint paths of total length @xmath56 connecting @xmath39 with @xmath95 in @xmath28 by the definition of a proper set of walks in @xmath57 hence , observing that each walk can be trivially pruned to a simple directed path with the same endpoints , we obtain our main result .", "[ theo : pdec ] the problem of whether or not there is a set of @xmath0 mutually vertex - disjoint simple directed paths of total length @xmath56 connecting @xmath39 with @xmath95 in the network @xmath28 can be decided by a randomized crew pram , with one - sided errors of exponentially small probability in @xmath2 running in @xmath150 time and using @xmath151 processors ."], ["in this section , we shall consider a more general situation where there are a positive integer @xmath152 and a cost function @xmath153 assigning to each of the @xmath154 edges @xmath59 in the network @xmath28 a cost @xmath155.$ ] the cost of a walk or a path is simply the sum of the costs of the edges forming it ( the cost of an edge is counted the number of times it appears on the walk or path ) .", "we would like to detect a proper set of @xmath0 walks in @xmath28 that achieves the minimum cost .", "for this reason , we consider the following generalization of the polynomial @xmath156 for @xmath157,$ ] let @xmath158 be the set of all proper sets of walks in the edge - costed network @xmath28 that have total cost not greater than @xmath159 next , for a walk @xmath41 in @xmath160 as previously , let @xmath61 be the monomial which is the product of @xmath58 over the occurrences of edges @xmath59 on @xmath62 the polynomial @xmath161 is defined by @xmath162    by using the proof method of lemma [ lem : zero ] , we obtain the following counterpart of this lemma for @xmath161 .    [ lem : czero ] for the edge - costed network @xmath28 , there is a proper set of @xmath0 mutually vertex - disjoint walks of total cost @xmath163 in @xmath28 iff @xmath161 is not identical to zero over a field of characteristic two .", "next , we obtain the following counterpart of lemma [ lem : pareval ] for @xmath161 .", "[ lem : cpareval ] @xmath161 can be evaluated for a given assignment @xmath8 of values over a field @xmath73 of characteristic two in @xmath164 time by a pram using @xmath165 processors .", "the proof reduces to that of lemma [ lem : pareval ] .", "we replace each directed edge @xmath59 of cost @xmath155 $ ] in the network @xmath28 by a directed path of length @xmath166 introducing @xmath167 additional vertices . with each edge on such a path , we associate a variable .", "we assign @xmath168 to the variable associated with the first edge on the path replacing @xmath169 and just @xmath21 of the field to the variables associated with the remaining edges on the path .", "the resulting network @xmath170 is of size @xmath171 let @xmath172 be the family of all proper sets of @xmath0 walks of total cost @xmath163 in the network @xmath170 .", "we can evaluate the polynomial @xmath173 in parallel analogously as @xmath63 in the proof of lemma [ lem : pareval ] .", "it remains to observe that the value of @xmath161 under the assignment @xmath8 is equal to that of @xmath174 under the aforementioned assignment .", "now , we are ready to derive our main result in this section .", "the minimum cost of a set of @xmath0 mutually vertex - disjoint simple directed paths connecting @xmath39 with @xmath95 in the network @xmath28 with edge costs in @xmath175 $ ] can be computed by a randomized crew pram , with errors of exponentially small probability in @xmath2 running in @xmath164 time and using @xmath176 processors .", "the minimum cost of the sought set of vertex - disjoint paths is in @xmath177.$ ] hence , by lemma [ lem : czero ] , it is sufficient to test the polynomials @xmath161 for non - identity with zero for all @xmath178 $ ] in parallel . by applying lemmata [ lem : zip ] and [ lem : cpareval ] in a manner analogous to the proof of theorem [ theo : pdec ] , we conclude that it can be done by a randomized crew pram , with one - sided errors of exponentially small in @xmath1 probability , running in @xmath164 time and using + @xmath179 processors ."], ["a straightforward approach of extending our randomized parallel method for deciding if there is a proper set of @xmath0 mutually vertex - disjoint walks ( of a bounded total cost ) between two sets of vertices of cardinality @xmath0 to include the finding variant could be roughly as follows . in parallel , for each @xmath0-tuple of respective neighbors of the @xmath0 start vertices in @xmath33 replace the set of start vertices by the @xmath0-tuple and apply our method recursively to the resulting network", ". if the test is positive , the first edges on the walks are known , and we can iterate the method .", "the problem with this approach is that its recursive depth is proportional to the maximum length of a walk in the resulting set of mutually vertex - disjoint walks between @xmath39 and @xmath37    also , it is not clear how one could implement a straightforward divide - and - conquer approach of guessing intermediate vertices in order to find a set of @xmath0 mutually - vertex disjoint walks of a given cost efficiently in parallel .", "we need more advanced methods to obtain a very fast parallelization of the finding variant .", "we shall modify the edge cost in the network @xmath28 in order to use the so called _ isolation lemma _ in a manner analogous to the rnc method of finding a perfect matching given in @xcite .", "( the isolation lemma @xcite ) .", "let @xmath180 be a family of subsets of a set with @xmath23 elements and let @xmath181 be a non - negative integer .", "suppose that each element @xmath19 of the set is independently assigned a weight @xmath182 uniformly at random from @xmath183 $ ] , and the weight of a subset @xmath40 in @xmath180 is defined as @xmath184 then , the probability that there is a unique set in @xmath180 of minimum weight is at least @xmath185    [ cor : ciso ] for each of the @xmath154 edges @xmath59 in the network @xmath160 modify its cost @xmath166 to @xmath186 where the weight @xmath187 is drawn uniformly at random from @xmath183 $ ] .", "then , the probability that there is a unique minimum - cost set of mutually vertex - disjoint paths connecting @xmath39 with @xmath95 in the edge weighted network @xmath28 is at least @xmath188    to use the isolation lemma , let the underlying set to consist of all edges in the network @xmath57 next , note that a set of mutually vertex - disjoint paths connecting @xmath39 with @xmath95 achieving a minimum cost consists of simple paths and thus it can be identified with the set of edges on the paths .", "let @xmath7 be the family of all sets of mutually vertex - disjoint simple paths connecting @xmath39 with @xmath95 in the network @xmath57 by the setting of new costs @xmath189 solely those sets in @xmath7 that achieved the minimum cost , say @xmath190 under the original costs @xmath166 can achieve a minimum cost under the new costs @xmath191", "so , we can set @xmath180 to the aforementioned sub - family of @xmath7 , and define the weight of a set of @xmath0 paths in @xmath180 as the sum of the weights @xmath187 of the edges @xmath59 on the paths in this set in order to use the isolation lemma . by the isolation lemma", ", there is a unique set @xmath40 in @xmath180 that achieves the minimum weight @xmath192 with the probability at least @xmath188 the corollary follows since each set @xmath40 in @xmath180 has the cost @xmath193 equal to @xmath194    throughout the rest of this section , we shall assume that each of the @xmath154 edges @xmath59 in the network @xmath28 is assigned the cost @xmath195 as in corollary [ cor : ciso ] and that @xmath196.$ ]    suppose that we know the minimum cost of a set of @xmath0 mutually vertex - disjoint paths connecting @xmath39 with @xmath95 in the network @xmath28 with the edge costs indicated , and such a minimum - cost set is unique .", "then , it is sufficient to show that we can test quickly in parallel if the network @xmath28 with an arbitrary edge removed still contains a set of @xmath0 mutually vertex - disjoint paths connecting @xmath39 with @xmath95 that achieves the minimum cost . by performing the test for each edge of @xmath28 in parallel , we can determine the set of edges forming the unique minimum - cost set of @xmath0 mutually vertex - disjoint paths connecting @xmath39 with @xmath95 .    to carry out these tests ,", "we need to generalize the polynomial @xmath161 to a polynomial @xmath197 , where @xmath59 is an edge in @xmath28 and @xmath198 is a cost constraint from @xmath199=[cn^{o(1)}]$ ] .", "let @xmath200 be the family of all proper sets of @xmath0 walks in the network @xmath28 with the edge @xmath59 removed that have total at most @xmath159 ( in the total cost of a set of walks , we count the cost of an edge the number of times equal to the sum of the multiplicities of the edge in the walks . )    as in the definition of @xmath63 assign a distinct variable @xmath58 to each edge @xmath59 in @xmath160 and for a walk @xmath201 let @xmath61 be the monomial , where @xmath58 has multiplicity equal to the number of occurrences of @xmath59 in @xmath62 the polynomial @xmath197 is defined by @xmath202    by using the proof method of lemma [ lem : zero ] , we obtain the following counterpart of this lemma for @xmath197 .    [ lem : gzero ] for the edge - costed network @xmath28 with @xmath154 edges , edge @xmath169 and @xmath203,$ ] there is a proper set of @xmath0 mutually vertex - disjoint walks of total cost @xmath163 in the network @xmath28 with the edge @xmath59 removed iff @xmath197 is not identical to zero over a field of characteristic two .", "next , we obtain the counterpart of lemma [ lem : pareval ] for @xmath197 following the proof of lemma [ lem : cpareval ] .", "[ lem : gcpareval ] @xmath197 can be evaluated for a given assignment of values over a field @xmath73 of characteristic two in @xmath164 time by a pram using @xmath204 processors .", "now , we are ready to derive our main result in this section .", "[ theo : cpfind ] there is a randomized pram returning almost certainly ( i.e. , with probability at least @xmath205 , where @xmath206 ) a minimum - cost set of @xmath0 mutually vertex - disjoint paths connecting @xmath39 with @xmath95 in the network @xmath28 with the original edge costs in @xmath175 $ ] ( iff such a set exists ) in @xmath164 time using @xmath204 processors .", "we set @xmath181 to , say , @xmath207 and specify the new edge costs @xmath195 in the network @xmath28 drawing the weights @xmath187 uniformly at random from @xmath183 $ ] as in corollary [ cor : ciso ] .", "next , for each @xmath208=[cn^{o(1)}],$ ] we proceed in parallel as follows . for each edge @xmath59 of the network @xmath28", ", we test the polynomial @xmath197 for the non - identity with zero by using lemma [ lem : zip ] and lemma [ lem : gcpareval ] ( we can perform a linear in @xmath1 number of such tests in parallel in order to decrease the probability of the one - sided error to an exponentially small one ) .", "next , we verify if the edges that passed the test positively yield a set of @xmath0 mutually vertex - disjoint paths connecting @xmath39 with @xmath95 .", "for example , it can be done by checking for each endpoint of the edges outside @xmath209 if it is shared by exactly two of the edges , and then computing and examining the transitive closure of the graph induced by the edges ( see @xcite ) .", "if so , we save the resulting set of paths of total ( new ) cost @xmath210 by corollary [ cor : ciso ] , there is a @xmath211 $ ] for which the above procedure will find such a set of paths that achieves the minimum ( original ) cost with probability at least @xmath212"], ["the following lemma is a straightforward generalization of a folklore reduction of maximum integral flow to a corresponding disjoint connecting path problem ( for instance cf .", "@xcite ) to include minimum - cost integral flow . we shall call a flow proper , if it ships each flow unit along a simple path from the source to the sink .", "[ lem : red ] the problem of whether or not there is a proper integral flow of value @xmath0 and cost @xmath213 from a distinguished source vertex @xmath19 to a distinguished sink vertex @xmath20 in a directed network with @xmath1 vertices .", "integral edge capacities and edge costs in @xmath175 $ ] can be ( many - one ) reduced to that of whether or not there is set of @xmath0 mutually vertex - disjoint simple directed paths of total cost @xmath214 , where @xmath215 connecting two distinguished sets of @xmath0 vertices in a directed network on @xmath216 vertices in @xmath217 time by a crew pram using @xmath218 processors .", "let @xmath219 be the directed network with integral edge capacities , edges costs in @xmath175 $ ] and the distinguished source vertex @xmath19 and sink vertex @xmath220 since we are interested in a flow of value @xmath0 , we can assume w.l.o.g that all edge capacities do not exceed @xmath221    we form a directed network @xmath222 on the basis of the network @xmath223 as follows .", "let @xmath224 next , let @xmath225 be the set of edges in @xmath223 incoming into @xmath226 , and let @xmath227 be the set of edges in @xmath223 leaving @xmath228 for each @xmath229 and @xmath230,$ ] we create the vertex @xmath231 analogously , for each @xmath232 and @xmath233,$ ] we create the vertex @xmath234 furthermore , we direct an edge from each vertex @xmath235 to each vertex @xmath236 . to each such an edge , we assign the cost @xmath148 also , for each edge @xmath237 of @xmath238 we direct an edge from @xmath239 to @xmath240 for @xmath230.$ ] to each such an edge , we assign the cost @xmath241 see fig .", "[ fig : paths1 ]     and the corresponding part of the network @xmath242,height=188 ]    let @xmath243 be the set of vertices of the form @xmath244 , and let @xmath245 denote the set of vertices of the form @xmath246 create an additional set @xmath39 of @xmath0 vertices and from each vertex in @xmath39 direct an edge to each vertex in @xmath243 .", "symmetrically , create another additional set @xmath95 of @xmath0 vertices and from each vertex in @xmath245 direct an edge to each vertex in @xmath37    it is easy to observe that there is a proper integral flow of value @xmath0 and cost @xmath213 from @xmath19 to @xmath20 in the network @xmath223 iff there is a set of @xmath0 mutually vertex - disjoint simple paths of total cost @xmath214 connecting @xmath39 with @xmath95 in the network @xmath247 such that @xmath248    now it is sufficient to note that the construction of @xmath222 , @xmath39 and @xmath95 on the basis of @xmath223 easily implemented by a crew pram in @xmath217-time using @xmath218 processors , where @xmath1 is the number of vertices in @xmath249    by combining theorem [ theo : pdec ] with lemma [ lem : red ] , we obtain our first main result .", "the minimum cost of a flow of value @xmath0 in a network with @xmath1 vertices , a sink and a source , integral edge capacities and positive integral edge costs in @xmath175 $ ] can be found by a randomized pram , with errors of exponentially small probability in @xmath2 running in @xmath250 time and using @xmath204 processors .    by combining in turn theorem [ theo : cpfind ] with the finding variant of lemma [ lem : red ] using exactly the same reduction", ", we obtain our second main result .", "[ theo : ffind ] there is a randomized pram algorithm returning almost certainly a minimum - cost flow of value @xmath0 ( iff a flow of value @xmath0 exists ) in a network with @xmath1 vertices , a sink and a source , integral edge capacities and edge costs in @xmath251,$ ] in @xmath3 time using @xmath4 processors .", "the problem of finding a minimum - cost flow of value @xmath252 in a network with @xmath1 vertices , a sink and a source , and integral edge capacities bounded polynomially in @xmath1 admits an @xmath6 algorithm ."], ["we have resented a new approach to the minimum - cost integral flow problem . in particular , it yields an @xmath6 algorithm when the flow supply is ( at most ) logarithmic in the size of the network .", "i. koutis .", "faster algebraic algorithms for path and packing problems .", "35th annual international colloquium on automata , languages and programming ( icalp ) , lecture notes in computer science 5555 , pp .", "653 - 664 , 2009 .", "orlin and c. stein .", "parallel algorithms for the assignment and minimum - cost flow problems .", "operations research letters , * 14 * , pp .", "181 - 186 , 1993 .", "reif ( editor ) .", "synthesis of parallel algorithms .", "morgan - kauffman , 1993 ."]]}
{"article_id": "1510.04083", "article_text": ["following edward snowden s revelations , privacy and anonymity technologies have been increasingly often in the news , with a growing number of users becoming aware  loosely speaking  of privacy and encryption notions  @xcite .", "service providers have rolled out , or announced they will , more privacy - enhancing tools , e.g. , support for end - to - end encryption  @xcite and https by default  @xcite . at the same time , a number of smartphone apps and mobile social networks have entered the market , promising to offer features like anonymity , ephemerality , and/or end - to - end encryption ( e2ee ) .", "while it is not that uncommon to stumble upon claims like `` military - grade encryption '' or `` nsa - proof ''  @xcite in the description of these apps , little work thus far has actually analyzed the guarantees they provide .", "this motivates the need for a systematic study of a careful selection of such apps . to this end , we compile a list of 18 apps that offer e2ee , anonymity and/or ephemerality , focusing on 8 popular ones ( confide , frankly chat , secret , snapchat , telegram , whisper , wickr , and yik yak ) .", "we review their functionalities and perform an empirical evaluation , based on _", "static _ and _ dynamic _ analysis , aimed to compare the claims of the selected apps against results of our analysis .", "highlights of our findings include that `` anonymous '' social network apps whisper and yik yak actually identify users with distinct user ids that are persistent .", "users previous activities are restored to their device after they uninstall and reinstall them , and information collected by these apps could be used to de - anonymize users .", "we also find that the ephemeral - messaging app snapchat does not always delete messages from its servers  in fact , previous `` expired '' chat messages are surprisingly included in packets sent to the clients .", "then , we report that all actions performed by a user on frankly chat can be observed from the request url , which is actually transmitted in the clear .", "we start by building a list of smartphone apps that are categorized as `` anonymous '' on product hunt  @xcite , and those popular among friends and colleagues .", "we then look at their descriptions and at _ similar apps _ on the google play , and focus on those described as offering end - to - end encryption , anonymity and/or ephemerality , as defined below :    * * _ anonymity : _ * is defined as the property that a subject is not identifiable within a set of subjects , known as the anonymity set  @xcite , e.g. , as provided by tor  @xcite for anonymous communications . in the context of this paper , the term anonymity will be used to denote that users are anonymous w.r.t .", "other users of the service or w.r.t . the app service provider . * * _", "end - to - end encryption ( e2ee ) : _ * data exchanged between two communicating parties is encrypted in a way that only the sender and the intended recipient can decrypt it , so , e.g. , eavesdroppers and service providers can not read or modify messages . * * _ ephemerality : _ * in cryptography , it denotes the property that encryption keys change with every message or after a certain period . instead , here", "ephemerality is used to indicate that messages are not available to recipients from the user interface after a period of time  @xcite .", "for instance , in apps like snapchat , messages `` disappear '' from the app ( but may still be stored at the server ) a few seconds after they are read .", "* first list . *", "our first list contains 18 apps , listed in table  [ table : first ] , where we also report their first release date , number of downloads as reported by google play store , the kind(s ) of content that can be shared via the apps ( e.g. , text , videos , files ) , and whether the apps create persistent social links .", "note that our first selection does not include popular apps like whatsapp , since it attempts , but does not guarantee , to provide e2ee for all users  @xcite .      among the 18 apps presented in table  [", "table : first ] , we then select a few popular ones , as discussed below", ".    * selection criteria . * from our corpus", ", we focus on apps with the most downloads that offer ephemerality , anonymity , e2ee , or , preferably , a combination of them .", "we exclude silent circle and tigertext as they require , respectively , paid subscription and a registered company email .", "we reduce our selection to 8 apps : confide , frankly chat , secret , snapchat , telegram , whisper , wickr , and yik yak ( bold entries in table  [ table : first ] ) .", "next , we provide an overview of their advertised functionalities , complementing information in the table  [ table : first ] .", "( note that descriptions below are taken either from the google play store or the apps respective websites . )", "* _ confide : _ * offers end - to - end encryption and ephemerality .", "it allows users to share text , photos , and documents from their device and integrates with dropbox and google drive .", "it provides read receipts and notification of screenshot capture attempts .", "messages are not displayed on the app until the recipient `` wands '' over them with a finger , so that only a limited portion of the message is revealed at a time .", "after a portion of the message is read , it is grayed out screenshots are also disabled on android .", "messages that have not been read are kept on the server for a maximum of 30 days .", "* _ frankly chat : _ * is a chat app allowing users to send ephemeral messages ( text , picture , video or audio ) , anonymous group chats , and un - send messages that the recipient has not opened .", "messages disappear after 10 seconds but users can `` pin '' their chats disabling ephemerality .", "both parties do not need to have the app installed to receive messages : a link is sent to the recipient via email , when clicked , reveals the message .", "messages are deleted from the server after 24 hours  whether they are read or not .", "* _ secret : _ * _ ( discontinued may 2015 ) _ lets users post anonymously to other _", "nearby _ users .", "users can view _ secrets _ from other locations but can only comment on those from their nearby location .", "users can chat privately with friends and engage in a group chat with the chat history disappearing after a period of inactivity .", "* _ snapchat : _ * is an app that allows users send text , photos and videos that are displayed for 1 to 10 seconds ( as set by the user ) before they `` disappear '' , i.e. , they are no longer available to their friends .", "if the recipient takes a screenshot , the sender is notified .", "users can also view _ stories _", ", i.e. , a collection of snaps around the same theme , and a so - called  _ discover _ , i.e. , accessing snaps from different selected editorials .    * _ telegram : _ * is a messaging app that lets users exchange text , photos , videos , and files .", "it also provides users with an option to engage in a `` secret chat '' , which provides e2ee and optional ephemerality .", "senders are notified if the recipient takes a screenshot .", "account information , along with all messages , media , contacts stored at telegram servers are deleted after 6 months of login inactivity .", "* _ whisper : _ * is a location - based mobile social network that allows users to anonymously share texts displayed atop images , which are either selected by the users or suggested by the app .", "users can view and respond to _ whispers _ either as a private message or via another _ whisper_.    * _ wickr : _ * is a chat app supporting text , audio , video , photos , and files , with user - defined ephemerality ( maximum 6 days ) .", "it also allows users to engage in group chats , shred deleted files securely , and prevents screenshots on android and claims to anonymize users by removing metadata ( such as persistent identifiers or geo - location ) from their contents .    *", "_ yik yak : _ * is a local bulletin - board social network allowing nearby users to post _ yaks _ anonymously .", "users clustered within a 10-mile radius are considered local and can post , view , reply to , and up / down vote yaks but can only view _ yaks _ outside their locality", "we now present the results of a static analysis of the 8 apps , aiming to analyze ssl / tls implementations and look for potential information leakage .", "we perform static analysis using dex2jar  @xcite , decompiling the .apk files to .jar files , from which we extract the related java classes using jd - gui  @xcite .", "we then search for ssl / tls keywords like ` trustmanager `  @xcite , ` hostnameverifier `  @xcite , ` sslsocketfactory `  @xcite , and ` httpsurlconnection ` @xcite .", "then , we inspect the ` trustmanager ` and ` hostnameverifier ` interfaces used to accept or reject a server s credentials : the former manages the certificates of all certificate authorities ( cas ) used in assessing a certificate s validity , while the latter performs hostname verification whenever a url s hostname does not match the hostname in the certificate .      several sockets are usually created to transport data to different hostnames in an app , therefore , sockets in an app may have different ssl implementations .", "we observe different ssl implementations in the 8 apps , and summarize our findings below .", "* non - customized ssl implementation .", "* app developers can choose to use any one of five defined ` hostnameverifier ` subclass for hostname verification , and use ` trustmanager ` initialized with a keystore of ca certificates trusted by the android os to determine whether a certificate is valid or not , or customize certificate validation by defining their own logic for accepting or rejecting a certificate .", "all the 8 apps in our corpus contain some non - customized ssl implementations .", "telegram and yik yak only use non - customized ssl code , with the former relying on ` browsercompathostnameverifier ` class and building sockets from default ` sslsocketfactory ` . confide and snapchat both use the ` browsercompathostnameverifier ` class , while wickr has instances of all ` hostnameverifier ` subclasses but uses the ` browsercompathostnameverifier ` class on most of its sockets .", "snapchat does not customize its ` trustmanager ` either and registers a scheme from the default ` socketfactory ` .", "secret uses sockets from the default ` sslsocketfactory ` but employs regex pattern matching for hostname verification", ".    * vulnerable trustmanager / hostnameverifier . * frankly chat , whisper , and wickr all contain ` trustmanager ` and ` hostnameverifier ` that accept all certificates or hostnames .", "alas , this makes it possible for an adversary to perform _ man - in - the - middle ( mitm ) _ attacks and retrieve information sent on the sockets that use the vulnerable ` trustmanager ` and/or ` hostnameverifier ` .", "vulnerable ` hostnameverifier ` in frankly chat returns ` true ` without performing any check , while wickr uses the ` allowallhostnameverifier ` subclass which is also used in whisper by bugsense crash reporter .", "* certificate pinning . *", "confide , frankly chat , and whisper implement certificate pinning .", "confide pins the expected ca certificate which is also accessible from the decompiled apk , whereas , whisper uses the hash of the pinned certificate appended with the domain name to make certificate validation decisions . for frankly chat", ", a single certificate is expected and its hash is checked for in the received certificate chain .", "frankly chat also initialize another ` trustmanager ` with a keystore that loads certificate from file .", "0.47       0.47", "next , we present the results of our dynamic analysis aimed to scrutinize features `` promised '' by the 8 apps in our corpus , as well as to confirm whether the vulnerabilities found statically are also observed dynamically .", "we conduct our experiments on a lg nexus 4 running android 5.1 , connected to a wi - fi access point under our control .", "( note that the wi - fi network was secured using wpa2 to prevent unauthorized connections and ensure that only intended traffic was captured . )", "our intention is to examine what a random attacker can access from an app advertised as privacy - enhancing , and what can be deduced as regards privacy - enhancing claims .", "hence , we assume an adversary that can not elevate her privilege nor have access to a rooted device .", "we perform actions including sign - up , login , profile editing , sending / reading messages , while monitoring traffic transmitted and received by the apps .", "we collect traffic using wireshark and analyze unencrypted traffic to check for sensitive information transmitted in the clear .", "we also rely on http proxies such as fiddler  @xcite and sslsplit  @xcite to mount man - in - the - middle ( mitm ) attacks and decrypt https traffic .", "proxying is supported in two ways :    1 .   _", "regular proxy : _ we install the fiddler http proxy  @xcite on a windows 8.1 laptop ( which also acts as wi - fi access point ) , listening on port 8888 , and manually configure the smartphone to connect to the proxy .", "figure  [ fig : fiddler ] illustrates our proxy setup using fiddler .", "we also install fiddler s ca certificate on the smartphone and laptop to allow https traffic decryption .", "transparent proxy : _ some android apps are programmed to ignore proxy settings , so fiddler does not accept / forward their packets .", "this happens with telegram , wickr ( non - css / js ) , and frankly chat ( chat packets ) .", "therefore , we set up a transparent proxy as shown in figure  [ fig : mitmproxy ] using sslsplit mitm proxy  @xcite set to listen on port 9014 on a linux desktop running fedora 22 , which also acts as a wi - fi access point .", "we use _ iptables _ to redirect to port 9014 all traffic to ports 80 , 443 , and 5228 ( gcm ) . as sslsplit", "uses a ca certificate to generate leaf certificates for the https servers each app connects to , we generate and install a ca certificate on the smartphone , and pass it to sslsplit running on the linux machine .", "we now present the results of our dynamic analysis , which are also summarized in table  [ table : proxy ] .    * no proxy . *", "we start by simply analyzing traffic captured by wireshark and observe that secret and frankly chat send sensitive information in the clear .", "specifically , in frankly chat , the android advertising i d ( a unique identifier ) is transmitted in the clear , via an http get request , along with device name .", "the list of actions a user performs on frankly chat can also be observed from the request url .", "secret instead leaks google maps location requests ( and responses ) via http get .", "* regular proxy . * using fiddler as a mitm proxy , we notice that confide and whisper do not complete connection with their servers due to certificate pinning .", "note that whisper started implementing pinning after an update on april 22 , 2015 .", "prior to that , one could capture whisper traffic via fiddler and access location and user i d .", "we also notice that frankly chat hashes passwords using md5 without salt , while snapchat sends usernames and passwords without hashing .", "although inconsistently , snapchat also sends previous `` expired '' chat messages to the other party even though these are not displayed on the ui .", "decrypted traffic from secret , whisper , and yik yak show that these apps associate unique user ids to each user , respectively , _ clientid _ , _ wuid _ , and _ user id_. we test the persistence of these ids and find that , even if the apps cache on the device is cleared through the android interface , and the apps uninstalled and reinstalled , whisper and yik yak retain the user i d from the uninstalled account and restore all previous _ whispers _ and _ yaks _ from the account . on whisper", ", we manually delete its wuid and state files ( in the /sdcard / whisper directory ) before reinstalling the app : this successfully clears all previous _ whispers _ and a new wuid file is generated .", "however , it does not completely de - associate the device from the `` old '' account as the `` new '' account would still get notifications of private messages from conversations started by the `` old '' account . on the contrary , clearing secret s cache unlinks previous messages , even without uninstalling the app .", "telegram and wickr ignore the proxy settings , i.e. , traffic does not pass through our proxy .", "frankly chat also ignore the proxy when sending chat messages but not for traffic generated by other actions .", "* transparent proxy . * using sslsplit , we decrypt ssl - encrypted traffic from wickr and telegram .", "we do not find any sensitive information or chat messages being transmitted as the traffic is indeed encrypted .", "apps for which ssl - encrypted traffic is recovered using fiddler exhibit the same behavior on the transparent proxy , with confide and whisper not connecting due to pinning .", "we observe that certificate pinning is implemented on the socket used to transmit chat messages on frankly chat , as we can not send chat messages but perform other actions , e.g. , editing profiles and adding new friends .", "we also uninstall the ca certificate from the device to observe whether non - trusted certificate are accepted , and find that none of the apps established an https connection , which implies the apps do not use trustmangers accepting any certificate as valid as reported in  @xcite .", "we now discuss the implications of our analysis , in light of the properties promised by the 8 studied apps .", "* anonymity w.r.t", ". other users . * posts on secret and yik yak ( resp . , _ secrets _ and _ yaks _ ) are not displayed along with any identifier , thus making users anonymous w.r.t . other users .", "whereas , on whisper , due to the presence of a _ display name _ ( non - unique identifier shown to other users ) and its `` nearby '' function , communities can be formed as a result of viewing and responding to _ whispers _ from nearby locations . thus , it may be possible to link _ whispers _ to a display name , while at the same time querying the distance to the target , as highlighted in  @xcite .", "a user who thinks is anonymous is more likely to share sensitive content she might not share on non - anonymous osn platforms , which makes `` anonymous '' apps potential targets of scammers / blackmailers that can identify users .", "this motivates us to examine the possibility of creating social links between users , i.e. , linking a user and a set of actions .", "we find that this is not possible on yik yak as there are no one - to - one conversations .", "also , when the yik yak stream is monitored by a non - participating user , user ids observed are symbolic to the real unique user i d .", "the symbolic user i d is only associated to one _ yak _ , hence one can not use it to link a user as the i d differs across _ yaks _ by the same user .", "frankly chat optionally offers @xmath0-anonymity during a group chat with @xmath0 + 1 friends . due to the social link already present in the group", "( users chat with friends ) , psychological differences make it possible to identify who says what .", "* anonymity w.r.t . service provider . * all apps associate identifiers to its users , which allows them to link each user across multiple sessions .", "wickr claims to strip any metadata that could allow them to identify their users , thereby making users anonymous and impossible to track @xcite , but we can not verify this claim since all traffic is encrypted end - to - end .", "we observe different levels of persistence of user ids in secret , whisper , and yik yak , as mentioned earlier .", "secret stores identifiers on users device , so an identifier would cease to persist beyond data and cache clearance . whereas , for whisper and yik yak , we have two hypotheses as to why user ids survive when the app is uninstalled and later reinstalled : either they store identifiers on their servers and restore them to a device on re - installation , or they create the user ids from the same device information using a deterministic function .", "this observation indicates that whisper and yik yak s user ids are linked to device information , thus making users persistently linkable .", "while whisper and yik yak do reveal the information they collect from users in their privacy policy , previous work shows that the overwhelming majority of users do not read ( or anyway understand ) privacy policies  @xcite .", "both apps collect information including device i d , ip address , geo - location , which can be used to track users .", "this , along with profiles from analytics providers ( which both apps embed ) , can be used to de - anonymize users age , gender , and other traits with a high degree of accuracy @xcite .", "finally , note that whisper s description on google play , while including terms like ` anonymous profiles ' and ` anonymous social network ' , is actually ambiguous as to whether they refer to anonymity w.r.t to whisper or other users ( or both )", ".    * location restriction . *", "secret and yik yak s restriction on feeds a user can see ( and interact with ) can simply be defeated , e.g. , as android lets users to use mock locations in developer mode . in combination with an app that feeds gps locations chosen by the user ( e.g. , _ fake gps _ ) , this allow them to access geo - tagged messages from anywhere .    * ephemerality . *", "confide , frankly chat , snapchat , telegram , and wickr offer message ephemerality with varying time intervals .", "confide claims messages disappear after it is read once  @xcite but this is not the case as messages only `` disappear '' after a user navigates away .", "this implies the recipient can keep the message for longer as long as they do not navigate away from the opened message . in frankly chat ,", "messages `` disappear '' after 10 seconds ( even though users can pin messages ) .", "ephemerality on telegram only applies to `` secret chats '' and the expiration time is defined by the user .", "snapchat and wickr also let users determine how long their message last , with snapchat defining a range of 110s ( default 3s ) .", "on snapchat , previous chat messages are actually part of the response received from the server , even though they are not displayed on the client s ui .", "this indicates that read messages are actually not deleted from snapchat servers immediately , despite what is stated in snapchat s privacy policy @xcite . since confide and frankly chat implement certificate pinning , we can not examine if responses from the server during chat contain past messages .", "also , telegram and wickr encrypt data before transmission , thus we can not make any analysis from intercepted packets .", "of all the apps offering ephemerality , only confide and wickr instruct the android os to prevent screen capture from a recipient .", "obviously , however , the recipient can still take a photo with another camera , and video recording would defeat confide s wand - based approach .", "confide can claim to offer plausible deniability if a photo is taken , as messages are not displayed along with the name of the sender , hence , pictures would not preserve the link between the message and the identity of the sender .", "frankly chat , snapchat , and telegram only notify the sender that the recipient has taken a screenshot , thus ephemerality claims are only valid assuming the recipient is not willing to violate a social contract between them and the sender . also", ", if messages are not completely wiped from the server , the provider is obviously still subject to subpoena and/or vulnerable to hacking .", "* end - to - end encryption . *", "confide and wickr claim to employ e2ee by default , using aes-128 and aes-256 , respectively .", "we can confirm e2ee in wickr but not in confide , since certificate pinning prevents interception of traffic .", "also , telegram offers e2ee for `` secret chat '' using aes-256 and client - server encryption ( i.e. only the server and both clients can decrypt traffic ) which also prevents mitm attacks for non - secret chats . in both secret and non - secret", "chat , telegram uses a proprietary protocol , mtproto , and transmit traffic over ssl although its webpage states otherwise .", "telegram and wickr s implementations also claim to support perfect forward secrecy @xcite . finally , note that recent criticism of telegram s security in the press do not affect the claims of telegram that we choose to analyze , i.e. , e2ee and ephemerality in `` secret chats . ''", "this section reviews related work , specifically , ( i ) measurement studies of chat apps and location - based social networks , ( ii ) apps vulnerabilities , and ( iii ) investigations of users behavior .    * measurement - based studies . * wang et al .", "@xcite analyze user interaction in whisper , motivated by the absence of persistent social links , content moderation , and user engagement .", "they also highlight a vulnerability that allows an attacker to detect a user s location by attaching a script to a _ whisper _ querying whisper s db .", "correa et al .", "@xcite define the concept of _ anonymity sensitivity _ for social media posts and measure it across non - anonymous ( e.g. , twitter ) and anonymous ( e.g. , whisper ) services , aiming to study linguistic differences between anonymous and non - anonymous social media sites as well as to analyze content posted on anonymous social media and the extent user demographics affect perception and measurements of sensitivity .", "peddinti et al .", "@xcite analyze users anonymity choices during their activity on quora , identifying categories of questions for which users are more likely to seek anonymity .", "they also perform an analysis of twitter to study the prevalence and behavior of so - called `` anonymous '' and `` identifiable '' users , as classified by amazon mechanical turk workers , and find a correlation between content sensitivity and a user s choice to be anonymous .", "stuzman et al .", "@xcite observe a significant growth in anonymity - seeking behavior on online social media in 2013 , while roesner et al .", "@xcite analyze why people use snapchat : they survey 127 adults and find that privacy is not the major driver of adoption , but the _ `` fun '' _ of self - destructing messages .", "* flaws . *", "prior work has also looked at related apps security flaws : in late 2013 , researchers from gibson security discovered a flaw in snapchat s api that allows an adversary to reconstruct snapchat s user base ( including names , aliases , phone numbers ) within one day and mass creation of bogus accounts  @xcite .", "zimmerman  @xcite highlights the issue of linkability of anonymous identifiers in wickr .", "recently , unger et al .", "@xcite systematize security and usability of chat and call apps providing end - to - end encryption . also , prior work  @xcite has studied libraries , interfaces , classes , and methods used by apps to make security decisions , specifically , w.r.t .", "vulnerabilities in sockets used to transmit user data .", "* user behavior .", "* pielot and oliver  @xcite study the motivations behind the use of snapchat by teenagers .", "they create two personas and , by engaging with other users , they find that teens use snapchat as they are excited by the ephemerality , see fewer risks , and non - commitment to persistent messengers .", "roesner et al .", "@xcite analyze why people use snapchat : they survey 127 adults and find that security and privacy are not the major drivers of adoption , but rather the _", "`` fun '' _ of self - destructing messages .", "hosseinmardi et al .", "@xcite look at cyberbullying on a semi - anonymous network , i.e. , last.fm , while stuzman et al .", "@xcite observe a significant growth in anonymity - seeking behavior on online social media in 2013 .", "shein  @xcite interview a few experts and commented on the rise of apps for `` ephemeral data '' ( e.g. , snapchat , gryphn , wickr ) , pointing out that users do not use ephemeral messaging because they have something to hide , rather , because they do not want to add digital artifacts to their digital `` detritus . ''    * privacy perceptions . *", "liu et al .", "@xcite measured the discrepancy between desired and actual privacy settings of facebook users , with a user study involving 200 participants .", "authors found that perception matched reality only 37% of the time , and that default settings were used for 36% of the profiles .", "ayalon and toch  @xcite investigated the relationship between information sharing , information aging , and privacy .", "they conducted a survey of 193 facebook users and posited that relevance , willingness to share / alter posts decreases with time .", "they also found that users are more willing to share recent than old events . while kraus et al .", "@xcite focus on users perception of security and privacy on smartphones , it reveals psychological effects that are seen as threats from users perspective that are usually not considered by mitigation developers .", "finally , bauer et al .", "@xcite studied the relationship of time and information relevance and privacy and found that facebook users were not really interested in the concept of ephemeral data .", "with recent reports of government snooping and increasingly detrimental hacks , more and more apps have entered the market advertised as providing some privacy features . as some of these are now used by millions of users", ", we set to study more carefully the features they offer .", "more specifically , we presented an analysis of 8 popular social networking apps namely confide , frankly chat , secret , snapchat , telegram , whisper , wickr , and yik yak that are marketed as offering some privacy properties .", "starting from a taxonomy of 18 apps , we focused on 8 of them due to their popularity .", "we performed a functional , static , and dynamic analysis , aiming to analyze the properties promised by the apps .", "we found that anonymous social networks whisper and yik yak actually identify their users with distinct ids that are persistent as previous activities like chats , _ whispers _ and _ yaks _ are restored to the device even if the user uninstalls and reinstalls the app .", "this behavior shows that , although they do not require users to provide their email or phone number , they can still persistently link  and possibly de - anonymize  users .", "we also highlighted that , while snapchat promises that messages will `` disappear '' after 10 seconds , they are not immediately deleted from its servers , as old messages are actually included in responses sent to the clients .", "finally , we confirmed that apps such as telegram and wickr do offer end - to - end encrypted chat messaging .    in future work", ", we plan to extend the analysis to more apps .", "we downloaded the metadata of 1.4 million apps using playdrone s measurements @xcite and found 455 apps that might be offering anonymity , ephemerality , or end - to - end encryption . as it would be demanding to manually evaluate them as we did in this paper", ", we will explore how to automate the analysis .", "* acknowledgments . *", "we wish to thank balachander krishnamurthy for motivating our research and for numerous helpful discussions , ruba abu - salma for feedback on an earlier version of the manuscript , and pressid for supporting lucky onwuzurike .", "this research is partly supported by a xerox s university affairs committee award and the `` h2020-msca - itn-2015 '' project privacy&us project ( ref ."], "abstract_text": ["<S> as social networking takes to the mobile world , smartphone apps provide users with ever - changing ways to interact with each other . over the past couple of years , an increasing number of apps have entered the market offering end - to - end encryption , self - destructing messages , or some degree of anonymity </S>", "<S> . however , little work thus far has examined the properties they offer . </S>", "<S> to this end , this paper presents a taxonomy of 18 of these apps : we first look at the features they promise in their appeal to broaden their reach and focus on 8 of the more popular ones . </S>", "<S> we present a technical evaluation , based on static and dynamic analysis , and identify a number of gaps between the claims and reality of their promises . </S>"], "labels": null, "section_names": ["introduction", "static analysis", "dynamic analysis", "discussion", "related work", "conclusion"], "sections": [["following edward snowden s revelations , privacy and anonymity technologies have been increasingly often in the news , with a growing number of users becoming aware  loosely speaking  of privacy and encryption notions  @xcite .", "service providers have rolled out , or announced they will , more privacy - enhancing tools , e.g. , support for end - to - end encryption  @xcite and https by default  @xcite . at the same time , a number of smartphone apps and mobile social networks have entered the market , promising to offer features like anonymity , ephemerality , and/or end - to - end encryption ( e2ee ) .", "while it is not that uncommon to stumble upon claims like `` military - grade encryption '' or `` nsa - proof ''  @xcite in the description of these apps , little work thus far has actually analyzed the guarantees they provide .", "this motivates the need for a systematic study of a careful selection of such apps . to this end , we compile a list of 18 apps that offer e2ee , anonymity and/or ephemerality , focusing on 8 popular ones ( confide , frankly chat , secret , snapchat , telegram , whisper , wickr , and yik yak ) .", "we review their functionalities and perform an empirical evaluation , based on _", "static _ and _ dynamic _ analysis , aimed to compare the claims of the selected apps against results of our analysis .", "highlights of our findings include that `` anonymous '' social network apps whisper and yik yak actually identify users with distinct user ids that are persistent .", "users previous activities are restored to their device after they uninstall and reinstall them , and information collected by these apps could be used to de - anonymize users .", "we also find that the ephemeral - messaging app snapchat does not always delete messages from its servers  in fact , previous `` expired '' chat messages are surprisingly included in packets sent to the clients .", "then , we report that all actions performed by a user on frankly chat can be observed from the request url , which is actually transmitted in the clear .", "we start by building a list of smartphone apps that are categorized as `` anonymous '' on product hunt  @xcite , and those popular among friends and colleagues .", "we then look at their descriptions and at _ similar apps _ on the google play , and focus on those described as offering end - to - end encryption , anonymity and/or ephemerality , as defined below :    * * _ anonymity : _ * is defined as the property that a subject is not identifiable within a set of subjects , known as the anonymity set  @xcite , e.g. , as provided by tor  @xcite for anonymous communications . in the context of this paper , the term anonymity will be used to denote that users are anonymous w.r.t .", "other users of the service or w.r.t . the app service provider . * * _", "end - to - end encryption ( e2ee ) : _ * data exchanged between two communicating parties is encrypted in a way that only the sender and the intended recipient can decrypt it , so , e.g. , eavesdroppers and service providers can not read or modify messages . * * _ ephemerality : _ * in cryptography , it denotes the property that encryption keys change with every message or after a certain period . instead , here", "ephemerality is used to indicate that messages are not available to recipients from the user interface after a period of time  @xcite .", "for instance , in apps like snapchat , messages `` disappear '' from the app ( but may still be stored at the server ) a few seconds after they are read .", "* first list . *", "our first list contains 18 apps , listed in table  [ table : first ] , where we also report their first release date , number of downloads as reported by google play store , the kind(s ) of content that can be shared via the apps ( e.g. , text , videos , files ) , and whether the apps create persistent social links .", "note that our first selection does not include popular apps like whatsapp , since it attempts , but does not guarantee , to provide e2ee for all users  @xcite .      among the 18 apps presented in table  [", "table : first ] , we then select a few popular ones , as discussed below", ".    * selection criteria . * from our corpus", ", we focus on apps with the most downloads that offer ephemerality , anonymity , e2ee , or , preferably , a combination of them .", "we exclude silent circle and tigertext as they require , respectively , paid subscription and a registered company email .", "we reduce our selection to 8 apps : confide , frankly chat , secret , snapchat , telegram , whisper , wickr , and yik yak ( bold entries in table  [ table : first ] ) .", "next , we provide an overview of their advertised functionalities , complementing information in the table  [ table : first ] .", "( note that descriptions below are taken either from the google play store or the apps respective websites . )", "* _ confide : _ * offers end - to - end encryption and ephemerality .", "it allows users to share text , photos , and documents from their device and integrates with dropbox and google drive .", "it provides read receipts and notification of screenshot capture attempts .", "messages are not displayed on the app until the recipient `` wands '' over them with a finger , so that only a limited portion of the message is revealed at a time .", "after a portion of the message is read , it is grayed out screenshots are also disabled on android .", "messages that have not been read are kept on the server for a maximum of 30 days .", "* _ frankly chat : _ * is a chat app allowing users to send ephemeral messages ( text , picture , video or audio ) , anonymous group chats , and un - send messages that the recipient has not opened .", "messages disappear after 10 seconds but users can `` pin '' their chats disabling ephemerality .", "both parties do not need to have the app installed to receive messages : a link is sent to the recipient via email , when clicked , reveals the message .", "messages are deleted from the server after 24 hours  whether they are read or not .", "* _ secret : _ * _ ( discontinued may 2015 ) _ lets users post anonymously to other _", "nearby _ users .", "users can view _ secrets _ from other locations but can only comment on those from their nearby location .", "users can chat privately with friends and engage in a group chat with the chat history disappearing after a period of inactivity .", "* _ snapchat : _ * is an app that allows users send text , photos and videos that are displayed for 1 to 10 seconds ( as set by the user ) before they `` disappear '' , i.e. , they are no longer available to their friends .", "if the recipient takes a screenshot , the sender is notified .", "users can also view _ stories _", ", i.e. , a collection of snaps around the same theme , and a so - called  _ discover _ , i.e. , accessing snaps from different selected editorials .    * _ telegram : _ * is a messaging app that lets users exchange text , photos , videos , and files .", "it also provides users with an option to engage in a `` secret chat '' , which provides e2ee and optional ephemerality .", "senders are notified if the recipient takes a screenshot .", "account information , along with all messages , media , contacts stored at telegram servers are deleted after 6 months of login inactivity .", "* _ whisper : _ * is a location - based mobile social network that allows users to anonymously share texts displayed atop images , which are either selected by the users or suggested by the app .", "users can view and respond to _ whispers _ either as a private message or via another _ whisper_.    * _ wickr : _ * is a chat app supporting text , audio , video , photos , and files , with user - defined ephemerality ( maximum 6 days ) .", "it also allows users to engage in group chats , shred deleted files securely , and prevents screenshots on android and claims to anonymize users by removing metadata ( such as persistent identifiers or geo - location ) from their contents .    *", "_ yik yak : _ * is a local bulletin - board social network allowing nearby users to post _ yaks _ anonymously .", "users clustered within a 10-mile radius are considered local and can post , view , reply to , and up / down vote yaks but can only view _ yaks _ outside their locality"], ["we now present the results of a static analysis of the 8 apps , aiming to analyze ssl / tls implementations and look for potential information leakage .", "we perform static analysis using dex2jar  @xcite , decompiling the .apk files to .jar files , from which we extract the related java classes using jd - gui  @xcite .", "we then search for ssl / tls keywords like ` trustmanager `  @xcite , ` hostnameverifier `  @xcite , ` sslsocketfactory `  @xcite , and ` httpsurlconnection ` @xcite .", "then , we inspect the ` trustmanager ` and ` hostnameverifier ` interfaces used to accept or reject a server s credentials : the former manages the certificates of all certificate authorities ( cas ) used in assessing a certificate s validity , while the latter performs hostname verification whenever a url s hostname does not match the hostname in the certificate .      several sockets are usually created to transport data to different hostnames in an app , therefore , sockets in an app may have different ssl implementations .", "we observe different ssl implementations in the 8 apps , and summarize our findings below .", "* non - customized ssl implementation .", "* app developers can choose to use any one of five defined ` hostnameverifier ` subclass for hostname verification , and use ` trustmanager ` initialized with a keystore of ca certificates trusted by the android os to determine whether a certificate is valid or not , or customize certificate validation by defining their own logic for accepting or rejecting a certificate .", "all the 8 apps in our corpus contain some non - customized ssl implementations .", "telegram and yik yak only use non - customized ssl code , with the former relying on ` browsercompathostnameverifier ` class and building sockets from default ` sslsocketfactory ` . confide and snapchat both use the ` browsercompathostnameverifier ` class , while wickr has instances of all ` hostnameverifier ` subclasses but uses the ` browsercompathostnameverifier ` class on most of its sockets .", "snapchat does not customize its ` trustmanager ` either and registers a scheme from the default ` socketfactory ` .", "secret uses sockets from the default ` sslsocketfactory ` but employs regex pattern matching for hostname verification", ".    * vulnerable trustmanager / hostnameverifier . * frankly chat , whisper , and wickr all contain ` trustmanager ` and ` hostnameverifier ` that accept all certificates or hostnames .", "alas , this makes it possible for an adversary to perform _ man - in - the - middle ( mitm ) _ attacks and retrieve information sent on the sockets that use the vulnerable ` trustmanager ` and/or ` hostnameverifier ` .", "vulnerable ` hostnameverifier ` in frankly chat returns ` true ` without performing any check , while wickr uses the ` allowallhostnameverifier ` subclass which is also used in whisper by bugsense crash reporter .", "* certificate pinning . *", "confide , frankly chat , and whisper implement certificate pinning .", "confide pins the expected ca certificate which is also accessible from the decompiled apk , whereas , whisper uses the hash of the pinned certificate appended with the domain name to make certificate validation decisions . for frankly chat", ", a single certificate is expected and its hash is checked for in the received certificate chain .", "frankly chat also initialize another ` trustmanager ` with a keystore that loads certificate from file .", "0.47       0.47"], ["next , we present the results of our dynamic analysis aimed to scrutinize features `` promised '' by the 8 apps in our corpus , as well as to confirm whether the vulnerabilities found statically are also observed dynamically .", "we conduct our experiments on a lg nexus 4 running android 5.1 , connected to a wi - fi access point under our control .", "( note that the wi - fi network was secured using wpa2 to prevent unauthorized connections and ensure that only intended traffic was captured . )", "our intention is to examine what a random attacker can access from an app advertised as privacy - enhancing , and what can be deduced as regards privacy - enhancing claims .", "hence , we assume an adversary that can not elevate her privilege nor have access to a rooted device .", "we perform actions including sign - up , login , profile editing , sending / reading messages , while monitoring traffic transmitted and received by the apps .", "we collect traffic using wireshark and analyze unencrypted traffic to check for sensitive information transmitted in the clear .", "we also rely on http proxies such as fiddler  @xcite and sslsplit  @xcite to mount man - in - the - middle ( mitm ) attacks and decrypt https traffic .", "proxying is supported in two ways :    1 .   _", "regular proxy : _ we install the fiddler http proxy  @xcite on a windows 8.1 laptop ( which also acts as wi - fi access point ) , listening on port 8888 , and manually configure the smartphone to connect to the proxy .", "figure  [ fig : fiddler ] illustrates our proxy setup using fiddler .", "we also install fiddler s ca certificate on the smartphone and laptop to allow https traffic decryption .", "transparent proxy : _ some android apps are programmed to ignore proxy settings , so fiddler does not accept / forward their packets .", "this happens with telegram , wickr ( non - css / js ) , and frankly chat ( chat packets ) .", "therefore , we set up a transparent proxy as shown in figure  [ fig : mitmproxy ] using sslsplit mitm proxy  @xcite set to listen on port 9014 on a linux desktop running fedora 22 , which also acts as a wi - fi access point .", "we use _ iptables _ to redirect to port 9014 all traffic to ports 80 , 443 , and 5228 ( gcm ) . as sslsplit", "uses a ca certificate to generate leaf certificates for the https servers each app connects to , we generate and install a ca certificate on the smartphone , and pass it to sslsplit running on the linux machine .", "we now present the results of our dynamic analysis , which are also summarized in table  [ table : proxy ] .    * no proxy . *", "we start by simply analyzing traffic captured by wireshark and observe that secret and frankly chat send sensitive information in the clear .", "specifically , in frankly chat , the android advertising i d ( a unique identifier ) is transmitted in the clear , via an http get request , along with device name .", "the list of actions a user performs on frankly chat can also be observed from the request url .", "secret instead leaks google maps location requests ( and responses ) via http get .", "* regular proxy . * using fiddler as a mitm proxy , we notice that confide and whisper do not complete connection with their servers due to certificate pinning .", "note that whisper started implementing pinning after an update on april 22 , 2015 .", "prior to that , one could capture whisper traffic via fiddler and access location and user i d .", "we also notice that frankly chat hashes passwords using md5 without salt , while snapchat sends usernames and passwords without hashing .", "although inconsistently , snapchat also sends previous `` expired '' chat messages to the other party even though these are not displayed on the ui .", "decrypted traffic from secret , whisper , and yik yak show that these apps associate unique user ids to each user , respectively , _ clientid _ , _ wuid _ , and _ user id_. we test the persistence of these ids and find that , even if the apps cache on the device is cleared through the android interface , and the apps uninstalled and reinstalled , whisper and yik yak retain the user i d from the uninstalled account and restore all previous _ whispers _ and _ yaks _ from the account . on whisper", ", we manually delete its wuid and state files ( in the /sdcard / whisper directory ) before reinstalling the app : this successfully clears all previous _ whispers _ and a new wuid file is generated .", "however , it does not completely de - associate the device from the `` old '' account as the `` new '' account would still get notifications of private messages from conversations started by the `` old '' account . on the contrary , clearing secret s cache unlinks previous messages , even without uninstalling the app .", "telegram and wickr ignore the proxy settings , i.e. , traffic does not pass through our proxy .", "frankly chat also ignore the proxy when sending chat messages but not for traffic generated by other actions .", "* transparent proxy . * using sslsplit , we decrypt ssl - encrypted traffic from wickr and telegram .", "we do not find any sensitive information or chat messages being transmitted as the traffic is indeed encrypted .", "apps for which ssl - encrypted traffic is recovered using fiddler exhibit the same behavior on the transparent proxy , with confide and whisper not connecting due to pinning .", "we observe that certificate pinning is implemented on the socket used to transmit chat messages on frankly chat , as we can not send chat messages but perform other actions , e.g. , editing profiles and adding new friends .", "we also uninstall the ca certificate from the device to observe whether non - trusted certificate are accepted , and find that none of the apps established an https connection , which implies the apps do not use trustmangers accepting any certificate as valid as reported in  @xcite ."], ["we now discuss the implications of our analysis , in light of the properties promised by the 8 studied apps .", "* anonymity w.r.t", ". other users . * posts on secret and yik yak ( resp . , _ secrets _ and _ yaks _ ) are not displayed along with any identifier , thus making users anonymous w.r.t . other users .", "whereas , on whisper , due to the presence of a _ display name _ ( non - unique identifier shown to other users ) and its `` nearby '' function , communities can be formed as a result of viewing and responding to _ whispers _ from nearby locations . thus , it may be possible to link _ whispers _ to a display name , while at the same time querying the distance to the target , as highlighted in  @xcite .", "a user who thinks is anonymous is more likely to share sensitive content she might not share on non - anonymous osn platforms , which makes `` anonymous '' apps potential targets of scammers / blackmailers that can identify users .", "this motivates us to examine the possibility of creating social links between users , i.e. , linking a user and a set of actions .", "we find that this is not possible on yik yak as there are no one - to - one conversations .", "also , when the yik yak stream is monitored by a non - participating user , user ids observed are symbolic to the real unique user i d .", "the symbolic user i d is only associated to one _ yak _ , hence one can not use it to link a user as the i d differs across _ yaks _ by the same user .", "frankly chat optionally offers @xmath0-anonymity during a group chat with @xmath0 + 1 friends . due to the social link already present in the group", "( users chat with friends ) , psychological differences make it possible to identify who says what .", "* anonymity w.r.t . service provider . * all apps associate identifiers to its users , which allows them to link each user across multiple sessions .", "wickr claims to strip any metadata that could allow them to identify their users , thereby making users anonymous and impossible to track @xcite , but we can not verify this claim since all traffic is encrypted end - to - end .", "we observe different levels of persistence of user ids in secret , whisper , and yik yak , as mentioned earlier .", "secret stores identifiers on users device , so an identifier would cease to persist beyond data and cache clearance . whereas , for whisper and yik yak , we have two hypotheses as to why user ids survive when the app is uninstalled and later reinstalled : either they store identifiers on their servers and restore them to a device on re - installation , or they create the user ids from the same device information using a deterministic function .", "this observation indicates that whisper and yik yak s user ids are linked to device information , thus making users persistently linkable .", "while whisper and yik yak do reveal the information they collect from users in their privacy policy , previous work shows that the overwhelming majority of users do not read ( or anyway understand ) privacy policies  @xcite .", "both apps collect information including device i d , ip address , geo - location , which can be used to track users .", "this , along with profiles from analytics providers ( which both apps embed ) , can be used to de - anonymize users age , gender , and other traits with a high degree of accuracy @xcite .", "finally , note that whisper s description on google play , while including terms like ` anonymous profiles ' and ` anonymous social network ' , is actually ambiguous as to whether they refer to anonymity w.r.t to whisper or other users ( or both )", ".    * location restriction . *", "secret and yik yak s restriction on feeds a user can see ( and interact with ) can simply be defeated , e.g. , as android lets users to use mock locations in developer mode . in combination with an app that feeds gps locations chosen by the user ( e.g. , _ fake gps _ ) , this allow them to access geo - tagged messages from anywhere .    * ephemerality . *", "confide , frankly chat , snapchat , telegram , and wickr offer message ephemerality with varying time intervals .", "confide claims messages disappear after it is read once  @xcite but this is not the case as messages only `` disappear '' after a user navigates away .", "this implies the recipient can keep the message for longer as long as they do not navigate away from the opened message . in frankly chat ,", "messages `` disappear '' after 10 seconds ( even though users can pin messages ) .", "ephemerality on telegram only applies to `` secret chats '' and the expiration time is defined by the user .", "snapchat and wickr also let users determine how long their message last , with snapchat defining a range of 110s ( default 3s ) .", "on snapchat , previous chat messages are actually part of the response received from the server , even though they are not displayed on the client s ui .", "this indicates that read messages are actually not deleted from snapchat servers immediately , despite what is stated in snapchat s privacy policy @xcite . since confide and frankly chat implement certificate pinning , we can not examine if responses from the server during chat contain past messages .", "also , telegram and wickr encrypt data before transmission , thus we can not make any analysis from intercepted packets .", "of all the apps offering ephemerality , only confide and wickr instruct the android os to prevent screen capture from a recipient .", "obviously , however , the recipient can still take a photo with another camera , and video recording would defeat confide s wand - based approach .", "confide can claim to offer plausible deniability if a photo is taken , as messages are not displayed along with the name of the sender , hence , pictures would not preserve the link between the message and the identity of the sender .", "frankly chat , snapchat , and telegram only notify the sender that the recipient has taken a screenshot , thus ephemerality claims are only valid assuming the recipient is not willing to violate a social contract between them and the sender . also", ", if messages are not completely wiped from the server , the provider is obviously still subject to subpoena and/or vulnerable to hacking .", "* end - to - end encryption . *", "confide and wickr claim to employ e2ee by default , using aes-128 and aes-256 , respectively .", "we can confirm e2ee in wickr but not in confide , since certificate pinning prevents interception of traffic .", "also , telegram offers e2ee for `` secret chat '' using aes-256 and client - server encryption ( i.e. only the server and both clients can decrypt traffic ) which also prevents mitm attacks for non - secret chats . in both secret and non - secret", "chat , telegram uses a proprietary protocol , mtproto , and transmit traffic over ssl although its webpage states otherwise .", "telegram and wickr s implementations also claim to support perfect forward secrecy @xcite . finally , note that recent criticism of telegram s security in the press do not affect the claims of telegram that we choose to analyze , i.e. , e2ee and ephemerality in `` secret chats . ''"], ["this section reviews related work , specifically , ( i ) measurement studies of chat apps and location - based social networks , ( ii ) apps vulnerabilities , and ( iii ) investigations of users behavior .    * measurement - based studies . * wang et al .", "@xcite analyze user interaction in whisper , motivated by the absence of persistent social links , content moderation , and user engagement .", "they also highlight a vulnerability that allows an attacker to detect a user s location by attaching a script to a _ whisper _ querying whisper s db .", "correa et al .", "@xcite define the concept of _ anonymity sensitivity _ for social media posts and measure it across non - anonymous ( e.g. , twitter ) and anonymous ( e.g. , whisper ) services , aiming to study linguistic differences between anonymous and non - anonymous social media sites as well as to analyze content posted on anonymous social media and the extent user demographics affect perception and measurements of sensitivity .", "peddinti et al .", "@xcite analyze users anonymity choices during their activity on quora , identifying categories of questions for which users are more likely to seek anonymity .", "they also perform an analysis of twitter to study the prevalence and behavior of so - called `` anonymous '' and `` identifiable '' users , as classified by amazon mechanical turk workers , and find a correlation between content sensitivity and a user s choice to be anonymous .", "stuzman et al .", "@xcite observe a significant growth in anonymity - seeking behavior on online social media in 2013 , while roesner et al .", "@xcite analyze why people use snapchat : they survey 127 adults and find that privacy is not the major driver of adoption , but the _ `` fun '' _ of self - destructing messages .", "* flaws . *", "prior work has also looked at related apps security flaws : in late 2013 , researchers from gibson security discovered a flaw in snapchat s api that allows an adversary to reconstruct snapchat s user base ( including names , aliases , phone numbers ) within one day and mass creation of bogus accounts  @xcite .", "zimmerman  @xcite highlights the issue of linkability of anonymous identifiers in wickr .", "recently , unger et al .", "@xcite systematize security and usability of chat and call apps providing end - to - end encryption . also , prior work  @xcite has studied libraries , interfaces , classes , and methods used by apps to make security decisions , specifically , w.r.t .", "vulnerabilities in sockets used to transmit user data .", "* user behavior .", "* pielot and oliver  @xcite study the motivations behind the use of snapchat by teenagers .", "they create two personas and , by engaging with other users , they find that teens use snapchat as they are excited by the ephemerality , see fewer risks , and non - commitment to persistent messengers .", "roesner et al .", "@xcite analyze why people use snapchat : they survey 127 adults and find that security and privacy are not the major drivers of adoption , but rather the _", "`` fun '' _ of self - destructing messages .", "hosseinmardi et al .", "@xcite look at cyberbullying on a semi - anonymous network , i.e. , last.fm , while stuzman et al .", "@xcite observe a significant growth in anonymity - seeking behavior on online social media in 2013 .", "shein  @xcite interview a few experts and commented on the rise of apps for `` ephemeral data '' ( e.g. , snapchat , gryphn , wickr ) , pointing out that users do not use ephemeral messaging because they have something to hide , rather , because they do not want to add digital artifacts to their digital `` detritus . ''    * privacy perceptions . *", "liu et al .", "@xcite measured the discrepancy between desired and actual privacy settings of facebook users , with a user study involving 200 participants .", "authors found that perception matched reality only 37% of the time , and that default settings were used for 36% of the profiles .", "ayalon and toch  @xcite investigated the relationship between information sharing , information aging , and privacy .", "they conducted a survey of 193 facebook users and posited that relevance , willingness to share / alter posts decreases with time .", "they also found that users are more willing to share recent than old events . while kraus et al .", "@xcite focus on users perception of security and privacy on smartphones , it reveals psychological effects that are seen as threats from users perspective that are usually not considered by mitigation developers .", "finally , bauer et al .", "@xcite studied the relationship of time and information relevance and privacy and found that facebook users were not really interested in the concept of ephemeral data ."], ["with recent reports of government snooping and increasingly detrimental hacks , more and more apps have entered the market advertised as providing some privacy features . as some of these are now used by millions of users", ", we set to study more carefully the features they offer .", "more specifically , we presented an analysis of 8 popular social networking apps namely confide , frankly chat , secret , snapchat , telegram , whisper , wickr , and yik yak that are marketed as offering some privacy properties .", "starting from a taxonomy of 18 apps , we focused on 8 of them due to their popularity .", "we performed a functional , static , and dynamic analysis , aiming to analyze the properties promised by the apps .", "we found that anonymous social networks whisper and yik yak actually identify their users with distinct ids that are persistent as previous activities like chats , _ whispers _ and _ yaks _ are restored to the device even if the user uninstalls and reinstalls the app .", "this behavior shows that , although they do not require users to provide their email or phone number , they can still persistently link  and possibly de - anonymize  users .", "we also highlighted that , while snapchat promises that messages will `` disappear '' after 10 seconds , they are not immediately deleted from its servers , as old messages are actually included in responses sent to the clients .", "finally , we confirmed that apps such as telegram and wickr do offer end - to - end encrypted chat messaging .    in future work", ", we plan to extend the analysis to more apps .", "we downloaded the metadata of 1.4 million apps using playdrone s measurements @xcite and found 455 apps that might be offering anonymity , ephemerality , or end - to - end encryption . as it would be demanding to manually evaluate them as we did in this paper", ", we will explore how to automate the analysis .", "* acknowledgments . *", "we wish to thank balachander krishnamurthy for motivating our research and for numerous helpful discussions , ruba abu - salma for feedback on an earlier version of the manuscript , and pressid for supporting lucky onwuzurike .", "this research is partly supported by a xerox s university affairs committee award and the `` h2020-msca - itn-2015 '' project privacy&us project ( ref ."]]}
{"article_id": "1010.1358", "article_text": ["random privacy amplification based on the universal@xmath3 condition@xcite has been studied by many authors@xcite .", "this technique is originally aimed for random number extraction@xcite .", "it can be applied to secret key distillation ( agreement ) with public communication@xcite and wire - tap channel@xcite , which treats the secure communication in the presence of an eavesdropper .", "( for details of its application , see e.g. hayashi @xcite . ) when random privacy amplification is implemented by a universal@xmath3 hash function , it can yield protocols for the above tasks with a relatively small amount of calculation . similar to the study @xcite for random privacy amplification based on the universal@xmath3 condition , hayashi@xcite focused only on the mutual information with the eavesdropper .", "however , as the secrecy criterion , many papers in cryptography community @xcite dopt the half of the @xmath0 norm distance , which is also called the variation distance or eve s distinguishability .", "because this criterion is closely related to universally composable security @xcite , it is required to evaluate the leaked information based on the @xmath0 norm distance from cryptography community viewpoint .    in this paper , we adopt the @xmath0 norm distance as the secrecy criterion , and evaluate the secrecy for random privacy amplification while hayashi@xcite adopts the mutual information criterion . in the independent and identical distributed case , when the rate of generating random numbers is smaller than the entropy of the original information source , it is possible to generate the random variable whose @xmath0 norm distance to the uniform random number approaches zero asymptotically", ". however , in the real setting , we can manipulate only a finite size of random variables .", "so , the speed of this convergence is very important . in the community of information theory , in order to discuss the speed , we often focus on the the exponential rate of decrease .", "this rate is called the exponent , and is widely discussed among several topics in information theory , e.g. , channel coding@xcite , source coding@xcite , and mutual information criterion in wire - tap channel@xcite .", "however , the exponent has not been discussed in the community of cryptography as an important criterion .", "the purpose of this paper is establishing a systematic evaluating method for exponent for the @xmath0 norm distance in secure protocols .    in subsection [ s2a ] , for a given generating rate and a given source distribution , in the i.i.d .", "setting , we derive a lower bound of the exponent of the average of the @xmath0 norm distance between the generated random number and the uniform random number when universal@xmath3 hash function is applied", ". then , we introduce a stronger condition for hash functions , which is called strongly universal@xmath3 function , and show that our lower bound is tight under this condition . since our bound realizes the optimal exponent , it is thought to be powerful even for the finite length setting .", "we consider the @xmath4-independent and identical extension , and show that the exponential rate of decrease for this bound is tight under a stronger condition by using the type method@xcite . however , if our protocol generating the random number depends on the original distribution , there is a possibility to improve the exponent while it is known that asymptotic generation can not be improved@xcite . in subsection", "[ s3 ] , we derive the optimal exponent in this setting by using the cramer s theorem @xcite and the type method @xcite . comparing these two exponents , we can compare the performances between the protocol based on universal@xmath3 hash function and the protocol depending on the information source .    in information theory ,", "the part constructing the protocol achieving the rate is called the direct part . and", "the proof for non - existence surpassing the rate is called the converse part . concerning the exponent of the @xmath0 norm distance with universal@xmath3 hash function , the direct part is relatively simple while the converse part is more complicated .", "the converse part is regarded as an important topic in information theory , while it has been regarded as a minor topic in other research areas .", "in the direct part , we focus on bennett et al@xcite s evaluation for random privacy amplification , which employs the rnyi entropy of order 2 .", "this evaluation was also obtained by hastad et al @xcite and is often called leftover hash lemma . using the same discussion as renner @xcite", ", we derive an upper bound for the @xmath0 norm distance .", "renner @xcite applied @xmath5 approximation idea ( smoothing ) to min - entropy , which bounds the rnyi entropy of order 2 . in this paper , we apply the same idea to the rnyi entropy of order 2 .", "however , for the evaluation of the smoothing error , we use a method different from the method invented by holenstein - renner@xcite , but our method is standard in large deviation theory@xcite .", "in fact , as is shown in subsection [ s2b ] , our evaluation is much better than that by holenstein - renner@xcite .", "then , we obtain a bound based on the rnyi entropy of order @xmath1 as the main theorem ( theorem [ thm1 ] ) .", "note that , hayashi@xcite obtained a similar result using the rnyi entropy of order @xmath1 , but it evaluates the shannon entropy not the @xmath0 norm distance .", "contrastly , in the converse part , we employ type method , which was invented by csiszr and krner @xcite .", "the discussion of this part is quite technical , but conventional in information theory . concerning the exponent of the protocol depending on the original distribution ,", "the direct part can be shown by the combination of a new invented lemma ( lemma [ lemdi ] ) and cramer s theorem in large deviation theory@xcite .", "the converse part can be shown by very technical application of the type method@xcite .    in section [ s42 ] , we consider the case when an eavesdropper has a random variable correlated to the random variable of the authorized user . in this case , when the authorized user applies universal@xmath3 hash function to his random variable , he obtain a secure random variable .", "when we apply theorem [ thm1 ] to the security by @xmath0 norm distance in this setting , we obtain a tighter evaluation than existing evaluation than that directly obtained from hayashi@xcite .    in section [ s2 ] , we focus on wire - tap channel model , whose capacity has been calculated by wyner @xcite and csiszr and krner @xcite .", "csiszr @xcite showed the strong security , and hayashi @xcite derived bounds for both exponential rates of decrease for the mutual information between alice and eve and the security criterion based on the @xmath0 norm distance , which is simply called the @xmath0 security criterion .", "he obtained a bound for the exponential rate of decrease concerning the @xmath0 security criterion .", "however , their approaches are based on completely random coding . using the method of information spectrum@xcite ,", "hayashi @xcite showed strong security when universal@xmath3 hash function is applied in the privacy amplification process of wire - tap channel .", "but , the analysis on @xcite was based on the mutual information not the @xmath0 security criterion . in the paper@xcite , he invented a systematic method to construct a wire - tap channel code based on universal@xmath3 hash function , and evaluated the performance based on a privacy amplification theorem concerning the shannon entropy given in @xcite .", "so , we can expect that theorem [ thm1 ] plays the same role in evaluation of the @xmath0 security criterion in wire - tap channel model as a privacy amplification theorem concerning the shannon entropy given in @xcite . in this paper", ", we apply theorem [ thm1 ] to wire - tap channel model , and obtain the evaluation of the exponent of the @xmath0 security criterion . in fact", ", we can evaluate the exponent of the @xmath0 security criterion from the evaluation for exponent concerning the mutual information in @xcite by applying the pinsker inequality to the mutual information .", "it is also shown in section [ s6 - 1 ] , that the evaluation obtained in this paper is better than that by @xcite and that obtained by application of pinsker inequality .    in section [ s6 ] , the same discussion as sections v and vi in @xcite", "can be applied to the @xmath0 security criterion with a suitable modification . in the additive noise case ,", "the obtained bound can be attained by linear operation .", "further , we obtain the bound for the @xmath0 security criterion in one - way secret key distillation by the same discussion as sections vi in @xcite . in appendix [ as2 ] , we prove theorem [ thm2 ] mentioned in subsection [ s2a ] . in appendix", "[ al9 - 9 - 1 ] , we prove lemma [ l9 - 9 - 1 ] given in subsection [ s3 ] .", "firstly , we consider the uniform random number generation problem from a biased random number @xmath6 , which obeys a probability distribution @xmath7 when its cardinality @xmath8 is finite .", "there are two types protocols for this problem .", "one is a protocol specialized for the given distribution @xmath7 .", "the other is a universal protocol that does not depends on the given distribution @xmath7 .", "the aim of this section is evaluate the performance of the latter setting . in the latter setting , our protocol is given by a function @xmath9 from @xmath10 to @xmath11 . in order to evaluate the protocol @xmath9 , we use the @xmath0 distance ( the variational distance ) @xmath12 and the @xmath13 distance @xmath14 these definitions can be extended when the total measure is less than @xmath15 i.e. , @xmath16 . in the following , we call such @xmath7 a sub - distribution .", "this extension for sub distributions is crucial for the later discussion .", "then , the quality of the random number obeying the distribution @xmath7 is evaluated by @xmath17 where @xmath18 is the uniform distribution on @xmath10 .", "we also use the quantitty @xmath19 and the rnyi entropy order @xmath1 : @xmath20 the @xmath13 distance is written by using the rnyi entropy order @xmath21 as follows .", "@xmath22    now , we focus on an ensemble of the functions @xmath23 from @xmath10 to @xmath24 , where @xmath25 denotes a random variable describing the stochastic behavior of the function @xmath23 .", "an ensemble of the functions @xmath23 is called universal@xmath3 hash when it satisfies the following condition@xcite :    @xmath26 , the collision probability that @xmath27 is at most @xmath28 .", "we sometimes require the following additional condition :    for any @xmath25 , the cardinality of @xmath29 does not depend on @xmath30 .", "this condition will be used in section iii .", "indeed , when the cardinality @xmath8 is a power of a prime power @xmath31 and @xmath32 is another power of the same prime power @xmath31 , as is shown in appendix ii of hayashi @xcite , the ensemble @xmath33 can be given by the concatenation of toeplitz matrix and the identity @xmath34@xcite only with @xmath35 random variables taking values in the finite field @xmath36 .", "that is , the function can be obtained by the multiplication of the random matrix @xmath34 taking values in @xmath36 . in this case ,", "condition [ c12 ] can be confirmed because the rank of @xmath34 is constant . for condition [ c1 ] ,", "see appendix ii of hayashi @xcite .", "bennett et al@xcite essentially showed the following lemma .", "a universal@xmath3 function @xmath23 satisfies @xmath37    this was also shown by hastad et al @xcite and is often called leftover hash lemma .", "now , we follow the derivation of theorem 5.5.1 of renner @xcite when one classical random variable is given .", "the schwarz - inequality implies that @xmath38 the jensen inequality yields that @xmath39 substituting ( [ 5 - 14 - 3 ] ) and ( [ 5 - 14 - 4 ] ) into the above inequality , we obtain @xmath40    using ( [ 5 - 14 - 6 ] ) , we can show the following theorem as a generalization of ( [ 5 - 14 - 6 ] ) .", "a universal@xmath3 function @xmath23 satisfies @xmath41    substituting @xmath42 , we obtain @xmath43 since the difference between ( [ 5 - 14 - 6 ] ) and ( [ 5 - 14 - 7 ] ) is only the coefficient , theorem [ thm1 ] can be regarded as a kind of generalization of bennett et al@xcite s result ( [ 5 - 14 - 4 ] ) .", "for any @xmath44 , we choose subset @xmath45 , and define the subdistribution @xmath46 by @xmath47 since @xmath48 , @xmath49 the inequality ( [ 5 - 14 - 6 ] ) yields @xmath50 for @xmath51 , we can evaluate @xmath52 and @xmath53 as @xmath54 combining ( [ 5 - 14 - 10 ] ) , ( [ 5 - 14 - 8 ] ) , and ( [ 5 - 14 - 9 ] ) , for @xmath55 , we obtain @xmath56 where we substitute @xmath57 into @xmath58 .", "next , we consider the case when our distribution @xmath59 is given by the @xmath4-fold independent and identical distribution of @xmath60 , i.e , @xmath61 . when the random number generation rate @xmath62 is @xmath63 , we focus on the _ exponential rate of decrease _ of @xmath64 , and consider the supremum .", "when an ensemble @xmath65 is a universal@xmath3 hash functions from @xmath66 to @xmath67 , theorem [ thm1 ] yields that @xmath68 on the other hand , when we apply the pinsker inequality@xcite to the upper bound for the mutual information obtained in hayashi@xcite , we obtain another bound @xmath69 , which is smaller than ( [ 4 - 19 - 2 ] ) .", "the right hand side of ( [ 4 - 19 - 2 ] ) is equal to the optimal exponential rate in the fixed - length source coding .", "the following lemma is known @xcite :    assume that @xmath60 is a probability distribution and the random number generation rate @xmath63 is greater than the critical rate @xmath70 , where @xmath71 .", "using kullback - leibler divergence @xmath72 , we have the following expression .", "@xmath73    here", "the last equation of ( [ 9 - 7 - 1 ] ) can be checked in the following way .", "@xmath74 is equal to the minimum exponent for correct decoding probability when the compression rate is @xmath63@xcite .", "this minimum exponent is equal to @xmath75@xcite .", "hence , we obtain the last equation .    in oder to show the tightness of the exponential rate decrease ( [ 9 - 7 - 1 ] ) under the universal@xmath3 condition , we consider the following ensemble .    for any @xmath76 , @xmath77 .", "the random variable @xmath78 is independent of @xmath79 for different arbitrary two elements @xmath80 .    under the strongly universal@xmath3 ensemble , and any subset @xmath81 with @xmath82 satisfies @xmath83    its proof is given in appendix [ as2 ] .", "using theorem [ thm2 ] and lemma [ lem9 ] with @xmath84 and applying the type method for fixed length source coding @xcite , we can show the following proposition :    let @xmath85 be the set of empirical distributions on @xmath10 with @xmath4 trials .", "assume the following conditions : @xmath86 .", "when @xmath87 , any sequence of strongly universal@xmath3 ensemble @xmath88 from @xmath66 to @xmath67 satisfies the equation @xmath89    when @xmath90 , applying the type method for fixed length source coding @xcite to the random variable @xmath91 , we obtain @xmath92 using theorem [ thm2 ] with @xmath93 , we obtain @xmath94 thus , @xmath95 in particular , when @xmath87 , the equations @xmath96 hold .", "since @xmath87 , lemma [ lem9 ] yields that @xmath89    therefore , we can conclude that our exponential rate of decrease is tight for the strongly universal@xmath3 condition when @xmath87 .", "in the above derivation , the key point is evaluating @xmath97 in the @xmath4-i.i.d .", "setting , i.e. , @xmath98 . in the community of cryptography , they often use holenstein - renner @xcite evaluation for @xmath53 in the @xmath4-i.i.d . setting .", "they proved the following theorem .    when @xmath99 , @xmath100 further , when @xmath101 and @xmath102 , @xmath103 when @xmath104 , the inequality yields the following evaluation .", "when @xmath105 , @xmath106 for even @xmath4 .", "our evaluation ( [ 5 - 14 - 9 ] ) of @xmath98 contains the parameter @xmath107 .", "since this parameter is arbitrary , it is natural to compare the upper bound @xmath108 given by ( [ 5 - 14 - 9 ] ) with that by theorem [ holenstein ] . that is , using ( [ 5 - 14 - 9 ] ) , we obtain the exponential evaluation @xmath109 while theorem [ holenstein ] yields that @xmath110 in this case , the upper bound is @xmath111 for @xmath101 and @xmath112 for @xmath113 .", "in fact , the probability @xmath97 is the key quantity in the method of information spectrum , which is a unified method in information theory@xcite . when the method of information spectrum is applied to the i.i.d .", "source , the probability @xmath97 is evaluated by applying cramr theorem ( see @xcite ) to the random variable @xmath114 .", "then , we obtain @xmath115 for @xmath116 . since @xmath117 is concave , when @xmath118 , the maximization can be attained in @xmath119 $ ] , i.e. ,", "@xmath120 which implies that our evaluation ( [ 5 - 14 - 9 ] ) gives the tight bound for exponential rate of decrease for the probability @xmath98 .", "in fact , the difference among these bounds is numerically given in fig . [ f2 ] . therefore , we can conclude that our evaluation ( [ 5 - 14 - 9 ] ) is much better than that by holenstein - renner @xcite .", "that is , combination of lemma 1 and ( [ 5 - 14 - 9 ] ) is essential for deriving the tight exponential bound .", "next , we consider a function @xmath9 from @xmath10 to @xmath121 specialized to a given probability distribution @xmath7 . this problem is called intrinsic randomness , which was studied with general source by vembu and verd @xcite .", "hayashi@xcite discussed the relation between the second order asymptotic rate and central limit theorem . in the following , for the comparison with the exponential rate of decrease for ( [ 9 - 7 - 1 ] )", ", we derive the optimal exponential rate of decrease for a given rate generating uniform random number .", "the following lemma is useful for this purpose .", "any probability distribution @xmath7 and any function @xmath9 from @xmath10 to @xmath121 satisfy that @xmath122    when @xmath123 is not empty , @xmath124 thus , we obtain @xmath125 which implies ( [ 9 - 8 - 1 ] ) .", "concerning the opposite inequality , we consider the case when our distribution @xmath59 is given by the @xmath4-fold independent and identical distribution of @xmath60 , i.e , @xmath61 . in this setting", ", we have the following lemma .    for any probability distribution @xmath7 ,", "there exists a function @xmath126 from @xmath66 to @xmath127 such that @xmath128 where @xmath85 is the set of empirical distribution with @xmath4 trials , and @xmath129 is the set of data with the empirical distribution @xmath130 .    in the first step ,", "we define the function @xmath126 . in the second step ,", "we show that the function satisfies ( [ 9 - 9 - 1 ] ) .    using the integer @xmath131", ", we divide @xmath85 into three parts : @xmath132 the condition that @xmath133 is equivalent with the condition that @xmath134 for @xmath135 .", "hence , @xmath136 so , @xmath137 since @xmath138 we have @xmath139 therefore , we can choose @xmath140 on @xmath141 satisfying the following conditions .", "( 1 ) for @xmath142 , @xmath143 . ( 2 )", "@xmath144 injective for @xmath145 .", "( 3 ) @xmath146 for @xmath147 .", "further , we choose @xmath140 satisfying the additional condition .", "( 4 ) any type @xmath147 satisfies that @xmath148 for @xmath149 .", "then , for @xmath147 , we obtain @xmath150 next , we define @xmath126 on the whole set by modifying @xmath140 as follows .", "( 5 ) @xmath126 is the same as @xmath140 on @xmath151 .", "( 6 ) @xmath152 .    therefore , our remaining task is to evaluate the value @xmath153 , where @xmath154_+,\\\\ [ x]_+&:=\\left\\ { \\begin{array}{ll }   x & \\hbox { if } x \\ge 0 \\\\ 0 & \\hbox { if } x < 0 .", "\\end{array }   \\right.\\end{aligned}\\ ] ] then , ( [ 2 - 2 - 6 ] ) implies that @xmath155 for @xmath156 , ( [ 2 - 2 - 1 ] ) implies @xmath157 for @xmath158 , @xmath159    combining ( [ 2 - 2 - 2 ] ) , ( [ 2 - 2 - 3 ] ) , and ( [ 2 - 2 - 4 ] ) , we obtain @xmath160 which implies ( [ 9 - 9 - 1 ] ) .    using the above two lemmas , we obtain the following theorem .", "assume that @xmath161 .", "@xmath162 where the above minimum is taken among functions @xmath126 from @xmath163 to @xmath164", ".    combining lemma [ lem9 ] and theorems [ thm2 ] and [ t9 - 9 - 1 ] , we can compare the performances between random universal protocol and specialized protocol .", "so , our exponential rate of decrease for the protocol based on universal@xmath3 hash functions is slightly smaller than the optimal exponential rate of decrease for specialized protocols .    using cramer s theorem@xcite", ", we obtain @xmath165 further , @xmath166 so , the remaining task is show that @xmath167 which can be shown from lemma [ l9 - 9 - 1 ] .", "( both cases in lemma [ l9 - 9 - 1 ] are required for this derivation . )", "when @xmath161 , @xmath168 when @xmath169 , @xmath170    its proof is given in appendix [ al9 - 9 - 1 ] .", "next , we consider the secure key generation problem from a common random number @xmath6 which has been partially eavesdropped on by eve . for this problem , it is assumed that alice and bob share a common random number @xmath6 , and eve has another random number @xmath171 , which is correlated to the random number @xmath172 .", "the task is to extract a common random number @xmath173 from the random number @xmath6 , which is almost independent of eve s random number @xmath171 . here", ", alice and bob are only allowed to apply the same function @xmath9 to the common random number @xmath6 .", "then , when the initial random variables @xmath174 and @xmath175 obey the distribution @xmath176 , eve s distinguishability can be represented by the following value : @xmath177 where @xmath178 is the product distribution of both marginal distributions @xmath179 and @xmath180 , and @xmath181 is the uniform distribution on @xmath121 . while the half of this value directly gives the probaility that eve can distingushes the alice s information , we call @xmath182 eve s distinguishability in the following .", "this criterion was proposed by @xcite and was used by @xcite . since the half of this quantity @xmath182 is closely related to universally composable security , we adopt it as the secrecy criterion in this paper . as another criterion , we sometimes treat @xmath183 since @xmath184 , we have @xmath185 further , @xmath179 is the uniform distribution , the above both criteria coincide with each other .    in order to evaluate the average performance", ", we define the quantity @xmath186 thus , using theorem [ thm1 ] and putting @xmath187 , we obtain the inequality : @xmath188 for @xmath189 and any universal@xmath3 hash function @xmath33 . note", "that when eve s random variable @xmath175 takes a continuous value in the set @xmath190 , the relation ( [ 4 - 25 - 1 ] ) holds by defining @xmath191 in the following way .", "@xmath192    next , we consider the case when our distribution @xmath193 is given by the @xmath4-fold independent and identical distribution of @xmath194 , i.e , @xmath195 .", "ahlswede and csiszr @xcite showed that the optimal generation rate @xmath196 equals the conditional entropy @xmath197 .", "that is , the generation rate @xmath198 is smaller than @xmath197 .", "the quantity @xmath199 goes to zero . in order to treat the speed of this convergence , we focus on the supremum of the _ exponential rate of decrease ( exponent ) _ for @xmath199 for a given @xmath63 @xmath200", "since the relation @xmath201 holds , the inequality ( [ 4 - 25 - 1 ] ) implies that @xmath202 when we apply pinsker inequality to the lower bound given in ( 5 ) of @xcite , we obtain @xmath203 where we define the function @xmath204 for @xmath2 $ ] . since @xmath205 , the right hand sides of ( [ 4 - 16 - 4 ] ) and ( [ 2 - 3 - 1 ] ) are strictly greater than @xmath15 for @xmath206 . concerning the comparison of both bounds , the following lemma holds .", "@xmath207    for @xmath206 .", "for example , we consider the following case : @xmath208 equals @xmath190 , the set @xmath208 has the module structure , and the conditional distribution @xmath209 has the form @xmath210 . then , both bounds can be simplified to @xmath211 in particular , the both exponents are numerically plotted in fig .", "[ f3 ] when @xmath212 , and @xmath213 , @xmath214 .", "we choose @xmath215 .", "then , applying jensen inequality to the concave function @xmath216 , we have @xmath217 where @xmath218 .", "thus , obtain ( [ 2 - 4 - 2 ] ) .", "next , we consider the wire - tap channel model , in which the eavesdropper ( wire - tapper ) , eve and the authorized receiver bob receive information from the authorized sender alice . in this case , in order for eve to have less information , alice chooses a suitable encoding .", "this problem is formulated as follows .", "let @xmath219 and @xmath220 be the probability spaces of bob and eve , and @xmath221 be the set of alphabets sent by alice .", "then , the main channel from alice to bob is described by @xmath222 , and the wire - tapper channel from alice to eve is described by @xmath223 . in this setting , in order to send the secret message in @xmath121 subject to the uniform distribution , alice chooses @xmath32 distributions @xmath224 on @xmath221 , and she generates @xmath225 subject to @xmath226 when she wants to send the message @xmath227 .", "bob prepares @xmath32 disjoint subsets @xmath228 of @xmath219 and judges that a message is @xmath30 if @xmath229 belongs to @xmath230 .", "therefore , the triplet @xmath231 is called a code , and is described by @xmath232 .", "its performance is given by the following three quantities .", "the first is the size @xmath32 , which is denoted by @xmath233 .", "the second is the average error probability @xmath234 : @xmath235 and the third is eve s distinguishability @xmath236 : @xmath237 ) \\\\", "w^e_{\\phi}(e )   : = & \\sum_i \\frac{1}{m } w_{q_i}^e(e ) , \\quad w^e[\\phi](i , e )   : = \\frac{1}{m } w_{q_i}^e(e).\\end{aligned}\\ ] ] the quantity @xmath236 gives an upper bound for the probability that eve can succeed in distinguishing whether alice s information belongs to a given subset .", "so , the value can be regarded as eve s distinguishability . in order to calculate these values", ", we introduce the following quantities .", "@xmath238 where @xmath239 .", "the following lemma gives the property of @xmath240 .", "when the random variable @xmath241 takes a continuous value in the set @xmath242 while @xmath243 takes discrete value , the above definition can be changed to @xmath244 these definitions do not depend on the choice of the measure on @xmath242 . that is , when @xmath245 for a positive function @xmath9 , @xmath246    the function @xmath247 is convex for @xmath248 $ ] , and is concave for @xmath249 $ ] .", "see lemma 1 of hayashi @xcite .    then , we obtain the following theorem .", "there exists a code @xmath232 for any integers @xmath250 , and any probability distribution @xmath251 on @xmath221 such that @xmath252 and @xmath253    we can construct the code satisfying the above conditions by the same way as section iii of @xcite .", "this theorem can be shown by replacing the concavity of @xmath254 and ( 3 ) in @xcite by the concavity of @xmath255 and the inequality ( [ 4 - 25 - 1 ] ) , respectively if we take care of the following point .", "when @xmath251 is the uniform distribution on the set @xmath256 and the joint distribution @xmath257 is given by @xmath258 , the equations @xmath259 hold . in the discussion in section iii of @xcite ,", "the main point is ( 12 ) of @xcite , which is essentially equivalent with ( [ 12 - 26 - 1-a ] ) . in the proof of theorem", "[ 3 - 6 ] , we need to modify ( 12 ) of @xcite by using ( [ 12 - 26 - 1 ] ) .    in the @xmath4-fold discrete memoryless channels @xmath260 and @xmath261 of the channels @xmath262 and @xmath263 , the additive equation @xmath264 holds .", "thus , there exists a code @xmath265 for any integers @xmath266 , and any probability distribution @xmath251 on @xmath221 such that @xmath267 and @xmath268 since @xmath269 , the rate @xmath270 can be asymptotically attained .", "therefore , when the sacrifice information rate is @xmath63 , i.e. , @xmath271 , the exponential rate for decrease of eve s distinguishability is greater than @xmath272 .    in the @xmath4-fold discrete memoryless channels @xmath260 and @xmath261 of the channels @xmath262 and @xmath263 , the additive equation @xmath264 holds .", "thus , there exists a code @xmath265 for any integers @xmath266 , and any probability distribution @xmath251 on @xmath221 such that @xmath267 and @xmath268 since @xmath269 , the rate @xmath270 can be asymptotically attained .", "therefore , when the sacrifice information rate is @xmath63 , i.e. , @xmath271 , the exponential rate of decrease for eve s distinguishability is greater than @xmath272 .", "now , we compare the lower obtained bound @xmath273 for the exponential rate of decrease for eve s distinguishability with existing lower bounds @xcite .", "as existing lower bounds , hayashi @xcite derived a lower bound of this exponential rate of decrease @xmath274 .", "hayashi @xcite also derived a lower bound @xmath275 of this exponential rate of decrease for the mutual information .", "application of pinsker inequality to this bound yields the bound @xmath276 , which is smaller than our lower bound @xmath277 because @xmath278 for @xmath51 .    in the following ,", "we compare the two bounds @xmath273 and @xmath277 for this purpose , we treat @xmath279 and @xmath280 for @xmath189 .", "reverse hlder inequality @xcite with the measurable space @xmath281 is given as @xmath282 for @xmath283 . using this inequality ,", "we obtain @xmath284 w_p(y)^{-s } \\\\ \\ge &   \\left (   \\sum_y   \\left [ \\sum_x p(x ) ( w_x(y))^{1+s } \\right]^{\\frac{1}{1+s } } \\right)^{1+s } \\cdot   \\left ( \\sum_y   w_p(y)^{-s\\cdot -\\frac{1}{s } } \\right)^{-s } \\\\ = & \\left (   \\sum_y   \\left [ \\sum_x p(x ) ( w_x(y))^{1+s } \\right]^{\\frac{1}{1+s } } \\right)^{1+s}.\\end{aligned}\\ ] ] substituting @xmath285 , we obtain @xmath286 w_p(y)^{\\frac{-t}{1-t } } \\\\ \\ge & \\left (   \\sum_y   \\left [ \\sum_x p(x ) ( w_x(y))^{\\frac{1}{1-t } } \\right]^{1-t } \\right)^{\\frac{1}{1-t}},\\end{aligned}\\ ] ] which implies @xmath287 w_p(y)^{\\frac{-t}{1-t } } \\right)^{1-t } \\\\ \\ge & \\sum_y   \\left [ \\sum_x p(x ) ( w_x(y))^{\\frac{1}{1-t } } \\right]^{1-t } = e^{\\phi(t|w^e , p)}.\\end{aligned}\\ ] ] thus , our bound @xmath273 for the exponential rate of decrease is better than the existing bound @xmath277 @xcite .", "assume that @xmath288 .", "we consider the following channel .", "@xmath289 when @xmath290 , @xmath291 then , the three bounds @xmath292 , @xmath293 , and @xmath294 with @xmath295 are numerically compared as in fig .", "[ f4 ] .", "next , we consider a more specific case .", "when @xmath296 and @xmath297 is a module and @xmath298 , the channel @xmath299 is called _", "any additive channel @xmath263 satisfies @xmath300 for the uniform distribution @xmath301 on @xmath297 because @xmath302 we consider a more general case .", "eve is assumed to have two random variables @xmath303 and @xmath304 .", "the first random variable @xmath305 is the output of an additive channel depending on the second variable @xmath304 .", "that is , the channel @xmath306 can be written as @xmath307 , where @xmath308 is a joint distribution .", "hereinafter , this channel model is called a general additive channel .", "this channel is also called a regular channel@xcite . for this channel model , the equalities @xmath309", "hold because @xmath310", "furthermore , in a practical sense , we need to take into account the decoding time . for this purpose , we often restrict our codes to linear codes . in the following ,", "we consider the case where the sender s space @xmath221 has the structure of a module . here", ", we should remark that hayashi @xcite considered the completely random ensemble of codes .", "he did not consider universal@xmath3 condition .", "so , we can not derive the following discussion from his result @xcite while his bound for exponential rate is the same as our bound when eve s channel is additive or general additive .", "such a required code can be constructed by the same discussion as section v of @xcite . when we generate codes @xmath232 by the same method as in @xcite ,", "the average of @xmath236 is bounded by @xmath311 .", "this result can be applied to one - way secret key distillation by the same way as section vi of @xcite . in this problem , alice , bob , and eve", "are assumed to have initial random variables @xmath312 , @xmath313 , and @xmath314 , respectively . the task for alice and bob is to share a common random variable almost independent of eve s random variable @xmath315 by using a public communication . for this purpose", ", we assume that alice and bob can perform local data processing in the both sides and alice can send messages to bob via public channel .", "that is , only one - way communication is allowed .", "we call such a combination of these operations a code and denote it by @xmath232 .", "the quality is evaluated by three quantities : the size of the final common random variable , the probability that their final variables coincide , and eve s distinguishability @xmath236 of the final joint distribution between alice and eve .", "then , we can construct the same code @xmath232 as given in section vi of@xcite for any numbers @xmath32 and @xmath316 .", "it follows from the similar discussion that this code satisfies that @xmath317 and @xmath318", "we have derived the tight evaluation for exponent for the average of the @xmath0 norm distance between the generated random number and the uniform random number when universal@xmath3 hash function is applied and the key generation rate is less than the critical rate @xmath319 . using this evaluation ,", "we have obtained an upper bound for eve s distinguishability in secret key generation from a common random number without communication when a universal@xmath3 hash function is applied . since our bound is based on the rnyi entropy of order @xmath1 for @xmath2 $ ] , it can be regarded as an extension of bennett et al @xcite s result with the rnyi entropy of order 2 .    applying this bound to the wire - tap channel , we obtain an upper bound for eve s distinguishability , which yields an exponential upper bound .", "this exponent improves on the existing exponent @xcite .", "further , when the error correction code is given by a linear code and when the channel is additive or general additive , the privacy amplification is given by a concatenation of toeplitz matrix and the identity matrix .", "this method can be applied to secret key distillation with public communication .", "the author is grateful to professors ryutaroh matsumoto and takeshi koshiba for a helpful comments .", "this research was partially supported by a mext grant - in - aid for young scientists ( a ) no .", "the centre for quantum technologies is funded by the singapore ministry of education and the national research foundation as part of the research centres of excellence programme .", "first , for a fixed element @xmath320 , we consider the condition @xmath321 we denote the probability that this condition holds and the expectation under this condition by @xmath322 and @xmath323 .", "then , @xmath324 and @xmath325      now , using the function @xmath342 , we make a code for the wire - tap channel based on the random coding method . for this purpose", ", we make a protocol to share a random number .", "first , we generate the random code @xmath343 with size @xmath344 , which is described by the @xmath344 independent and identical random variables @xmath345 subject to the distribution @xmath251 on @xmath221 . for integers @xmath346 and @xmath347 , let @xmath348 be the maximum likelihood decoder of the code @xmath343 .", "gallager @xcite showed that the ensemble expectation of the average error probability concerning decoding the input message @xmath174 is less than @xmath349 for @xmath350 . after sending the random variable @xmath174 taking values in the set with the cardinality @xmath351 ,", "alice and bob apply the above universal@xmath3 function @xmath23 to the random variable @xmath174 and generate another piece of data of size @xmath32 . here", ", we assume that the ensemble @xmath352 satisfies condition [ c12 ]", ". then , alice and bob share random variable @xmath353 with size @xmath32 .", "this protocol is denoted by @xmath354    let @xmath175 be the random variable of the output of eve s channel @xmath263 . for simplicity , we simplify the uniform distribution @xmath355 on the message space @xmath356 and the distribution @xmath357 on @xmath221 by @xmath358 and @xmath359 .", "therefore , since the random variable @xmath174 obeys the uniform distribution @xmath358 , the joint distribution @xmath360 concerning @xmath174 and @xmath175 satisfies @xmath361 for a given code @xmath343 , we apply the inequality ( [ 4 - 25 - 1 ] ) to eve s distinguishability . then , @xmath362 for @xmath363 .", "the concavity of @xmath279 ( lemma [ l1 ] ) guarantees that @xmath364 for @xmath363 .", "now , we make a code for wire - tap channel by modifying the above protocol @xmath354 .", "first , we choose the distribution @xmath226 to be the uniform distribution on @xmath29 .", "when alice wants to send the secret message @xmath30 , before sending the random variable @xmath174 , alice generates the random number @xmath174 subject to the distribution @xmath226 .", "alice sends the random variable @xmath174 .", "bob recovers the random variable @xmath174 , and applies the function @xmath23 .", "then , bob decodes alice s message @xmath30 , and this code for wire - tap channel @xmath365 is denoted by @xmath366 . since the ensemble @xmath352 satisfies condition [ c12 ] and the secret message @xmath30 obeys the uniform distribution on @xmath367 , this protocol @xmath366 has the same performance as the above protocol @xmath354 .    finally , we consider what code is derived from the above random coding discussion .", "using the markov inequality , we obtain @xmath368 therefore , the existence of a good code is guaranteed in the following way .", "that is , we give the concrete performance of a code whose existence is shown in the above random coding method .", "we choose @xmath326 such that @xmath327 , where @xmath328 .", "when @xmath130 satisfies @xmath329 , @xmath330 hence , @xmath331 the last equation follows from the concavity of @xmath332 concerning @xmath333 .", "assume that @xmath161 .", "then , @xmath334 . when @xmath335 , @xmath336 which implies ( [ 9 - 9 - 4 ] ) .", "assume that @xmath169 .", "when @xmath335 , @xmath337 further , when @xmath338 , @xmath339 which implies ( [ 9 - 9 - 5 ] ) .", "further , the concavity of @xmath340 and the condition @xmath169 imply that @xmath341 .", "thus , we obtain ( [ 2 - 2 - 8 ] ) .", "first , we regard a submodule @xmath369 as an encoding for the usual sent message , and focus on its decoding @xmath370 by the authorized receiver .", "we construct a code for a wire - tap channel @xmath371}\\}_{[x]\\in c_1/c_2 } , \\{\\cd_{[x]}\\}_{[x]\\in c_1/c_2})$ ] based on a submodule @xmath372 of @xmath373 as follows .", "the encoding @xmath374}$ ] is given as the uniform distribution on the coset @xmath375:=x+c_2 $ ] , and the decoding @xmath376}$ ] is given as the subset @xmath377 .", "next , we assume that a submodule @xmath378 of @xmath373 with cardinality @xmath379 is generated by a random variable @xmath25 satisfying the following condition .", "any element @xmath380 is included in @xmath378 with probability at most @xmath381 .", "then , the performance of the constructed code is evaluated by the following theorem .", "choose the subcode @xmath378 according to condition [ c3 ] .", "we construct the code @xmath382 by choosing the distribution @xmath374}$ ] to be the uniform distribution on @xmath375 $ ] for @xmath375\\in c_1/c_2({{\\bf x}})$ ] .", "then , we obtain @xmath383 where @xmath384 is the uniform distribution on the subset @xmath385 .    this inequality can be shown by ( [ 4 - 25 - 1 ] ) as follows .", "now , we define the joint distribution @xmath386 .", "the choice of @xmath374}$ ] corresponds to a hashing operation satisfying condition 1 .", "then , ( [ 4 - 25 - 1 ] ) yields that @xmath387 is bounded by @xmath388 , which implies ( [ 4 - 27 - 1 ] ) .    next , we consider a special class of channels .", "when the channel @xmath263 is additive , i.e. , @xmath389 , the equation @xmath390 holds for any @xmath391 .", "thus , the concavity of @xmath279 ( lemma [ l1 ] ) implies that @xmath392 thus , combining ( [ 4 - 27 - 1 ] ) , ( [ 2 - 13 - 1 ] ) , and ( [ 12 - 26 - 2 ] ) , we obtain @xmath393 for @xmath394 .", "similarly , when the channel @xmath263 is general additive , i.e. , @xmath395 , combining ( [ 4 - 27 - 1 ] ) , ( [ 2 - 13 - 1 ] ) , and ( [ 5 - 14 - 1 ] ) , we obtain @xmath396 for @xmath394 .    in the following discussion , we assume that @xmath221 is an @xmath4-dimensional vector space @xmath397 over the finite field @xmath36 . in this assumption , as is mentioned in @xcite , the bound can be attained by combination of linear code and the concatenation of toeplitz matrix and the identity @xmath34 of the size @xmath398 .", "next , we explain the detail of construction of one - way secret key distillation protocol . first , alice generates another uniform random variable @xmath391 and sends the random variable @xmath399 .", "then , the distribution of the random variables @xmath400 ( @xmath401 ) accessible to bob ( eve ) can be regarded as the output distribution of the channel @xmath402 ( @xmath403 ) .", "the channels @xmath404 and @xmath405 are given as follows .", "@xmath406 where @xmath407 ( @xmath408 ) is the joint probability between alice s initial random variable @xmath172 and bob s ( eve s ) initial random variable @xmath409 ( @xmath315 ) .", "hence , the channel @xmath263 is general additive .      in particular , when @xmath221 is an @xmath4-dimensional vector space @xmath397 over the finite field @xmath36 and the joint distribution between @xmath174 and @xmath413(@xmath175 ) is the @xmath4-fold independent and identical distribution ( i.i.d . ) of @xmath414 ( @xmath176 ) , respectively , the relations @xmath415 and @xmath416 hold .", "thus , there exists a code @xmath265 for any integers @xmath266 , and any probability distribution @xmath251 on @xmath221 such that @xmath267 and @xmath417 hence , the achievable rate of this protocol is equal to @xmath418 which was obtained by maurer@xcite and ahlswede - csiszr@xcite . here", ", since the channels @xmath262 and @xmath263 can be regarded as general additive , we can apply the discussion in section [ s3 ] .", "that is , the bound ( [ 2 - 13 - 2 ] ) can be attained with the combination of a linear code and random privacy amplification , which is given in section [ s3 ] .", "u. maurer and s. wolf , `` infromation - theoretic key agreement : from weak to strong secrecy for free , '' _ advances in cryptology  eurocrypt 2000 _ , lecture notes in computer science , vol.1807 , pp.351 - 368 , springer - verlag ( 2000 ) .", "r. renner and s. wolf , `` simple and tight bounds for information reconciliation and privacy amplification , '' _ asiacrypt 2005 _ , lecture notes in computer science , springer - verlag , vol .", "3788 , pp .", "199 - 216 , 2005 .", "s. watanabe , t. saitou , r. matsumoto , t. uyematsu `` strongly secure privacy amplification can not be obtained by encoder of slepian - wolf code , '' _ proceedings of the 2009 ieee international symposium on information theory _ , volume 2 , seoul , korea , pp . 1298 - 1302 ( 2009 ) ( arxiv:0906.2582 )            a. winter , a. c. a. nascimento , and h. imai , `` commitment capacity of discrete memoryless channels , '' _ proc .", "9th cirencester crypto and coding conf .", "_ , lncs 2989 , pp 35 - 51 , springer , berlin 2003 ; cs.cr/0304014 ( 2003 )    m. hayashi , `` general non - asymptotic and asymptotic formulas in channel resolvability and identification capacity and its application to wire - tap channel , '' _ ieee trans .", "inform . theory _", ", vol . * 52*(4 ) , 15621575 , 2006 .", "kuptsov , `` holder inequality '' , in hazewinkel , michiel , encyclopaedia of mathematics , springer , ( 2001 ) .", "t. holenstein and r. renner , `` on the randomness of independent experiments , '' arxiv : cs/0608007 ( 2006 )"], "abstract_text": ["<S> we adopt @xmath0 distance as eve s distinguishability in secret key generation from a common random number without communication . under this secrecy criterion , using the rnyi entropy of order @xmath1 for @xmath2 $ ] , we derive a new upper bound of eve s distinguishability under the application of the universal@xmath3 hash function . </S>", "<S> it is also shown that this bound gives the tight exponential rate of decrease in the independent and identical distribution . </S>", "<S> the result is applied to wire - tap channel model and secret key distillation ( agreement ) by public discussion .    </S>", "<S> sacrifice bits , @xmath0 norm distance , secret key distillation , universal hash function , wire - tap channel </S>"], "labels": null, "section_names": ["introduction", "uniform random number generation", "specialized protocol for uniform random number generation", "secret key generation without communication", "the wire-tap channel in a general framework", "comparison with existing bound", "further applications", "discussion", "acknowledgments", "proof of theorem", "proof of lemma", "construction of linear codes", "application to one-way secret key distillation"], "sections": [["random privacy amplification based on the universal@xmath3 condition@xcite has been studied by many authors@xcite .", "this technique is originally aimed for random number extraction@xcite .", "it can be applied to secret key distillation ( agreement ) with public communication@xcite and wire - tap channel@xcite , which treats the secure communication in the presence of an eavesdropper .", "( for details of its application , see e.g. hayashi @xcite . ) when random privacy amplification is implemented by a universal@xmath3 hash function , it can yield protocols for the above tasks with a relatively small amount of calculation . similar to the study @xcite for random privacy amplification based on the universal@xmath3 condition , hayashi@xcite focused only on the mutual information with the eavesdropper .", "however , as the secrecy criterion , many papers in cryptography community @xcite dopt the half of the @xmath0 norm distance , which is also called the variation distance or eve s distinguishability .", "because this criterion is closely related to universally composable security @xcite , it is required to evaluate the leaked information based on the @xmath0 norm distance from cryptography community viewpoint .    in this paper , we adopt the @xmath0 norm distance as the secrecy criterion , and evaluate the secrecy for random privacy amplification while hayashi@xcite adopts the mutual information criterion . in the independent and identical distributed case , when the rate of generating random numbers is smaller than the entropy of the original information source , it is possible to generate the random variable whose @xmath0 norm distance to the uniform random number approaches zero asymptotically", ". however , in the real setting , we can manipulate only a finite size of random variables .", "so , the speed of this convergence is very important . in the community of information theory , in order to discuss the speed , we often focus on the the exponential rate of decrease .", "this rate is called the exponent , and is widely discussed among several topics in information theory , e.g. , channel coding@xcite , source coding@xcite , and mutual information criterion in wire - tap channel@xcite .", "however , the exponent has not been discussed in the community of cryptography as an important criterion .", "the purpose of this paper is establishing a systematic evaluating method for exponent for the @xmath0 norm distance in secure protocols .    in subsection [ s2a ] , for a given generating rate and a given source distribution , in the i.i.d .", "setting , we derive a lower bound of the exponent of the average of the @xmath0 norm distance between the generated random number and the uniform random number when universal@xmath3 hash function is applied", ". then , we introduce a stronger condition for hash functions , which is called strongly universal@xmath3 function , and show that our lower bound is tight under this condition . since our bound realizes the optimal exponent , it is thought to be powerful even for the finite length setting .", "we consider the @xmath4-independent and identical extension , and show that the exponential rate of decrease for this bound is tight under a stronger condition by using the type method@xcite . however , if our protocol generating the random number depends on the original distribution , there is a possibility to improve the exponent while it is known that asymptotic generation can not be improved@xcite . in subsection", "[ s3 ] , we derive the optimal exponent in this setting by using the cramer s theorem @xcite and the type method @xcite . comparing these two exponents , we can compare the performances between the protocol based on universal@xmath3 hash function and the protocol depending on the information source .    in information theory ,", "the part constructing the protocol achieving the rate is called the direct part . and", "the proof for non - existence surpassing the rate is called the converse part . concerning the exponent of the @xmath0 norm distance with universal@xmath3 hash function , the direct part is relatively simple while the converse part is more complicated .", "the converse part is regarded as an important topic in information theory , while it has been regarded as a minor topic in other research areas .", "in the direct part , we focus on bennett et al@xcite s evaluation for random privacy amplification , which employs the rnyi entropy of order 2 .", "this evaluation was also obtained by hastad et al @xcite and is often called leftover hash lemma . using the same discussion as renner @xcite", ", we derive an upper bound for the @xmath0 norm distance .", "renner @xcite applied @xmath5 approximation idea ( smoothing ) to min - entropy , which bounds the rnyi entropy of order 2 . in this paper , we apply the same idea to the rnyi entropy of order 2 .", "however , for the evaluation of the smoothing error , we use a method different from the method invented by holenstein - renner@xcite , but our method is standard in large deviation theory@xcite .", "in fact , as is shown in subsection [ s2b ] , our evaluation is much better than that by holenstein - renner@xcite .", "then , we obtain a bound based on the rnyi entropy of order @xmath1 as the main theorem ( theorem [ thm1 ] ) .", "note that , hayashi@xcite obtained a similar result using the rnyi entropy of order @xmath1 , but it evaluates the shannon entropy not the @xmath0 norm distance .", "contrastly , in the converse part , we employ type method , which was invented by csiszr and krner @xcite .", "the discussion of this part is quite technical , but conventional in information theory . concerning the exponent of the protocol depending on the original distribution ,", "the direct part can be shown by the combination of a new invented lemma ( lemma [ lemdi ] ) and cramer s theorem in large deviation theory@xcite .", "the converse part can be shown by very technical application of the type method@xcite .    in section [ s42 ] , we consider the case when an eavesdropper has a random variable correlated to the random variable of the authorized user . in this case , when the authorized user applies universal@xmath3 hash function to his random variable , he obtain a secure random variable .", "when we apply theorem [ thm1 ] to the security by @xmath0 norm distance in this setting , we obtain a tighter evaluation than existing evaluation than that directly obtained from hayashi@xcite .    in section [ s2 ] , we focus on wire - tap channel model , whose capacity has been calculated by wyner @xcite and csiszr and krner @xcite .", "csiszr @xcite showed the strong security , and hayashi @xcite derived bounds for both exponential rates of decrease for the mutual information between alice and eve and the security criterion based on the @xmath0 norm distance , which is simply called the @xmath0 security criterion .", "he obtained a bound for the exponential rate of decrease concerning the @xmath0 security criterion .", "however , their approaches are based on completely random coding . using the method of information spectrum@xcite ,", "hayashi @xcite showed strong security when universal@xmath3 hash function is applied in the privacy amplification process of wire - tap channel .", "but , the analysis on @xcite was based on the mutual information not the @xmath0 security criterion . in the paper@xcite , he invented a systematic method to construct a wire - tap channel code based on universal@xmath3 hash function , and evaluated the performance based on a privacy amplification theorem concerning the shannon entropy given in @xcite .", "so , we can expect that theorem [ thm1 ] plays the same role in evaluation of the @xmath0 security criterion in wire - tap channel model as a privacy amplification theorem concerning the shannon entropy given in @xcite . in this paper", ", we apply theorem [ thm1 ] to wire - tap channel model , and obtain the evaluation of the exponent of the @xmath0 security criterion . in fact", ", we can evaluate the exponent of the @xmath0 security criterion from the evaluation for exponent concerning the mutual information in @xcite by applying the pinsker inequality to the mutual information .", "it is also shown in section [ s6 - 1 ] , that the evaluation obtained in this paper is better than that by @xcite and that obtained by application of pinsker inequality .    in section [ s6 ] , the same discussion as sections v and vi in @xcite", "can be applied to the @xmath0 security criterion with a suitable modification . in the additive noise case ,", "the obtained bound can be attained by linear operation .", "further , we obtain the bound for the @xmath0 security criterion in one - way secret key distillation by the same discussion as sections vi in @xcite . in appendix [ as2 ] , we prove theorem [ thm2 ] mentioned in subsection [ s2a ] . in appendix", "[ al9 - 9 - 1 ] , we prove lemma [ l9 - 9 - 1 ] given in subsection [ s3 ] ."], ["firstly , we consider the uniform random number generation problem from a biased random number @xmath6 , which obeys a probability distribution @xmath7 when its cardinality @xmath8 is finite .", "there are two types protocols for this problem .", "one is a protocol specialized for the given distribution @xmath7 .", "the other is a universal protocol that does not depends on the given distribution @xmath7 .", "the aim of this section is evaluate the performance of the latter setting . in the latter setting , our protocol is given by a function @xmath9 from @xmath10 to @xmath11 . in order to evaluate the protocol @xmath9 , we use the @xmath0 distance ( the variational distance ) @xmath12 and the @xmath13 distance @xmath14 these definitions can be extended when the total measure is less than @xmath15 i.e. , @xmath16 . in the following , we call such @xmath7 a sub - distribution .", "this extension for sub distributions is crucial for the later discussion .", "then , the quality of the random number obeying the distribution @xmath7 is evaluated by @xmath17 where @xmath18 is the uniform distribution on @xmath10 .", "we also use the quantitty @xmath19 and the rnyi entropy order @xmath1 : @xmath20 the @xmath13 distance is written by using the rnyi entropy order @xmath21 as follows .", "@xmath22    now , we focus on an ensemble of the functions @xmath23 from @xmath10 to @xmath24 , where @xmath25 denotes a random variable describing the stochastic behavior of the function @xmath23 .", "an ensemble of the functions @xmath23 is called universal@xmath3 hash when it satisfies the following condition@xcite :    @xmath26 , the collision probability that @xmath27 is at most @xmath28 .", "we sometimes require the following additional condition :    for any @xmath25 , the cardinality of @xmath29 does not depend on @xmath30 .", "this condition will be used in section iii .", "indeed , when the cardinality @xmath8 is a power of a prime power @xmath31 and @xmath32 is another power of the same prime power @xmath31 , as is shown in appendix ii of hayashi @xcite , the ensemble @xmath33 can be given by the concatenation of toeplitz matrix and the identity @xmath34@xcite only with @xmath35 random variables taking values in the finite field @xmath36 .", "that is , the function can be obtained by the multiplication of the random matrix @xmath34 taking values in @xmath36 . in this case ,", "condition [ c12 ] can be confirmed because the rank of @xmath34 is constant . for condition [ c1 ] ,", "see appendix ii of hayashi @xcite .", "bennett et al@xcite essentially showed the following lemma .", "a universal@xmath3 function @xmath23 satisfies @xmath37    this was also shown by hastad et al @xcite and is often called leftover hash lemma .", "now , we follow the derivation of theorem 5.5.1 of renner @xcite when one classical random variable is given .", "the schwarz - inequality implies that @xmath38 the jensen inequality yields that @xmath39 substituting ( [ 5 - 14 - 3 ] ) and ( [ 5 - 14 - 4 ] ) into the above inequality , we obtain @xmath40    using ( [ 5 - 14 - 6 ] ) , we can show the following theorem as a generalization of ( [ 5 - 14 - 6 ] ) .", "a universal@xmath3 function @xmath23 satisfies @xmath41    substituting @xmath42 , we obtain @xmath43 since the difference between ( [ 5 - 14 - 6 ] ) and ( [ 5 - 14 - 7 ] ) is only the coefficient , theorem [ thm1 ] can be regarded as a kind of generalization of bennett et al@xcite s result ( [ 5 - 14 - 4 ] ) .", "for any @xmath44 , we choose subset @xmath45 , and define the subdistribution @xmath46 by @xmath47 since @xmath48 , @xmath49 the inequality ( [ 5 - 14 - 6 ] ) yields @xmath50 for @xmath51 , we can evaluate @xmath52 and @xmath53 as @xmath54 combining ( [ 5 - 14 - 10 ] ) , ( [ 5 - 14 - 8 ] ) , and ( [ 5 - 14 - 9 ] ) , for @xmath55 , we obtain @xmath56 where we substitute @xmath57 into @xmath58 .", "next , we consider the case when our distribution @xmath59 is given by the @xmath4-fold independent and identical distribution of @xmath60 , i.e , @xmath61 . when the random number generation rate @xmath62 is @xmath63 , we focus on the _ exponential rate of decrease _ of @xmath64 , and consider the supremum .", "when an ensemble @xmath65 is a universal@xmath3 hash functions from @xmath66 to @xmath67 , theorem [ thm1 ] yields that @xmath68 on the other hand , when we apply the pinsker inequality@xcite to the upper bound for the mutual information obtained in hayashi@xcite , we obtain another bound @xmath69 , which is smaller than ( [ 4 - 19 - 2 ] ) .", "the right hand side of ( [ 4 - 19 - 2 ] ) is equal to the optimal exponential rate in the fixed - length source coding .", "the following lemma is known @xcite :    assume that @xmath60 is a probability distribution and the random number generation rate @xmath63 is greater than the critical rate @xmath70 , where @xmath71 .", "using kullback - leibler divergence @xmath72 , we have the following expression .", "@xmath73    here", "the last equation of ( [ 9 - 7 - 1 ] ) can be checked in the following way .", "@xmath74 is equal to the minimum exponent for correct decoding probability when the compression rate is @xmath63@xcite .", "this minimum exponent is equal to @xmath75@xcite .", "hence , we obtain the last equation .    in oder to show the tightness of the exponential rate decrease ( [ 9 - 7 - 1 ] ) under the universal@xmath3 condition , we consider the following ensemble .    for any @xmath76 , @xmath77 .", "the random variable @xmath78 is independent of @xmath79 for different arbitrary two elements @xmath80 .    under the strongly universal@xmath3 ensemble , and any subset @xmath81 with @xmath82 satisfies @xmath83    its proof is given in appendix [ as2 ] .", "using theorem [ thm2 ] and lemma [ lem9 ] with @xmath84 and applying the type method for fixed length source coding @xcite , we can show the following proposition :    let @xmath85 be the set of empirical distributions on @xmath10 with @xmath4 trials .", "assume the following conditions : @xmath86 .", "when @xmath87 , any sequence of strongly universal@xmath3 ensemble @xmath88 from @xmath66 to @xmath67 satisfies the equation @xmath89    when @xmath90 , applying the type method for fixed length source coding @xcite to the random variable @xmath91 , we obtain @xmath92 using theorem [ thm2 ] with @xmath93 , we obtain @xmath94 thus , @xmath95 in particular , when @xmath87 , the equations @xmath96 hold .", "since @xmath87 , lemma [ lem9 ] yields that @xmath89    therefore , we can conclude that our exponential rate of decrease is tight for the strongly universal@xmath3 condition when @xmath87 .", "in the above derivation , the key point is evaluating @xmath97 in the @xmath4-i.i.d .", "setting , i.e. , @xmath98 . in the community of cryptography , they often use holenstein - renner @xcite evaluation for @xmath53 in the @xmath4-i.i.d . setting .", "they proved the following theorem .    when @xmath99 , @xmath100 further , when @xmath101 and @xmath102 , @xmath103 when @xmath104 , the inequality yields the following evaluation .", "when @xmath105 , @xmath106 for even @xmath4 .", "our evaluation ( [ 5 - 14 - 9 ] ) of @xmath98 contains the parameter @xmath107 .", "since this parameter is arbitrary , it is natural to compare the upper bound @xmath108 given by ( [ 5 - 14 - 9 ] ) with that by theorem [ holenstein ] . that is , using ( [ 5 - 14 - 9 ] ) , we obtain the exponential evaluation @xmath109 while theorem [ holenstein ] yields that @xmath110 in this case , the upper bound is @xmath111 for @xmath101 and @xmath112 for @xmath113 .", "in fact , the probability @xmath97 is the key quantity in the method of information spectrum , which is a unified method in information theory@xcite . when the method of information spectrum is applied to the i.i.d .", "source , the probability @xmath97 is evaluated by applying cramr theorem ( see @xcite ) to the random variable @xmath114 .", "then , we obtain @xmath115 for @xmath116 . since @xmath117 is concave , when @xmath118 , the maximization can be attained in @xmath119 $ ] , i.e. ,", "@xmath120 which implies that our evaluation ( [ 5 - 14 - 9 ] ) gives the tight bound for exponential rate of decrease for the probability @xmath98 .", "in fact , the difference among these bounds is numerically given in fig . [ f2 ] . therefore , we can conclude that our evaluation ( [ 5 - 14 - 9 ] ) is much better than that by holenstein - renner @xcite .", "that is , combination of lemma 1 and ( [ 5 - 14 - 9 ] ) is essential for deriving the tight exponential bound ."], ["next , we consider a function @xmath9 from @xmath10 to @xmath121 specialized to a given probability distribution @xmath7 . this problem is called intrinsic randomness , which was studied with general source by vembu and verd @xcite .", "hayashi@xcite discussed the relation between the second order asymptotic rate and central limit theorem . in the following , for the comparison with the exponential rate of decrease for ( [ 9 - 7 - 1 ] )", ", we derive the optimal exponential rate of decrease for a given rate generating uniform random number .", "the following lemma is useful for this purpose .", "any probability distribution @xmath7 and any function @xmath9 from @xmath10 to @xmath121 satisfy that @xmath122    when @xmath123 is not empty , @xmath124 thus , we obtain @xmath125 which implies ( [ 9 - 8 - 1 ] ) .", "concerning the opposite inequality , we consider the case when our distribution @xmath59 is given by the @xmath4-fold independent and identical distribution of @xmath60 , i.e , @xmath61 . in this setting", ", we have the following lemma .    for any probability distribution @xmath7 ,", "there exists a function @xmath126 from @xmath66 to @xmath127 such that @xmath128 where @xmath85 is the set of empirical distribution with @xmath4 trials , and @xmath129 is the set of data with the empirical distribution @xmath130 .    in the first step ,", "we define the function @xmath126 . in the second step ,", "we show that the function satisfies ( [ 9 - 9 - 1 ] ) .    using the integer @xmath131", ", we divide @xmath85 into three parts : @xmath132 the condition that @xmath133 is equivalent with the condition that @xmath134 for @xmath135 .", "hence , @xmath136 so , @xmath137 since @xmath138 we have @xmath139 therefore , we can choose @xmath140 on @xmath141 satisfying the following conditions .", "( 1 ) for @xmath142 , @xmath143 . ( 2 )", "@xmath144 injective for @xmath145 .", "( 3 ) @xmath146 for @xmath147 .", "further , we choose @xmath140 satisfying the additional condition .", "( 4 ) any type @xmath147 satisfies that @xmath148 for @xmath149 .", "then , for @xmath147 , we obtain @xmath150 next , we define @xmath126 on the whole set by modifying @xmath140 as follows .", "( 5 ) @xmath126 is the same as @xmath140 on @xmath151 .", "( 6 ) @xmath152 .    therefore , our remaining task is to evaluate the value @xmath153 , where @xmath154_+,\\\\ [ x]_+&:=\\left\\ { \\begin{array}{ll }   x & \\hbox { if } x \\ge 0 \\\\ 0 & \\hbox { if } x < 0 .", "\\end{array }   \\right.\\end{aligned}\\ ] ] then , ( [ 2 - 2 - 6 ] ) implies that @xmath155 for @xmath156 , ( [ 2 - 2 - 1 ] ) implies @xmath157 for @xmath158 , @xmath159    combining ( [ 2 - 2 - 2 ] ) , ( [ 2 - 2 - 3 ] ) , and ( [ 2 - 2 - 4 ] ) , we obtain @xmath160 which implies ( [ 9 - 9 - 1 ] ) .    using the above two lemmas , we obtain the following theorem .", "assume that @xmath161 .", "@xmath162 where the above minimum is taken among functions @xmath126 from @xmath163 to @xmath164", ".    combining lemma [ lem9 ] and theorems [ thm2 ] and [ t9 - 9 - 1 ] , we can compare the performances between random universal protocol and specialized protocol .", "so , our exponential rate of decrease for the protocol based on universal@xmath3 hash functions is slightly smaller than the optimal exponential rate of decrease for specialized protocols .    using cramer s theorem@xcite", ", we obtain @xmath165 further , @xmath166 so , the remaining task is show that @xmath167 which can be shown from lemma [ l9 - 9 - 1 ] .", "( both cases in lemma [ l9 - 9 - 1 ] are required for this derivation . )", "when @xmath161 , @xmath168 when @xmath169 , @xmath170    its proof is given in appendix [ al9 - 9 - 1 ] ."], ["next , we consider the secure key generation problem from a common random number @xmath6 which has been partially eavesdropped on by eve . for this problem , it is assumed that alice and bob share a common random number @xmath6 , and eve has another random number @xmath171 , which is correlated to the random number @xmath172 .", "the task is to extract a common random number @xmath173 from the random number @xmath6 , which is almost independent of eve s random number @xmath171 . here", ", alice and bob are only allowed to apply the same function @xmath9 to the common random number @xmath6 .", "then , when the initial random variables @xmath174 and @xmath175 obey the distribution @xmath176 , eve s distinguishability can be represented by the following value : @xmath177 where @xmath178 is the product distribution of both marginal distributions @xmath179 and @xmath180 , and @xmath181 is the uniform distribution on @xmath121 . while the half of this value directly gives the probaility that eve can distingushes the alice s information , we call @xmath182 eve s distinguishability in the following .", "this criterion was proposed by @xcite and was used by @xcite . since the half of this quantity @xmath182 is closely related to universally composable security , we adopt it as the secrecy criterion in this paper . as another criterion , we sometimes treat @xmath183 since @xmath184 , we have @xmath185 further , @xmath179 is the uniform distribution , the above both criteria coincide with each other .    in order to evaluate the average performance", ", we define the quantity @xmath186 thus , using theorem [ thm1 ] and putting @xmath187 , we obtain the inequality : @xmath188 for @xmath189 and any universal@xmath3 hash function @xmath33 . note", "that when eve s random variable @xmath175 takes a continuous value in the set @xmath190 , the relation ( [ 4 - 25 - 1 ] ) holds by defining @xmath191 in the following way .", "@xmath192    next , we consider the case when our distribution @xmath193 is given by the @xmath4-fold independent and identical distribution of @xmath194 , i.e , @xmath195 .", "ahlswede and csiszr @xcite showed that the optimal generation rate @xmath196 equals the conditional entropy @xmath197 .", "that is , the generation rate @xmath198 is smaller than @xmath197 .", "the quantity @xmath199 goes to zero . in order to treat the speed of this convergence , we focus on the supremum of the _ exponential rate of decrease ( exponent ) _ for @xmath199 for a given @xmath63 @xmath200", "since the relation @xmath201 holds , the inequality ( [ 4 - 25 - 1 ] ) implies that @xmath202 when we apply pinsker inequality to the lower bound given in ( 5 ) of @xcite , we obtain @xmath203 where we define the function @xmath204 for @xmath2 $ ] . since @xmath205 , the right hand sides of ( [ 4 - 16 - 4 ] ) and ( [ 2 - 3 - 1 ] ) are strictly greater than @xmath15 for @xmath206 . concerning the comparison of both bounds , the following lemma holds .", "@xmath207    for @xmath206 .", "for example , we consider the following case : @xmath208 equals @xmath190 , the set @xmath208 has the module structure , and the conditional distribution @xmath209 has the form @xmath210 . then , both bounds can be simplified to @xmath211 in particular , the both exponents are numerically plotted in fig .", "[ f3 ] when @xmath212 , and @xmath213 , @xmath214 .", "we choose @xmath215 .", "then , applying jensen inequality to the concave function @xmath216 , we have @xmath217 where @xmath218 .", "thus , obtain ( [ 2 - 4 - 2 ] ) ."], ["next , we consider the wire - tap channel model , in which the eavesdropper ( wire - tapper ) , eve and the authorized receiver bob receive information from the authorized sender alice . in this case , in order for eve to have less information , alice chooses a suitable encoding .", "this problem is formulated as follows .", "let @xmath219 and @xmath220 be the probability spaces of bob and eve , and @xmath221 be the set of alphabets sent by alice .", "then , the main channel from alice to bob is described by @xmath222 , and the wire - tapper channel from alice to eve is described by @xmath223 . in this setting , in order to send the secret message in @xmath121 subject to the uniform distribution , alice chooses @xmath32 distributions @xmath224 on @xmath221 , and she generates @xmath225 subject to @xmath226 when she wants to send the message @xmath227 .", "bob prepares @xmath32 disjoint subsets @xmath228 of @xmath219 and judges that a message is @xmath30 if @xmath229 belongs to @xmath230 .", "therefore , the triplet @xmath231 is called a code , and is described by @xmath232 .", "its performance is given by the following three quantities .", "the first is the size @xmath32 , which is denoted by @xmath233 .", "the second is the average error probability @xmath234 : @xmath235 and the third is eve s distinguishability @xmath236 : @xmath237 ) \\\\", "w^e_{\\phi}(e )   : = & \\sum_i \\frac{1}{m } w_{q_i}^e(e ) , \\quad w^e[\\phi](i , e )   : = \\frac{1}{m } w_{q_i}^e(e).\\end{aligned}\\ ] ] the quantity @xmath236 gives an upper bound for the probability that eve can succeed in distinguishing whether alice s information belongs to a given subset .", "so , the value can be regarded as eve s distinguishability . in order to calculate these values", ", we introduce the following quantities .", "@xmath238 where @xmath239 .", "the following lemma gives the property of @xmath240 .", "when the random variable @xmath241 takes a continuous value in the set @xmath242 while @xmath243 takes discrete value , the above definition can be changed to @xmath244 these definitions do not depend on the choice of the measure on @xmath242 . that is , when @xmath245 for a positive function @xmath9 , @xmath246    the function @xmath247 is convex for @xmath248 $ ] , and is concave for @xmath249 $ ] .", "see lemma 1 of hayashi @xcite .    then , we obtain the following theorem .", "there exists a code @xmath232 for any integers @xmath250 , and any probability distribution @xmath251 on @xmath221 such that @xmath252 and @xmath253    we can construct the code satisfying the above conditions by the same way as section iii of @xcite .", "this theorem can be shown by replacing the concavity of @xmath254 and ( 3 ) in @xcite by the concavity of @xmath255 and the inequality ( [ 4 - 25 - 1 ] ) , respectively if we take care of the following point .", "when @xmath251 is the uniform distribution on the set @xmath256 and the joint distribution @xmath257 is given by @xmath258 , the equations @xmath259 hold . in the discussion in section iii of @xcite ,", "the main point is ( 12 ) of @xcite , which is essentially equivalent with ( [ 12 - 26 - 1-a ] ) . in the proof of theorem", "[ 3 - 6 ] , we need to modify ( 12 ) of @xcite by using ( [ 12 - 26 - 1 ] ) .    in the @xmath4-fold discrete memoryless channels @xmath260 and @xmath261 of the channels @xmath262 and @xmath263 , the additive equation @xmath264 holds .", "thus , there exists a code @xmath265 for any integers @xmath266 , and any probability distribution @xmath251 on @xmath221 such that @xmath267 and @xmath268 since @xmath269 , the rate @xmath270 can be asymptotically attained .", "therefore , when the sacrifice information rate is @xmath63 , i.e. , @xmath271 , the exponential rate for decrease of eve s distinguishability is greater than @xmath272 .    in the @xmath4-fold discrete memoryless channels @xmath260 and @xmath261 of the channels @xmath262 and @xmath263 , the additive equation @xmath264 holds .", "thus , there exists a code @xmath265 for any integers @xmath266 , and any probability distribution @xmath251 on @xmath221 such that @xmath267 and @xmath268 since @xmath269 , the rate @xmath270 can be asymptotically attained .", "therefore , when the sacrifice information rate is @xmath63 , i.e. , @xmath271 , the exponential rate of decrease for eve s distinguishability is greater than @xmath272 ."], ["now , we compare the lower obtained bound @xmath273 for the exponential rate of decrease for eve s distinguishability with existing lower bounds @xcite .", "as existing lower bounds , hayashi @xcite derived a lower bound of this exponential rate of decrease @xmath274 .", "hayashi @xcite also derived a lower bound @xmath275 of this exponential rate of decrease for the mutual information .", "application of pinsker inequality to this bound yields the bound @xmath276 , which is smaller than our lower bound @xmath277 because @xmath278 for @xmath51 .    in the following ,", "we compare the two bounds @xmath273 and @xmath277 for this purpose , we treat @xmath279 and @xmath280 for @xmath189 .", "reverse hlder inequality @xcite with the measurable space @xmath281 is given as @xmath282 for @xmath283 . using this inequality ,", "we obtain @xmath284 w_p(y)^{-s } \\\\ \\ge &   \\left (   \\sum_y   \\left [ \\sum_x p(x ) ( w_x(y))^{1+s } \\right]^{\\frac{1}{1+s } } \\right)^{1+s } \\cdot   \\left ( \\sum_y   w_p(y)^{-s\\cdot -\\frac{1}{s } } \\right)^{-s } \\\\ = & \\left (   \\sum_y   \\left [ \\sum_x p(x ) ( w_x(y))^{1+s } \\right]^{\\frac{1}{1+s } } \\right)^{1+s}.\\end{aligned}\\ ] ] substituting @xmath285 , we obtain @xmath286 w_p(y)^{\\frac{-t}{1-t } } \\\\ \\ge & \\left (   \\sum_y   \\left [ \\sum_x p(x ) ( w_x(y))^{\\frac{1}{1-t } } \\right]^{1-t } \\right)^{\\frac{1}{1-t}},\\end{aligned}\\ ] ] which implies @xmath287 w_p(y)^{\\frac{-t}{1-t } } \\right)^{1-t } \\\\ \\ge & \\sum_y   \\left [ \\sum_x p(x ) ( w_x(y))^{\\frac{1}{1-t } } \\right]^{1-t } = e^{\\phi(t|w^e , p)}.\\end{aligned}\\ ] ] thus , our bound @xmath273 for the exponential rate of decrease is better than the existing bound @xmath277 @xcite .", "assume that @xmath288 .", "we consider the following channel .", "@xmath289 when @xmath290 , @xmath291 then , the three bounds @xmath292 , @xmath293 , and @xmath294 with @xmath295 are numerically compared as in fig .", "[ f4 ] .", "next , we consider a more specific case .", "when @xmath296 and @xmath297 is a module and @xmath298 , the channel @xmath299 is called _", "any additive channel @xmath263 satisfies @xmath300 for the uniform distribution @xmath301 on @xmath297 because @xmath302 we consider a more general case .", "eve is assumed to have two random variables @xmath303 and @xmath304 .", "the first random variable @xmath305 is the output of an additive channel depending on the second variable @xmath304 .", "that is , the channel @xmath306 can be written as @xmath307 , where @xmath308 is a joint distribution .", "hereinafter , this channel model is called a general additive channel .", "this channel is also called a regular channel@xcite . for this channel model , the equalities @xmath309", "hold because @xmath310"], ["furthermore , in a practical sense , we need to take into account the decoding time . for this purpose , we often restrict our codes to linear codes . in the following ,", "we consider the case where the sender s space @xmath221 has the structure of a module . here", ", we should remark that hayashi @xcite considered the completely random ensemble of codes .", "he did not consider universal@xmath3 condition .", "so , we can not derive the following discussion from his result @xcite while his bound for exponential rate is the same as our bound when eve s channel is additive or general additive .", "such a required code can be constructed by the same discussion as section v of @xcite . when we generate codes @xmath232 by the same method as in @xcite ,", "the average of @xmath236 is bounded by @xmath311 .", "this result can be applied to one - way secret key distillation by the same way as section vi of @xcite . in this problem , alice , bob , and eve", "are assumed to have initial random variables @xmath312 , @xmath313 , and @xmath314 , respectively . the task for alice and bob is to share a common random variable almost independent of eve s random variable @xmath315 by using a public communication . for this purpose", ", we assume that alice and bob can perform local data processing in the both sides and alice can send messages to bob via public channel .", "that is , only one - way communication is allowed .", "we call such a combination of these operations a code and denote it by @xmath232 .", "the quality is evaluated by three quantities : the size of the final common random variable , the probability that their final variables coincide , and eve s distinguishability @xmath236 of the final joint distribution between alice and eve .", "then , we can construct the same code @xmath232 as given in section vi of@xcite for any numbers @xmath32 and @xmath316 .", "it follows from the similar discussion that this code satisfies that @xmath317 and @xmath318"], ["we have derived the tight evaluation for exponent for the average of the @xmath0 norm distance between the generated random number and the uniform random number when universal@xmath3 hash function is applied and the key generation rate is less than the critical rate @xmath319 . using this evaluation ,", "we have obtained an upper bound for eve s distinguishability in secret key generation from a common random number without communication when a universal@xmath3 hash function is applied . since our bound is based on the rnyi entropy of order @xmath1 for @xmath2 $ ] , it can be regarded as an extension of bennett et al @xcite s result with the rnyi entropy of order 2 .    applying this bound to the wire - tap channel , we obtain an upper bound for eve s distinguishability , which yields an exponential upper bound .", "this exponent improves on the existing exponent @xcite .", "further , when the error correction code is given by a linear code and when the channel is additive or general additive , the privacy amplification is given by a concatenation of toeplitz matrix and the identity matrix .", "this method can be applied to secret key distillation with public communication ."], ["the author is grateful to professors ryutaroh matsumoto and takeshi koshiba for a helpful comments .", "this research was partially supported by a mext grant - in - aid for young scientists ( a ) no .", "the centre for quantum technologies is funded by the singapore ministry of education and the national research foundation as part of the research centres of excellence programme ."], ["first , for a fixed element @xmath320 , we consider the condition @xmath321 we denote the probability that this condition holds and the expectation under this condition by @xmath322 and @xmath323 .", "then , @xmath324 and @xmath325      now , using the function @xmath342 , we make a code for the wire - tap channel based on the random coding method . for this purpose", ", we make a protocol to share a random number .", "first , we generate the random code @xmath343 with size @xmath344 , which is described by the @xmath344 independent and identical random variables @xmath345 subject to the distribution @xmath251 on @xmath221 . for integers @xmath346 and @xmath347 , let @xmath348 be the maximum likelihood decoder of the code @xmath343 .", "gallager @xcite showed that the ensemble expectation of the average error probability concerning decoding the input message @xmath174 is less than @xmath349 for @xmath350 . after sending the random variable @xmath174 taking values in the set with the cardinality @xmath351 ,", "alice and bob apply the above universal@xmath3 function @xmath23 to the random variable @xmath174 and generate another piece of data of size @xmath32 . here", ", we assume that the ensemble @xmath352 satisfies condition [ c12 ]", ". then , alice and bob share random variable @xmath353 with size @xmath32 .", "this protocol is denoted by @xmath354    let @xmath175 be the random variable of the output of eve s channel @xmath263 . for simplicity , we simplify the uniform distribution @xmath355 on the message space @xmath356 and the distribution @xmath357 on @xmath221 by @xmath358 and @xmath359 .", "therefore , since the random variable @xmath174 obeys the uniform distribution @xmath358 , the joint distribution @xmath360 concerning @xmath174 and @xmath175 satisfies @xmath361 for a given code @xmath343 , we apply the inequality ( [ 4 - 25 - 1 ] ) to eve s distinguishability . then , @xmath362 for @xmath363 .", "the concavity of @xmath279 ( lemma [ l1 ] ) guarantees that @xmath364 for @xmath363 .", "now , we make a code for wire - tap channel by modifying the above protocol @xmath354 .", "first , we choose the distribution @xmath226 to be the uniform distribution on @xmath29 .", "when alice wants to send the secret message @xmath30 , before sending the random variable @xmath174 , alice generates the random number @xmath174 subject to the distribution @xmath226 .", "alice sends the random variable @xmath174 .", "bob recovers the random variable @xmath174 , and applies the function @xmath23 .", "then , bob decodes alice s message @xmath30 , and this code for wire - tap channel @xmath365 is denoted by @xmath366 . since the ensemble @xmath352 satisfies condition [ c12 ] and the secret message @xmath30 obeys the uniform distribution on @xmath367 , this protocol @xmath366 has the same performance as the above protocol @xmath354 .    finally , we consider what code is derived from the above random coding discussion .", "using the markov inequality , we obtain @xmath368 therefore , the existence of a good code is guaranteed in the following way .", "that is , we give the concrete performance of a code whose existence is shown in the above random coding method ."], ["we choose @xmath326 such that @xmath327 , where @xmath328 .", "when @xmath130 satisfies @xmath329 , @xmath330 hence , @xmath331 the last equation follows from the concavity of @xmath332 concerning @xmath333 .", "assume that @xmath161 .", "then , @xmath334 . when @xmath335 , @xmath336 which implies ( [ 9 - 9 - 4 ] ) .", "assume that @xmath169 .", "when @xmath335 , @xmath337 further , when @xmath338 , @xmath339 which implies ( [ 9 - 9 - 5 ] ) .", "further , the concavity of @xmath340 and the condition @xmath169 imply that @xmath341 .", "thus , we obtain ( [ 2 - 2 - 8 ] ) ."], ["first , we regard a submodule @xmath369 as an encoding for the usual sent message , and focus on its decoding @xmath370 by the authorized receiver .", "we construct a code for a wire - tap channel @xmath371}\\}_{[x]\\in c_1/c_2 } , \\{\\cd_{[x]}\\}_{[x]\\in c_1/c_2})$ ] based on a submodule @xmath372 of @xmath373 as follows .", "the encoding @xmath374}$ ] is given as the uniform distribution on the coset @xmath375:=x+c_2 $ ] , and the decoding @xmath376}$ ] is given as the subset @xmath377 .", "next , we assume that a submodule @xmath378 of @xmath373 with cardinality @xmath379 is generated by a random variable @xmath25 satisfying the following condition .", "any element @xmath380 is included in @xmath378 with probability at most @xmath381 .", "then , the performance of the constructed code is evaluated by the following theorem .", "choose the subcode @xmath378 according to condition [ c3 ] .", "we construct the code @xmath382 by choosing the distribution @xmath374}$ ] to be the uniform distribution on @xmath375 $ ] for @xmath375\\in c_1/c_2({{\\bf x}})$ ] .", "then , we obtain @xmath383 where @xmath384 is the uniform distribution on the subset @xmath385 .    this inequality can be shown by ( [ 4 - 25 - 1 ] ) as follows .", "now , we define the joint distribution @xmath386 .", "the choice of @xmath374}$ ] corresponds to a hashing operation satisfying condition 1 .", "then , ( [ 4 - 25 - 1 ] ) yields that @xmath387 is bounded by @xmath388 , which implies ( [ 4 - 27 - 1 ] ) .    next , we consider a special class of channels .", "when the channel @xmath263 is additive , i.e. , @xmath389 , the equation @xmath390 holds for any @xmath391 .", "thus , the concavity of @xmath279 ( lemma [ l1 ] ) implies that @xmath392 thus , combining ( [ 4 - 27 - 1 ] ) , ( [ 2 - 13 - 1 ] ) , and ( [ 12 - 26 - 2 ] ) , we obtain @xmath393 for @xmath394 .", "similarly , when the channel @xmath263 is general additive , i.e. , @xmath395 , combining ( [ 4 - 27 - 1 ] ) , ( [ 2 - 13 - 1 ] ) , and ( [ 5 - 14 - 1 ] ) , we obtain @xmath396 for @xmath394 .    in the following discussion , we assume that @xmath221 is an @xmath4-dimensional vector space @xmath397 over the finite field @xmath36 . in this assumption , as is mentioned in @xcite , the bound can be attained by combination of linear code and the concatenation of toeplitz matrix and the identity @xmath34 of the size @xmath398 ."], ["next , we explain the detail of construction of one - way secret key distillation protocol . first , alice generates another uniform random variable @xmath391 and sends the random variable @xmath399 .", "then , the distribution of the random variables @xmath400 ( @xmath401 ) accessible to bob ( eve ) can be regarded as the output distribution of the channel @xmath402 ( @xmath403 ) .", "the channels @xmath404 and @xmath405 are given as follows .", "@xmath406 where @xmath407 ( @xmath408 ) is the joint probability between alice s initial random variable @xmath172 and bob s ( eve s ) initial random variable @xmath409 ( @xmath315 ) .", "hence , the channel @xmath263 is general additive .      in particular , when @xmath221 is an @xmath4-dimensional vector space @xmath397 over the finite field @xmath36 and the joint distribution between @xmath174 and @xmath413(@xmath175 ) is the @xmath4-fold independent and identical distribution ( i.i.d . ) of @xmath414 ( @xmath176 ) , respectively , the relations @xmath415 and @xmath416 hold .", "thus , there exists a code @xmath265 for any integers @xmath266 , and any probability distribution @xmath251 on @xmath221 such that @xmath267 and @xmath417 hence , the achievable rate of this protocol is equal to @xmath418 which was obtained by maurer@xcite and ahlswede - csiszr@xcite . here", ", since the channels @xmath262 and @xmath263 can be regarded as general additive , we can apply the discussion in section [ s3 ] .", "that is , the bound ( [ 2 - 13 - 2 ] ) can be attained with the combination of a linear code and random privacy amplification , which is given in section [ s3 ] .", "u. maurer and s. wolf , `` infromation - theoretic key agreement : from weak to strong secrecy for free , '' _ advances in cryptology  eurocrypt 2000 _ , lecture notes in computer science , vol.1807 , pp.351 - 368 , springer - verlag ( 2000 ) .", "r. renner and s. wolf , `` simple and tight bounds for information reconciliation and privacy amplification , '' _ asiacrypt 2005 _ , lecture notes in computer science , springer - verlag , vol .", "3788 , pp .", "199 - 216 , 2005 .", "s. watanabe , t. saitou , r. matsumoto , t. uyematsu `` strongly secure privacy amplification can not be obtained by encoder of slepian - wolf code , '' _ proceedings of the 2009 ieee international symposium on information theory _ , volume 2 , seoul , korea , pp . 1298 - 1302 ( 2009 ) ( arxiv:0906.2582 )            a. winter , a. c. a. nascimento , and h. imai , `` commitment capacity of discrete memoryless channels , '' _ proc .", "9th cirencester crypto and coding conf .", "_ , lncs 2989 , pp 35 - 51 , springer , berlin 2003 ; cs.cr/0304014 ( 2003 )    m. hayashi , `` general non - asymptotic and asymptotic formulas in channel resolvability and identification capacity and its application to wire - tap channel , '' _ ieee trans .", "inform . theory _", ", vol . * 52*(4 ) , 15621575 , 2006 .", "kuptsov , `` holder inequality '' , in hazewinkel , michiel , encyclopaedia of mathematics , springer , ( 2001 ) .", "t. holenstein and r. renner , `` on the randomness of independent experiments , '' arxiv : cs/0608007 ( 2006 )"]]}
{"article_id": "1410.3812", "article_text": ["the wiretap channel was first introduced by wyner @xcite , in which a legitimate transmitter ( alice ) wishes to send messages to a legitimate receiver ( bob ) secretly in the presence of an eavesdropper ( eve ) .", "wyner @xcite characterized the capacity equivocation region for the degraded wiretap channel , in which the received signal at eve is a degraded version of the received signal at bob .", "later , csiszr and krner @xcite characterized the capacity equivocation region for general , not necessarily degraded , wiretap channels .", "these works are based on information - theoretic random coding schemes .", "polar coding , invented by arikan @xcite , is the first code that provably achieves the capacity of the binary - input discrete symmetric output channels ( b - dmc ) .", "the idea of polar coding has been extended to lossless source coding @xcite , lossy source coding @xcite , and to multi - user scenarios , such as , multiple access channel @xcite , broadcast channel @xcite , interference channel @xcite , and slepian - wolf coding problem @xcite .    on a b - dmc", ", polarization results in two kinds of sub - channels @xcite .", "the first kind is good sub - channels . the capacity for these sub - channels approaches @xmath0 bit per channel use .", "the second kind is bad sub - channels .", "the channel output for these sub - channels is independent of the channel input ; therefore the capacity for these sub - channels approaches @xmath1 . in particular ,", "if a b - dmc a is degraded with respect to a b - dmc b , then the good sub - channels of a must be a subset of the good sub - channels of b @xcite .", "we call this the _ subset property_.    polar coding schemes for _ degraded _ wiretap channels with", "_ symmetric _", "main and eavesdropper channels are developed using the subset property in @xcite .", "for degraded wiretap channels , the good sub - channels of eve is a subset of the good sub - channels of bob .", "the polar coding scheme is designed to transmit the confusion messages ( random bits ) on the sub - channels simultaneously good for bob and eve , and to transmit the secret messages on the sub - channels only good for bob . however , for non - degraded wiretap channels , the subset property no longer holds @xcite , i.e. , the good sub - channels of eve is not necessary a subset of the good sub - channels of bob .", "moreover , the secrecy capacity achieving input distribution is not necessarily a uniform distribution .", "therefore , the polar coding schemes in @xcite can not directly extend to the non - degraded wiretap channel .    by applying the two recently developed techniques for polar codes", ", we can achieve the secrecy capacity of the general wiretap channel .", "the first is universal polar codes @xcite .", "universal polar coding allows us to align the good sub - channels of bob and eve together .", "therefore , we can artificially construct the subset property for the non - degraded wiretap channel .", "then , alice transmits the random bits on the sub - channels simultaneously good for bob and eve , and the secret message on the sub - channels only good for bob .", "the second is polar coding for asymmetric models @xcite , which allows us to deal with the non - uniform input distribution .", "different from b - dmc , polarization for asymmetric channel results in three different kinds of sub - channels .", "another polar coding scheme for the general wiretap channel is provided in @xcite , which uses a concatenated code consisting of two polar codes .", "the inner layer ensures that the transmitted message can be reliably decoded by bob , and the outer layer guarantees that the message is kept secret from eve .", "our work jointly handles these two goals in one shot .", "hence , the decoding error probability of our scheme is approximately @xmath2 , whereas it is @xmath3 in @xcite .", "moreover , for practical code construction , there is still no efficient way to characterize the outer index set ( * ? ? ?", "* sec  iii .", "c. ) , while our coding scheme can be efficiently constructed by @xcite .", "a wiretap channel consists of a legitimate transmitter ( alice ) who wishes to send messages to a legitimate receiver ( bob ) secretly in the presence of an the eavesdropper ( eve ) .", "the channel between alice and bob is called the main channel , and the channel between alice and eve is called the eavesdropper channel .", "let @xmath4 denote the single - letter input to the main and eavesdropper channels .", "let @xmath5 and @xmath6 denote the corresponding single - letter outputs of the main and the eavesdropper channels , respectively .", "@xmath7 represents the message to be sent to bob and kept secret from eve with @xmath8 .", "let @xmath9 denote the probability of error for bob s decoding .", "the equivocation rate is given by @xmath10 , which reflects the uncertainty of the message given eavesdropper s channel observation .", "a rate - equivocation pair @xmath11 is achievable if as @xmath12 , @xmath13 and @xmath14 .", "perfect ( weak ) secrecy is achieved if @xmath15 @xcite .", "therefore , perfect secrecy is achieved if @xmath16 , and the _ secrecy capacity _", "@xmath17 is the highest achievable perfect secrecy rate @xmath18 , which is also the highest possible equivocation rate @xcite .", "csiszr and krner characterized the secrecy capacity for the general wiretap channel , which is @xcite @xmath19    in the following , we assume that we already know the optimal input distribution @xcite , i.e. , we know the optimal @xmath20 , @xmath4 that achieve @xmath17 .", "although we focus on developing a coding scheme for binary inputs below , there is no difficulty to extend the work to @xmath21-ary inputs @xcite .", "let @xmath22 be the joint distribution of a pair of random variables @xmath23 , where @xmath24 is a binary random variable and @xmath20 is any finite alphabet random variable .", "let us define the bhattacharyya parameter as follows @xmath25 let @xmath26 , where @xmath27 denotes @xmath28 independent copies of the random variable @xmath4 with @xmath29 , and @xmath30 where @xmath31 and @xmath32 denotes the kronecker product of matrices for @xmath33 .", "@xcite shows as @xmath34 , @xmath35 is almost independent of @xmath36 and uniformly distributed , or otherwise @xmath35 is almost determined by @xmath36 .", "therefore , @xmath37 $ ] , the index set @xmath38 , is almost polarized into two sets @xmath39 and @xmath40 : @xmath41:z(u_i|u^{i-1 } ) \\geq 1- \\delta_n \\ }", "\\notag \\\\ \\mathcal{l}_x & = \\ { i\\in[n]:z(u_i|u^{i-1 } ) \\leq \\delta_n \\ } , \\label{h_x_l_x_def}\\end{aligned}\\ ] ] where @xmath42 and @xmath43 .", "moreover , @xmath44    let @xmath45 be a discrete memoryless channel with a binary input @xmath4 and finite alphabet output @xmath5 . here , @xmath45 does not have to be a symmetric channel . fix a distribution @xmath46 for @xmath4 .", "@xcite generalizes the above argument to achieve a rate close to @xmath47 .", "consider two subsets of @xmath37 $ ] , @xmath48 and @xmath49 , defined as follows @xmath50:z(u_i|u^{i-1 } , y^n ) \\geq 1- \\delta_n   \\ } \\notag \\\\ \\mathcal{l}_{x|y } & = \\", "{ i\\in[n]:z(u_i|u^{i-1 } , y^n ) \\leq \\delta_n   \\ } , \\label{h_x|y_l_x|y_def}\\end{aligned}\\ ] ] similar to , we have @xmath51    with and , we define the following three sets @xmath52 in the following , we call the set @xmath53 the _ information set _ , and sets @xmath54 and @xmath55 the _", "frozen set_. although we call them the _ frozen set _ , @xmath54 and", "@xmath55 have different operational meanings which will be illustrated below .", "note that for the symmetric channel capacity achieving code design , @xmath55 is an empty set @xcite .    to achieve rate @xmath47 for channel @xmath45 ,", "let us consider the following coding scheme .", "first , the encoder transmits the information bits in the index set @xmath53 . for @xmath56 in , since @xmath57 , @xmath35 is almost independent of @xmath36 and uniformly distributed .", "therefore , the encoder can freely assign values to @xmath58 , where @xmath58 denotes a sub - vector @xmath59 .", "moreover , since @xmath60 , @xmath35 is almost determined by @xmath36 and @xmath61 , which means that given the channel output @xmath61 , @xmath35 is decoded in a successive manner .", "second , for @xmath62 in , @xmath35 is almost independent of @xmath36 and uniformly distributed , and given the channel output @xmath61 , @xmath35 can not be reliably decoded .", "the encoder transmits @xmath63 with a uniformly random sequence and the randomness is shared between the transmitter and the receiver .", "last , for @xmath64 in , @xmath35 is almost determined by @xmath36 .", "the values of @xmath65 are computed in successive order through the following randomized map @xmath66    by and , it is easy to verify that @xmath67 moreover , by applying successive cancellation decoder , the block error probability @xmath68 can be upper bounded by @xmath69 for any @xmath43 , with complexity @xmath70 . therefore , the rate @xmath47 is achieved .", "consider two b - dmcs @xmath71 and @xmath72 , and assume that these two channels have identical capacities , i.e. , @xmath73 .", "let @xmath26 , and denote @xmath74 and @xmath75 as the information set defined in , i.e. , @xmath76 : z(u_i|u^{i-1 } , y^n )   \\leq \\delta_n", "\\ }   \\notag \\\\ \\mathcal{q } & = \\", "{ i \\in [ n ] : z(u_i|u^{i-1 } , z^n )   \\leq \\delta_n   \\ } , \\notag\\end{aligned}\\ ] ] where @xmath42 and @xmath43 .", "since we assume @xmath73 , we also have @xmath77 .    in general , the differences @xmath78 and @xmath79 are not empty sets @xcite ; therefore , it is not straightforward to apply standard polar coding to achieve the capacity of the compound channel consisting of @xmath45 and @xmath80 .", "@xcite proposes a method , called _ chaining construction _ , to solve this problem .", "( chaining construction @xcite ) [ chain ] let @xmath81 .", "the @xmath82-chain of @xmath74 and @xmath75 is a code of length @xmath83 that consists of @xmath82 polar blocks of length @xmath28 . in each of the @xmath82 blocks , the set @xmath84", "is set to be an information set . in the @xmath85th block , @xmath86 ,", "the set @xmath78 is also set to be an information set .", "moreover , the set @xmath78 in the @xmath85th block is chained to the set @xmath79 in the @xmath87th block in the sense that the information is repeated in these two sets .", "all other indices are frozen .", "therefore , in each block , the set @xmath88 is frozen , and the set @xmath79 in the @xmath0st block and the set @xmath78 in the @xmath82th block are frozen , too .", "note that @xmath89 denotes the complement of a set .", "the rate of the chaining construction is @xmath90    next , we discuss the decoding procedure for the compound channel consisting of @xmath45 and @xmath80 . if the channel @xmath45 is used , then we decode from the first block . on the other hand ,", "if the channel @xmath80 is used , then we decode from the last block .", "first , suppose that channel @xmath45 is used and a code of length @xmath83 has been received . for this case , we decode from the first block . in the @xmath0st block", ", we put all the information bits in the set @xmath74 , thus the decoder can decode correctly . for the @xmath91nd block , through chaining construction ,", "the set @xmath78 in the @xmath0st block is chained to the set @xmath79 in the @xmath91nd block , and the set @xmath88 is frozen .", "equivalently , the decoder only needs to decode the bits in the set @xmath74 , which can be correctly decoded .", "the same procedure holds until the @xmath92th block . for the @xmath82th block ,", "the information bits are only put in the set @xmath84 , and the remaining part has been determined .", "hence , information bits can be reliably decoded .", "the main rate loss for the chaining construction comes from the last block .", "second , consider the case that the channel @xmath80 is used . in this case , we decode from the last block . in the @xmath82th block ,", "since the information bits are put in the set @xmath75 , reliable decoding is guaranteed . for the @xmath92th block , due to the chaining process , the set @xmath79 in the @xmath82th block is chained to the set @xmath78 in the @xmath92th block , and note that the set @xmath88 is frozen .", "the decoder only needs to decode the information bits in the set @xmath75 , thus correct decoding is ensured .", "this procedure is applied until the @xmath91nd block . for the @xmath0st block , information bits which have not been determined fall in the set @xmath84 ,", "thus the decoder can decode them correctly .    in summary ,", "for a fixed @xmath82 , if we let @xmath12 , we can achieve the rate in with arbitrary small error probability , which also means that the rate @xmath93 can be achieved . additionally ,", "if we let @xmath94 , then the rate @xmath95 , which is the capacity of the compound channel consisting of channels @xmath45 and @xmath80 , can be achieved .", "assume now that we know the optimal distributions to achieve the secrecy capacity @xmath17 in , i.e. , we know the optimal @xmath20 and @xmath4 . for illustration , we consider the case of a binary input channel , i.e. , @xmath96 .", "the cardinality bound for channel prefixing @xmath20 , is @xmath97 .", "let @xmath98 .", "consider the following sets : @xmath99:z(u_i|u^{i-1 } ) \\geq 1- \\delta_n \\ } \\notag \\\\ \\mathcal{l}_v & = \\ { i\\in[n]:z(u_i|u^{i-1 } ) \\leq \\delta_n \\ } , \\label{h_v_l_v_def } \\\\", "\\mathcal{h}_{v|y } & = \\ { i\\in[n]:z(u_i|u^{i-1 } , y^n ) \\geq 1- \\delta_n   \\ } \\notag \\\\ \\mathcal{l}_{v|y } & = \\ { i\\in[n]:z(u_i|u^{i-1 } , y^n ) \\leq \\delta_n   \\ } , \\label{h_v|y_l_v|y_def } \\\\", "\\mathcal{h}_{v|z } & = \\ { i\\in[n]:z(u_i|u^{i-1 } , z^n )", "\\geq 1- \\delta_n   \\ } \\notag \\\\ \\mathcal{l}_{v|z } & = \\ { i\\in[n]:z(u_i|u^{i-1 } , z^n ) \\leq", "\\delta_n   \\ } , \\label{h_v|z_l_v|z_def}\\end{aligned}\\ ] ] where @xmath42 and @xmath43 .", "the set @xmath37 $ ] can be partitioned into the following four sets : @xmath100 from a successive decoding point of view , the sub - channels corresponding to the set @xmath101 are simultaneously good for bob and eve .", "the sub - channels in the set @xmath102 are good for bob but bad for eve . on the other hand ,", "the sub - channels in the set @xmath103 are good for eve but bad for bob .", "last , the sub - channels in the set @xmath104 are bad for both bob and eve .    similar to  , we have : @xmath105 by , we have @xmath106    for the _ symmetric _ and _ degraded _ wiretap channel @xcite , @xmath103 is an empty set , since the degraded property of the channel causes @xmath107 @xcite .", "however , for the general wiretap channel , @xmath103 is no longer an empty set , and @xmath108 can not be negligible @xcite .    here , we consider the positive secrecy capacity case , thus we have @xmath109 .", "choose a set , @xmath110 , such that @xmath111 and @xmath112 .", "define the set @xmath113 as : @xmath114 from , we have @xmath115    we construct the code as follows .", "consider an _", "m - chain _ polar code in definition  [ chain ] . for @xmath86 , the set @xmath116 in the @xmath85th block", "is chained to @xmath117 in the @xmath87th block as in fig .", "[ fig_chain ] . for each of the @xmath82 blocks , the set @xmath118", "is set to be frozen .", "moreover , the set @xmath117 in the @xmath0st block is set to be frozen in the sense that @xmath119 , and the set @xmath116 in the @xmath82th block is also set to be frozen in the sense that @xmath120 . in fig .", "[ fig_chain ] , we use a red cross to denote a frozen set .", "we put the secret information bits in the set @xmath113 in each block .", "therefore , the set @xmath113 is used for secret message transmission . for blocks", "@xmath86 , we put uniformly distributed random bits to @xmath116 to serve as the confusion messages . through the chaining construction , the confusion messages are also chained to the set @xmath117 in block @xmath121", "moreover , the set @xmath122 in each block are also filled with random bits to serve as confusion message . for the frozen sets , if the index belongs to @xmath123 or @xmath124 , then we put uniformly distributed random bits and share the randomness with the decoder ( bob and eve ) .", "last , if the index belongs to @xmath55 , then we determine the value according to the randomized map defined in .", "we summarize the encoding procedure as follows .    * encoding procedure : *    for each block , the secret information bits are put in @xmath125 , and determine the bits in @xmath126 by .    for the @xmath0st block ,    1 .", "put uniformly distributed random bits to @xmath127 .", "put uniformly distributed random bits to @xmath128 , and share the randomness with the decoder .    for the @xmath129th block , @xmath130 ,    1 .", "put uniformly distributed random bits to @xmath127 .", "2 .   chaining construction : repeat the bits in @xmath116 of the @xmath131th block to the bits in @xmath132 .", "put uniformly distributed random bits to @xmath133 , and share the randomness with the decoder .    for the @xmath82th block ,    1 .", "put uniformly distributed random bits to @xmath134 .", "2 .   chaining construction : repeat the bits in @xmath116 of the @xmath92th block to the bits in @xmath132 .", "put uniformly distributed random bits to @xmath135 , and share the randomness with the decoder .", "note that in the chaining construction we require the bits in @xmath136 equal the bits in @xmath137 .", "since we fill uniformly distributed random bits to @xmath137 , we simultaneously fill random bits to @xmath136 . due to the fact that @xmath138 , we can freely choose bits in this set .", "* decoding procedure : *    bob decodes from the @xmath0st block . in each block , if @xmath64 , then @xmath139 . for the @xmath0st block , @xmath140 for the @xmath129th block , @xmath130 , @xmath141 for the @xmath82th block , @xmath142      from", ", we know as @xmath12 , our coding scheme can achieve the secrecy rate in .", "moreover , when bob applies the decoding procedure described in sec .", "[ sec_scheme_design ] , according to , the block error probability of the whole _", "m - chain _ block can be upper bounded by @xmath143 for any @xmath43 with complexity @xmath70 .", "therefore , the secrecy rate in can be achieved reliably .", "we first introduce necessary notation for the calculation of the equivocation rate . in the encoding process , we consider @xmath82 blocks each with block length @xmath28 .", "let @xmath144 denote what eve receives .", "for each block , we perform @xmath145 , therefore , for the total of @xmath82 blocks , we have @xmath146 and @xmath147 .", "let @xmath148 denote the secret message , and @xmath149 denote the confusion message .", "let the subscript @xmath85 of a set denote the set in the @xmath85th block .", "for example , @xmath150 denotes the set @xmath113 in the @xmath85th block , and @xmath151 denotes the set @xmath152 in the @xmath129th block . since secret message is put in @xmath150 , @xmath153 , we have @xmath154 . also , the confusion message is put in @xmath155 and @xmath156 .", "therefore , we have @xmath157 .", "we can calculate the equivocation rate as follows : @xmath158 which is equivalent to @xmath159 here , is due to chain rule of conditional entropy , is due to the definition of mutual information , comes from the data processing inequality , is due to the independence of the secret message and the confusion message . in , we bound each terms on the right hand side as follows :    for the first term , we have @xmath160 @xmath161", ". therefore , @xmath162 .    to bound the second term , suppose eve obtains @xmath148 and @xmath144 , and wants to decode @xmath149 .", "by symmetry of chaining construction , eve can apply similar decoding rule as described in sec .", "[ sec_scheme_design ] . however , this time eve decodes from the @xmath82th block , then the block error probability of the whole _", "m - chain _ block can be upper bounded by @xmath163 for @xmath43 . hence , by applying fano s inequality , we have @xmath164 .", "\\notag\\end{aligned}\\ ] ] therefore , as @xmath165 , @xmath166 .    for the last term , as @xmath165 , by and", ", we have @xmath167 .", "hence , as @xmath168 , @xmath169 .    after we bound the right hand side of , we know as @xmath165 and @xmath168 , @xmath170 .", "therefore , the weak secrecy constraint is achieved .", "we proposed a polar coding scheme that achieves the secrecy capacity of the general wiretap channel , by using the chaining construction technique and polar coding for asymmetric channels .", "compared to previous work , our construction has better decoding error probability and can be constructed more efficiently .", "finally , we note that this chaining construction based polar coding scheme can be extended to achieve _ strong _", "secrecy guarantees as presented in @xcite .", "e.  arikan , `` channel polarization : a method for constructing capacity - achieving codes for symmetric binary - input memoryless channels , '' _ ieee trans .", "inf . theory _ ,", "55 , no .  7 , pp . 30513073 , jul ."], "abstract_text": ["<S> information - theoretic work for wiretap channels is mostly based on random coding schemes . designing practical coding schemes to achieve information - theoretic security </S>", "<S> is an important problem . by applying the two recently developed techniques for polar codes , </S>", "<S> we propose a polar coding scheme to achieve the secrecy capacity of the general wiretap channel . </S>"], "labels": null, "section_names": ["introduction", "wiretap channel model", "polar codes", "polar coding for the general wiretap channel", "conclusion"], "sections": [["the wiretap channel was first introduced by wyner @xcite , in which a legitimate transmitter ( alice ) wishes to send messages to a legitimate receiver ( bob ) secretly in the presence of an eavesdropper ( eve ) .", "wyner @xcite characterized the capacity equivocation region for the degraded wiretap channel , in which the received signal at eve is a degraded version of the received signal at bob .", "later , csiszr and krner @xcite characterized the capacity equivocation region for general , not necessarily degraded , wiretap channels .", "these works are based on information - theoretic random coding schemes .", "polar coding , invented by arikan @xcite , is the first code that provably achieves the capacity of the binary - input discrete symmetric output channels ( b - dmc ) .", "the idea of polar coding has been extended to lossless source coding @xcite , lossy source coding @xcite , and to multi - user scenarios , such as , multiple access channel @xcite , broadcast channel @xcite , interference channel @xcite , and slepian - wolf coding problem @xcite .    on a b - dmc", ", polarization results in two kinds of sub - channels @xcite .", "the first kind is good sub - channels . the capacity for these sub - channels approaches @xmath0 bit per channel use .", "the second kind is bad sub - channels .", "the channel output for these sub - channels is independent of the channel input ; therefore the capacity for these sub - channels approaches @xmath1 . in particular ,", "if a b - dmc a is degraded with respect to a b - dmc b , then the good sub - channels of a must be a subset of the good sub - channels of b @xcite .", "we call this the _ subset property_.    polar coding schemes for _ degraded _ wiretap channels with", "_ symmetric _", "main and eavesdropper channels are developed using the subset property in @xcite .", "for degraded wiretap channels , the good sub - channels of eve is a subset of the good sub - channels of bob .", "the polar coding scheme is designed to transmit the confusion messages ( random bits ) on the sub - channels simultaneously good for bob and eve , and to transmit the secret messages on the sub - channels only good for bob . however , for non - degraded wiretap channels , the subset property no longer holds @xcite , i.e. , the good sub - channels of eve is not necessary a subset of the good sub - channels of bob .", "moreover , the secrecy capacity achieving input distribution is not necessarily a uniform distribution .", "therefore , the polar coding schemes in @xcite can not directly extend to the non - degraded wiretap channel .    by applying the two recently developed techniques for polar codes", ", we can achieve the secrecy capacity of the general wiretap channel .", "the first is universal polar codes @xcite .", "universal polar coding allows us to align the good sub - channels of bob and eve together .", "therefore , we can artificially construct the subset property for the non - degraded wiretap channel .", "then , alice transmits the random bits on the sub - channels simultaneously good for bob and eve , and the secret message on the sub - channels only good for bob .", "the second is polar coding for asymmetric models @xcite , which allows us to deal with the non - uniform input distribution .", "different from b - dmc , polarization for asymmetric channel results in three different kinds of sub - channels .", "another polar coding scheme for the general wiretap channel is provided in @xcite , which uses a concatenated code consisting of two polar codes .", "the inner layer ensures that the transmitted message can be reliably decoded by bob , and the outer layer guarantees that the message is kept secret from eve .", "our work jointly handles these two goals in one shot .", "hence , the decoding error probability of our scheme is approximately @xmath2 , whereas it is @xmath3 in @xcite .", "moreover , for practical code construction , there is still no efficient way to characterize the outer index set ( * ? ? ?", "* sec  iii .", "c. ) , while our coding scheme can be efficiently constructed by @xcite ."], ["a wiretap channel consists of a legitimate transmitter ( alice ) who wishes to send messages to a legitimate receiver ( bob ) secretly in the presence of an the eavesdropper ( eve ) .", "the channel between alice and bob is called the main channel , and the channel between alice and eve is called the eavesdropper channel .", "let @xmath4 denote the single - letter input to the main and eavesdropper channels .", "let @xmath5 and @xmath6 denote the corresponding single - letter outputs of the main and the eavesdropper channels , respectively .", "@xmath7 represents the message to be sent to bob and kept secret from eve with @xmath8 .", "let @xmath9 denote the probability of error for bob s decoding .", "the equivocation rate is given by @xmath10 , which reflects the uncertainty of the message given eavesdropper s channel observation .", "a rate - equivocation pair @xmath11 is achievable if as @xmath12 , @xmath13 and @xmath14 .", "perfect ( weak ) secrecy is achieved if @xmath15 @xcite .", "therefore , perfect secrecy is achieved if @xmath16 , and the _ secrecy capacity _", "@xmath17 is the highest achievable perfect secrecy rate @xmath18 , which is also the highest possible equivocation rate @xcite .", "csiszr and krner characterized the secrecy capacity for the general wiretap channel , which is @xcite @xmath19    in the following , we assume that we already know the optimal input distribution @xcite , i.e. , we know the optimal @xmath20 , @xmath4 that achieve @xmath17 .", "although we focus on developing a coding scheme for binary inputs below , there is no difficulty to extend the work to @xmath21-ary inputs @xcite ."], ["let @xmath22 be the joint distribution of a pair of random variables @xmath23 , where @xmath24 is a binary random variable and @xmath20 is any finite alphabet random variable .", "let us define the bhattacharyya parameter as follows @xmath25 let @xmath26 , where @xmath27 denotes @xmath28 independent copies of the random variable @xmath4 with @xmath29 , and @xmath30 where @xmath31 and @xmath32 denotes the kronecker product of matrices for @xmath33 .", "@xcite shows as @xmath34 , @xmath35 is almost independent of @xmath36 and uniformly distributed , or otherwise @xmath35 is almost determined by @xmath36 .", "therefore , @xmath37 $ ] , the index set @xmath38 , is almost polarized into two sets @xmath39 and @xmath40 : @xmath41:z(u_i|u^{i-1 } ) \\geq 1- \\delta_n \\ }", "\\notag \\\\ \\mathcal{l}_x & = \\ { i\\in[n]:z(u_i|u^{i-1 } ) \\leq \\delta_n \\ } , \\label{h_x_l_x_def}\\end{aligned}\\ ] ] where @xmath42 and @xmath43 .", "moreover , @xmath44    let @xmath45 be a discrete memoryless channel with a binary input @xmath4 and finite alphabet output @xmath5 . here , @xmath45 does not have to be a symmetric channel . fix a distribution @xmath46 for @xmath4 .", "@xcite generalizes the above argument to achieve a rate close to @xmath47 .", "consider two subsets of @xmath37 $ ] , @xmath48 and @xmath49 , defined as follows @xmath50:z(u_i|u^{i-1 } , y^n ) \\geq 1- \\delta_n   \\ } \\notag \\\\ \\mathcal{l}_{x|y } & = \\", "{ i\\in[n]:z(u_i|u^{i-1 } , y^n ) \\leq \\delta_n   \\ } , \\label{h_x|y_l_x|y_def}\\end{aligned}\\ ] ] similar to , we have @xmath51    with and , we define the following three sets @xmath52 in the following , we call the set @xmath53 the _ information set _ , and sets @xmath54 and @xmath55 the _", "frozen set_. although we call them the _ frozen set _ , @xmath54 and", "@xmath55 have different operational meanings which will be illustrated below .", "note that for the symmetric channel capacity achieving code design , @xmath55 is an empty set @xcite .    to achieve rate @xmath47 for channel @xmath45 ,", "let us consider the following coding scheme .", "first , the encoder transmits the information bits in the index set @xmath53 . for @xmath56 in , since @xmath57 , @xmath35 is almost independent of @xmath36 and uniformly distributed .", "therefore , the encoder can freely assign values to @xmath58 , where @xmath58 denotes a sub - vector @xmath59 .", "moreover , since @xmath60 , @xmath35 is almost determined by @xmath36 and @xmath61 , which means that given the channel output @xmath61 , @xmath35 is decoded in a successive manner .", "second , for @xmath62 in , @xmath35 is almost independent of @xmath36 and uniformly distributed , and given the channel output @xmath61 , @xmath35 can not be reliably decoded .", "the encoder transmits @xmath63 with a uniformly random sequence and the randomness is shared between the transmitter and the receiver .", "last , for @xmath64 in , @xmath35 is almost determined by @xmath36 .", "the values of @xmath65 are computed in successive order through the following randomized map @xmath66    by and , it is easy to verify that @xmath67 moreover , by applying successive cancellation decoder , the block error probability @xmath68 can be upper bounded by @xmath69 for any @xmath43 , with complexity @xmath70 . therefore , the rate @xmath47 is achieved .", "consider two b - dmcs @xmath71 and @xmath72 , and assume that these two channels have identical capacities , i.e. , @xmath73 .", "let @xmath26 , and denote @xmath74 and @xmath75 as the information set defined in , i.e. , @xmath76 : z(u_i|u^{i-1 } , y^n )   \\leq \\delta_n", "\\ }   \\notag \\\\ \\mathcal{q } & = \\", "{ i \\in [ n ] : z(u_i|u^{i-1 } , z^n )   \\leq \\delta_n   \\ } , \\notag\\end{aligned}\\ ] ] where @xmath42 and @xmath43 .", "since we assume @xmath73 , we also have @xmath77 .    in general , the differences @xmath78 and @xmath79 are not empty sets @xcite ; therefore , it is not straightforward to apply standard polar coding to achieve the capacity of the compound channel consisting of @xmath45 and @xmath80 .", "@xcite proposes a method , called _ chaining construction _ , to solve this problem .", "( chaining construction @xcite ) [ chain ] let @xmath81 .", "the @xmath82-chain of @xmath74 and @xmath75 is a code of length @xmath83 that consists of @xmath82 polar blocks of length @xmath28 . in each of the @xmath82 blocks , the set @xmath84", "is set to be an information set . in the @xmath85th block , @xmath86 ,", "the set @xmath78 is also set to be an information set .", "moreover , the set @xmath78 in the @xmath85th block is chained to the set @xmath79 in the @xmath87th block in the sense that the information is repeated in these two sets .", "all other indices are frozen .", "therefore , in each block , the set @xmath88 is frozen , and the set @xmath79 in the @xmath0st block and the set @xmath78 in the @xmath82th block are frozen , too .", "note that @xmath89 denotes the complement of a set .", "the rate of the chaining construction is @xmath90    next , we discuss the decoding procedure for the compound channel consisting of @xmath45 and @xmath80 . if the channel @xmath45 is used , then we decode from the first block . on the other hand ,", "if the channel @xmath80 is used , then we decode from the last block .", "first , suppose that channel @xmath45 is used and a code of length @xmath83 has been received . for this case , we decode from the first block . in the @xmath0st block", ", we put all the information bits in the set @xmath74 , thus the decoder can decode correctly . for the @xmath91nd block , through chaining construction ,", "the set @xmath78 in the @xmath0st block is chained to the set @xmath79 in the @xmath91nd block , and the set @xmath88 is frozen .", "equivalently , the decoder only needs to decode the bits in the set @xmath74 , which can be correctly decoded .", "the same procedure holds until the @xmath92th block . for the @xmath82th block ,", "the information bits are only put in the set @xmath84 , and the remaining part has been determined .", "hence , information bits can be reliably decoded .", "the main rate loss for the chaining construction comes from the last block .", "second , consider the case that the channel @xmath80 is used . in this case , we decode from the last block . in the @xmath82th block ,", "since the information bits are put in the set @xmath75 , reliable decoding is guaranteed . for the @xmath92th block , due to the chaining process , the set @xmath79 in the @xmath82th block is chained to the set @xmath78 in the @xmath92th block , and note that the set @xmath88 is frozen .", "the decoder only needs to decode the information bits in the set @xmath75 , thus correct decoding is ensured .", "this procedure is applied until the @xmath91nd block . for the @xmath0st block , information bits which have not been determined fall in the set @xmath84 ,", "thus the decoder can decode them correctly .    in summary ,", "for a fixed @xmath82 , if we let @xmath12 , we can achieve the rate in with arbitrary small error probability , which also means that the rate @xmath93 can be achieved . additionally ,", "if we let @xmath94 , then the rate @xmath95 , which is the capacity of the compound channel consisting of channels @xmath45 and @xmath80 , can be achieved ."], ["assume now that we know the optimal distributions to achieve the secrecy capacity @xmath17 in , i.e. , we know the optimal @xmath20 and @xmath4 . for illustration , we consider the case of a binary input channel , i.e. , @xmath96 .", "the cardinality bound for channel prefixing @xmath20 , is @xmath97 .", "let @xmath98 .", "consider the following sets : @xmath99:z(u_i|u^{i-1 } ) \\geq 1- \\delta_n \\ } \\notag \\\\ \\mathcal{l}_v & = \\ { i\\in[n]:z(u_i|u^{i-1 } ) \\leq \\delta_n \\ } , \\label{h_v_l_v_def } \\\\", "\\mathcal{h}_{v|y } & = \\ { i\\in[n]:z(u_i|u^{i-1 } , y^n ) \\geq 1- \\delta_n   \\ } \\notag \\\\ \\mathcal{l}_{v|y } & = \\ { i\\in[n]:z(u_i|u^{i-1 } , y^n ) \\leq \\delta_n   \\ } , \\label{h_v|y_l_v|y_def } \\\\", "\\mathcal{h}_{v|z } & = \\ { i\\in[n]:z(u_i|u^{i-1 } , z^n )", "\\geq 1- \\delta_n   \\ } \\notag \\\\ \\mathcal{l}_{v|z } & = \\ { i\\in[n]:z(u_i|u^{i-1 } , z^n ) \\leq", "\\delta_n   \\ } , \\label{h_v|z_l_v|z_def}\\end{aligned}\\ ] ] where @xmath42 and @xmath43 .", "the set @xmath37 $ ] can be partitioned into the following four sets : @xmath100 from a successive decoding point of view , the sub - channels corresponding to the set @xmath101 are simultaneously good for bob and eve .", "the sub - channels in the set @xmath102 are good for bob but bad for eve . on the other hand ,", "the sub - channels in the set @xmath103 are good for eve but bad for bob .", "last , the sub - channels in the set @xmath104 are bad for both bob and eve .    similar to  , we have : @xmath105 by , we have @xmath106    for the _ symmetric _ and _ degraded _ wiretap channel @xcite , @xmath103 is an empty set , since the degraded property of the channel causes @xmath107 @xcite .", "however , for the general wiretap channel , @xmath103 is no longer an empty set , and @xmath108 can not be negligible @xcite .    here , we consider the positive secrecy capacity case , thus we have @xmath109 .", "choose a set , @xmath110 , such that @xmath111 and @xmath112 .", "define the set @xmath113 as : @xmath114 from , we have @xmath115    we construct the code as follows .", "consider an _", "m - chain _ polar code in definition  [ chain ] . for @xmath86 , the set @xmath116 in the @xmath85th block", "is chained to @xmath117 in the @xmath87th block as in fig .", "[ fig_chain ] . for each of the @xmath82 blocks , the set @xmath118", "is set to be frozen .", "moreover , the set @xmath117 in the @xmath0st block is set to be frozen in the sense that @xmath119 , and the set @xmath116 in the @xmath82th block is also set to be frozen in the sense that @xmath120 . in fig .", "[ fig_chain ] , we use a red cross to denote a frozen set .", "we put the secret information bits in the set @xmath113 in each block .", "therefore , the set @xmath113 is used for secret message transmission . for blocks", "@xmath86 , we put uniformly distributed random bits to @xmath116 to serve as the confusion messages . through the chaining construction , the confusion messages are also chained to the set @xmath117 in block @xmath121", "moreover , the set @xmath122 in each block are also filled with random bits to serve as confusion message . for the frozen sets , if the index belongs to @xmath123 or @xmath124 , then we put uniformly distributed random bits and share the randomness with the decoder ( bob and eve ) .", "last , if the index belongs to @xmath55 , then we determine the value according to the randomized map defined in .", "we summarize the encoding procedure as follows .    * encoding procedure : *    for each block , the secret information bits are put in @xmath125 , and determine the bits in @xmath126 by .    for the @xmath0st block ,    1 .", "put uniformly distributed random bits to @xmath127 .", "put uniformly distributed random bits to @xmath128 , and share the randomness with the decoder .    for the @xmath129th block , @xmath130 ,    1 .", "put uniformly distributed random bits to @xmath127 .", "2 .   chaining construction : repeat the bits in @xmath116 of the @xmath131th block to the bits in @xmath132 .", "put uniformly distributed random bits to @xmath133 , and share the randomness with the decoder .    for the @xmath82th block ,    1 .", "put uniformly distributed random bits to @xmath134 .", "2 .   chaining construction : repeat the bits in @xmath116 of the @xmath92th block to the bits in @xmath132 .", "put uniformly distributed random bits to @xmath135 , and share the randomness with the decoder .", "note that in the chaining construction we require the bits in @xmath136 equal the bits in @xmath137 .", "since we fill uniformly distributed random bits to @xmath137 , we simultaneously fill random bits to @xmath136 . due to the fact that @xmath138 , we can freely choose bits in this set .", "* decoding procedure : *    bob decodes from the @xmath0st block . in each block , if @xmath64 , then @xmath139 . for the @xmath0st block , @xmath140 for the @xmath129th block , @xmath130 , @xmath141 for the @xmath82th block , @xmath142      from", ", we know as @xmath12 , our coding scheme can achieve the secrecy rate in .", "moreover , when bob applies the decoding procedure described in sec .", "[ sec_scheme_design ] , according to , the block error probability of the whole _", "m - chain _ block can be upper bounded by @xmath143 for any @xmath43 with complexity @xmath70 .", "therefore , the secrecy rate in can be achieved reliably .", "we first introduce necessary notation for the calculation of the equivocation rate . in the encoding process , we consider @xmath82 blocks each with block length @xmath28 .", "let @xmath144 denote what eve receives .", "for each block , we perform @xmath145 , therefore , for the total of @xmath82 blocks , we have @xmath146 and @xmath147 .", "let @xmath148 denote the secret message , and @xmath149 denote the confusion message .", "let the subscript @xmath85 of a set denote the set in the @xmath85th block .", "for example , @xmath150 denotes the set @xmath113 in the @xmath85th block , and @xmath151 denotes the set @xmath152 in the @xmath129th block . since secret message is put in @xmath150 , @xmath153 , we have @xmath154 . also , the confusion message is put in @xmath155 and @xmath156 .", "therefore , we have @xmath157 .", "we can calculate the equivocation rate as follows : @xmath158 which is equivalent to @xmath159 here , is due to chain rule of conditional entropy , is due to the definition of mutual information , comes from the data processing inequality , is due to the independence of the secret message and the confusion message . in , we bound each terms on the right hand side as follows :    for the first term , we have @xmath160 @xmath161", ". therefore , @xmath162 .    to bound the second term , suppose eve obtains @xmath148 and @xmath144 , and wants to decode @xmath149 .", "by symmetry of chaining construction , eve can apply similar decoding rule as described in sec .", "[ sec_scheme_design ] . however , this time eve decodes from the @xmath82th block , then the block error probability of the whole _", "m - chain _ block can be upper bounded by @xmath163 for @xmath43 . hence , by applying fano s inequality , we have @xmath164 .", "\\notag\\end{aligned}\\ ] ] therefore , as @xmath165 , @xmath166 .    for the last term , as @xmath165 , by and", ", we have @xmath167 .", "hence , as @xmath168 , @xmath169 .    after we bound the right hand side of , we know as @xmath165 and @xmath168 , @xmath170 .", "therefore , the weak secrecy constraint is achieved ."], ["we proposed a polar coding scheme that achieves the secrecy capacity of the general wiretap channel , by using the chaining construction technique and polar coding for asymmetric channels .", "compared to previous work , our construction has better decoding error probability and can be constructed more efficiently .", "finally , we note that this chaining construction based polar coding scheme can be extended to achieve _ strong _", "secrecy guarantees as presented in @xcite .", "e.  arikan , `` channel polarization : a method for constructing capacity - achieving codes for symmetric binary - input memoryless channels , '' _ ieee trans .", "inf . theory _ ,", "55 , no .  7 , pp . 30513073 , jul ."]]}
{"article_id": "1402.3198", "article_text": ["enriching microdata with spatial information opens up numerous additional approaches for analysis . in the area of epidemiology", ", this insight goes back at least to the middle of the 19th century when john snow identified a contaminated water pump in london as the source of a cholera outbreak by linking the cases of mortality to their location and visualising these locations and the positions of surrounding water pumps on a map  @xcite .    in recent years", ", spatial analysis techniques have become increasingly attractive in the social sciences as well  @xcite .", "however , when personal microdata containing sensitive information ( e.g. , gathered in a survey or health study ) are published for research purposes , the anonymity of the individuals has to be guaranteed .", "it has been pointed out in  @xcite that _ location is often one of the critical pieces of information for a successful re - identification attack_. therefore , usually only microdata that contain spatial information in an aggregated form are released , which restricts the choice of applicable techniques for analysis drastically .", "in particular , distance calculations that are based on aggregated data become difficult and imprecise  @xcite , especially for entities that are closely related .", "since many data mining techniques and methods in spatial analysis require accurate distance computations , it is necessary to investigate the extent to which additionally published ( approximate ) inter - record distances influence the risk of identity disclosure and how a possible non - acceptable increase of this risk can be prevented .", "our work presented in this article provides a novel approach for tackling these questions .", "we introduce a flexible natural graph model for microdata with known inter - record distances .", "the search for a maximum common subgraph between two such graph models is interpreted as a novel kind of linkage attack on such microdata .", "we discuss the relative merits of our method in comparison to the usual linkage attacks on the basis of a small - scale example ( example  [ example : poets ] in section  [ sec : linkage_attack ] ) .", "furthermore , in the special case of geographical distances , it is shown that , on the basis of simulated data , a non - negligible risk of identity disclosure exists if @xmath0-distributed gaussian noise is added to the input coordinates for too small values of @xmath1 . for larger values of @xmath1 ( which lead to sufficiently anonymised data ) , however , the data become nearly useless for further analysis .", "these results reflect a trade - off between data utility and disclosure risk through the proposed attack .      in section [ sec : background ] , we refer to related work .", "the preparatory work is given in section [ sec : graph_model ] as well as a graph model for microdata in a metric space which forms the basis of the graph theoretic linkage attack introduced in section [ sec : linkage_attack ] . in section [ sec : experimental_results ] , this attack is evaluated by means of a simulation study .", "we conclude and discuss the possible directions for future research in section [ sec : conclusion ] .", "as already indicated in the introductory section above , the original motivation for the work presented in this article stems back to the wish to also make the wide variety of distance - based methods ( e.g. from spatial statistics ) applicable for microdata that are published for scientific purposes .", "since it is intuitively compelling that naive release of the exact distances between individuals can increase the risk of deanonymisation , the interest question , however , is how might the knowledge of approximate distances change the risk of identity disclosure , i.e. the chance of a data snooper attempting to identify some of the entities .", "in general , the analysis of such deanonymisation attacks on microdata and the development of tools for their anonymisation is a central topic of _ statistical disclosure control _", "it is universally acknowledged that a necessary but insufficient first step during the process of anonymisation consists in the removal of all attributes that can be used to identify an individual entity unambiguously ( this step is usually referred to as _ deidentification _ ) .", "such attributes ( e.g. , ` social insurance number ` ) are called _", "( direct ) identifiers _ , in contrast to _ quasi - identifiers _ , which do not have the power to nullify an individual s anonymity on their own , a distinction which has to be ascribed to dalenius  @xcite .    by using a combination of quasi - identifiers , however , it might be possible to assign an entity from the underlying population to a specific record of a published microdata file unambigously .", "for example , in  @xcite it was shown that based on 1990 us census data , 87% of the population of the united states are uniquely determined by their values with respect to the quasi - identifier set \\{`5-digit zip code ` , ` gender ` , ` date of birth`}. this fact motivates a mode of attack that is commonly referred to as _ linkage attack _", "@xcite : in this scenario , it is assumed that a data snooper has access to an external auxiliary microdata file ( called _ identification file _ ) containing both direct identifiers and quasi - identifiers as attributes . by making use of the quasi - identifiers , the snooper attempts to identify entities by linking records from the identification file to records from the published microdata file ( termed _ target file _ ) .", "a real - life example of linkage via quasi - identifiers is due to sweeney  @xcite : she was able to detect the record corresponding to the governor of massachusetts in a published health data file by linkage with a publicly obtainable voter registration list .", "even though theoretical results on linkage attacks were recently obtained in  @xcite , the concept of @xmath2-anonymity had already been proposed as a remedy against linkage attacks in  @xcite .", "the basic idea of @xmath2-anonymity is to modify the records in the released microdata such that every record coincides with at least @xmath3 other records with respect to the quasi - identifiers .", "for this reason , an unambiguous linkage between the identification and target file will not be possible . the graph theoretic linkage attack introduced in section [ sec : linkage_attack ]", "contains the classical linkage attack via quasi - identifiers as a subroutine , however , it provides a way to resolve at least some of the ambiguous matches .", "several papers on _ privacy preserving data mining _ have already discussed privacy issues with respect to the distance - preserving transformations of microdata . however , in these articles it is generally assumed that the considered distances can be directly calculated from the microdata , whereas our focus is on microdata enriched with supplementary distances between the entities that can not be calculated from the microdata itself .", "moreover , in most cases only specific kinds of distances have been considered ( e.g. , @xmath4-distance in  @xcite or the euclidean ( i.e. @xmath5- ) distance in  @xcite ) .    in contrast , the attack proposed in this paper can be applied to any kind of distance function ( notwithstanding that the special case of spatial distances motivated our research and is exclusively referred to in our examples ) .", "furthermore , a distance - preserving technique for the anonymisation of binary vectors is discussed in  @xcite .", "in contrast to our approach , in that article the distance information alone is not assumed to increase the risk of identity disclosure .", "there is a vast literature on the problem of identity disclosure when dealing with spatially referenced data .", "the opportunities and challenges with regard to spatial data in the context of social sciences are discussed in great detail in  @xcite and  @xcite .", "articles  @xcite and  @xcite give illustrative examples of how naive publishing of spatially referenced data can lead to a violation of anonymity : in both cases , the respective authors were able to reconstruct many of the original addresses successfully from published low resolution maps . a currently flourishing branch of research deals with anonymisation techniques for datasets containing mobility traces of individuals  @xcite ( e.g. , obtained via mobile phone tracking ) .", "this topic is usually referred to as _ location privacy _  @xcite .    in this article , however , we consider the deanonymisation risk that arises from the knowledge of the ( approximate ) distances between fixed spatial points assigned to the entities in a microdata table .", "various methods for the anonymisation of geographic point data ( not necessarily taking additional covariates into consideration as in our case ) have been discussed under the term of _ geographical masks_. @xcite and  @xcite provide comprehensive outlines of the existing methods .", "a noteworthy method is due to wieland et al .", "@xcite , who developed a method based on linear programming that moves each point in the dataset as little as possible under a given quantitative risk of re - identification . however , the aim of nearly all proposed anonymisation techniques for spatially referenced data consists in distorting the spatial distribution with respect to the underlying geographical area as little as possible , whereas attempts predominantly focusing on the preservation of distances have not yet been discussed in the context of spatial data .", "it appears to be obvious that neglecting the underlying geographical area might yield more accurate results regarding distance calculations .", "the use of a graph model in this article might suggest a strong connection between our approach and the methods discussed in the area of _ social network anonymisation _  @xcite . however , we model the microdata with known inter - record distances using a complete graph with vertex labels and edge weights , which is a very specific model in contrast to the more general graph models commonly used in social network analysis .", "indeed , the graphs modelling social networks are usually a long way off from being complete and their edges are not usually weighted .", "for example , in  @xcite the underlying graph model considers discrete edge labels instead of real valued weights only .", "furthermore , active attacks ( consisting in the addition of nodes to the published network by an intruder ) as in  @xcite do not seem to be sensible when investigating the risk of identity disclosure for published microdata . however , the active attack proposed in  @xcite is related to the one in this paper because it also makes use of graph algorithmic building blocks .", "it consists in the detection of a subgraph in a larger graph , whereas the attack in this paper is based on finding the common subgraphs of two different graphs .      to the best of our knowledge", ", this paper is the first one to make use of a graph model for a microdata file and the distances between its records .", "finding a matching between two such graph models constitutes the basic principle of the graph theoretic linkage attack proposed in this article and is an often considered problem in the _ pattern recognition _ field and its various areas of application ( see  @xcite as a source providing an extensive outline ) .", "fundamental to our presentation is the article by levi  @xcite , which motivates to transform the problem of finding the ( maximum ) common subgraphs of two graphs into a ( maximum ) clique detection problem , and its adaption in  @xcite where the original approach by levi has been relaxed in order to deal with approximate common subgraphs as well .", "this transformation to the maximum clique detection problem is of particular interest due to its various fields of application ( e.g. biochemistry  @xcite ) .", "the problem of finding a maximum clique in a graph is known to be np - hard  @xcite and a great deal of attention has been paid to the development of techniques for solving this problem either exactly or at least approximately  @xcite . for the simulation study in section", "[ sec : experimental_results ] of this paper , we made use of the maximum clique detection algorithm introduced by konc and janei in  @xcite .", "exploring the limits of our approach in view of its scalability towards very large files is postponed to future research .", "a metric space is a pair @xmath6 , where @xmath7 is a set and @xmath8 is a ( distance ) function @xmath9 satisfying the following three conditions : ( i ) @xmath10 and @xmath11 whenever @xmath12 , ( ii ) @xmath13 and ( iii ) @xmath14 .", "we assume that the deduplicated microdata table @xmath15 at hand contains information with respect to an attribute set @xmath16 about @xmath17 entities from an underlying population .", "the fact that the distances between the entities of @xmath15 are known can be modelled in mathematical terms by means of a function @xmath18:=\\{1,\\ldots , n_t\\ } \\to ( x , d)$ ] , @xmath19 which maps the @xmath20th record / entity of @xmath15 to a point @xmath21 in a metric space @xmath7 such that the distance between records @xmath20 and @xmath22 of @xmath15 is equal to @xmath23 . the distances between all the entities can then be stored in the @xmath24 distance matrix @xmath25 .", "such a pair @xmath26 is hereafter referred to as _ microdata in a metric space_.    note that we did not state any assumptions on the function @xmath27 such as injectivity or surjectivity .", "it is easy to see that @xmath28 $ ] itself becomes a metric space by the pullback of @xmath8 via @xmath27 if and only if @xmath27 is injective ( see page 81 in  @xcite ) .", "in general , @xmath28 $ ] becomes a pseudometric space only . from our point of view , this flexibility regarding @xmath27 is intended as the records of a microdata table often only form a pseudometric instead of a metric space , which is illustrated by the following example : consider microdata about individuals which have been gathered in a scientific survey . if two respondents share a common residence , the geographical distance between these respondents will be equal to zero and thus the set of respondents with the related distances between them forms a pseudometric space only .", "thus , the distance matrix @xmath29 is not assumed to be a proper distance matrix , i.e. zeroes outside the diagonal are permitted .", "given a set @xmath30 , we denote the set of its two - element subsets by @xmath31 ^ 2 $ ] .", "( simple undirected ) graph _ @xmath32 consists of a set @xmath33 ( whose elements are termed _ vertices _ ) and a set @xmath34 ^ 2 $ ] of _ edges_. the cardinality @xmath35 of @xmath33 is called the order of @xmath36 . two distinct vertices @xmath37 and @xmath38 of @xmath33 are _ adjacent _ if @xmath39 .", "the existence of an edge between @xmath37 and @xmath38 will sometimes be denoted by @xmath40 as a shorthand .", "a graph is called _ complete _ if any two of its vertices are adjacent .", "a graph @xmath41 with @xmath42 and @xmath43 ^ 2 \\cap e$ ] is a _ subgraph _ of @xmath32 .", "if @xmath44 ^ 2 \\cap e$ ] holds , the graph @xmath45 is called an _ induced subgraph _ of @xmath36 or we say that the subset @xmath46 of vertices induces @xmath45 in @xmath36 which is denoted by @xmath47 $ ] . a subset of the vertex set @xmath33 is a _ clique _ if the subgraph induced by these vertices is complete . a clique containing @xmath2 elements", "is termed a @xmath2__-clique__. a clique is _ maximal _ if it is not contained in a larger clique .", "a clique is _ maximum _ if there is no other clique containing more vertices .", "clearly , a maximum clique is always maximal , but generally not vice versa .", "the notion of a vertex - labelled and edge - weighted graph is of fundamental importance to the graph model for microdata in a metric space introduced below .", "this notion is just a special case of the more general notion of an _ attributed graph _ which is frequently used in the pattern recognition community  @xcite .", "[ def : graph ] let @xmath48 be a set of vertex labels .", "vertex - labelled _ and _ edge - weighted graph _ is a four - tuple @xmath49 , where @xmath33 is the vertex set , @xmath50 ^ 2 $ ] the edge set , @xmath51 the vertex - labelling function and @xmath52 a weight function which assigns real numbers to the edges .", "let @xmath26 be microdata in a metric space and @xmath53 the number of records in @xmath15 as above .", "an associated vertex - labelled and edge - weighted graph @xmath54 can be defined as follows : set @xmath55 , @xmath56 ^ 2 $ ] and define @xmath57 via @xmath58 ; the labelling function @xmath59 assigns a certain part of the information stored in @xmath15 for a record to the corresponding vertex of the graph @xmath36 ( see example  [ example : graph_model ] below ) .", "note that the simple undirected graph @xmath60 obtained from @xmath36 by forgetting vertex labels and edge weights is the complete graph @xmath61 with @xmath53 vertices .", "this graph theoretical structure appears adequate for modelling microdata in a metric space : loops , i.e. edges linking a vertex with itself , are not necessary because @xmath62 for any vertex @xmath63 and undirected edges are sufficient for reflecting the distance from the corresponding edge weights due to the symmetry @xmath64 of the distance matrix @xmath25 .", "obviously , it would be easy to widen this model , e.g. by introducing directed edges , if this were necessary for a specific application .", "[ example : graph_model ] consider the imaginary microdata provided by table  [ table : example_microdata ] containing personal microdata with respect to the attributes ` name ` , ` sex ` , ` birth location ` and ` year of birth ` .", "the function @xmath27 maps each individual to the geographic coordinates ( longitude @xmath65 and latitude @xmath66 in degrees ) of the correspoding birth location with respect to the world geographic system wgs 84 , i.e. @xmath67    assuming a spherical shape with radius @xmath68 for the earth and converting degrees to radians , the geographical distance @xmath8 between two locations @xmath69 , @xmath70 can be calculated as @xmath71 where @xmath72 using this formula leads to the following distance matrix @xmath29 :    @xmath73    the corresponding graph model is then given by @xmath74 , @xmath56 ^ 2 $ ] and the edge weights are defined via @xmath75 .", "we define the vertex labelling function by assigning the information regarding the attributes ` sex ` and ` year of birth ` to each vertex , i.e. formally , we have @xmath76 .", "the resulting vertex - labelled and edge - weighted graph can be visualised as in figure [ fig : example_model ] .    [ cols=\"^,^,^,^\",options=\"header \" , ]      all the experiments reported here were performed using r and the exact maximum clique detection algorithm proposed in  @xcite .", "all the accompanying visualisations were created in r.      the matches and non - matches between the target and identification file gathered by the proposed graph theoretical linkage attack were classified as true positives ( successful deanonymisation ) , false positives ( failed deanonymisation ) , false negatives ( records belonging to the same entity have been missed ) and true negatives ( records have been correctly classified as belonging to distinct entities ) .", "the quality measures considered are based on the number of true positives ( * tp * ) , false positives ( * fp * ) and false negatives ( * fn * ) .", "more precisely , we consider @xmath77 which are two standard measures in the evaluation of data linkage processes  @xcite .      in our experiments , we varied the noise parameter @xmath1 as well as the threshold parameter @xmath78 .", "for each parameter setup , the simulation was repeated @xmath79 times .", "the mean of precision and recall over all iterations for the chosen parameter setups can be found tables  [ table : results_prec_1 ] and  [ table : results_rec_1 ] .", "visualisations of these results can be found in figures  [ fig : results ] and  [ fig : results_dep_alpha ] .", "in addition , typical outcomes of the graph theoretic linkage attack are visualised in figure  [ fig : typical_results ] .    *", "1l*10r @xmath80  @xmath81 & & & & & & & & & & + 0.1 & 0.7800 & 0.7215 & 0.3415 & 0.2752 & 0.3232 & 0.2193 & 0.1549 & 0.1571 & 0.1136 & 0.1584 + 0.2 & 0.9717 & 0.8929 & 0.8769 & 0.8229 & 0.7569 & 0.6121 & 0.5657 & 0.5094 & 0.4780 & 0.4785 + 0.3 & 0.9831 & 0.9513 & 0.9047 & 0.8502 & 0.8255 & 0.7728 & 0.6862 & 0.6567 & 0.6073 & 0.5975 + 0.4 & 0.9829 & 0.9558 & 0.9037 & 0.8700 & 0.8358 & 0.7766 & 0.7411 & 0.6651 & 0.6428 & 0.6410 + 0.5 & 0.9808 & 0.9458 & 0.9133 & 0.8721 & 0.8374 & 0.7834 & 0.7505 & 0.6832 & 0.6526 & 0.6274 + 0.6 & 0.9830 & 0.9436 & 0.9102 & 0.8725 & 0.8315 & 0.7780 & 0.7430 & 0.6974 & 0.6604 & 0.6229 + 0.7 & 0.9803 & 0.9405 & 0.9087 & 0.8675 & 0.8255 & 0.7707 & 0.7434 & 0.6948 & 0.6556 & 0.6086 + 0.8 & 0.9795 & 0.9373 & 0.9008 & 0.8539 & 0.8027 & 0.7666 & 0.7248 & 0.6894 & 0.6484 & 0.6065 + 0.9 & 0.9764 & 0.9304 & 0.8884 & 0.8351 & 0.7954 & 0.7513 & 0.7017 & 0.6605 & 0.6159 & 0.5876 +    * 1l*10r @xmath80  @xmath81 & & & & & & & & & & + 0.1 & 0.0664 & 0.0612 & 0.0302 & 0.0284 & 0.0326 & 0.0228 & 0.0168 & 0.0174 & 0.0130 & 0.0196 + 0.2 & 0.1166 & 0.1034 & 0.1126 & 0.1144 & 0.1120 & 0.0870 & 0.0864 & 0.0770 & 0.0714 & 0.0818 + 0.3 & 0.1586 & 0.1552 & 0.1556 & 0.1592 & 0.1642 & 0.1416 & 0.1398 & 0.1354 & 0.1292 & 0.1390 + 0.4 & 0.2084 & 0.2204 & 0.2112 & 0.2164 & 0.2162 & 0.1934 & 0.1966 & 0.1894 & 0.1828 & 0.2026 + 0.5 & 0.2774 & 0.2874 & 0.2754 & 0.2880 & 0.2722 & 0.2628 & 0.2526 & 0.2518 & 0.2404 & 0.2564 + 0.6 & 0.3622 & 0.3714 & 0.3308 & 0.3582 & 0.3364 & 0.3322 & 0.3166 & 0.3146 & 0.3084 & 0.3132 + 0.7 & 0.4402 & 0.4490 & 0.4250 & 0.4408 & 0.4198 & 0.4080 & 0.3976 & 0.3992 & 0.3754 & 0.3936 + 0.8 & 0.5638 & 0.5538 & 0.5420 & 0.5408 & 0.5096 & 0.5110 & 0.5204 & 0.5068 & 0.5104 & 0.4966 + 0.9 & 0.7202 & 0.7146 & 0.6960 & 0.6790 & 0.6568 & 0.6568 & 0.6838 & 0.6454 & 0.6658 & 0.6468 +    .5   for different values of @xmath78 ( see tables  [ table : results_prec_1 ] and  [ table : results_rec_1]).,title=\"fig:\",scaledwidth=102.0% ]    .5   for different values of @xmath78 ( see tables  [ table : results_prec_1 ] and  [ table : results_rec_1]).,title=\"fig:\",scaledwidth=102.0% ]    .5   for different values of @xmath1 ( see tables  [ table : results_prec_1 ] and  [ table : results_rec_1]).,title=\"fig:\",scaledwidth=102.0% ]    .5   for different values of @xmath1 ( see tables  [ table : results_prec_1 ] and  [ table : results_rec_1]).,title=\"fig:\",scaledwidth=102.0% ]    .5   ( chosen by the data holder of the target file ) and the threshold parameter @xmath78 ( chosen by the data snooper ) .", "line segments between the target and identification file indicate matches made by the data snooper ( the green lines indicate true , the red ones false positives ) .", "larger values of @xmath78 lead to more matches and a increase in recall , larger values of @xmath1 to a decrease in precision.,title=\"fig:\",scaledwidth=102.0% ]    .5   ( chosen by the data holder of the target file ) and the threshold parameter @xmath78 ( chosen by the data snooper ) . line segments between the target and identification file indicate matches made by the data snooper ( the green lines indicate true , the red ones false positives ) .", "larger values of @xmath78 lead to more matches and a increase in recall , larger values of @xmath1 to a decrease in precision.,title=\"fig:\",scaledwidth=102.0% ]    .5   ( chosen by the data holder of the target file ) and the threshold parameter @xmath78 ( chosen by the data snooper ) .", "line segments between the target and identification file indicate matches made by the data snooper ( the green lines indicate true , the red ones false positives ) .", "larger values of @xmath78 lead to more matches and a increase in recall , larger values of @xmath1 to a decrease in precision.,title=\"fig:\",scaledwidth=102.0% ]    .5   ( chosen by the data holder of the target file ) and the threshold parameter @xmath78 ( chosen by the data snooper ) .", "line segments between the target and identification file indicate matches made by the data snooper ( the green lines indicate true , the red ones false positives ) .", "larger values of @xmath78 lead to more matches and a increase in recall , larger values of @xmath1 to a decrease in precision.,title=\"fig:\",scaledwidth=102.0% ]      the main effect of the threshold parameter @xmath78 concerns the recall .", "the probability of detecting a common edge of the target and identification graph is approximately equal to @xmath78 .", "for this reason , higher values of @xmath78 lead to a higher recall ( see table  [ table : results_rec_1 ] and figures  [ fig : results ] and  [ fig : results_dep_alpha ] ) .", "simultaneously , the effect of @xmath78 on the precision appears to be twofold : on the one hand , for increasing @xmath78 a larger portion of the overlap between the identification and target file can be successfully detected by the snooper , which makes false positives less likely ( leading to a larger precision ) . on the other hand , for too high values of @xmath78 also the chance for non - common edges of the target and identification graph ( but which coincide with respect to the vertex labels of their endpoints )", "to be classified as common edges increases leading to a slight decrease in precision .", "the latter phenomenon , together with the increase in recall for increasing @xmath78 mentioned above , would reflect a trade - off between precision and recall , which is a well - known phenomenon in data linkage  @xcite .", "thus , combining these two thoughts , for increasing @xmath78 the precision should rapidly increase initially and then slightly decrease when @xmath78 becomes too large .", "this expectation is confirmed by our experiments ( see table  [ table : results_prec_1 ] and figures  [ fig : results ] and  [ fig : results_dep_alpha ] ) , although the decrease in precision when @xmath78 becomes too large is not significant for the considered values of @xmath1 .    from the definition of @xmath82 ( see the paragraph _ fine - tuning of the attack _ above ) , it is supposed that the recall does not change significantly in dependence on @xmath1 because the probability of correctly detecting an edge should be nearly @xmath78 ( which is independent of @xmath1 ) .", "this non - dependence is impressively confirmed by the performed simulations and illustrated in figures  [ fig : results ] and [ fig : results_dep_alpha ] .", "however , @xmath1 strongly influences the precision ( for larger values of @xmath1 the precision evidently decreases ) : the data snooper has to accept false positives ( resulting in less precision ) if she / he wants to achieve a certain predetermined recall .    altogether ,", "the simulations show that , in principle , a sufficient level of anonymity can be achieved by the addition of random noise to the input coordinates before computing the distance matrix . however , this anonymity is not free , which is illustrated by means of the risk - utility ( r - u ) confidentiality map in figure  [ fig : ru ] , where the risk of identity disclosure ( measured by the average precision of the linkage attack ; see also the discussion below ) is plotted against the utility ( measured as the reciprocal of the sample variance of the distance deviation @xmath83 for the current value of @xmath1 as recorded in table  [ table : distance_deviation ] ) . in the datasets considered in the simulation study", "@xmath1 would have to be chosen large enough to guarantee at least some degree of anonymity , that useful analyses based on the distances would become difficult . for this reason , the development of distance modification techniques that guarantee a certain degree of anonymity , and make it possible to also conduct useful analyses on the anonymised data , will be an important aspect of future research .    .", "the threshold parameter @xmath78 was chosen equal to @xmath84.,scaledwidth=99.0% ]    note that in our specific example , the snooper would primarily attempt to achieve a high precision : in the case of geographic distances , a point is uniquely determined by the exact distances to three other points . if the snooper could deanonymise at least three entities successfully , exploiting this fact would be a good starting point to identify even more individuals . for arbitrary metric spaces ,", "such a relationship does not hold in general , albeit the successful deanonymisation of some entities would also alleviate a snooper s work in this more general case .", "obviously , for distance modification techniques other than perturbation of the input coordinates , a snooper will have to modify the graph theoretical linkage attack , especially the definition of @xmath82 .", "however , due to kerckhoffs principle , it has to be assumed that the snooper at least knows the distance modification technique used by the holder of the target file and exploits this knowledge in the precise construction of the attack .", "for instance , if noise is not added to the input coordinates before computing the distance matrix but rather to the distance matrix itself ( a technique discussed in  @xcite ) , the attack has to be slightly adapted . in this case , when defining the relation @xmath82 the quantiles of the noise distribution can be used directly , thereby making the empirical study on distance deviations originating from perturbation of the input coordinates unnecessary .", "moreover , in this specific case it might be reasonable to further modify the attack by relaxing the ( relatively strong ) notion of a maximum clique to the less restrictive notion of a maximum quasi - clique , a relaxation which has been successfully applied in  @xcite for the purpose of protein classification . in a similar way", ", our attack can be adapted to many other anonymisation techniques and thus provides a useful and flexible tool for the analysis of methods for distance - preserving anonymisation .", "in this article , we have introduced a novel graph theoretic linkage attack on microdata with additionally published ( approximate ) inter - record distances . in the special case of spatial distances , we have demonstrated  on the basis of our test data  that the release of distances increases the risk of identity disclosure unreasonably even if geographical coordinates have been perturbed by random gaussian noise before the distances are calculated .", "furthermore , we showed that augmenting the standard deviation of the added random noise will gradually lead to a sufficient level of anonymity , but also make the perturbed distances useless for further analysis .", "therefore , the development and analysis of anonymisation techniques for microdata in a metric space that guarantee a certain degree of anonymity but distort the distances as little as possible ( particularly with regard to the applicability of data mining techniques ) will be an important aspect of future research .", "this research has been supported by a grant from the german research foundation ( dfg ) to rainer schnell .", "i am grateful to rainer schnell for valuable suggestions that helped to improve the presentation of this paper", ".    99    m.p .", "armstrong , g. rushton and d.l .", "zimmerman ( 1999 ) geographically masking health data to preserve confidentiality .", "_ statistics in medicine _ 18:497525 .", "l. backstrom , c. dwork and j. kleinberg ( 2007 ) wherefore art thou r3579x ? : anonymized social networks , hidden patterns , and structural steganography .", "_ acm proceedings of the 16th international conference on world wide web _ , 181190 .", "beyer , a.f .", "saftlas , a.b .", "wallis , c. peek - asa and g. rushton ( 2011 ) a probabilistic sampling method ( psm ) for estimating geographic distance to health services when only the region of residence is known .", "_ international journal of health geographics _ 10:4 .", "bomze , m. budinich , p.m. pardalos and m. pelillo ( 1999 ) the maximum clique problem . in : d", "du and p. pardalos ( eds . ) : _ handbook of combinatorial optimization _ , 174 .", "brownstein , c.a .", "cassa , i.s .", "kohane and k.d .", "mandl ( 2006 ) an unsupervised classification method for inferring original case locations from low - resolution disease maps .", "_ international journal of health geographics _ 5:56 .", "h. bunke and k. riesen ( 2012 ) towards the unification of structural and statistical pattern recognition .", "_ pattern recognition letters _ 33:811825 .", "s. chester , b.m .", "kapron , g. srivastava and s. venkatesh ( 2013 ) complexity of social network anonymization .", "_ social network analysis and mining _ 3:151166 .", "d. conte , p. foggia , c. sansone and m. vento ( 2004 ) thirty years of graph matching in pattern recognition . _ international journal of pattern recognition and artificial intelligence _ 18:265298 .", "p. christen and k. goiser ( 2007 ) quality and complexity measures for data linkage and deduplication . in : f. guillet and h.j .", "hamilton ( eds . ) : _ quality measures in data mining _ , 127151 .", "g. csardi and t. nepusz ( 2006 ) the igraph software package for complex network research .", "_ interjournal , complex systems _", "http://igraph.sf.net .", "curtis , j.w .", "mills and m. leitner ( 2006 ) spatial confidentiality and gis : re - engineering mortality locations from published maps about hurricane katrina .", "_ international journal of health geographics _ 5:44 .", "t. dalenius ( 1986 ) finding a needle in a haystack  or identifying anonymous census records .", "_ journal of official statistics _ 2(3):329336 .", "deza and e. deza ( 2009 ) encyclopedia of distances .", "_ springer_.    g.t .", "duncan , m. elliot and j .- j .", "salazar - gonzlez ( 2011 ) statistical confidentiality : principles and practise .", "_ springer_.    k. el emam and l. arbuckle ( 2013 ) anonymizing health data .", "_ oreilly_.    t. fober , g. klebe and e. hllermeier ( 2013 ) local clique merging : an extension of the maximum common subgraph measure with applications in structural bioinformatics . in : b.  lausen ,", "d. van den poel and a. utsch ( eds . ) : _ algorithms from and for nature and life _ , 279286 .", "s. gambs , m .- o . killijian and m. nez del prado cortez ( 2011 ) show me how you move and i will tell you who you are . _ transactions on data privacy", "_ 4:103126 .", "garey and d.s .", "johnson ( 1979 ) computers and intractability : a guide to the theory of np - completeness .", "_ freeman_.    m.p . gutmann and p. c. stern ( 2007 ) putting people on the map : protecting confidentiality with linked social - spatial data .", "_ national academies press_.    m.p .", "gutmann , k. witkowski , c. colyer , j.m .", "orourke and j. mcnally ( 2008 ) providing spatial data for secondary analysis : issues and current practises relating to confidentiality", ". _ population research and policy review _ 27:639665 .", "a. hundepool , j. domingo - ferrer , l. franconi , s. giessing , e. schulte nordholt , k. spicer and p .-", "de wolf ( 2012 ) statistical disclosure control . _ wiley series in survey methodology_.    d. kahle and h. wickham ( 2013 ) ggmap : a package for spatial visualization with google maps and openstreetmap .", "r package version 2.3 .", "http://cran.r-project.org/package=ggmap .", "k. kenthapadi , a. korolova , i. mironov and n. mishra ( 2013 ) privacy via the johnson - lindenstrauss transform .", "_ journal of privacy and confidentiality _ 5(1):3971 .", "j. konc and d. janei ( 2007 ) an improved branch and bound algorithm for the maximum clique problem .", "_ match commun . math .", "_ 58:569590 .", "j. krumm ( 2009 ) a survey of computational location privacy .", "_ personal and ubiquitous computing _ 13(6):391399 .", "g. levi ( 1973 ) a note on the derivation of maximal common subgraphs of two directed or undirected graphs .", "_ calcolo _ 9(4):341352 .", "k. liu , c. giannella and h. kargupta ( 2006 ) an attacker s view of distance preserving maps for privacy preserving data mining . in : j.", "frnkranz , t. scheffer and m. spiliopoulou ( eds . ) : _ knowledge discovery in databases : 10th european conference on principles and practice of knowledge discovery in databases _ , 297308 .", "m. merener ( 2012 ) theoretical results on de - anonymization via linkage attacks .", "_ transactions on data privacy _ 5(2):377402 .    c. m.", "okeefe ( 2012 ) confidentialising maps of mixed point and diffuse spatial data . in : j. domingo - ferrer and i. tinnirello ( eds . ) : _ privacy in statistical databases _ , 226240 .", "parker and e.k .", "asencio ( 2008 ) gis and spatial analysis for the social sciences : coding , mapping , and modeling .", "_ routledge_.    f.a.p .", "petitcolas ( 2011 ) kerckhoffs principle . in : h.c.a .", "van tilborg and s. jajodie ( eds . ) : _ encyclopedia of cryptography and security _", "s. rane , w. sun and a. vetro ( 2010 ) privacy - preserving approximation of l1 distance for multimedia applications .", "_ ieee international conference on multimedia and expo ( icme ) _ , 492497 .", "p. samarati and l. sweeney ( 1998 ) protecting privacy when disclosing information : @xmath2-anonymity and its enforcement through generalization and suppression .", "_ technical report , sri international_.    j. snow ( 1855 ) on the mode of communication of cholera .", "_ john churchill_.    l. sweeney ( 2000 ) uniqueness of simple demographics in the us population .", "_ technical report_.    l. sweeney ( 2002 ) @xmath2-anonymity : a model for protecting privacy .", "_ international journal of uncertainty , fuzziness and knowledge - based systems _ 10:557570 .", "wieland , c.a .", "cassa , k.d .", "mandl and b. berger ( 2008 ) revealing the spatial distribution of a disease while preserving privacy .", "_ proceedings of the national academy of sciences _ 105:1760817613 .", "e. zheleva and l. getoor ( 2011 ) privacy in social networks : a survey . in : c.c .", "aggarwal ( ed . ) : _ social network data analytics _", ", 277306 .", "the distances between birth locations ` loc ` are stored in the distance matrix @xmath85 : @xmath86    geocoding of the locations from table  [ table : poets_intruder ] using the r package ` ggmap ` and calculation of the mutual distances via the command ` spdists ` from the package ` sp ` yields the distance matrix @xmath87 :"], "abstract_text": ["<S> certain methods of analysis require the knowledge of the spatial distances between entities whose data are stored in a microdata table . </S>", "<S> for instance , such knowledge is necessary and sufficient to perform data mining tasks such as nearest neighbour searches or clustering . </S>", "<S> however , when inter - record distances are published in addition to the microdata for research purposes , the risk of identity disclosure has to be taken into consideration again . in order to tackle this problem , we introduce a flexible graph model for microdata in a metric space and propose a linkage attack based on realistic assumptions of a data snooper s background knowledge . </S>", "<S> this attack is based on the idea of finding a maximum approximate common subgraph of two vertex - labelled and edge - weighted graphs . by adapting a standard argument from algorithmic graph theory to our setup , </S>", "<S> this task is transformed to the maximum clique detection problem in a corresponding product graph . using a toy example and experimental results on simulated data </S>", "<S> show that publishing even approximate distances could increase the risk of identity disclosure unreasonably .    </S>", "<S> _ keywords _ : anonymity , identity disclosure , linkage attack , maximum approximate common subgraph problem </S>"], "labels": null, "section_names": ["introduction", "related work", "a graph model for microdata in a metric space", "conclusion", "acknowledgements", "example dataset: european poets"], "sections": [["enriching microdata with spatial information opens up numerous additional approaches for analysis . in the area of epidemiology", ", this insight goes back at least to the middle of the 19th century when john snow identified a contaminated water pump in london as the source of a cholera outbreak by linking the cases of mortality to their location and visualising these locations and the positions of surrounding water pumps on a map  @xcite .    in recent years", ", spatial analysis techniques have become increasingly attractive in the social sciences as well  @xcite .", "however , when personal microdata containing sensitive information ( e.g. , gathered in a survey or health study ) are published for research purposes , the anonymity of the individuals has to be guaranteed .", "it has been pointed out in  @xcite that _ location is often one of the critical pieces of information for a successful re - identification attack_. therefore , usually only microdata that contain spatial information in an aggregated form are released , which restricts the choice of applicable techniques for analysis drastically .", "in particular , distance calculations that are based on aggregated data become difficult and imprecise  @xcite , especially for entities that are closely related .", "since many data mining techniques and methods in spatial analysis require accurate distance computations , it is necessary to investigate the extent to which additionally published ( approximate ) inter - record distances influence the risk of identity disclosure and how a possible non - acceptable increase of this risk can be prevented .", "our work presented in this article provides a novel approach for tackling these questions .", "we introduce a flexible natural graph model for microdata with known inter - record distances .", "the search for a maximum common subgraph between two such graph models is interpreted as a novel kind of linkage attack on such microdata .", "we discuss the relative merits of our method in comparison to the usual linkage attacks on the basis of a small - scale example ( example  [ example : poets ] in section  [ sec : linkage_attack ] ) .", "furthermore , in the special case of geographical distances , it is shown that , on the basis of simulated data , a non - negligible risk of identity disclosure exists if @xmath0-distributed gaussian noise is added to the input coordinates for too small values of @xmath1 . for larger values of @xmath1 ( which lead to sufficiently anonymised data ) , however , the data become nearly useless for further analysis .", "these results reflect a trade - off between data utility and disclosure risk through the proposed attack .      in section [ sec : background ] , we refer to related work .", "the preparatory work is given in section [ sec : graph_model ] as well as a graph model for microdata in a metric space which forms the basis of the graph theoretic linkage attack introduced in section [ sec : linkage_attack ] . in section [ sec : experimental_results ] , this attack is evaluated by means of a simulation study .", "we conclude and discuss the possible directions for future research in section [ sec : conclusion ] ."], ["as already indicated in the introductory section above , the original motivation for the work presented in this article stems back to the wish to also make the wide variety of distance - based methods ( e.g. from spatial statistics ) applicable for microdata that are published for scientific purposes .", "since it is intuitively compelling that naive release of the exact distances between individuals can increase the risk of deanonymisation , the interest question , however , is how might the knowledge of approximate distances change the risk of identity disclosure , i.e. the chance of a data snooper attempting to identify some of the entities .", "in general , the analysis of such deanonymisation attacks on microdata and the development of tools for their anonymisation is a central topic of _ statistical disclosure control _", "it is universally acknowledged that a necessary but insufficient first step during the process of anonymisation consists in the removal of all attributes that can be used to identify an individual entity unambiguously ( this step is usually referred to as _ deidentification _ ) .", "such attributes ( e.g. , ` social insurance number ` ) are called _", "( direct ) identifiers _ , in contrast to _ quasi - identifiers _ , which do not have the power to nullify an individual s anonymity on their own , a distinction which has to be ascribed to dalenius  @xcite .    by using a combination of quasi - identifiers , however , it might be possible to assign an entity from the underlying population to a specific record of a published microdata file unambigously .", "for example , in  @xcite it was shown that based on 1990 us census data , 87% of the population of the united states are uniquely determined by their values with respect to the quasi - identifier set \\{`5-digit zip code ` , ` gender ` , ` date of birth`}. this fact motivates a mode of attack that is commonly referred to as _ linkage attack _", "@xcite : in this scenario , it is assumed that a data snooper has access to an external auxiliary microdata file ( called _ identification file _ ) containing both direct identifiers and quasi - identifiers as attributes . by making use of the quasi - identifiers , the snooper attempts to identify entities by linking records from the identification file to records from the published microdata file ( termed _ target file _ ) .", "a real - life example of linkage via quasi - identifiers is due to sweeney  @xcite : she was able to detect the record corresponding to the governor of massachusetts in a published health data file by linkage with a publicly obtainable voter registration list .", "even though theoretical results on linkage attacks were recently obtained in  @xcite , the concept of @xmath2-anonymity had already been proposed as a remedy against linkage attacks in  @xcite .", "the basic idea of @xmath2-anonymity is to modify the records in the released microdata such that every record coincides with at least @xmath3 other records with respect to the quasi - identifiers .", "for this reason , an unambiguous linkage between the identification and target file will not be possible . the graph theoretic linkage attack introduced in section [ sec : linkage_attack ]", "contains the classical linkage attack via quasi - identifiers as a subroutine , however , it provides a way to resolve at least some of the ambiguous matches .", "several papers on _ privacy preserving data mining _ have already discussed privacy issues with respect to the distance - preserving transformations of microdata . however , in these articles it is generally assumed that the considered distances can be directly calculated from the microdata , whereas our focus is on microdata enriched with supplementary distances between the entities that can not be calculated from the microdata itself .", "moreover , in most cases only specific kinds of distances have been considered ( e.g. , @xmath4-distance in  @xcite or the euclidean ( i.e. @xmath5- ) distance in  @xcite ) .    in contrast , the attack proposed in this paper can be applied to any kind of distance function ( notwithstanding that the special case of spatial distances motivated our research and is exclusively referred to in our examples ) .", "furthermore , a distance - preserving technique for the anonymisation of binary vectors is discussed in  @xcite .", "in contrast to our approach , in that article the distance information alone is not assumed to increase the risk of identity disclosure .", "there is a vast literature on the problem of identity disclosure when dealing with spatially referenced data .", "the opportunities and challenges with regard to spatial data in the context of social sciences are discussed in great detail in  @xcite and  @xcite .", "articles  @xcite and  @xcite give illustrative examples of how naive publishing of spatially referenced data can lead to a violation of anonymity : in both cases , the respective authors were able to reconstruct many of the original addresses successfully from published low resolution maps . a currently flourishing branch of research deals with anonymisation techniques for datasets containing mobility traces of individuals  @xcite ( e.g. , obtained via mobile phone tracking ) .", "this topic is usually referred to as _ location privacy _  @xcite .    in this article , however , we consider the deanonymisation risk that arises from the knowledge of the ( approximate ) distances between fixed spatial points assigned to the entities in a microdata table .", "various methods for the anonymisation of geographic point data ( not necessarily taking additional covariates into consideration as in our case ) have been discussed under the term of _ geographical masks_. @xcite and  @xcite provide comprehensive outlines of the existing methods .", "a noteworthy method is due to wieland et al .", "@xcite , who developed a method based on linear programming that moves each point in the dataset as little as possible under a given quantitative risk of re - identification . however , the aim of nearly all proposed anonymisation techniques for spatially referenced data consists in distorting the spatial distribution with respect to the underlying geographical area as little as possible , whereas attempts predominantly focusing on the preservation of distances have not yet been discussed in the context of spatial data .", "it appears to be obvious that neglecting the underlying geographical area might yield more accurate results regarding distance calculations .", "the use of a graph model in this article might suggest a strong connection between our approach and the methods discussed in the area of _ social network anonymisation _  @xcite . however , we model the microdata with known inter - record distances using a complete graph with vertex labels and edge weights , which is a very specific model in contrast to the more general graph models commonly used in social network analysis .", "indeed , the graphs modelling social networks are usually a long way off from being complete and their edges are not usually weighted .", "for example , in  @xcite the underlying graph model considers discrete edge labels instead of real valued weights only .", "furthermore , active attacks ( consisting in the addition of nodes to the published network by an intruder ) as in  @xcite do not seem to be sensible when investigating the risk of identity disclosure for published microdata . however , the active attack proposed in  @xcite is related to the one in this paper because it also makes use of graph algorithmic building blocks .", "it consists in the detection of a subgraph in a larger graph , whereas the attack in this paper is based on finding the common subgraphs of two different graphs .      to the best of our knowledge", ", this paper is the first one to make use of a graph model for a microdata file and the distances between its records .", "finding a matching between two such graph models constitutes the basic principle of the graph theoretic linkage attack proposed in this article and is an often considered problem in the _ pattern recognition _ field and its various areas of application ( see  @xcite as a source providing an extensive outline ) .", "fundamental to our presentation is the article by levi  @xcite , which motivates to transform the problem of finding the ( maximum ) common subgraphs of two graphs into a ( maximum ) clique detection problem , and its adaption in  @xcite where the original approach by levi has been relaxed in order to deal with approximate common subgraphs as well .", "this transformation to the maximum clique detection problem is of particular interest due to its various fields of application ( e.g. biochemistry  @xcite ) .", "the problem of finding a maximum clique in a graph is known to be np - hard  @xcite and a great deal of attention has been paid to the development of techniques for solving this problem either exactly or at least approximately  @xcite . for the simulation study in section", "[ sec : experimental_results ] of this paper , we made use of the maximum clique detection algorithm introduced by konc and janei in  @xcite .", "exploring the limits of our approach in view of its scalability towards very large files is postponed to future research ."], ["a metric space is a pair @xmath6 , where @xmath7 is a set and @xmath8 is a ( distance ) function @xmath9 satisfying the following three conditions : ( i ) @xmath10 and @xmath11 whenever @xmath12 , ( ii ) @xmath13 and ( iii ) @xmath14 .", "we assume that the deduplicated microdata table @xmath15 at hand contains information with respect to an attribute set @xmath16 about @xmath17 entities from an underlying population .", "the fact that the distances between the entities of @xmath15 are known can be modelled in mathematical terms by means of a function @xmath18:=\\{1,\\ldots , n_t\\ } \\to ( x , d)$ ] , @xmath19 which maps the @xmath20th record / entity of @xmath15 to a point @xmath21 in a metric space @xmath7 such that the distance between records @xmath20 and @xmath22 of @xmath15 is equal to @xmath23 . the distances between all the entities can then be stored in the @xmath24 distance matrix @xmath25 .", "such a pair @xmath26 is hereafter referred to as _ microdata in a metric space_.    note that we did not state any assumptions on the function @xmath27 such as injectivity or surjectivity .", "it is easy to see that @xmath28 $ ] itself becomes a metric space by the pullback of @xmath8 via @xmath27 if and only if @xmath27 is injective ( see page 81 in  @xcite ) .", "in general , @xmath28 $ ] becomes a pseudometric space only . from our point of view , this flexibility regarding @xmath27 is intended as the records of a microdata table often only form a pseudometric instead of a metric space , which is illustrated by the following example : consider microdata about individuals which have been gathered in a scientific survey . if two respondents share a common residence , the geographical distance between these respondents will be equal to zero and thus the set of respondents with the related distances between them forms a pseudometric space only .", "thus , the distance matrix @xmath29 is not assumed to be a proper distance matrix , i.e. zeroes outside the diagonal are permitted .", "given a set @xmath30 , we denote the set of its two - element subsets by @xmath31 ^ 2 $ ] .", "( simple undirected ) graph _ @xmath32 consists of a set @xmath33 ( whose elements are termed _ vertices _ ) and a set @xmath34 ^ 2 $ ] of _ edges_. the cardinality @xmath35 of @xmath33 is called the order of @xmath36 . two distinct vertices @xmath37 and @xmath38 of @xmath33 are _ adjacent _ if @xmath39 .", "the existence of an edge between @xmath37 and @xmath38 will sometimes be denoted by @xmath40 as a shorthand .", "a graph is called _ complete _ if any two of its vertices are adjacent .", "a graph @xmath41 with @xmath42 and @xmath43 ^ 2 \\cap e$ ] is a _ subgraph _ of @xmath32 .", "if @xmath44 ^ 2 \\cap e$ ] holds , the graph @xmath45 is called an _ induced subgraph _ of @xmath36 or we say that the subset @xmath46 of vertices induces @xmath45 in @xmath36 which is denoted by @xmath47 $ ] . a subset of the vertex set @xmath33 is a _ clique _ if the subgraph induced by these vertices is complete . a clique containing @xmath2 elements", "is termed a @xmath2__-clique__. a clique is _ maximal _ if it is not contained in a larger clique .", "a clique is _ maximum _ if there is no other clique containing more vertices .", "clearly , a maximum clique is always maximal , but generally not vice versa .", "the notion of a vertex - labelled and edge - weighted graph is of fundamental importance to the graph model for microdata in a metric space introduced below .", "this notion is just a special case of the more general notion of an _ attributed graph _ which is frequently used in the pattern recognition community  @xcite .", "[ def : graph ] let @xmath48 be a set of vertex labels .", "vertex - labelled _ and _ edge - weighted graph _ is a four - tuple @xmath49 , where @xmath33 is the vertex set , @xmath50 ^ 2 $ ] the edge set , @xmath51 the vertex - labelling function and @xmath52 a weight function which assigns real numbers to the edges .", "let @xmath26 be microdata in a metric space and @xmath53 the number of records in @xmath15 as above .", "an associated vertex - labelled and edge - weighted graph @xmath54 can be defined as follows : set @xmath55 , @xmath56 ^ 2 $ ] and define @xmath57 via @xmath58 ; the labelling function @xmath59 assigns a certain part of the information stored in @xmath15 for a record to the corresponding vertex of the graph @xmath36 ( see example  [ example : graph_model ] below ) .", "note that the simple undirected graph @xmath60 obtained from @xmath36 by forgetting vertex labels and edge weights is the complete graph @xmath61 with @xmath53 vertices .", "this graph theoretical structure appears adequate for modelling microdata in a metric space : loops , i.e. edges linking a vertex with itself , are not necessary because @xmath62 for any vertex @xmath63 and undirected edges are sufficient for reflecting the distance from the corresponding edge weights due to the symmetry @xmath64 of the distance matrix @xmath25 .", "obviously , it would be easy to widen this model , e.g. by introducing directed edges , if this were necessary for a specific application .", "[ example : graph_model ] consider the imaginary microdata provided by table  [ table : example_microdata ] containing personal microdata with respect to the attributes ` name ` , ` sex ` , ` birth location ` and ` year of birth ` .", "the function @xmath27 maps each individual to the geographic coordinates ( longitude @xmath65 and latitude @xmath66 in degrees ) of the correspoding birth location with respect to the world geographic system wgs 84 , i.e. @xmath67    assuming a spherical shape with radius @xmath68 for the earth and converting degrees to radians , the geographical distance @xmath8 between two locations @xmath69 , @xmath70 can be calculated as @xmath71 where @xmath72 using this formula leads to the following distance matrix @xmath29 :    @xmath73    the corresponding graph model is then given by @xmath74 , @xmath56 ^ 2 $ ] and the edge weights are defined via @xmath75 .", "we define the vertex labelling function by assigning the information regarding the attributes ` sex ` and ` year of birth ` to each vertex , i.e. formally , we have @xmath76 .", "the resulting vertex - labelled and edge - weighted graph can be visualised as in figure [ fig : example_model ] .    [ cols=\"^,^,^,^\",options=\"header \" , ]      all the experiments reported here were performed using r and the exact maximum clique detection algorithm proposed in  @xcite .", "all the accompanying visualisations were created in r.      the matches and non - matches between the target and identification file gathered by the proposed graph theoretical linkage attack were classified as true positives ( successful deanonymisation ) , false positives ( failed deanonymisation ) , false negatives ( records belonging to the same entity have been missed ) and true negatives ( records have been correctly classified as belonging to distinct entities ) .", "the quality measures considered are based on the number of true positives ( * tp * ) , false positives ( * fp * ) and false negatives ( * fn * ) .", "more precisely , we consider @xmath77 which are two standard measures in the evaluation of data linkage processes  @xcite .      in our experiments , we varied the noise parameter @xmath1 as well as the threshold parameter @xmath78 .", "for each parameter setup , the simulation was repeated @xmath79 times .", "the mean of precision and recall over all iterations for the chosen parameter setups can be found tables  [ table : results_prec_1 ] and  [ table : results_rec_1 ] .", "visualisations of these results can be found in figures  [ fig : results ] and  [ fig : results_dep_alpha ] .", "in addition , typical outcomes of the graph theoretic linkage attack are visualised in figure  [ fig : typical_results ] .    *", "1l*10r @xmath80  @xmath81 & & & & & & & & & & + 0.1 & 0.7800 & 0.7215 & 0.3415 & 0.2752 & 0.3232 & 0.2193 & 0.1549 & 0.1571 & 0.1136 & 0.1584 + 0.2 & 0.9717 & 0.8929 & 0.8769 & 0.8229 & 0.7569 & 0.6121 & 0.5657 & 0.5094 & 0.4780 & 0.4785 + 0.3 & 0.9831 & 0.9513 & 0.9047 & 0.8502 & 0.8255 & 0.7728 & 0.6862 & 0.6567 & 0.6073 & 0.5975 + 0.4 & 0.9829 & 0.9558 & 0.9037 & 0.8700 & 0.8358 & 0.7766 & 0.7411 & 0.6651 & 0.6428 & 0.6410 + 0.5 & 0.9808 & 0.9458 & 0.9133 & 0.8721 & 0.8374 & 0.7834 & 0.7505 & 0.6832 & 0.6526 & 0.6274 + 0.6 & 0.9830 & 0.9436 & 0.9102 & 0.8725 & 0.8315 & 0.7780 & 0.7430 & 0.6974 & 0.6604 & 0.6229 + 0.7 & 0.9803 & 0.9405 & 0.9087 & 0.8675 & 0.8255 & 0.7707 & 0.7434 & 0.6948 & 0.6556 & 0.6086 + 0.8 & 0.9795 & 0.9373 & 0.9008 & 0.8539 & 0.8027 & 0.7666 & 0.7248 & 0.6894 & 0.6484 & 0.6065 + 0.9 & 0.9764 & 0.9304 & 0.8884 & 0.8351 & 0.7954 & 0.7513 & 0.7017 & 0.6605 & 0.6159 & 0.5876 +    * 1l*10r @xmath80  @xmath81 & & & & & & & & & & + 0.1 & 0.0664 & 0.0612 & 0.0302 & 0.0284 & 0.0326 & 0.0228 & 0.0168 & 0.0174 & 0.0130 & 0.0196 + 0.2 & 0.1166 & 0.1034 & 0.1126 & 0.1144 & 0.1120 & 0.0870 & 0.0864 & 0.0770 & 0.0714 & 0.0818 + 0.3 & 0.1586 & 0.1552 & 0.1556 & 0.1592 & 0.1642 & 0.1416 & 0.1398 & 0.1354 & 0.1292 & 0.1390 + 0.4 & 0.2084 & 0.2204 & 0.2112 & 0.2164 & 0.2162 & 0.1934 & 0.1966 & 0.1894 & 0.1828 & 0.2026 + 0.5 & 0.2774 & 0.2874 & 0.2754 & 0.2880 & 0.2722 & 0.2628 & 0.2526 & 0.2518 & 0.2404 & 0.2564 + 0.6 & 0.3622 & 0.3714 & 0.3308 & 0.3582 & 0.3364 & 0.3322 & 0.3166 & 0.3146 & 0.3084 & 0.3132 + 0.7 & 0.4402 & 0.4490 & 0.4250 & 0.4408 & 0.4198 & 0.4080 & 0.3976 & 0.3992 & 0.3754 & 0.3936 + 0.8 & 0.5638 & 0.5538 & 0.5420 & 0.5408 & 0.5096 & 0.5110 & 0.5204 & 0.5068 & 0.5104 & 0.4966 + 0.9 & 0.7202 & 0.7146 & 0.6960 & 0.6790 & 0.6568 & 0.6568 & 0.6838 & 0.6454 & 0.6658 & 0.6468 +    .5   for different values of @xmath78 ( see tables  [ table : results_prec_1 ] and  [ table : results_rec_1]).,title=\"fig:\",scaledwidth=102.0% ]    .5   for different values of @xmath78 ( see tables  [ table : results_prec_1 ] and  [ table : results_rec_1]).,title=\"fig:\",scaledwidth=102.0% ]    .5   for different values of @xmath1 ( see tables  [ table : results_prec_1 ] and  [ table : results_rec_1]).,title=\"fig:\",scaledwidth=102.0% ]    .5   for different values of @xmath1 ( see tables  [ table : results_prec_1 ] and  [ table : results_rec_1]).,title=\"fig:\",scaledwidth=102.0% ]    .5   ( chosen by the data holder of the target file ) and the threshold parameter @xmath78 ( chosen by the data snooper ) .", "line segments between the target and identification file indicate matches made by the data snooper ( the green lines indicate true , the red ones false positives ) .", "larger values of @xmath78 lead to more matches and a increase in recall , larger values of @xmath1 to a decrease in precision.,title=\"fig:\",scaledwidth=102.0% ]    .5   ( chosen by the data holder of the target file ) and the threshold parameter @xmath78 ( chosen by the data snooper ) . line segments between the target and identification file indicate matches made by the data snooper ( the green lines indicate true , the red ones false positives ) .", "larger values of @xmath78 lead to more matches and a increase in recall , larger values of @xmath1 to a decrease in precision.,title=\"fig:\",scaledwidth=102.0% ]    .5   ( chosen by the data holder of the target file ) and the threshold parameter @xmath78 ( chosen by the data snooper ) .", "line segments between the target and identification file indicate matches made by the data snooper ( the green lines indicate true , the red ones false positives ) .", "larger values of @xmath78 lead to more matches and a increase in recall , larger values of @xmath1 to a decrease in precision.,title=\"fig:\",scaledwidth=102.0% ]    .5   ( chosen by the data holder of the target file ) and the threshold parameter @xmath78 ( chosen by the data snooper ) .", "line segments between the target and identification file indicate matches made by the data snooper ( the green lines indicate true , the red ones false positives ) .", "larger values of @xmath78 lead to more matches and a increase in recall , larger values of @xmath1 to a decrease in precision.,title=\"fig:\",scaledwidth=102.0% ]      the main effect of the threshold parameter @xmath78 concerns the recall .", "the probability of detecting a common edge of the target and identification graph is approximately equal to @xmath78 .", "for this reason , higher values of @xmath78 lead to a higher recall ( see table  [ table : results_rec_1 ] and figures  [ fig : results ] and  [ fig : results_dep_alpha ] ) .", "simultaneously , the effect of @xmath78 on the precision appears to be twofold : on the one hand , for increasing @xmath78 a larger portion of the overlap between the identification and target file can be successfully detected by the snooper , which makes false positives less likely ( leading to a larger precision ) . on the other hand , for too high values of @xmath78 also the chance for non - common edges of the target and identification graph ( but which coincide with respect to the vertex labels of their endpoints )", "to be classified as common edges increases leading to a slight decrease in precision .", "the latter phenomenon , together with the increase in recall for increasing @xmath78 mentioned above , would reflect a trade - off between precision and recall , which is a well - known phenomenon in data linkage  @xcite .", "thus , combining these two thoughts , for increasing @xmath78 the precision should rapidly increase initially and then slightly decrease when @xmath78 becomes too large .", "this expectation is confirmed by our experiments ( see table  [ table : results_prec_1 ] and figures  [ fig : results ] and  [ fig : results_dep_alpha ] ) , although the decrease in precision when @xmath78 becomes too large is not significant for the considered values of @xmath1 .    from the definition of @xmath82 ( see the paragraph _ fine - tuning of the attack _ above ) , it is supposed that the recall does not change significantly in dependence on @xmath1 because the probability of correctly detecting an edge should be nearly @xmath78 ( which is independent of @xmath1 ) .", "this non - dependence is impressively confirmed by the performed simulations and illustrated in figures  [ fig : results ] and [ fig : results_dep_alpha ] .", "however , @xmath1 strongly influences the precision ( for larger values of @xmath1 the precision evidently decreases ) : the data snooper has to accept false positives ( resulting in less precision ) if she / he wants to achieve a certain predetermined recall .    altogether ,", "the simulations show that , in principle , a sufficient level of anonymity can be achieved by the addition of random noise to the input coordinates before computing the distance matrix . however , this anonymity is not free , which is illustrated by means of the risk - utility ( r - u ) confidentiality map in figure  [ fig : ru ] , where the risk of identity disclosure ( measured by the average precision of the linkage attack ; see also the discussion below ) is plotted against the utility ( measured as the reciprocal of the sample variance of the distance deviation @xmath83 for the current value of @xmath1 as recorded in table  [ table : distance_deviation ] ) . in the datasets considered in the simulation study", "@xmath1 would have to be chosen large enough to guarantee at least some degree of anonymity , that useful analyses based on the distances would become difficult . for this reason , the development of distance modification techniques that guarantee a certain degree of anonymity , and make it possible to also conduct useful analyses on the anonymised data , will be an important aspect of future research .    .", "the threshold parameter @xmath78 was chosen equal to @xmath84.,scaledwidth=99.0% ]    note that in our specific example , the snooper would primarily attempt to achieve a high precision : in the case of geographic distances , a point is uniquely determined by the exact distances to three other points . if the snooper could deanonymise at least three entities successfully , exploiting this fact would be a good starting point to identify even more individuals . for arbitrary metric spaces ,", "such a relationship does not hold in general , albeit the successful deanonymisation of some entities would also alleviate a snooper s work in this more general case .", "obviously , for distance modification techniques other than perturbation of the input coordinates , a snooper will have to modify the graph theoretical linkage attack , especially the definition of @xmath82 .", "however , due to kerckhoffs principle , it has to be assumed that the snooper at least knows the distance modification technique used by the holder of the target file and exploits this knowledge in the precise construction of the attack .", "for instance , if noise is not added to the input coordinates before computing the distance matrix but rather to the distance matrix itself ( a technique discussed in  @xcite ) , the attack has to be slightly adapted . in this case , when defining the relation @xmath82 the quantiles of the noise distribution can be used directly , thereby making the empirical study on distance deviations originating from perturbation of the input coordinates unnecessary .", "moreover , in this specific case it might be reasonable to further modify the attack by relaxing the ( relatively strong ) notion of a maximum clique to the less restrictive notion of a maximum quasi - clique , a relaxation which has been successfully applied in  @xcite for the purpose of protein classification . in a similar way", ", our attack can be adapted to many other anonymisation techniques and thus provides a useful and flexible tool for the analysis of methods for distance - preserving anonymisation ."], ["in this article , we have introduced a novel graph theoretic linkage attack on microdata with additionally published ( approximate ) inter - record distances . in the special case of spatial distances , we have demonstrated  on the basis of our test data  that the release of distances increases the risk of identity disclosure unreasonably even if geographical coordinates have been perturbed by random gaussian noise before the distances are calculated .", "furthermore , we showed that augmenting the standard deviation of the added random noise will gradually lead to a sufficient level of anonymity , but also make the perturbed distances useless for further analysis .", "therefore , the development and analysis of anonymisation techniques for microdata in a metric space that guarantee a certain degree of anonymity but distort the distances as little as possible ( particularly with regard to the applicability of data mining techniques ) will be an important aspect of future research ."], ["this research has been supported by a grant from the german research foundation ( dfg ) to rainer schnell .", "i am grateful to rainer schnell for valuable suggestions that helped to improve the presentation of this paper", ".    99    m.p .", "armstrong , g. rushton and d.l .", "zimmerman ( 1999 ) geographically masking health data to preserve confidentiality .", "_ statistics in medicine _ 18:497525 .", "l. backstrom , c. dwork and j. kleinberg ( 2007 ) wherefore art thou r3579x ? : anonymized social networks , hidden patterns , and structural steganography .", "_ acm proceedings of the 16th international conference on world wide web _ , 181190 .", "beyer , a.f .", "saftlas , a.b .", "wallis , c. peek - asa and g. rushton ( 2011 ) a probabilistic sampling method ( psm ) for estimating geographic distance to health services when only the region of residence is known .", "_ international journal of health geographics _ 10:4 .", "bomze , m. budinich , p.m. pardalos and m. pelillo ( 1999 ) the maximum clique problem . in : d", "du and p. pardalos ( eds . ) : _ handbook of combinatorial optimization _ , 174 .", "brownstein , c.a .", "cassa , i.s .", "kohane and k.d .", "mandl ( 2006 ) an unsupervised classification method for inferring original case locations from low - resolution disease maps .", "_ international journal of health geographics _ 5:56 .", "h. bunke and k. riesen ( 2012 ) towards the unification of structural and statistical pattern recognition .", "_ pattern recognition letters _ 33:811825 .", "s. chester , b.m .", "kapron , g. srivastava and s. venkatesh ( 2013 ) complexity of social network anonymization .", "_ social network analysis and mining _ 3:151166 .", "d. conte , p. foggia , c. sansone and m. vento ( 2004 ) thirty years of graph matching in pattern recognition . _ international journal of pattern recognition and artificial intelligence _ 18:265298 .", "p. christen and k. goiser ( 2007 ) quality and complexity measures for data linkage and deduplication . in : f. guillet and h.j .", "hamilton ( eds . ) : _ quality measures in data mining _ , 127151 .", "g. csardi and t. nepusz ( 2006 ) the igraph software package for complex network research .", "_ interjournal , complex systems _", "http://igraph.sf.net .", "curtis , j.w .", "mills and m. leitner ( 2006 ) spatial confidentiality and gis : re - engineering mortality locations from published maps about hurricane katrina .", "_ international journal of health geographics _ 5:44 .", "t. dalenius ( 1986 ) finding a needle in a haystack  or identifying anonymous census records .", "_ journal of official statistics _ 2(3):329336 .", "deza and e. deza ( 2009 ) encyclopedia of distances .", "_ springer_.    g.t .", "duncan , m. elliot and j .- j .", "salazar - gonzlez ( 2011 ) statistical confidentiality : principles and practise .", "_ springer_.    k. el emam and l. arbuckle ( 2013 ) anonymizing health data .", "_ oreilly_.    t. fober , g. klebe and e. hllermeier ( 2013 ) local clique merging : an extension of the maximum common subgraph measure with applications in structural bioinformatics . in : b.  lausen ,", "d. van den poel and a. utsch ( eds . ) : _ algorithms from and for nature and life _ , 279286 .", "s. gambs , m .- o . killijian and m. nez del prado cortez ( 2011 ) show me how you move and i will tell you who you are . _ transactions on data privacy", "_ 4:103126 .", "garey and d.s .", "johnson ( 1979 ) computers and intractability : a guide to the theory of np - completeness .", "_ freeman_.    m.p . gutmann and p. c. stern ( 2007 ) putting people on the map : protecting confidentiality with linked social - spatial data .", "_ national academies press_.    m.p .", "gutmann , k. witkowski , c. colyer , j.m .", "orourke and j. mcnally ( 2008 ) providing spatial data for secondary analysis : issues and current practises relating to confidentiality", ". _ population research and policy review _ 27:639665 .", "a. hundepool , j. domingo - ferrer , l. franconi , s. giessing , e. schulte nordholt , k. spicer and p .-", "de wolf ( 2012 ) statistical disclosure control . _ wiley series in survey methodology_.    d. kahle and h. wickham ( 2013 ) ggmap : a package for spatial visualization with google maps and openstreetmap .", "r package version 2.3 .", "http://cran.r-project.org/package=ggmap .", "k. kenthapadi , a. korolova , i. mironov and n. mishra ( 2013 ) privacy via the johnson - lindenstrauss transform .", "_ journal of privacy and confidentiality _ 5(1):3971 .", "j. konc and d. janei ( 2007 ) an improved branch and bound algorithm for the maximum clique problem .", "_ match commun . math .", "_ 58:569590 .", "j. krumm ( 2009 ) a survey of computational location privacy .", "_ personal and ubiquitous computing _ 13(6):391399 .", "g. levi ( 1973 ) a note on the derivation of maximal common subgraphs of two directed or undirected graphs .", "_ calcolo _ 9(4):341352 .", "k. liu , c. giannella and h. kargupta ( 2006 ) an attacker s view of distance preserving maps for privacy preserving data mining . in : j.", "frnkranz , t. scheffer and m. spiliopoulou ( eds . ) : _ knowledge discovery in databases : 10th european conference on principles and practice of knowledge discovery in databases _ , 297308 .", "m. merener ( 2012 ) theoretical results on de - anonymization via linkage attacks .", "_ transactions on data privacy _ 5(2):377402 .    c. m.", "okeefe ( 2012 ) confidentialising maps of mixed point and diffuse spatial data . in : j. domingo - ferrer and i. tinnirello ( eds . ) : _ privacy in statistical databases _ , 226240 .", "parker and e.k .", "asencio ( 2008 ) gis and spatial analysis for the social sciences : coding , mapping , and modeling .", "_ routledge_.    f.a.p .", "petitcolas ( 2011 ) kerckhoffs principle . in : h.c.a .", "van tilborg and s. jajodie ( eds . ) : _ encyclopedia of cryptography and security _", "s. rane , w. sun and a. vetro ( 2010 ) privacy - preserving approximation of l1 distance for multimedia applications .", "_ ieee international conference on multimedia and expo ( icme ) _ , 492497 .", "p. samarati and l. sweeney ( 1998 ) protecting privacy when disclosing information : @xmath2-anonymity and its enforcement through generalization and suppression .", "_ technical report , sri international_.    j. snow ( 1855 ) on the mode of communication of cholera .", "_ john churchill_.    l. sweeney ( 2000 ) uniqueness of simple demographics in the us population .", "_ technical report_.    l. sweeney ( 2002 ) @xmath2-anonymity : a model for protecting privacy .", "_ international journal of uncertainty , fuzziness and knowledge - based systems _ 10:557570 .", "wieland , c.a .", "cassa , k.d .", "mandl and b. berger ( 2008 ) revealing the spatial distribution of a disease while preserving privacy .", "_ proceedings of the national academy of sciences _ 105:1760817613 .", "e. zheleva and l. getoor ( 2011 ) privacy in social networks : a survey . in : c.c .", "aggarwal ( ed . ) : _ social network data analytics _", ", 277306 ."], ["the distances between birth locations ` loc ` are stored in the distance matrix @xmath85 : @xmath86    geocoding of the locations from table  [ table : poets_intruder ] using the r package ` ggmap ` and calculation of the mutual distances via the command ` spdists ` from the package ` sp ` yields the distance matrix @xmath87 :"]]}
{"article_id": "1605.06143", "article_text": ["consider different data providers holding vertically partitioned data .", "each data provider holds different information about the same set of individuals and there is a common identifier ( social security number , or any other sort of i d ) that allows one to cross - reference individuals across data providers .", "for example , the data providers contain a data of a set of individuals : gene `` banks '' hold genetic information , police departments hold criminal records , financial institutions keep record of credit history , hospitals record health history of patience for future diagnosis , etc .", "those data providers might be geographically spread , belong to different organizations and have their specific privacy and security requirements . for instance , a data provider of genetic information might not allow a public access to its data while allowing access for medical doctors to their patients data , or police departments that allow data access for international law enforcements , but deny access to anyone outside the departments . in such cases , it is usually impossible to gather all the data in one place , either due to the size of the data or due to privacy restrictions on data sharing . a client issuing a query to a different data providers in public cloud settings ,", "might be required to pay for utilized cpu time and network traffic . in many cases ,", "such client will be willing to trade performance ( in other words , cost ) for accuracy of the answer .", "especially , if the trade off is controlled by the client , which can define the accepted error in the received answer .    in this paper", "we propose a method of trading off performance for accuracy using sampling .", "we suggest a specific way to perform sampling in vertically split datasets and calculate sample size given acceptable error .", "performance improvements of the method are shown on a calculation of intersection set cardinality , together with a proposed heuristic algorithm .", "we also discuss a case of non - cooperative data providers and adaptation of the proposed algorithm for differential privacy calculations .    * our goal", "* : given a set of @xmath0 data providers @xmath1 which data records share a common identifier @xmath2 and a set of predicates @xmath3 , find the size of intersection @xmath4 where @xmath5 is a set of all records in @xmath6 that satisfy predicate @xmath7 .", "we assume that the number of records that are not present in all datasets is very small compared to the total number of records and thus , for the sake of simplicity , the data providers in @xmath8 are assumed to contain the same records with size denoted by @xmath9 , i.e. vertical split of the data .", "the size of the intersection might not be calculated precisely , but up to a given accuracy @xmath10 provided by the client querying the data providers .", "for example , the client that performs a query is interested in an integer percentage of people from the entire population having both criminal record and specific genetic mutation , i.e. the results can be rounded to a closest percentage .", "intuitively , this relaxed requirement should result in more computationally and network efficient algorithms .", "the optimization of computation time and network traffic are important , as mentioned above , in the commonly used public - cloud deployment ; the user is charged for computation time ( also called cpu time ) and network traffic .", "thus , minimization of those parameters is a very attractive algorithm property and present a trade - off with accuracy of the intersection set size .    * related work and our contribution : * conjunctive queries over distributed databases", "were extensively researched ( see as an example  @xcite ) .", "previous research has references both the aggregation techniques and performance issues of such queries .", "however , those works are based on an assumption that databases freely share the data , which does not hold in a settings of non - cooperative data providers .", "there were a number of works describing privacy preserving calculations of intersection size , see  @xcite .", "most of those works described a protocol that performs an exact calculation of the intersection set using secure multiparty computations ( mpc ) , first investigated by  @xcite and later generalized to multiparty computation .", "performing mpc protocols allows efficient calculation of the intersection size , however , when used by itself , it is also posssible to leak information about specific items in the datasets .", "for instance , the user can issue a query knowing with a single possible answer and check whether the intersection set is empty or not .", "such technique allow the user to identify the presence or absence of a specific item in the datasets .", "a technique that can be used to hide a presence of individual items in the dataset is differential privacy ( @xcite ) .", "while some of the above works ( @xcite ) performed inexact calculations of the intersection set , the source of approximation was to preserve differential privacy of the results .", "for instance , @xcite applied oblivious transfer ( ot ) protocol to approximate the size of the intersection of two databases , where ot has lead to the inexact result .", "our approach is to utilize the relaxed requirement of providing inexact result to improve the computational complexity of queries", ".    sampling as a way to cope with huge volumes of data or to increase computation performance were considered in a number of different contexts .", "( @xcite ) and section 4.2 of  @xcite discussed sampling as a means to cope with massive datasets by creation of a histogram - based estimators .", "once the estimator is created , when possible , the database performed estimation calculations on the histogram .", "reservoir stream sampling algorithms ( @xcite ) were developed to provide a sample of online data .", "the techniques fill in the sample ( `` reservoir '' ) by knocking out existing sample items from the sample with reducing probability as the sample begins to fill .", "( @xcite ) initiated usage of document sampling for document similarity comparison on internet scale .", "ideas were later developed into min - wise sampling techniques ( @xcite ) , which appear to be more suited for horizontal split datasets .", "in addition , a considerable corpus of work exists on concentration inequalities in scope of machine learning .", "concentration inequalities investigate the relation between the size of the sample and its statistical similarity to the entire dataset .", "pac - bayes bounds on hypothesis error as a function of sample size were provided in  @xcite .    in this paper", ", we consider the case of approximate calculations in distributed databases where the data is split vertically .", "we suggest an algorithm that takes advantage of a lack of accuracy in a distributed answer to considerably speed up the queries .", "we show a use - case of an algorithm on intersection set cardinality calculations both with and without privacy considerations .", "the algorithm adopts sampling techniques from ( @xcite ) and uses techniques similar to pac - bayes bounds from ( @xcite ) to decide on a mimimal , representative sample size .", "performance improvement might be especially significant in privacy preserving setting , where the calculations take considerable time .", "specifically our contributions are :    * suggested efficient method for approximate , distributed calculations with vertical - split datasets .", "* proposed a method of choosing sampling size given a required error level and provided simulation results of comparing the error level of different sample sizes .", "* suggested a heuristic algorithm of speeding up the intersection set size calculation based on bounds convergence and showed its adoptation for privacy preserving intersection set size calculation .", "the rest of the article is structured in the following way .", "section  [ sec : naive algorithm ] defines a nave algorithm for calculation of exact intersection set size , section  [ sec : sampling ] relaxes the requirement to calculate the exact size of intersection set and defines theoretical bounds for _ inexact _ intersection .", "section  [ sec : experiments ] presents results of experiments that validate the proposed sampling method .", "section  [ sec : bounded estimation ] describes a heuristic algorithm for calculation of intersection based on the bounds of intersection size .", "finally , section  [ sec : privacy ] discusses privacy issues in the described algorithms .", "the paper is concluded in section  [ sec : concluding remarks ] .", "the idea of the nave algorithm is simple : iterate over data providers @xmath11 and exchange a monotonously decreasing set of keys that satisfy all predicates up to the current iteration .", "the data provider then checks which of the received keys answer the corresponding predicate and returns a new list to the client .", "the client continues the process until all data providers were queried and the intersection set is found . in this way", ", the client calculates the intersection set iteratively .", "denote this set by @xmath12 where @xmath13 is the iteration number .", "in other words , @xmath14 where @xmath5 is a set of records in @xmath6 that satisfy predicate @xmath7 , and @xmath15 .    the nave algorithm can also be performed in a parallel way , where the client receives a set of records that satisfy the corresponding predicate from each data provider .", "the client then calculates the intersection set .", "comparing the sequential and parallel nave algorithms , sequential algorithm optimizes the network load and also improves cpu time , but not a wall - clock time of the algorithm ( i.e. , the time between the beginning of the operation and the time the client gets the final answer ) . unless stated otherwise , in the rest of the section we consider only the sequential version of the algorithm .    despite its virtue of being simple", ", the nave algorithm has a few drawbacks .    *", "the amount of information transferred between the client and the data providers is relatively large , as the entire set of keys in the intersection set is sent .", "* the first data providers evaluate the given predicate over their entire dataset , which is expensive . *", "the wall - clock time of the sequential algorithm is linear in a number of data providers and the size of the data . *", "the algorithm completely exposes information both between data providers and between data providers to the client .", "as mentioned above , those drawbacks are even more extreme in a current common practice , where a public cloud infrastructure is used for calculations , as in a public cloud infrastructure the client is charged for computation time ( cpu time ) and for network traffic . thus , there is a monetary incentive to minimize those two values .", "both the sequential and the parallel nave algorithms described above calculate the intersection set exactly , thus having a relatively high network traffic and cpu time demands .", "the next section discusses a way to reduce computation and network complexity by calculation of an estimation of the intersection size .", "relaxing the requirement for exact calculation of the intersection , this section discusses a way to calculate a smaller intersection set whose size can be extrapolated to the intersection size of the entire population . instead of calculating the intersection of the entire datasets ,", "perform a sample and calculate the intersection of the sample . then , scale up the size of the sample intersection to the size of the intersection for the entire population .", "the results of scale up will be an estimate for the intersection size .", "the requirements from the sample are clear : the sample should be as small as possible while truthfully representing the entire population ( given the definition and the error of representation ) .", "those two requirements present a trade - off between sample size and the accuracy of algorithm results .", "a number of different sampling techniques were developed over the years : on - line ( reservoir ) stream sampling algorithms ( @xcite ) , histogram - based estimators ( @xcite ) and min - wise sampling techniques ( @xcite ) which are more suited for horizontal split datasets . in our settings", ", it seems natural to choose hash function based technique , which in a sense a randomized version of histogram .    to provide sufficient improvements in performance", ", the size of the sample ( denoted by @xmath16 ) should be much smaller than the average size of the datasets ( @xmath17 ) : @xmath18 .", "therefore , if each data provider performs its own sampling , the overlap between the samples will be very small .", "in other words , the probability of the data record ( person in our example ) appearing in the sample is low . for every data provider @xmath13 @xmath19 where @xmath20 is a sample from the dataset of the data provider @xmath13 .", "leading to the probability of the item appearing in all samples being ( assuming that all data providers contain the same records ) : @xmath21 where @xmath22 and @xmath23 are a sample size and a dataset size of a data provider @xmath24 .", "assuming , for simplicity , that the sample size and dataset size are equal for all data providers , the probability becomes : @xmath25 as this probability is low , the size of the intersection will be essentially zero for most of the samples .", "the solution is to make all data providers create the same sample . in order to do that , we will use a sampling technique based on hashing ( @xcite ) .", "the technique works in the following way :    1 .", "the client defines an accuracy threshold for the calculation .", "2 .   using techniques described below ,", "the size of the sample is determined : @xmath16 .", "the number of hash buckets is @xmath26 .", "4 .   pick a hash function @xmath27 that will distribute datasets of data providers @xmath6 into @xmath28 buckets .", "@xmath27 hashes only the i d of the records and not the entire data , as the i d is the shared information across different data providers . 5 .", "the client sends each data provider 3 parameters : @xmath27 , the number of the bucket that was chosen randomly and predicate @xmath7 .", "this will ensure that all data providers use the same sample .", "each data provider evaluates its predicate on the records in a given bucket ( @xmath5 ) and sends the results to the client .", "the client performs the intersection calculation on the received results .", "as client acceptable error defines sample size , which is the bucket size , it is possible to pre - calculate a number of buckets for different error value .", "this will eliminate the need for sequential scan of the entire database on each query .", "however , it will also mean that the client will get approximately the required error .", "notice that just like the nave algorithm sampling can be done both in parallel and sequential ways . following the similar idea of the nave algorithm , in the parallel version of sampling algorithm", "each data provider evaluates the predicate over the entire given bucket , where in the sequential version , the client sends the current intersection set ( which is a subset of the given bucket ) to the next data provider , which evaluates the given predicate only over the received subset .", "however , in contrast to the nave algorithm , where the sequential variant significantly reduces the network load and cpu time , in sampling algorithm the reduction is much smaller , as the core underlying idea of the sampling algorithm is minimization of the amount of records participating in the intersection calculation .", "importantly , the accuracy of the intersection size estimation is the same in both sequential and parallel versions of the sampling algorithm .", "sampling improves both the computation time and network traffic over the nave algorithm by the factor of @xmath29 .", "thus , one important question still remains unanswered : what should be the size of the sample given an accuracy threshold ?", "the following sections define bounds on the sample size such that with high probability it will represent behavior similar to the entire dataset .", "two different cases are considered : sample size is small compared to the dataset size ( section  [ subsec : small sample ] ) and sample size is comparable to dataset size ( section  [ subsec : big sample ] ) . notice that in both cases the sampling is performed in the same way as explained above .", "however the bound used to calculate the sample size is different for those two cases .", "selection of the sample size is driven by the trade - off between the accuracy of the sample and the performance of calculations .", "the bigger the sample , the more accurate it is , but also the larger the computation time required to calculate the intersection . using a bound on the error as a function of the sample size , it is possible , given a limit on an acceptable error , to choose the size of the sample set .", "when a sample size is very small compared to the dataset size , the sampling can be approximated by independent sampling of the same size with replacement .", "the approximation can be done , as the probability of any record being a part of the sample goes from @xmath30 for the first pick to @xmath31 for the last pick .", "if @xmath32 then @xmath33 , and therefore , the probability is approximately ( @xmath30 ) , which is the same probability for a record to be picked when sampling with replacement .", "the reason to perform such approximation is due to a fact that it is simpler to provide a bound for independent samples with replacement rather than for sampling without replacement .", "the size of the sample should be big enough that the sample will be a good representative of the entire dataset for the intersection calculations .", "how can the size be estimated ?", "let us consider a sampling from the dataset @xmath6 according to the uniform distribution .", "notice that even though the sampling is done according to uniform distribution , as described in the sampling algorithm ( section  [ sec : sampling ] ) , the datasets themselves might be drawn from other distributions .", "the provided bounds still hold , as the sample `` similarity '' to the entire dataset depends only on its size . if a different , non - uniform sampling technique is used , the bound might be changed to use more general bernstein inequalities ( @xcite ) .", "define @xmath34 to be a random variable that the sampled item @xmath35 satisfies condition @xmath36 .", "then , the average value of the sample , denoted by @xmath37 , should be close to the mean value , @xmath38 , of the entire dataset .", "notice that in the binary case , the average is simply the number of data records that satisfy a given predicate divided by the data size . in order to bound the difference between the average value of the sample and the mean value of the dataset , we can use concentration inequalities .", "namely using results by hoeffding ( @xcite ) if @xmath39 are independent and @xmath40 , then : @xmath41    for simplicity , for now we will consider only the right - side of the equation , i.e. @xmath42 where @xmath16 is the size of the sample and @xmath10 measures the `` resemblance '' of the sample to the mean of the dataset . notice that the equation does not depend on the size of the dataset , as it is considered much larger than the sample .", "the bound in equation  [ eq : short hoeffding ] is used by the client when it issues queries to the data providers by defining both the acceptable error ( @xmath10 ) and the target probability of exceeding the acceptable error .      in cases where sampling size is comparable to the dataset size , it is possible to develop bounds directly for sampling without replacement . in the rest of the section , we show two bounds on sampling without replacement , one is based on a reduction of `` randomness '' of the data and the second one is based on a direct counting technique .", "those bounds can be expected to be tighter than those based on reduction to independence or bounds for sampling with replacement .", "the reason for this as follows .", "assume that @xmath0 points were sampled out of @xmath43 points without replacement .", "the next point is to be sampled from a set of @xmath44 rather than @xmath43 points , which would be the case in sampling with replacement .", "the successive reduction in the size of the sampled set reduces the `` randomness '' of the newly sampled point as compared to the independent case , and also introduces dependency between samples .", "whereas , the bound provided in equation  [ eq : short hoeffding ] does not depend on the dataset size and does not take the reduction in population size into consideration .", "this intuition is at the heart of serfling s improved bound ( @xcite ) which is stated next .", "the result holds for general bounded loss functions and is established by a careful utilization of martingale techniques combined with chernoff s bounding method .", "[ theorem : serfling ] ( serfling ) let @xmath45 be a finite set of non - negative bounded real numbers , @xmath46 .", "let @xmath47 be a random variables obtaining their values by sampling @xmath48 uniformly at random * without * replacement .", "set @xmath49 .", "then , @xmath50 similar bounds hold for @xmath51 .", "compared to the bound in equation  [ eq : short hoeffding ] , the above bound is always tighter when @xmath52 , i.e. when @xmath53 .    in our case @xmath54", "s are binary variables and the bound could be improved further by using a proof based on a counting argument ( @xcite ) .", "figure  [ fig : bounds comparison ] presents a single example of comparison between the above bounds ( [ eq : short hoeffding ] and [ eq : serfling ] ) . as expected , the serfling bound is tighter than the hoeffding bound and thus , using this bound for calculation of sample size will result in a smaller sample set .", ", @xmath55 .", "the error value is preset and then the sample size that fits this value is picked . in tighter bounds , the sample size will be smaller.,scaledwidth=50.0% ]    now we can briefly describe the process of using those bounds .", "we assume that the client attempts to minimize the size of the sampling sets , as this also minimizes the network load and cpu time , both of which are chargeable in public cloud environments .", "the client defines a value of an acceptable error for the combined distributed query ( @xmath56 ) and a `` confidence '' of the error .", "the bounds defined in ( [ eq : short hoeffding ] ) or ( [ eq : serfling ] ) provide a `` confidence '' that the error will be smaller than @xmath10 given a sample size .", "now the client uses the bound from ( [ eq : short hoeffding ] ) or ( [ eq : serfling ] ) to determine the minimum sampling size that allow the required error with `` high enough '' confidence . using a predefined confidence and a chosen bound , the client can find the minimum value of sample size that will provide the required error with the required confidence .", "for example , consider figure  [ fig : bounds comparison ] , where for dataset size of @xmath57 the client choose @xmath58 and confidence of @xmath59 .", "thus , using the simplified hoeffding bound ( [ eq : short hoeffding ] ) the minimal sample size is @xmath60 and using the serfling bound ( [ eq : serfling ] ) the minimal sample size is @xmath61 . of a sample size . ]    after the intersection of sample sets is calculated , there is a need to estimate the size of the intersection of the entire datasets . as mentioned above , the mean of the binary random variable is also the number of records that belong to a set defined by the predicate divided by the size of the sample .", "thus , the ratio of the records in a sample that satisfy a given predicate is close to the ratio of the records that satisfy the predicate in the entire dataset of a given data provider .", "this leads to the conclusion that the ratio of the sample intersection set size to the sample size should be the same as the ratio of the intersection set size to the size of entire dataset .", "let @xmath62 be a sample of @xmath6 of size @xmath16 , then @xmath63    notice that even though the absolute error in the intersection estimation of the entire dataset is proportionally larger than the error in estimation of a sample , the relative error will remain the same .", "the reason for the error to remain the same in a process of intersection calculation is due to the sampling method .", "even though the sample from each data provider has the same ( potential ) relative error , the error is unique for the sample and thus , it does not accumulates when the intersection is calculated .", "as @xmath10 in the above bounds ( equation  [ eq : short hoeffding ] and  [ eq : serfling ] ) is a bound on difference in estimation , it is a relative error . therefore , if the client defines an acceptable error ( @xmath56 ) as an absolute error , it can be easily translated : @xmath64 .", "we have performed a number of experiments to show the utility and error resulted from sampling .", "the experiments were performed on simulated data and validated on the adult dataset ( @xcite ) .", "the described sampling technique is targeted at large datasets with many records , such that calculation of predicates over the entire dataset is wasteful . for such datasets , it is much more practical to test the algorithm on simulated data , which can be generated on any required scale .      for experiments on simulated data ,", "a number of datasets were generated with random values of the predicates and then the intersection size was calculated according to each algorithm .", "the methodology works as follows .    *", "generate a dataset of a given size .", "all data providers assumed to share the same set of records with possibly a different data per record .", "* for each data provider generate a set of predicate values randomly , such that the frequency of `` true '' values is as defined .", "* calculate the intersection iteratively , i.e. by addition of a single datasets at a time .", "this is a both a simpler way of implementation ( as opposite to parallel calculation ) and also allows observation of convergence rate of the intersection size .", "all experiments were averaged over 10 runs , standard deviation was calculated and drawn on the resulting graphs .    figure  [ fig : algorithms comparison ] shows the intersection size as calculated by the sequential nave algorithm and estimation by sampling of various sizes .", "the graph provides a high - level , visual practical validity of the approach .", "even a relatively small sample sizes ( @xmath65 of the dataset size ) estimate intersection size close to the real value .    in order to focus on the error caused by sampling and", "show the difference in sample size more clearly , we have performed a number of experiments showing relative error between estimation and the real intersection size .", "the nave algorithm calculated was taken as a baseline for error calculations .", "the sampling algorithm was executed with a number of sample sizes , each one showing the relative error from the nave algorithm results .", "figures  [ fig : algorithms error comparison ] , [ fig : algorithms error comparison 1 m ] , [ fig : algorithms error comparison 100k r05 ] , and  [ fig : algorithms error comparison 1 m r05 ] show the relative error of the sampling algorithm with various sample sizes .", "the y axis shows the error of the sampling , i.e. |sampled estimation - intersection size| / ( intersection size ) , while the x axis is the number of datasets in the intersection .", "the graphs compare datasets sizes of 100,000 ( figures  [ fig : algorithms error comparison ] and [ fig : algorithms error comparison 100k r05 ] ) and 1,000,000 ( figures  [ fig : algorithms error comparison 1 m ] and [ fig : algorithms error comparison 1 m r05 ] ) records with predicate satisfaction frequency of @xmath66 and @xmath67 , i.e. in each data provider , @xmath68 or @xmath69 of the records satisfy the corresponding predicate .", "as mentioned above , each graph is an average of 10 different runs and standard deviation is depicted by error bars on the graph .", "the graphs show that , as expected , larger samples result both in smaller error and smaller standard deviation . however , it also can be seen , that the error quickly converges as a function of sample size . in some cases , even for sample of @xmath65 from entire dataset ,", "it is possible to achieve reasonable error .", "overall , sampling @xmath70 or @xmath71 of the dataset resulted in errors of approximately @xmath59 from the intersection size , which is an order of a single percent .", "thus , queries of a type : `` what is the approximate percentage of people ... ? '' can be answered using only tenth of a data .    , number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath66 .", "the y axis shows the size the intersection estimation .", "the x axis show the number of the data sets that participated in the intersection .", "error bars show standard deviation over 10 different executions.,scaledwidth=50.0% ]    , number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath66 .", "error bars show standard deviation over 10 different executions.,scaledwidth=50.0% ]    , number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath66 .", "error bars show standard deviation over 10 different executions.,scaledwidth=50.0% ]    , number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath67 .", "error bars show standard deviation over 10 different executions.,scaledwidth=50.0% ]    , number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath67 .", "error bars show standard deviation over 10 different executions.,scaledwidth=50.0% ]      in addition to experiments on simulated datasets , we have tested the methodology on adult dataset from uci machine learning repository ( @xcite ) .", "the dataset contains data from census bureau , where each record has data about different person .", "this fits a description of the setup we have described .", "the dataset contains 32,562 instances .", "as a test case we have used an intersection of the following predicates : age @xmath73 30 , marital status : never - married , sex : female or male in two different tests and income @xmath74 50k .", "the exact intersection set size is 252 for males and 139 for females .", "since the dataset was not used for classification task , it allowed us to use income as one of the predicates .", "in addition , the intersection was of 4 predicates , as addition of more predicates resulted in a small or empty intersection set due to a limited dataset size .", "samples of different sizes were drawn with sample size ratio going from 0.1 to 0.5 .", "the intersection set size was calculated from the drawn sample .", "figure  [ fig : adult sampling ] depicts the results of the testing .", "while the accuracy of the estimation does not necessary improves by taking larger samples , the standard deviation becomes smaller .", "the accuracy improvement is most probably caused by the small size of the intersection , while improvement in standard deviation fits the results shown in simulated datasets .", "overall , the accuracy of the intersection set size calculation verify the validity of the approach .", "previous sections described exact and approximate ways of calculating the size of the intersection .", "this section presents a heuristic algorithm that attempts to optimize the calculation of the intersection by not performing the calculations for all datasets .", "the simplest case of heuristic is when the intersection is calculated iteratively and at some point the intersection set is empty .", "clearly , the calculation can be stopped at this stage .", "following is a heuristic algorithm for the intersection calculation that attempts to stop at the earliest possible point .", "the size of the intersection depends on the sizes of the sets from each data provider that answer the corresponding predicate and on the correlation between those sizes .", "below we suggest a heuristic algorithm for estimation of the intersection set size .", "the algorithm starts with an accuracy parameter , the ratio of records that satisfy the relevant predicate in each data set , and the lower and the upper bounds on the intersection size .", "the algorithm iteratively tightens the bounds until the difference between the bounds is smaller or equal to the accuracy parameter .", "then the algorithm stops and returns the middle value of the range between the lower and the upper bound .", "let @xmath7 be the predicate that is associated with a data provider @xmath6 .", "then , @xmath75 is the set of members in @xmath6 that satisfy @xmath7 .", "also , @xmath76 will denote the fraction of the members that satisfy @xmath7 in @xmath6 , i.e. @xmath77 .", "now , let us define the bounds on the size of the intersection set @xmath78 for the sake of the algorithm iterations . for simplicity , we assume that data provider datasets @xmath79 contain _ exactly _ the same records .", "the bounds will hold when the difference in members between datasets is small .    as the intersection size is at most the size of the smallest predicate set ,", "the size of the intersection is bounded from above by the following bound : @xmath80    the lower bound on the intersection of two sets @xmath81 and @xmath82 is ( @xcite ) @xmath83 in case of 3 sets , the lower bound becomes : @xmath84 which in general is @xmath85 where @xmath86 is the average size of the datasets ( as they contain the same set of records ) .", "as described above , the iterative step of the algorithm is to tighten the bounds on the intersection size .", "the algorithm will then stop once the difference between bounds is smaller than the required accuracy . as the upper bound ( equation  [ eq : upper bound ] ) is the minimal predicate set , an iteration step will attempt to make the minimum value smaller by calculating the intersection between the two smallest predicate sets .", "the intersection of two sets will be smaller or of the same size as minimal predicate set and will replace those two sets in the bound .", "calculating the intersection of two minimal predicate sets is also a good technique for increasing the lower bound ( equation  [ eq : lower bound ] ) as well .. in this case the lower bound of their intersection is 0 . ] notice that each iteration step of the algorithm also decreases the value of @xmath0 and thus removes the subtractive member of the lower bound .", "the algorithm steps are as follows .    1 .", "define a required accuracy threshold : @xmath87 as @xmath88 is a relative error , the iterations continue until an absolute error is smaller than @xmath89 .", "when the iterations stop , the middle of the bounds range is returned , therefore , the iterations stop when the distance between the bounds is less or equal to @xmath90 2 .", "calculate @xmath5 for every data provider @xmath13 .", "3 .   calculate the lower ( @xmath91 ) and the upper bounds ( @xmath92 ) , given @xmath7 .", "[ step : iteration ] while @xmath93 and the number of sets is larger than 1 : 1 .", "let @xmath94 and @xmath95 .", "in other words , pick two minimal predicate sets : @xmath96 and @xmath97 .", "[ step : intersect ] calculate the intersection between those two sets .", "3 .   replace @xmath98 and @xmath99 with the new predicate set : @xmath100 .", "4 .   update new values of @xmath92 and @xmath91 .", "if only one set remains @xmath101 ( @xmath102 ) , then the size of its predicate set , @xmath103 , is the size of the intersection set .", "6 .   if there is more than one set and @xmath104 , then the size of the intersection set to the middle value of @xmath105 $ ] range : @xmath106 .", "the algorithm clearly converges , as the iterations stop when the intersection size is calculated exactly . at this point", "the lower bound is equal to the upper bound and thus , the accuracy requirement will be satisfied .", "notice that if step  [ step : intersect ] results in an empty set , the algorithms also stops , as the lower and the upper bounds will be equal and zero .", "* example : * [ example : algorithm example ] as an example of the algorithm execution , assume @xmath107 , equal size of all data sets @xmath43 and the required accuracy of @xmath108", ". let the respective ratios be @xmath109 and @xmath110 .", "in this case , the upper bound on the intersection set size will be @xmath111 , whereas the lower bound will be @xmath112 , as the sum of all ratios is @xmath113 less than @xmath114 .    the first iteration of the algorithm will be to find the intersection of @xmath115 and @xmath116 .", "let us assume that their intersection results in @xmath117 .", "now , the upper bound of the intersection becomes @xmath118 , whereas the lower bound increases to @xmath119 .", "the iterations continue until the bounds converge to within @xmath120 .", "in general , the heuristic algorithm will work fine in cases where the ratio of records that satisfy the predicate is high across most data providers , but the intersection size might decrease relatively fast . see figure  [ fig : heuristic algorithm ] for example of convergence of the algorithm bounds on simulated data .", ", number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath121 ( the high ratio is required for the lower bound to be larger than zero for @xmath72 ) . notice that in simulated case , the intersection size is co - located with the upper bound of the intersection.,scaledwidth=50.0% ]      clearly , both the computation time and network traffic depend heavily on the calculation of the intersection between the two sets in step  [ step : intersect ] of the heuristic algorithm . to improve the performance of this step it is possible to use the sampling technique from section  [ sec : sampling ] . instead of performing intersection between two sets @xmath13 and @xmath96", ", the algorithm will calculate the intersection of two samples of those sets .", "however , sampling introduces an error into calculation of the intersection , which has to be related to the accuracy threshold @xmath88 of the heuristic algorithm .    assuming that the error in sampling distributes proportionally among datasets and noting that each iteration `` eliminates '' one dataset", ", we can define the following changes to the algorithm .", "every sample introduces error of @xmath122 . to accommodate this error ,", "the algorithm will decrease the required accuracy threshold by this amount on each iteration .", "this will ensure that required by user accuracy threshold will be honored .", "two steps are changed in the algorithm :    1 .", "step  [ step : iteration ] then becomes : + _ while @xmath123 where @xmath13 is the iteration number , and the number of sets is larger than 1 .", "step  [ step : intersect ] becomes : + _ calculate the intersection between those two sets : @xmath96 and @xmath24 by sampling with error threshold @xmath124 .", "_    the rest of the algorithm remains the same , including required accuracy threshold value .", "in the described setting there are 2 different types of actors : data providers and client .", "there are different privacy concerns between those actors .", "one privacy concern is preserving the privacy of data providers .", "the client querying the data providers should not learn the identity of the records that are part of the intersection set or the number of records that satisfy a specific predicate in any single data provider .", "notice that the client should be limited to a reasonable number of queries ( sub - linear in a data size ) , as it is not possible to answer a large number of arbitrary queries while preserving privacy ( @xcite ) .", "another privacy concern is in keeping a privacy of records in a specific data provider from other data providers .", "for instance , a data provider might be interested in hiding the presence of specific record from other data providers .", "first , it is imperative to note that the sampling ( see section  [ sec : sampling ] ) provides a very nave form of privacy . as every data", "set record has a @xmath125 chance of not being included in sample , the presence of the specific record in the data set is not immediately observable . yet , the fact that the client gets record identifiers of each data provider is undesirable .", "data providers can encrypt the sampling indices to hide the identity of the records that are sent to the client .", "even though an additional encryption will obscure the identity of those items from the client , it still will expose the presence of individual records to other data providers .", "in addition , using an additional knowledge , the client can easily learn the presence of a specific record in the data providers .", "one way to preserve privacy is to use secure multiparty computations to calculate the intersection set ( @xcite ) a simple method that uses commutative cryptography ( another approach is to use secret sharing ) is described in  @xcite .", "each data provider encrypts record identifiers using its own commutative key and passes the key to other data providers .", "once all data providers have encrypted all keys , it is possible to find the intersection using encrypted identifiers utilizing commutativity of the encryption and calculate its size .", "there are a few downsides to using secure multi party computations .", "first , performance is heavily impacted by performing secure computations using these methods .", "it is possible to aleviate the performance issues by using the sampling to reduce a number of records in the calculated intersection , as described in section  [ sec : sampling ] .", "thus , the idea is to perform the sampling algorithm and then calculate the intersection in a secure way using the mpc computations between the data providers .", "this method however , will still require calculation of the sampled intersection exactly , without the ability to halt when required accuracy was reached , as it is possible in heuristic algorithm .", "the second drawback is specific for the described use - case where the data is kept in different organizations .", "secure multi party computations require direct communication between data providers , which in some cases is very challenging in a real - world deployments due to both security and technological reasons .", "while it is still possible for the client to act as an intermediary between different data providers , such setup also doubles the network traffic .", "a different approach is only to allow communication between client and data provider with adopted algorithms .", "however , the most significant drawback of mpc is that the exact intersection size is calculated .", "this might allow the client to use additional information to infer a presence of a specific record in the dataset .", "the current de - facto privacy standard is differential privacy ( @xcite ) .", "informally , the idea of differential privacy is to protect the privacy of individual records in the dataset without any assumption on the additional knowledge .", "differential privacy is preserved if it is practically impossible to identify whether the record is present or absent in the dataset from the result of database queries .", "the most common methods of ensuring differential privacy rely either on addition of a carefully chosen amount of noise to the result or by using an exponential mechanism that chooses the output according to some specific probability distribution .", "the protocols that are based on secure multiparty computations perform the exact calculation of the set , which is impossible in differential - privacy settings .", "this led to several works describing differential - privacy - preserving calculations of the intersection size , see  @xcite .", "even though those algorithms preserve differential privacy , the requirement to provide an approximate answer in our case allows our scheme to optimize the algorithm for performance by performing secure computations on as little number of the records as possible due to sampling .    the most common method of ensuring differential privacy is an addition of a specifically crafted random noise to the released data ( @xcite ) . moreover , there are two different approaches for the private data release : interactive ( @xcite)and non - interactive ( @xcite ) .", "interactive data release is when the client sends data query to a data provider .", "the data provider then releases the data to the client while ensuring differential privacy of records in its database .", "the data provider might decide not to answer a specific queries or to stop answering queries from a given client , as this might impact the privacy . in non - interactive data release", ", the noise is applied only once and then the data is released to the client .", "the client can perform any number and any type of queries on the data .", "intuitively , non - interactive release requires to add more noise to the released data , as it has to be ready for any query , while interactive release can adopt added noise to the results of each query ( @xcite ) .      in the described above sampling algorithm ( section  [ sec : sampling ] ) , there are two intuitive locations to add random noise .", "first , it is possible to add noise in hashing function that assigns records to buckets .", "such noise will preserve privacy of individual records , as it will be impossible to distinguish whether a specific record is within the bucket or not present at all in the dataset . a second location to add the noise is during a query processing .", "since the data provider exposes only the number of records that satisfy a given predicate , the noise can be added to the output after the predicate was evaluated or to the predicate itself [ [ ehud : this is not clear , the noise should be fake ids ] ] .", "the difference between those two locations is exactly the difference between interactive and non - interactive data release . adding noise to the hashing function", "is a one - time operation and has no relationship to the result of the specific predicate .", "thus , this noise addition can be seen as non - interactive data release . on the other hand ,", "if the noise is added after the predicate is evaluated over records in a bucket , then the noise value can be adapted to the results of predicate evaluation .", "thus , the amount of noise can be directly related to the results of a specific query , like in interactive data release settings .    due to the above reasoning", ", the next section describes an addition of a noise to the predicate function results .", "differentially private calculation of a counting function ( referred in the paper as `` noisy sum '' ) , i.e. counting a number of records that satisfy a given predicate , was considered previously in ( @xcite ) .", "( @xcite ) showed that adding a laplacian noise @xmath126 to the sum query output : @xmath127 , ensures @xmath128differential private function .", "notice that the sensitivity of counting function is @xmath129 and thus , the distribution standard deviation is @xmath130 .    the fast algorithm for privacy - preserving intersection calculation is described below .", "heuristic algorithm from section  [ sec : bounded estimation ] is used as a basis for privacy preserving algorithm .    1 .   use a hash function @xmath27 to divide data into buckets . in case only a single bucket is used for all queries , only the agreed bucket is kept .", "2 .   when a predicate is received from the client , evaluate the predicate over the chosen bucket .", "3 .   add random noise according to laplace distribution to the size of the predicate set .", "the noisy set - size is shared with the client .", "the client gathers results from all data providers and calculate upper and lower bounds .", "[ step : chosen data providers ] the client continues to execute heuristic algorithm and picks two data providers , @xmath96 and @xmath0 , to perform intersection according to the algorithm step  [ step : intersect ] .", "perform privacy - preserving , secure intersection between two chosen providers using the below algorithm . 7 .", "update the bounds and continue until the bounds are close enough .", "the algorithm used for privacy - preserving , secure intersection set calculation is based on a work of ( @xcite ) , which allows a differentially private calculation of an intersection over multiple databases .", "the algorithm provides computational differential privacy , as it relies on homomorphic encryption and makes some assumptions on computational hardness .", "this section describes how the algorithm from ( @xcite ) can be used in our heuristic algorithm from section  [ sec : bounded estimation ] for inexact computations .", "the algorithm describes a basic operation _ private set - intersection cardinality ( psi - ca ) _ and then adds noise to ensure differential privacy , thus resulting in bn - psi - ca .", "psi - ca operation is based on usage of homomorphic cryptosystem that allows addition and multiplication by constant ( for instance , paillier s cryptosystem ( @xcite ) ) .", "when two data providers , @xmath13 and @xmath96 , attempt to calculate intersection , @xmath13 defines a polynomial whose roots are the members of its set , the polynomial coefficients are then encrypted and transfered to @xmath96 data provider , which calculates @xmath131 . in other words", ", it evaluates the polynomial on its set members , multiplies by a constant number @xmath132 and adds a special string of zeros @xmath133 .", "when transfered back to the first data provider ( @xmath13 ) , it calculates the number of zero string . in order to add differential privacy , a random number of dummy values", "are added to the transfered set by both parties ( for details see ( @xcite ) rounding the set size to the bucket size in our case . in the end", ", data provider @xmath13 learns the noised cardinality and data provider @xmath96 knows the amount of noise it added to the intersection set size .    adopting this algorithm for our case , the client performs the following actions .", "assume that two data providers are chosen for intersection in step  [ step : chosen data providers ] : @xmath96 and @xmath0 .    1 .", "perform bn - psi - ca algorithm to calculate a noisy intersection set .", "assume that without loss of generality , @xmath96 will hold the size of a noisy set , where @xmath0 will hold the amount of added noise .", "2 .   @xmath96 sends the size of a noisy set to the client .    in the following iterations of the heuristic algorithm , bn - psi - ca", "might be invoked to find intersection size of @xmath96 , @xmath0 and @xmath132 , and more additional data providers . as due to", "privacy issues , no single data provider holds the intersection dataset , it is necessary to perform intersection size calculations over again for any additional data provider .", "notice that in cases where data providers do not communicate directly , the client acts as an intermediary for the protocol . in the end", ", the client has a value of intersection size which is still preserving differential privacy .", "the client also keeps record that @xmath0 holds the amount of noise of the last intersection . as described in ( @xcite ) , the noise added in one intersection , might be removed if there is a need to calculate another intersection of @xmath96 and @xmath0 with additional data provider .", "( @xcite ) performance results show approximately linear dependency of an algorithm run - time in the number of records in the participating datasets .", "using results from section  [ sec : experiments ] , with a reasonable error , it is possible to reduce the number of records by a factor of 5 - 10 ( according to acceptable error ) , thus tens of minutes to single minutes .", "such a reduction might make a difference between practical and impractical system .", "the algorithm accelerate approximate intersection calculation by :    * computationally intensive calculations of intersection sizes are performed on samples .", "those calculations are demanding computationally , thus , any reduce in the size of datasets is important .", "[ [ ehud : we should try to find a third - year student to implement this ] ] * the intersection is not calculate exactly , but rather the algorithm stops when the difference between bounds is close enough .", "a steady and rapid increase in the amount of data has resulted in an abundance of various data providers .", "more data is kept for longer and in more places . in parallel ,", "the awareness of data privacy and security is also on a rise both from government regulation and personal perspective .", "this trend requires new algorithms and protocols for dealing with distributed data .", "those new methods should be both efficient and privacy preserving .", "the main practical downside of the current privacy preserving computations is the run - time complexity .", "both mpc , oblivious - transfer and private - information - retrieval methods that are used to perform distributed calculations in privacy - preserving manner significantly increase the running time of the query . while some advances in improving performance were made , the running time still remains a limiting factor .", "our proposed method uses the ability to provide an approximate answer to the supplied query and thus to reduce considerably the dataset that is used for computations .", "this decouples the use of sampling technique for reducing the size of the dataset used for calculations from the protocols designed to preserve privacy of individual records or data providers . as such ,", "most of the above protocols can be used in conjunction with sampling techniques to perform approximate calculations with improved performance .", "as showed in this work , when acceptable , calculating an approximate answer can significantly improve computation time . in those cases", "we have suggested a way to use this possibility for approximate answering by using sampling of the datasets .", "the sampling leads to a dramatic reduction in the size of data required for calculation : @xmath134 of the data as observed in simulations", ". such reduction of the data size is much more significant when calculations are done in a secure and privacy - preserving way .", "r.  agrawal , a.  evfimievski , and r.  srikant .", "information sharing across private databases . in _ proceedings of the 2003 acm", "sigmod international conference on management of data _ , sigmod 03 , pages 8697 , new york , ny , usa , 2003 .", "acm .                                c.  dwork , f.  mcsherry , k.  nissim , and a.  smith . calibrating noise to sensitivity in private data analysis . in _", "theory of cryptography , third theory of cryptography conference , tcc 2006 , new york , ny , usa , march 4 - 7 , 2006 , proceedings _ , pages 265284 , 2006 .", "p.  b. gibbons .", "distinct sampling for highly - accurate answers to distinct values queries and event reports . in _ in proceedings of the 27th international conference on very large data bases _ , pages 541550 .", "n.  mohammed , r.  chen , b.  c. fung , and p.  s. yu .", "differentially private data release for data mining . in _ proceedings of the 17th acm", "sigkdd international conference on knowledge discovery and data mining _ , kdd 11 , pages 493501 , new york , ny , usa , 2011 .", "a.  narayan and a.  haeberlen .", "djoin : differentially private join queries over distributed databases . in _ in proceedings of the 10th usenix symposium on operating systems design and implementation _ , 2012 .", "k.  nissim , s.  raskhodnikova , and a.  smith .", "smooth sensitivity and sampling in private data analysis . in _ proceedings of the thirty - ninth annual acm symposium on theory of computing _ , stoc 07 , pages 7584 , new york , ny , usa , 2007 .", "r.  pagh , m.  stckel , and d.  p. woodruff . is min - wise hashing optimal for summarizing set intersection ? in _ proceedings of the 33rd acm sigmod - sigact - sigart symposium on principles of database systems , pods14 , snowbird , ut , usa , june 22 - 27 , 2014 _ , pages 109120 , 2014 .", "d.  woodruff and q.  zhang .", "when distributed computation is communication expensive . in y.", "afek , editor , _ distributed computing _ , volume 8205 of _ lecture notes in computer science _ ,", "pages 1630 .", "springer berlin heidelberg , 2013 ."], "abstract_text": ["<S> in this paper , we present a general method for trade off between performance and accuracy of distributed calculations by performing data sampling . sampling was a topic of extensive research that recently received a boost of interest . </S>", "<S> we provide a sampling method targeted at separate , non - collaborating , vertically partitioned datasets . the method is exemplified and tested on approximation of intersection set both without and with privacy - preserving mechanism . </S>", "<S> an analysis of the bound on error as a function of the sample size is discussed and heuristic algorithm is suggested to further improve the performance . </S>", "<S> the algorithms were implemented and experimental results confirm the validity of the approach . </S>"], "labels": null, "section_names": ["introduction", "nave algorithm", "reducing data size by sampling", "experiments", "heuristic algorithm for bounded estimation", "privacy of intersection calculations", "concluding remarks"], "sections": [["consider different data providers holding vertically partitioned data .", "each data provider holds different information about the same set of individuals and there is a common identifier ( social security number , or any other sort of i d ) that allows one to cross - reference individuals across data providers .", "for example , the data providers contain a data of a set of individuals : gene `` banks '' hold genetic information , police departments hold criminal records , financial institutions keep record of credit history , hospitals record health history of patience for future diagnosis , etc .", "those data providers might be geographically spread , belong to different organizations and have their specific privacy and security requirements . for instance , a data provider of genetic information might not allow a public access to its data while allowing access for medical doctors to their patients data , or police departments that allow data access for international law enforcements , but deny access to anyone outside the departments . in such cases , it is usually impossible to gather all the data in one place , either due to the size of the data or due to privacy restrictions on data sharing . a client issuing a query to a different data providers in public cloud settings ,", "might be required to pay for utilized cpu time and network traffic . in many cases ,", "such client will be willing to trade performance ( in other words , cost ) for accuracy of the answer .", "especially , if the trade off is controlled by the client , which can define the accepted error in the received answer .    in this paper", "we propose a method of trading off performance for accuracy using sampling .", "we suggest a specific way to perform sampling in vertically split datasets and calculate sample size given acceptable error .", "performance improvements of the method are shown on a calculation of intersection set cardinality , together with a proposed heuristic algorithm .", "we also discuss a case of non - cooperative data providers and adaptation of the proposed algorithm for differential privacy calculations .    * our goal", "* : given a set of @xmath0 data providers @xmath1 which data records share a common identifier @xmath2 and a set of predicates @xmath3 , find the size of intersection @xmath4 where @xmath5 is a set of all records in @xmath6 that satisfy predicate @xmath7 .", "we assume that the number of records that are not present in all datasets is very small compared to the total number of records and thus , for the sake of simplicity , the data providers in @xmath8 are assumed to contain the same records with size denoted by @xmath9 , i.e. vertical split of the data .", "the size of the intersection might not be calculated precisely , but up to a given accuracy @xmath10 provided by the client querying the data providers .", "for example , the client that performs a query is interested in an integer percentage of people from the entire population having both criminal record and specific genetic mutation , i.e. the results can be rounded to a closest percentage .", "intuitively , this relaxed requirement should result in more computationally and network efficient algorithms .", "the optimization of computation time and network traffic are important , as mentioned above , in the commonly used public - cloud deployment ; the user is charged for computation time ( also called cpu time ) and network traffic .", "thus , minimization of those parameters is a very attractive algorithm property and present a trade - off with accuracy of the intersection set size .    * related work and our contribution : * conjunctive queries over distributed databases", "were extensively researched ( see as an example  @xcite ) .", "previous research has references both the aggregation techniques and performance issues of such queries .", "however , those works are based on an assumption that databases freely share the data , which does not hold in a settings of non - cooperative data providers .", "there were a number of works describing privacy preserving calculations of intersection size , see  @xcite .", "most of those works described a protocol that performs an exact calculation of the intersection set using secure multiparty computations ( mpc ) , first investigated by  @xcite and later generalized to multiparty computation .", "performing mpc protocols allows efficient calculation of the intersection size , however , when used by itself , it is also posssible to leak information about specific items in the datasets .", "for instance , the user can issue a query knowing with a single possible answer and check whether the intersection set is empty or not .", "such technique allow the user to identify the presence or absence of a specific item in the datasets .", "a technique that can be used to hide a presence of individual items in the dataset is differential privacy ( @xcite ) .", "while some of the above works ( @xcite ) performed inexact calculations of the intersection set , the source of approximation was to preserve differential privacy of the results .", "for instance , @xcite applied oblivious transfer ( ot ) protocol to approximate the size of the intersection of two databases , where ot has lead to the inexact result .", "our approach is to utilize the relaxed requirement of providing inexact result to improve the computational complexity of queries", ".    sampling as a way to cope with huge volumes of data or to increase computation performance were considered in a number of different contexts .", "( @xcite ) and section 4.2 of  @xcite discussed sampling as a means to cope with massive datasets by creation of a histogram - based estimators .", "once the estimator is created , when possible , the database performed estimation calculations on the histogram .", "reservoir stream sampling algorithms ( @xcite ) were developed to provide a sample of online data .", "the techniques fill in the sample ( `` reservoir '' ) by knocking out existing sample items from the sample with reducing probability as the sample begins to fill .", "( @xcite ) initiated usage of document sampling for document similarity comparison on internet scale .", "ideas were later developed into min - wise sampling techniques ( @xcite ) , which appear to be more suited for horizontal split datasets .", "in addition , a considerable corpus of work exists on concentration inequalities in scope of machine learning .", "concentration inequalities investigate the relation between the size of the sample and its statistical similarity to the entire dataset .", "pac - bayes bounds on hypothesis error as a function of sample size were provided in  @xcite .    in this paper", ", we consider the case of approximate calculations in distributed databases where the data is split vertically .", "we suggest an algorithm that takes advantage of a lack of accuracy in a distributed answer to considerably speed up the queries .", "we show a use - case of an algorithm on intersection set cardinality calculations both with and without privacy considerations .", "the algorithm adopts sampling techniques from ( @xcite ) and uses techniques similar to pac - bayes bounds from ( @xcite ) to decide on a mimimal , representative sample size .", "performance improvement might be especially significant in privacy preserving setting , where the calculations take considerable time .", "specifically our contributions are :    * suggested efficient method for approximate , distributed calculations with vertical - split datasets .", "* proposed a method of choosing sampling size given a required error level and provided simulation results of comparing the error level of different sample sizes .", "* suggested a heuristic algorithm of speeding up the intersection set size calculation based on bounds convergence and showed its adoptation for privacy preserving intersection set size calculation .", "the rest of the article is structured in the following way .", "section  [ sec : naive algorithm ] defines a nave algorithm for calculation of exact intersection set size , section  [ sec : sampling ] relaxes the requirement to calculate the exact size of intersection set and defines theoretical bounds for _ inexact _ intersection .", "section  [ sec : experiments ] presents results of experiments that validate the proposed sampling method .", "section  [ sec : bounded estimation ] describes a heuristic algorithm for calculation of intersection based on the bounds of intersection size .", "finally , section  [ sec : privacy ] discusses privacy issues in the described algorithms .", "the paper is concluded in section  [ sec : concluding remarks ] ."], ["the idea of the nave algorithm is simple : iterate over data providers @xmath11 and exchange a monotonously decreasing set of keys that satisfy all predicates up to the current iteration .", "the data provider then checks which of the received keys answer the corresponding predicate and returns a new list to the client .", "the client continues the process until all data providers were queried and the intersection set is found . in this way", ", the client calculates the intersection set iteratively .", "denote this set by @xmath12 where @xmath13 is the iteration number .", "in other words , @xmath14 where @xmath5 is a set of records in @xmath6 that satisfy predicate @xmath7 , and @xmath15 .    the nave algorithm can also be performed in a parallel way , where the client receives a set of records that satisfy the corresponding predicate from each data provider .", "the client then calculates the intersection set .", "comparing the sequential and parallel nave algorithms , sequential algorithm optimizes the network load and also improves cpu time , but not a wall - clock time of the algorithm ( i.e. , the time between the beginning of the operation and the time the client gets the final answer ) . unless stated otherwise , in the rest of the section we consider only the sequential version of the algorithm .    despite its virtue of being simple", ", the nave algorithm has a few drawbacks .    *", "the amount of information transferred between the client and the data providers is relatively large , as the entire set of keys in the intersection set is sent .", "* the first data providers evaluate the given predicate over their entire dataset , which is expensive . *", "the wall - clock time of the sequential algorithm is linear in a number of data providers and the size of the data . *", "the algorithm completely exposes information both between data providers and between data providers to the client .", "as mentioned above , those drawbacks are even more extreme in a current common practice , where a public cloud infrastructure is used for calculations , as in a public cloud infrastructure the client is charged for computation time ( cpu time ) and for network traffic . thus , there is a monetary incentive to minimize those two values .", "both the sequential and the parallel nave algorithms described above calculate the intersection set exactly , thus having a relatively high network traffic and cpu time demands .", "the next section discusses a way to reduce computation and network complexity by calculation of an estimation of the intersection size ."], ["relaxing the requirement for exact calculation of the intersection , this section discusses a way to calculate a smaller intersection set whose size can be extrapolated to the intersection size of the entire population . instead of calculating the intersection of the entire datasets ,", "perform a sample and calculate the intersection of the sample . then , scale up the size of the sample intersection to the size of the intersection for the entire population .", "the results of scale up will be an estimate for the intersection size .", "the requirements from the sample are clear : the sample should be as small as possible while truthfully representing the entire population ( given the definition and the error of representation ) .", "those two requirements present a trade - off between sample size and the accuracy of algorithm results .", "a number of different sampling techniques were developed over the years : on - line ( reservoir ) stream sampling algorithms ( @xcite ) , histogram - based estimators ( @xcite ) and min - wise sampling techniques ( @xcite ) which are more suited for horizontal split datasets . in our settings", ", it seems natural to choose hash function based technique , which in a sense a randomized version of histogram .    to provide sufficient improvements in performance", ", the size of the sample ( denoted by @xmath16 ) should be much smaller than the average size of the datasets ( @xmath17 ) : @xmath18 .", "therefore , if each data provider performs its own sampling , the overlap between the samples will be very small .", "in other words , the probability of the data record ( person in our example ) appearing in the sample is low . for every data provider @xmath13 @xmath19 where @xmath20 is a sample from the dataset of the data provider @xmath13 .", "leading to the probability of the item appearing in all samples being ( assuming that all data providers contain the same records ) : @xmath21 where @xmath22 and @xmath23 are a sample size and a dataset size of a data provider @xmath24 .", "assuming , for simplicity , that the sample size and dataset size are equal for all data providers , the probability becomes : @xmath25 as this probability is low , the size of the intersection will be essentially zero for most of the samples .", "the solution is to make all data providers create the same sample . in order to do that , we will use a sampling technique based on hashing ( @xcite ) .", "the technique works in the following way :    1 .", "the client defines an accuracy threshold for the calculation .", "2 .   using techniques described below ,", "the size of the sample is determined : @xmath16 .", "the number of hash buckets is @xmath26 .", "4 .   pick a hash function @xmath27 that will distribute datasets of data providers @xmath6 into @xmath28 buckets .", "@xmath27 hashes only the i d of the records and not the entire data , as the i d is the shared information across different data providers . 5 .", "the client sends each data provider 3 parameters : @xmath27 , the number of the bucket that was chosen randomly and predicate @xmath7 .", "this will ensure that all data providers use the same sample .", "each data provider evaluates its predicate on the records in a given bucket ( @xmath5 ) and sends the results to the client .", "the client performs the intersection calculation on the received results .", "as client acceptable error defines sample size , which is the bucket size , it is possible to pre - calculate a number of buckets for different error value .", "this will eliminate the need for sequential scan of the entire database on each query .", "however , it will also mean that the client will get approximately the required error .", "notice that just like the nave algorithm sampling can be done both in parallel and sequential ways . following the similar idea of the nave algorithm , in the parallel version of sampling algorithm", "each data provider evaluates the predicate over the entire given bucket , where in the sequential version , the client sends the current intersection set ( which is a subset of the given bucket ) to the next data provider , which evaluates the given predicate only over the received subset .", "however , in contrast to the nave algorithm , where the sequential variant significantly reduces the network load and cpu time , in sampling algorithm the reduction is much smaller , as the core underlying idea of the sampling algorithm is minimization of the amount of records participating in the intersection calculation .", "importantly , the accuracy of the intersection size estimation is the same in both sequential and parallel versions of the sampling algorithm .", "sampling improves both the computation time and network traffic over the nave algorithm by the factor of @xmath29 .", "thus , one important question still remains unanswered : what should be the size of the sample given an accuracy threshold ?", "the following sections define bounds on the sample size such that with high probability it will represent behavior similar to the entire dataset .", "two different cases are considered : sample size is small compared to the dataset size ( section  [ subsec : small sample ] ) and sample size is comparable to dataset size ( section  [ subsec : big sample ] ) . notice that in both cases the sampling is performed in the same way as explained above .", "however the bound used to calculate the sample size is different for those two cases .", "selection of the sample size is driven by the trade - off between the accuracy of the sample and the performance of calculations .", "the bigger the sample , the more accurate it is , but also the larger the computation time required to calculate the intersection . using a bound on the error as a function of the sample size , it is possible , given a limit on an acceptable error , to choose the size of the sample set .", "when a sample size is very small compared to the dataset size , the sampling can be approximated by independent sampling of the same size with replacement .", "the approximation can be done , as the probability of any record being a part of the sample goes from @xmath30 for the first pick to @xmath31 for the last pick .", "if @xmath32 then @xmath33 , and therefore , the probability is approximately ( @xmath30 ) , which is the same probability for a record to be picked when sampling with replacement .", "the reason to perform such approximation is due to a fact that it is simpler to provide a bound for independent samples with replacement rather than for sampling without replacement .", "the size of the sample should be big enough that the sample will be a good representative of the entire dataset for the intersection calculations .", "how can the size be estimated ?", "let us consider a sampling from the dataset @xmath6 according to the uniform distribution .", "notice that even though the sampling is done according to uniform distribution , as described in the sampling algorithm ( section  [ sec : sampling ] ) , the datasets themselves might be drawn from other distributions .", "the provided bounds still hold , as the sample `` similarity '' to the entire dataset depends only on its size . if a different , non - uniform sampling technique is used , the bound might be changed to use more general bernstein inequalities ( @xcite ) .", "define @xmath34 to be a random variable that the sampled item @xmath35 satisfies condition @xmath36 .", "then , the average value of the sample , denoted by @xmath37 , should be close to the mean value , @xmath38 , of the entire dataset .", "notice that in the binary case , the average is simply the number of data records that satisfy a given predicate divided by the data size . in order to bound the difference between the average value of the sample and the mean value of the dataset , we can use concentration inequalities .", "namely using results by hoeffding ( @xcite ) if @xmath39 are independent and @xmath40 , then : @xmath41    for simplicity , for now we will consider only the right - side of the equation , i.e. @xmath42 where @xmath16 is the size of the sample and @xmath10 measures the `` resemblance '' of the sample to the mean of the dataset . notice that the equation does not depend on the size of the dataset , as it is considered much larger than the sample .", "the bound in equation  [ eq : short hoeffding ] is used by the client when it issues queries to the data providers by defining both the acceptable error ( @xmath10 ) and the target probability of exceeding the acceptable error .      in cases where sampling size is comparable to the dataset size , it is possible to develop bounds directly for sampling without replacement . in the rest of the section , we show two bounds on sampling without replacement , one is based on a reduction of `` randomness '' of the data and the second one is based on a direct counting technique .", "those bounds can be expected to be tighter than those based on reduction to independence or bounds for sampling with replacement .", "the reason for this as follows .", "assume that @xmath0 points were sampled out of @xmath43 points without replacement .", "the next point is to be sampled from a set of @xmath44 rather than @xmath43 points , which would be the case in sampling with replacement .", "the successive reduction in the size of the sampled set reduces the `` randomness '' of the newly sampled point as compared to the independent case , and also introduces dependency between samples .", "whereas , the bound provided in equation  [ eq : short hoeffding ] does not depend on the dataset size and does not take the reduction in population size into consideration .", "this intuition is at the heart of serfling s improved bound ( @xcite ) which is stated next .", "the result holds for general bounded loss functions and is established by a careful utilization of martingale techniques combined with chernoff s bounding method .", "[ theorem : serfling ] ( serfling ) let @xmath45 be a finite set of non - negative bounded real numbers , @xmath46 .", "let @xmath47 be a random variables obtaining their values by sampling @xmath48 uniformly at random * without * replacement .", "set @xmath49 .", "then , @xmath50 similar bounds hold for @xmath51 .", "compared to the bound in equation  [ eq : short hoeffding ] , the above bound is always tighter when @xmath52 , i.e. when @xmath53 .    in our case @xmath54", "s are binary variables and the bound could be improved further by using a proof based on a counting argument ( @xcite ) .", "figure  [ fig : bounds comparison ] presents a single example of comparison between the above bounds ( [ eq : short hoeffding ] and [ eq : serfling ] ) . as expected , the serfling bound is tighter than the hoeffding bound and thus , using this bound for calculation of sample size will result in a smaller sample set .", ", @xmath55 .", "the error value is preset and then the sample size that fits this value is picked . in tighter bounds , the sample size will be smaller.,scaledwidth=50.0% ]    now we can briefly describe the process of using those bounds .", "we assume that the client attempts to minimize the size of the sampling sets , as this also minimizes the network load and cpu time , both of which are chargeable in public cloud environments .", "the client defines a value of an acceptable error for the combined distributed query ( @xmath56 ) and a `` confidence '' of the error .", "the bounds defined in ( [ eq : short hoeffding ] ) or ( [ eq : serfling ] ) provide a `` confidence '' that the error will be smaller than @xmath10 given a sample size .", "now the client uses the bound from ( [ eq : short hoeffding ] ) or ( [ eq : serfling ] ) to determine the minimum sampling size that allow the required error with `` high enough '' confidence . using a predefined confidence and a chosen bound , the client can find the minimum value of sample size that will provide the required error with the required confidence .", "for example , consider figure  [ fig : bounds comparison ] , where for dataset size of @xmath57 the client choose @xmath58 and confidence of @xmath59 .", "thus , using the simplified hoeffding bound ( [ eq : short hoeffding ] ) the minimal sample size is @xmath60 and using the serfling bound ( [ eq : serfling ] ) the minimal sample size is @xmath61 . of a sample size . ]    after the intersection of sample sets is calculated , there is a need to estimate the size of the intersection of the entire datasets . as mentioned above , the mean of the binary random variable is also the number of records that belong to a set defined by the predicate divided by the size of the sample .", "thus , the ratio of the records in a sample that satisfy a given predicate is close to the ratio of the records that satisfy the predicate in the entire dataset of a given data provider .", "this leads to the conclusion that the ratio of the sample intersection set size to the sample size should be the same as the ratio of the intersection set size to the size of entire dataset .", "let @xmath62 be a sample of @xmath6 of size @xmath16 , then @xmath63    notice that even though the absolute error in the intersection estimation of the entire dataset is proportionally larger than the error in estimation of a sample , the relative error will remain the same .", "the reason for the error to remain the same in a process of intersection calculation is due to the sampling method .", "even though the sample from each data provider has the same ( potential ) relative error , the error is unique for the sample and thus , it does not accumulates when the intersection is calculated .", "as @xmath10 in the above bounds ( equation  [ eq : short hoeffding ] and  [ eq : serfling ] ) is a bound on difference in estimation , it is a relative error . therefore , if the client defines an acceptable error ( @xmath56 ) as an absolute error , it can be easily translated : @xmath64 ."], ["we have performed a number of experiments to show the utility and error resulted from sampling .", "the experiments were performed on simulated data and validated on the adult dataset ( @xcite ) .", "the described sampling technique is targeted at large datasets with many records , such that calculation of predicates over the entire dataset is wasteful . for such datasets , it is much more practical to test the algorithm on simulated data , which can be generated on any required scale .      for experiments on simulated data ,", "a number of datasets were generated with random values of the predicates and then the intersection size was calculated according to each algorithm .", "the methodology works as follows .    *", "generate a dataset of a given size .", "all data providers assumed to share the same set of records with possibly a different data per record .", "* for each data provider generate a set of predicate values randomly , such that the frequency of `` true '' values is as defined .", "* calculate the intersection iteratively , i.e. by addition of a single datasets at a time .", "this is a both a simpler way of implementation ( as opposite to parallel calculation ) and also allows observation of convergence rate of the intersection size .", "all experiments were averaged over 10 runs , standard deviation was calculated and drawn on the resulting graphs .    figure  [ fig : algorithms comparison ] shows the intersection size as calculated by the sequential nave algorithm and estimation by sampling of various sizes .", "the graph provides a high - level , visual practical validity of the approach .", "even a relatively small sample sizes ( @xmath65 of the dataset size ) estimate intersection size close to the real value .    in order to focus on the error caused by sampling and", "show the difference in sample size more clearly , we have performed a number of experiments showing relative error between estimation and the real intersection size .", "the nave algorithm calculated was taken as a baseline for error calculations .", "the sampling algorithm was executed with a number of sample sizes , each one showing the relative error from the nave algorithm results .", "figures  [ fig : algorithms error comparison ] , [ fig : algorithms error comparison 1 m ] , [ fig : algorithms error comparison 100k r05 ] , and  [ fig : algorithms error comparison 1 m r05 ] show the relative error of the sampling algorithm with various sample sizes .", "the y axis shows the error of the sampling , i.e. |sampled estimation - intersection size| / ( intersection size ) , while the x axis is the number of datasets in the intersection .", "the graphs compare datasets sizes of 100,000 ( figures  [ fig : algorithms error comparison ] and [ fig : algorithms error comparison 100k r05 ] ) and 1,000,000 ( figures  [ fig : algorithms error comparison 1 m ] and [ fig : algorithms error comparison 1 m r05 ] ) records with predicate satisfaction frequency of @xmath66 and @xmath67 , i.e. in each data provider , @xmath68 or @xmath69 of the records satisfy the corresponding predicate .", "as mentioned above , each graph is an average of 10 different runs and standard deviation is depicted by error bars on the graph .", "the graphs show that , as expected , larger samples result both in smaller error and smaller standard deviation . however , it also can be seen , that the error quickly converges as a function of sample size . in some cases , even for sample of @xmath65 from entire dataset ,", "it is possible to achieve reasonable error .", "overall , sampling @xmath70 or @xmath71 of the dataset resulted in errors of approximately @xmath59 from the intersection size , which is an order of a single percent .", "thus , queries of a type : `` what is the approximate percentage of people ... ? '' can be answered using only tenth of a data .    , number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath66 .", "the y axis shows the size the intersection estimation .", "the x axis show the number of the data sets that participated in the intersection .", "error bars show standard deviation over 10 different executions.,scaledwidth=50.0% ]    , number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath66 .", "error bars show standard deviation over 10 different executions.,scaledwidth=50.0% ]    , number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath66 .", "error bars show standard deviation over 10 different executions.,scaledwidth=50.0% ]    , number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath67 .", "error bars show standard deviation over 10 different executions.,scaledwidth=50.0% ]    , number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath67 .", "error bars show standard deviation over 10 different executions.,scaledwidth=50.0% ]      in addition to experiments on simulated datasets , we have tested the methodology on adult dataset from uci machine learning repository ( @xcite ) .", "the dataset contains data from census bureau , where each record has data about different person .", "this fits a description of the setup we have described .", "the dataset contains 32,562 instances .", "as a test case we have used an intersection of the following predicates : age @xmath73 30 , marital status : never - married , sex : female or male in two different tests and income @xmath74 50k .", "the exact intersection set size is 252 for males and 139 for females .", "since the dataset was not used for classification task , it allowed us to use income as one of the predicates .", "in addition , the intersection was of 4 predicates , as addition of more predicates resulted in a small or empty intersection set due to a limited dataset size .", "samples of different sizes were drawn with sample size ratio going from 0.1 to 0.5 .", "the intersection set size was calculated from the drawn sample .", "figure  [ fig : adult sampling ] depicts the results of the testing .", "while the accuracy of the estimation does not necessary improves by taking larger samples , the standard deviation becomes smaller .", "the accuracy improvement is most probably caused by the small size of the intersection , while improvement in standard deviation fits the results shown in simulated datasets .", "overall , the accuracy of the intersection set size calculation verify the validity of the approach ."], ["previous sections described exact and approximate ways of calculating the size of the intersection .", "this section presents a heuristic algorithm that attempts to optimize the calculation of the intersection by not performing the calculations for all datasets .", "the simplest case of heuristic is when the intersection is calculated iteratively and at some point the intersection set is empty .", "clearly , the calculation can be stopped at this stage .", "following is a heuristic algorithm for the intersection calculation that attempts to stop at the earliest possible point .", "the size of the intersection depends on the sizes of the sets from each data provider that answer the corresponding predicate and on the correlation between those sizes .", "below we suggest a heuristic algorithm for estimation of the intersection set size .", "the algorithm starts with an accuracy parameter , the ratio of records that satisfy the relevant predicate in each data set , and the lower and the upper bounds on the intersection size .", "the algorithm iteratively tightens the bounds until the difference between the bounds is smaller or equal to the accuracy parameter .", "then the algorithm stops and returns the middle value of the range between the lower and the upper bound .", "let @xmath7 be the predicate that is associated with a data provider @xmath6 .", "then , @xmath75 is the set of members in @xmath6 that satisfy @xmath7 .", "also , @xmath76 will denote the fraction of the members that satisfy @xmath7 in @xmath6 , i.e. @xmath77 .", "now , let us define the bounds on the size of the intersection set @xmath78 for the sake of the algorithm iterations . for simplicity , we assume that data provider datasets @xmath79 contain _ exactly _ the same records .", "the bounds will hold when the difference in members between datasets is small .    as the intersection size is at most the size of the smallest predicate set ,", "the size of the intersection is bounded from above by the following bound : @xmath80    the lower bound on the intersection of two sets @xmath81 and @xmath82 is ( @xcite ) @xmath83 in case of 3 sets , the lower bound becomes : @xmath84 which in general is @xmath85 where @xmath86 is the average size of the datasets ( as they contain the same set of records ) .", "as described above , the iterative step of the algorithm is to tighten the bounds on the intersection size .", "the algorithm will then stop once the difference between bounds is smaller than the required accuracy . as the upper bound ( equation  [ eq : upper bound ] ) is the minimal predicate set , an iteration step will attempt to make the minimum value smaller by calculating the intersection between the two smallest predicate sets .", "the intersection of two sets will be smaller or of the same size as minimal predicate set and will replace those two sets in the bound .", "calculating the intersection of two minimal predicate sets is also a good technique for increasing the lower bound ( equation  [ eq : lower bound ] ) as well .. in this case the lower bound of their intersection is 0 . ] notice that each iteration step of the algorithm also decreases the value of @xmath0 and thus removes the subtractive member of the lower bound .", "the algorithm steps are as follows .    1 .", "define a required accuracy threshold : @xmath87 as @xmath88 is a relative error , the iterations continue until an absolute error is smaller than @xmath89 .", "when the iterations stop , the middle of the bounds range is returned , therefore , the iterations stop when the distance between the bounds is less or equal to @xmath90 2 .", "calculate @xmath5 for every data provider @xmath13 .", "3 .   calculate the lower ( @xmath91 ) and the upper bounds ( @xmath92 ) , given @xmath7 .", "[ step : iteration ] while @xmath93 and the number of sets is larger than 1 : 1 .", "let @xmath94 and @xmath95 .", "in other words , pick two minimal predicate sets : @xmath96 and @xmath97 .", "[ step : intersect ] calculate the intersection between those two sets .", "3 .   replace @xmath98 and @xmath99 with the new predicate set : @xmath100 .", "4 .   update new values of @xmath92 and @xmath91 .", "if only one set remains @xmath101 ( @xmath102 ) , then the size of its predicate set , @xmath103 , is the size of the intersection set .", "6 .   if there is more than one set and @xmath104 , then the size of the intersection set to the middle value of @xmath105 $ ] range : @xmath106 .", "the algorithm clearly converges , as the iterations stop when the intersection size is calculated exactly . at this point", "the lower bound is equal to the upper bound and thus , the accuracy requirement will be satisfied .", "notice that if step  [ step : intersect ] results in an empty set , the algorithms also stops , as the lower and the upper bounds will be equal and zero .", "* example : * [ example : algorithm example ] as an example of the algorithm execution , assume @xmath107 , equal size of all data sets @xmath43 and the required accuracy of @xmath108", ". let the respective ratios be @xmath109 and @xmath110 .", "in this case , the upper bound on the intersection set size will be @xmath111 , whereas the lower bound will be @xmath112 , as the sum of all ratios is @xmath113 less than @xmath114 .    the first iteration of the algorithm will be to find the intersection of @xmath115 and @xmath116 .", "let us assume that their intersection results in @xmath117 .", "now , the upper bound of the intersection becomes @xmath118 , whereas the lower bound increases to @xmath119 .", "the iterations continue until the bounds converge to within @xmath120 .", "in general , the heuristic algorithm will work fine in cases where the ratio of records that satisfy the predicate is high across most data providers , but the intersection size might decrease relatively fast . see figure  [ fig : heuristic algorithm ] for example of convergence of the algorithm bounds on simulated data .", ", number of datasets is @xmath72 , ratio of predicate satisfaction in each dataset is @xmath121 ( the high ratio is required for the lower bound to be larger than zero for @xmath72 ) . notice that in simulated case , the intersection size is co - located with the upper bound of the intersection.,scaledwidth=50.0% ]      clearly , both the computation time and network traffic depend heavily on the calculation of the intersection between the two sets in step  [ step : intersect ] of the heuristic algorithm . to improve the performance of this step it is possible to use the sampling technique from section  [ sec : sampling ] . instead of performing intersection between two sets @xmath13 and @xmath96", ", the algorithm will calculate the intersection of two samples of those sets .", "however , sampling introduces an error into calculation of the intersection , which has to be related to the accuracy threshold @xmath88 of the heuristic algorithm .    assuming that the error in sampling distributes proportionally among datasets and noting that each iteration `` eliminates '' one dataset", ", we can define the following changes to the algorithm .", "every sample introduces error of @xmath122 . to accommodate this error ,", "the algorithm will decrease the required accuracy threshold by this amount on each iteration .", "this will ensure that required by user accuracy threshold will be honored .", "two steps are changed in the algorithm :    1 .", "step  [ step : iteration ] then becomes : + _ while @xmath123 where @xmath13 is the iteration number , and the number of sets is larger than 1 .", "step  [ step : intersect ] becomes : + _ calculate the intersection between those two sets : @xmath96 and @xmath24 by sampling with error threshold @xmath124 .", "_    the rest of the algorithm remains the same , including required accuracy threshold value ."], ["in the described setting there are 2 different types of actors : data providers and client .", "there are different privacy concerns between those actors .", "one privacy concern is preserving the privacy of data providers .", "the client querying the data providers should not learn the identity of the records that are part of the intersection set or the number of records that satisfy a specific predicate in any single data provider .", "notice that the client should be limited to a reasonable number of queries ( sub - linear in a data size ) , as it is not possible to answer a large number of arbitrary queries while preserving privacy ( @xcite ) .", "another privacy concern is in keeping a privacy of records in a specific data provider from other data providers .", "for instance , a data provider might be interested in hiding the presence of specific record from other data providers .", "first , it is imperative to note that the sampling ( see section  [ sec : sampling ] ) provides a very nave form of privacy . as every data", "set record has a @xmath125 chance of not being included in sample , the presence of the specific record in the data set is not immediately observable . yet , the fact that the client gets record identifiers of each data provider is undesirable .", "data providers can encrypt the sampling indices to hide the identity of the records that are sent to the client .", "even though an additional encryption will obscure the identity of those items from the client , it still will expose the presence of individual records to other data providers .", "in addition , using an additional knowledge , the client can easily learn the presence of a specific record in the data providers .", "one way to preserve privacy is to use secure multiparty computations to calculate the intersection set ( @xcite ) a simple method that uses commutative cryptography ( another approach is to use secret sharing ) is described in  @xcite .", "each data provider encrypts record identifiers using its own commutative key and passes the key to other data providers .", "once all data providers have encrypted all keys , it is possible to find the intersection using encrypted identifiers utilizing commutativity of the encryption and calculate its size .", "there are a few downsides to using secure multi party computations .", "first , performance is heavily impacted by performing secure computations using these methods .", "it is possible to aleviate the performance issues by using the sampling to reduce a number of records in the calculated intersection , as described in section  [ sec : sampling ] .", "thus , the idea is to perform the sampling algorithm and then calculate the intersection in a secure way using the mpc computations between the data providers .", "this method however , will still require calculation of the sampled intersection exactly , without the ability to halt when required accuracy was reached , as it is possible in heuristic algorithm .", "the second drawback is specific for the described use - case where the data is kept in different organizations .", "secure multi party computations require direct communication between data providers , which in some cases is very challenging in a real - world deployments due to both security and technological reasons .", "while it is still possible for the client to act as an intermediary between different data providers , such setup also doubles the network traffic .", "a different approach is only to allow communication between client and data provider with adopted algorithms .", "however , the most significant drawback of mpc is that the exact intersection size is calculated .", "this might allow the client to use additional information to infer a presence of a specific record in the dataset .", "the current de - facto privacy standard is differential privacy ( @xcite ) .", "informally , the idea of differential privacy is to protect the privacy of individual records in the dataset without any assumption on the additional knowledge .", "differential privacy is preserved if it is practically impossible to identify whether the record is present or absent in the dataset from the result of database queries .", "the most common methods of ensuring differential privacy rely either on addition of a carefully chosen amount of noise to the result or by using an exponential mechanism that chooses the output according to some specific probability distribution .", "the protocols that are based on secure multiparty computations perform the exact calculation of the set , which is impossible in differential - privacy settings .", "this led to several works describing differential - privacy - preserving calculations of the intersection size , see  @xcite .", "even though those algorithms preserve differential privacy , the requirement to provide an approximate answer in our case allows our scheme to optimize the algorithm for performance by performing secure computations on as little number of the records as possible due to sampling .    the most common method of ensuring differential privacy is an addition of a specifically crafted random noise to the released data ( @xcite ) . moreover , there are two different approaches for the private data release : interactive ( @xcite)and non - interactive ( @xcite ) .", "interactive data release is when the client sends data query to a data provider .", "the data provider then releases the data to the client while ensuring differential privacy of records in its database .", "the data provider might decide not to answer a specific queries or to stop answering queries from a given client , as this might impact the privacy . in non - interactive data release", ", the noise is applied only once and then the data is released to the client .", "the client can perform any number and any type of queries on the data .", "intuitively , non - interactive release requires to add more noise to the released data , as it has to be ready for any query , while interactive release can adopt added noise to the results of each query ( @xcite ) .      in the described above sampling algorithm ( section  [ sec : sampling ] ) , there are two intuitive locations to add random noise .", "first , it is possible to add noise in hashing function that assigns records to buckets .", "such noise will preserve privacy of individual records , as it will be impossible to distinguish whether a specific record is within the bucket or not present at all in the dataset . a second location to add the noise is during a query processing .", "since the data provider exposes only the number of records that satisfy a given predicate , the noise can be added to the output after the predicate was evaluated or to the predicate itself [ [ ehud : this is not clear , the noise should be fake ids ] ] .", "the difference between those two locations is exactly the difference between interactive and non - interactive data release . adding noise to the hashing function", "is a one - time operation and has no relationship to the result of the specific predicate .", "thus , this noise addition can be seen as non - interactive data release . on the other hand ,", "if the noise is added after the predicate is evaluated over records in a bucket , then the noise value can be adapted to the results of predicate evaluation .", "thus , the amount of noise can be directly related to the results of a specific query , like in interactive data release settings .    due to the above reasoning", ", the next section describes an addition of a noise to the predicate function results .", "differentially private calculation of a counting function ( referred in the paper as `` noisy sum '' ) , i.e. counting a number of records that satisfy a given predicate , was considered previously in ( @xcite ) .", "( @xcite ) showed that adding a laplacian noise @xmath126 to the sum query output : @xmath127 , ensures @xmath128differential private function .", "notice that the sensitivity of counting function is @xmath129 and thus , the distribution standard deviation is @xmath130 .    the fast algorithm for privacy - preserving intersection calculation is described below .", "heuristic algorithm from section  [ sec : bounded estimation ] is used as a basis for privacy preserving algorithm .    1 .   use a hash function @xmath27 to divide data into buckets . in case only a single bucket is used for all queries , only the agreed bucket is kept .", "2 .   when a predicate is received from the client , evaluate the predicate over the chosen bucket .", "3 .   add random noise according to laplace distribution to the size of the predicate set .", "the noisy set - size is shared with the client .", "the client gathers results from all data providers and calculate upper and lower bounds .", "[ step : chosen data providers ] the client continues to execute heuristic algorithm and picks two data providers , @xmath96 and @xmath0 , to perform intersection according to the algorithm step  [ step : intersect ] .", "perform privacy - preserving , secure intersection between two chosen providers using the below algorithm . 7 .", "update the bounds and continue until the bounds are close enough .", "the algorithm used for privacy - preserving , secure intersection set calculation is based on a work of ( @xcite ) , which allows a differentially private calculation of an intersection over multiple databases .", "the algorithm provides computational differential privacy , as it relies on homomorphic encryption and makes some assumptions on computational hardness .", "this section describes how the algorithm from ( @xcite ) can be used in our heuristic algorithm from section  [ sec : bounded estimation ] for inexact computations .", "the algorithm describes a basic operation _ private set - intersection cardinality ( psi - ca ) _ and then adds noise to ensure differential privacy , thus resulting in bn - psi - ca .", "psi - ca operation is based on usage of homomorphic cryptosystem that allows addition and multiplication by constant ( for instance , paillier s cryptosystem ( @xcite ) ) .", "when two data providers , @xmath13 and @xmath96 , attempt to calculate intersection , @xmath13 defines a polynomial whose roots are the members of its set , the polynomial coefficients are then encrypted and transfered to @xmath96 data provider , which calculates @xmath131 . in other words", ", it evaluates the polynomial on its set members , multiplies by a constant number @xmath132 and adds a special string of zeros @xmath133 .", "when transfered back to the first data provider ( @xmath13 ) , it calculates the number of zero string . in order to add differential privacy , a random number of dummy values", "are added to the transfered set by both parties ( for details see ( @xcite ) rounding the set size to the bucket size in our case . in the end", ", data provider @xmath13 learns the noised cardinality and data provider @xmath96 knows the amount of noise it added to the intersection set size .    adopting this algorithm for our case , the client performs the following actions .", "assume that two data providers are chosen for intersection in step  [ step : chosen data providers ] : @xmath96 and @xmath0 .    1 .", "perform bn - psi - ca algorithm to calculate a noisy intersection set .", "assume that without loss of generality , @xmath96 will hold the size of a noisy set , where @xmath0 will hold the amount of added noise .", "2 .   @xmath96 sends the size of a noisy set to the client .    in the following iterations of the heuristic algorithm , bn - psi - ca", "might be invoked to find intersection size of @xmath96 , @xmath0 and @xmath132 , and more additional data providers . as due to", "privacy issues , no single data provider holds the intersection dataset , it is necessary to perform intersection size calculations over again for any additional data provider .", "notice that in cases where data providers do not communicate directly , the client acts as an intermediary for the protocol . in the end", ", the client has a value of intersection size which is still preserving differential privacy .", "the client also keeps record that @xmath0 holds the amount of noise of the last intersection . as described in ( @xcite ) , the noise added in one intersection , might be removed if there is a need to calculate another intersection of @xmath96 and @xmath0 with additional data provider .", "( @xcite ) performance results show approximately linear dependency of an algorithm run - time in the number of records in the participating datasets .", "using results from section  [ sec : experiments ] , with a reasonable error , it is possible to reduce the number of records by a factor of 5 - 10 ( according to acceptable error ) , thus tens of minutes to single minutes .", "such a reduction might make a difference between practical and impractical system .", "the algorithm accelerate approximate intersection calculation by :    * computationally intensive calculations of intersection sizes are performed on samples .", "those calculations are demanding computationally , thus , any reduce in the size of datasets is important .", "[ [ ehud : we should try to find a third - year student to implement this ] ] * the intersection is not calculate exactly , but rather the algorithm stops when the difference between bounds is close enough ."], ["a steady and rapid increase in the amount of data has resulted in an abundance of various data providers .", "more data is kept for longer and in more places . in parallel ,", "the awareness of data privacy and security is also on a rise both from government regulation and personal perspective .", "this trend requires new algorithms and protocols for dealing with distributed data .", "those new methods should be both efficient and privacy preserving .", "the main practical downside of the current privacy preserving computations is the run - time complexity .", "both mpc , oblivious - transfer and private - information - retrieval methods that are used to perform distributed calculations in privacy - preserving manner significantly increase the running time of the query . while some advances in improving performance were made , the running time still remains a limiting factor .", "our proposed method uses the ability to provide an approximate answer to the supplied query and thus to reduce considerably the dataset that is used for computations .", "this decouples the use of sampling technique for reducing the size of the dataset used for calculations from the protocols designed to preserve privacy of individual records or data providers . as such ,", "most of the above protocols can be used in conjunction with sampling techniques to perform approximate calculations with improved performance .", "as showed in this work , when acceptable , calculating an approximate answer can significantly improve computation time . in those cases", "we have suggested a way to use this possibility for approximate answering by using sampling of the datasets .", "the sampling leads to a dramatic reduction in the size of data required for calculation : @xmath134 of the data as observed in simulations", ". such reduction of the data size is much more significant when calculations are done in a secure and privacy - preserving way .", "r.  agrawal , a.  evfimievski , and r.  srikant .", "information sharing across private databases . in _ proceedings of the 2003 acm", "sigmod international conference on management of data _ , sigmod 03 , pages 8697 , new york , ny , usa , 2003 .", "acm .                                c.  dwork , f.  mcsherry , k.  nissim , and a.  smith . calibrating noise to sensitivity in private data analysis . in _", "theory of cryptography , third theory of cryptography conference , tcc 2006 , new york , ny , usa , march 4 - 7 , 2006 , proceedings _ , pages 265284 , 2006 .", "p.  b. gibbons .", "distinct sampling for highly - accurate answers to distinct values queries and event reports . in _ in proceedings of the 27th international conference on very large data bases _ , pages 541550 .", "n.  mohammed , r.  chen , b.  c. fung , and p.  s. yu .", "differentially private data release for data mining . in _ proceedings of the 17th acm", "sigkdd international conference on knowledge discovery and data mining _ , kdd 11 , pages 493501 , new york , ny , usa , 2011 .", "a.  narayan and a.  haeberlen .", "djoin : differentially private join queries over distributed databases . in _ in proceedings of the 10th usenix symposium on operating systems design and implementation _ , 2012 .", "k.  nissim , s.  raskhodnikova , and a.  smith .", "smooth sensitivity and sampling in private data analysis . in _ proceedings of the thirty - ninth annual acm symposium on theory of computing _ , stoc 07 , pages 7584 , new york , ny , usa , 2007 .", "r.  pagh , m.  stckel , and d.  p. woodruff . is min - wise hashing optimal for summarizing set intersection ? in _ proceedings of the 33rd acm sigmod - sigact - sigart symposium on principles of database systems , pods14 , snowbird , ut , usa , june 22 - 27 , 2014 _ , pages 109120 , 2014 .", "d.  woodruff and q.  zhang .", "when distributed computation is communication expensive . in y.", "afek , editor , _ distributed computing _ , volume 8205 of _ lecture notes in computer science _ ,", "pages 1630 .", "springer berlin heidelberg , 2013 ."]]}
{"article_id": "1606.00541", "article_text": ["in many scientific applications , we need to solve lower triangular problems and upper triangular problems , such as incomplete lu ( ilu ) preconditioners , domain decomposition preconditioners and gauss - seidel smoothers for algebraic multigrid solvers @xcite . the algorithms for these problems are serial in nature and difficult to parallelize on parallel platforms .", "gpu is one of these parallel devices , which is powerful in float point calculation and is over 10 times faster than latest cpu .", "recently , gpu has been popular in various numerical scientific applications .", "it is efficient for vector operations .", "researchers have developed linear solvers for gpu devices @xcite .", "however , the development of efficient parallel triangular solvers for gpu is still challenging @xcite .", "klie et al . (", "2011 ) investigated a triangular solver @xcite .", "they developed a level schedule method and a speedup of two was obtained .", "naumov ( 2011 ) from the nvidia company also developed parallel triangular solvers @xcite .", "he developed new parallel triangular solvers based on a graph analysis .", "the average speedup was also around two .    in this paper , we introduce our work on speeding triangular solvers", ". a new matrix format , hec ( hybrid ell and csr ) , is developed .", "a hec matrix includes two matrices , an ell matrix and a csr matrix .", "the ell part is in column - major order and is designed the way to increase the effective bandwidth of nvidia gpu . for the csr matrix ,", "each row contains at least one non - zero element .", "this design of the csr part reduces the complexity of the solution of triangular systems .", "in addition , parallel algorithms for solving the triangular systems are developed .", "the algorithms are motivated by the level schedule method described in @xcite .", "our parallel triangular solvers can be sped up to seven times faster .", "based on these modified algorithms , ilu(k ) , ilut and domain decomposition ( restricted additive schwarz ) preconditioners are developed .", "numerical experiments are performed .", "these experiments show that we can speed linear solvers around ten times faster .", "the layout is as follows . in  2 ,", "a new matrix format and algorithms for lower triangular problems and upper triangular problems are introduced . in ", "3 , parallel triangular solvers are employed to develop ilu preconditioners and domain decomposition preconditioners . in  4 , numerical tests are performed . at the end", ", conclusions are presented .", "for the most commonly used preconditioner ilu , the following triangular systems need to be solved : @xmath0 where @xmath1 and @xmath2 are a lower triangular matrix and an upper triangular matrix , respectively , @xmath3 is the right - hand side vector , @xmath4 is the unknown to be solved for , and @xmath5 is the intermediate unknown . the lower triangular problem , @xmath6 , is solved first , and then , by solving the upper triangular problem , @xmath7 , we can obtain the result @xmath4 . in this paper", ", we always assume that each row of @xmath1 and @xmath2 is sorted in ascending order according to their column indices .", "the matrix format we develop is denoted by hec ( hybrid ell and csr ) @xcite .", "its basic structure is demonstrated by figure  [ fig1 ] .", "an hec matrix contains two submatrices : an ell matrix , which was introduced in ellpack  @xcite , and a csr ( compressed sparse row ) matrix .", "the ell matrix has two submatrices , a column - indices matrix and a non - zeros matrix .", "the length of each row in these two matrices is the same .", "the ell matrix is in column - major order and is aligned when being stored on gpu .", "note that the data access pattern of global memory for nvidia tesla gpu is coalesced @xcite so the data access speed for the ell matrix is high .", "a disadvantage of the ell format is that it may waste memory if one row has too many non - zeros . in this paper , a csr submatrix", "is added to overcome this problem .", "a csr matrix contains three arrays , the first one for the offset of each row , the second one for column indices and the last one for non - zeros . for our hec format matrix , we store the regular part of a given triangular matrix @xmath1 in the ell part and the irregular part in the csr part . when we store the lower triangular matrix , each row of the csr matrix has at least one element , which is the diagonal element in the triangular matrix @xmath1 .          in this section ,", "we introduce our parallel lower triangular solver to solve @xmath8 the solver we develop is based on the level schedule method @xcite .", "the idea is to group unknowns @xmath9 into different levels so that all unknowns within the same level can be computed simultaneously @xcite . for the lower triangular problem , the level of @xmath9 is defined as @xmath10 where @xmath11 is the @xmath12th entry of @xmath1 , @xmath13 is zero initially and @xmath14 is the number of rows .", "we define @xmath15 , which is the union of all unknowns that have the same level . here", "we assume that each set @xmath16 is sorted in ascending order according to the indices of the unknowns belonging to @xmath16 .", "define @xmath17 the number of unknowns in set @xmath16 and @xmath18 the number of levels .", "now , a map @xmath19 can be defined as follows : @xmath20 where @xmath21 is the position @xmath9 in the set @xmath22 when @xmath9 belongs to @xmath22 . with the help of the map , @xmath19", ", we can reorder the triangular matrix @xmath1 to @xmath23 , where @xmath11 in @xmath1 is transformed to @xmath24 in @xmath23 .", "@xmath23 is still a lower triangular matrix . from this map", ", we find that if @xmath9 is next to @xmath25 in some set @xmath22 , the @xmath26th and @xmath27th rows of @xmath1 are next to each other in @xmath23 after reordering .", "it means that @xmath1 is reordered level by level , which implies that memory access in matrix @xmath23 is less irregular than that in matrix @xmath1 .", "therefore , @xmath23 has higher performance compared to @xmath1 when we solve a lower triangular problem .", "the whole algorithm is described in two steps , the preprocessing step and the solution step , respectively .", "the preprocessing step is described in algorithm [ alg1 ] . in this step", ", the level of each unknown is calculated first . according to these levels ,", "a map between @xmath1 and @xmath23 can be set up according to equation ( [ equ - map ] ) .", "then the matrix @xmath1 is reordered .", "we should mention that @xmath1 can be stored in any kind of matrix format .", "a general format is csr . at the end , @xmath23 is converted to the hec format and as we discussed above each row of the csr part has at least one element .    .", "@xmath28 ;    start = level(i ) ; end = level(i + 1 ) - 1 ; solve the @xmath27th row ;    @xmath29 ;    the second step is to solve the lower triangular problem .", "this step is described in algorithm [ alg2 ] , where @xmath30 is the start row position of level @xmath26 .", "first , the right - hand side @xmath3 is permutated according to the map @xmath19 we computed .", "then the triangular problem is solved level by level and the solution in the same level is simultaneous .", "each thread is responsible for one row . at the end", ", the final solution is obtained by a permutation .    to solve the upper triangular problem @xmath31", ", we introduce the following transferring map :    @xmath32 where @xmath14 is the number of rows in @xmath2 . with this map ,", "the upper triangular problem is transferred to a lower triangular problem , and the lower triangular solver is called to solve the problem .", "the ilu factorization for a sparse matrix @xmath33 computes a sparse lower triangular matrix @xmath1 and a sparse upper triangular matrix @xmath2 .", "if no fill - in is allowed , we obtain the so - called ilu(0 ) preconditioner .", "if fill - in is allowed , we obtain the ilu(k ) preconditioner , where @xmath34 is the fill - in level .", "another method is ilut(p , tol ) , which drops entries based on the numerical values @xmath35 of the fill - in elements and the maximal number @xmath36 of non - zeros in each row @xcite .", "the performance of parallel triangular solvers for ilu(k ) and ilut is dominated by original problems . in this paper , we implement block ilu(k ) and block ilut preconditioners . when the number of blocks is large enough , we will have sufficient parallel performance", ". if the matrix is not well partitioned and the number of blocks is too large , the effect of ilu(k ) and ilut will be weakened . when we partition a matrix , the graph library metis @xcite is employed .", "as we discussed above , the effect of ilu preconditioners is weakened if the number of blocks is too large .", "the domain decomposition preconditioner is implemented , which was developed by cai et al . @xcite .", "the domain decomposition preconditioner we implement is the so - called restricted additive schwarz ( ras ) method .", "overlap is introduced , and , therefore , this preconditioner is not as sensitive as block ilu preconditioners", ". it can lead to good parallel performance and good preconditioning . in this paper", ", we treat the original matrix as an undirected graph and this graph is partitioned by metis @xcite .", "the subdomain can be extended according to the topology of the graph .", "then each smaller problem can be solved by ilu(k ) or ilut . in this paper", ", we use ilu(0 ) .", "assume that the domain is decomposed into @xmath37 subdomains , and we have @xmath37 smaller problems , @xmath38 , @xmath39 , @xmath40 , and @xmath41 .", "we do not solve these problems one by one but we treat them as one bigger problem : @xmath42 each @xmath43 is factorized by the ilu method , where we have @xmath44 then equation ( [ ddm ] ) is solved by our triangular solvers .", "in this section , numerical experiments are presented , which are performed on our workstation with intel xeon x5570 cpus and nvidia tesla c2050/c2070 gpus .", "the operating system is centos 6.2 x86_64 with cuda toolkit 4.1 and gcc 4.4 .", "all cpu codes are compiled with -o2 option .", "the data type of a float point number is double .", "the linear solver is gmres(20 ) .", "bilu , bilut and ras denote the block ilu(0 ) , block ilut and restricted additive schwarz preconditioners , respectively .", "the matrix used in this example is from a three - dimensional poisson equation .", "the dimension is 1,000,000 and the number of non - zeros is 6,940,000 .", "the ilu preconditioners we use in this example are block ilu(0 ) and block ilut(7 , 0.1 ) .", "the performance data is collected in table [ tab - ex1 ] .", ".performance of the matrix from the poisson equation [ cols= \" < , < , < , < , < , < , < , < \" , ]     for the block ilu(0 ) , we can speed it over 3 times faster .", "when the number of blocks increases , our algorithm has better speedup .", "the whole solving part is sped around 8 times faster .", "bilut is a special preconditioner , since it is obtained according to the values of @xmath1 and @xmath2 ; sometimes , the sparse patterns of @xmath1 and @xmath2 are more irregular than those in bilu . from table", "[ tab - ex1 ] , the speedups are a little lower compared to bilu . however , bilut reflects the real data dependence , and its performance is better in general .", "this is demonstrated by table [ tab - ex1 ] .", "the cpu version bilut always takes less time than the cpu bilu .", "but for the gpu versions , their performance is similar .", "the bilut is sped around 2 times faster . the whole solving part", "is sped around 6 times faster .", "the ras preconditioner is always stable .", "it also has better speedup than bilu and bilut .", "the triangular solver is sped around 6 times faster and the average speedup is around 9 .", "matrix atmosmodd is taken from the university of florida sparse matrix collection @xcite and is derived from a computational fluid dynamics ( cfd ) problem .", "the dimension of atmosmodd is 1,270,432 and it has 8,814,880 non - zeros .", "the ilu preconditioners we use in this example are block ilu(0 ) and block ilut(7 , 0.01 ) .", "the performance data is collected in table [ tab - ex2 ] .", "llllllll pre & blocks & cpu ( s ) & gpu ( s ) & speedup & pre cpu ( s)&pre gpu ( s ) & speedup +    bilu & 16 & 20.61 & 2.63 & 7.79 & 0.0248 & 0.0072 & 3.45 + bilu & 128 & 23.94 & 2.80 & 8.50 & 0.0244 & 0.0072 & 3.40 + bilu & 512 & 24.13 & 2.72 & 8.82 & 0.0241 & 0.0070 & 3.46 + bilut & 16 & 14.70 & 2.37 & 6.16 & 0.028669 & 0.0114 & 2.51 + bilut & 128 & 16.58 & 2.43 & 6.79 & 0.028380 & 0.0100 & 2.84 + bilut & 512 & 19.91 & 2.64 & 7.50 & 0.027945 & 0.0113 & 2.47 + ras & 256 & 23.45 & 2.66 & 8.75 & 0.0428 & 0.0072 & 5.96 + ras & 2048 & 25.75 & 3.28 & 7.81 & 0.0546 & 0.0083 & 6.59 +    the performance of bilu , bilut and ras is similar to that of the same preconditioners in example 1 .", "when bilu is applied , the triangular solvers are sped over 3 times faster and the speedup of the whole solving phase is about 7 . because of the irregular non - zero pattern of bilut , the speedup of bilut is around 2 .", "the average speedup of the solving phase is about 6 .", "the speedup increases when the number of blocks grows .", "ras is as stable as in example 1 .", "the triangular solvers are sped around 6 , and the average speedup of the solving phase is around 8 .", "a matrix from spe10 is used @xcite .", "spe10 is a standard benchmark for the black oil simulator .", "the problem is highly heterogenous and it is designed the way so that it is hard to solve .", "the grid size for spe10 is 60x220x85 .", "the number of unknowns is 2,188,851 and the number of non - zeros is 29,915,573 .", "the ilu preconditioners we use in this example are block ilu(0 ) and block ilut(14 , 0.01 ) .", "the performance data is collected in table [ tab - ex3 ] .", "llllllll pre & blocks & cpu ( s ) & gpu ( s ) & speedup & pre cpu ( s)&pre gpu ( s ) & speedup +    bilu & 16 & 92.80 & 12.78 & 7.25 & 0.0421 & 0.0118 & 3.54 + bilu & 128 & 86.22 & 12.05 & 7.14 & 0.0423 & 0.0119 & 3.56 + bilu & 512 & 92.82 & 12.87 & 7.20 & 0.0424 & 0.0119 & 3.56 + bilut & 16 & 32.00 & 7.17 & 4.46 & 0.0645 & 0.0747 & 0.86 + bilut & 128 & 42.51 & 7.82 & 5.42 & 0.0647 & 0.0747 & 0.86 + bilut & 512 & 47.44 & 8.80 & 5.37 & 0.0645 & 0.0747 & 0.86 + ras & 256 & 106.61 & 14.36 & 7.41 & 0.100 & 0.0198 & 5.07 + ras & 1024 & 110.36 & 16.36 & 6.73 & 0.124 & 0.0242 & 5.11 +    from table [ tab - ex3 ] , we can speed the whole solving phase 6.2 times faster when ilu(0 ) is applied .", "the speedup increases if we increase the number of blocks .", "the average speedup of bilu is about 3 and the average speedup for the whole solving stage is about 7 . in this example", ", bilut is the best , which always takes the least running time .", "however , due to its irregular non - zero pattern , we fail to speed the triangular solvers .", "the ras preconditioner is always stable , and the average speedup of ras is about 5 , while the average speedup of the solving phase is around 6.5 .", "we have developed a new matrix format and its corresponding triangular solvers . based on them , the block ilu(k ) , block ilut and restricted additive schwarz preconditioners", "have been implemented .", "the block ilu(0 ) is sped over three times faster , the block ilut is sped around 2 times , and the ras preconditioner is sped up to 7 times faster .", "the latter preconditioner is very stable and it can be served as a general preconditioner for parallel platform ."], "abstract_text": ["<S> in this paper , we investigate gpu based parallel triangular solvers systematically . </S>", "<S> the parallel triangular solvers are fundamental to incomplete lu factorization family preconditioners and algebraic multigrid solvers . </S>", "<S> we develop a new matrix format suitable for gpu devices . </S>", "<S> parallel lower triangular solvers and upper triangular solvers are developed for this new data structure . with these solvers , ilu preconditioners and domain decomposition preconditioners </S>", "<S> are developed . </S>", "<S> numerical results show that we can speed triangular solvers around seven times faster .    </S>", "<S> solver , gpu , parallel , linear system </S>"], "labels": null, "section_names": ["introduction", "parallel triangular solvers", "preconditioners", "numerical results", "conclusions"], "sections": [["in many scientific applications , we need to solve lower triangular problems and upper triangular problems , such as incomplete lu ( ilu ) preconditioners , domain decomposition preconditioners and gauss - seidel smoothers for algebraic multigrid solvers @xcite . the algorithms for these problems are serial in nature and difficult to parallelize on parallel platforms .", "gpu is one of these parallel devices , which is powerful in float point calculation and is over 10 times faster than latest cpu .", "recently , gpu has been popular in various numerical scientific applications .", "it is efficient for vector operations .", "researchers have developed linear solvers for gpu devices @xcite .", "however , the development of efficient parallel triangular solvers for gpu is still challenging @xcite .", "klie et al . (", "2011 ) investigated a triangular solver @xcite .", "they developed a level schedule method and a speedup of two was obtained .", "naumov ( 2011 ) from the nvidia company also developed parallel triangular solvers @xcite .", "he developed new parallel triangular solvers based on a graph analysis .", "the average speedup was also around two .    in this paper , we introduce our work on speeding triangular solvers", ". a new matrix format , hec ( hybrid ell and csr ) , is developed .", "a hec matrix includes two matrices , an ell matrix and a csr matrix .", "the ell part is in column - major order and is designed the way to increase the effective bandwidth of nvidia gpu . for the csr matrix ,", "each row contains at least one non - zero element .", "this design of the csr part reduces the complexity of the solution of triangular systems .", "in addition , parallel algorithms for solving the triangular systems are developed .", "the algorithms are motivated by the level schedule method described in @xcite .", "our parallel triangular solvers can be sped up to seven times faster .", "based on these modified algorithms , ilu(k ) , ilut and domain decomposition ( restricted additive schwarz ) preconditioners are developed .", "numerical experiments are performed .", "these experiments show that we can speed linear solvers around ten times faster .", "the layout is as follows . in  2 ,", "a new matrix format and algorithms for lower triangular problems and upper triangular problems are introduced . in ", "3 , parallel triangular solvers are employed to develop ilu preconditioners and domain decomposition preconditioners . in  4 , numerical tests are performed . at the end", ", conclusions are presented ."], ["for the most commonly used preconditioner ilu , the following triangular systems need to be solved : @xmath0 where @xmath1 and @xmath2 are a lower triangular matrix and an upper triangular matrix , respectively , @xmath3 is the right - hand side vector , @xmath4 is the unknown to be solved for , and @xmath5 is the intermediate unknown . the lower triangular problem , @xmath6 , is solved first , and then , by solving the upper triangular problem , @xmath7 , we can obtain the result @xmath4 . in this paper", ", we always assume that each row of @xmath1 and @xmath2 is sorted in ascending order according to their column indices .", "the matrix format we develop is denoted by hec ( hybrid ell and csr ) @xcite .", "its basic structure is demonstrated by figure  [ fig1 ] .", "an hec matrix contains two submatrices : an ell matrix , which was introduced in ellpack  @xcite , and a csr ( compressed sparse row ) matrix .", "the ell matrix has two submatrices , a column - indices matrix and a non - zeros matrix .", "the length of each row in these two matrices is the same .", "the ell matrix is in column - major order and is aligned when being stored on gpu .", "note that the data access pattern of global memory for nvidia tesla gpu is coalesced @xcite so the data access speed for the ell matrix is high .", "a disadvantage of the ell format is that it may waste memory if one row has too many non - zeros . in this paper , a csr submatrix", "is added to overcome this problem .", "a csr matrix contains three arrays , the first one for the offset of each row , the second one for column indices and the last one for non - zeros . for our hec format matrix , we store the regular part of a given triangular matrix @xmath1 in the ell part and the irregular part in the csr part . when we store the lower triangular matrix , each row of the csr matrix has at least one element , which is the diagonal element in the triangular matrix @xmath1 .          in this section ,", "we introduce our parallel lower triangular solver to solve @xmath8 the solver we develop is based on the level schedule method @xcite .", "the idea is to group unknowns @xmath9 into different levels so that all unknowns within the same level can be computed simultaneously @xcite . for the lower triangular problem , the level of @xmath9 is defined as @xmath10 where @xmath11 is the @xmath12th entry of @xmath1 , @xmath13 is zero initially and @xmath14 is the number of rows .", "we define @xmath15 , which is the union of all unknowns that have the same level . here", "we assume that each set @xmath16 is sorted in ascending order according to the indices of the unknowns belonging to @xmath16 .", "define @xmath17 the number of unknowns in set @xmath16 and @xmath18 the number of levels .", "now , a map @xmath19 can be defined as follows : @xmath20 where @xmath21 is the position @xmath9 in the set @xmath22 when @xmath9 belongs to @xmath22 . with the help of the map , @xmath19", ", we can reorder the triangular matrix @xmath1 to @xmath23 , where @xmath11 in @xmath1 is transformed to @xmath24 in @xmath23 .", "@xmath23 is still a lower triangular matrix . from this map", ", we find that if @xmath9 is next to @xmath25 in some set @xmath22 , the @xmath26th and @xmath27th rows of @xmath1 are next to each other in @xmath23 after reordering .", "it means that @xmath1 is reordered level by level , which implies that memory access in matrix @xmath23 is less irregular than that in matrix @xmath1 .", "therefore , @xmath23 has higher performance compared to @xmath1 when we solve a lower triangular problem .", "the whole algorithm is described in two steps , the preprocessing step and the solution step , respectively .", "the preprocessing step is described in algorithm [ alg1 ] . in this step", ", the level of each unknown is calculated first . according to these levels ,", "a map between @xmath1 and @xmath23 can be set up according to equation ( [ equ - map ] ) .", "then the matrix @xmath1 is reordered .", "we should mention that @xmath1 can be stored in any kind of matrix format .", "a general format is csr . at the end , @xmath23 is converted to the hec format and as we discussed above each row of the csr part has at least one element .    .", "@xmath28 ;    start = level(i ) ; end = level(i + 1 ) - 1 ; solve the @xmath27th row ;    @xmath29 ;    the second step is to solve the lower triangular problem .", "this step is described in algorithm [ alg2 ] , where @xmath30 is the start row position of level @xmath26 .", "first , the right - hand side @xmath3 is permutated according to the map @xmath19 we computed .", "then the triangular problem is solved level by level and the solution in the same level is simultaneous .", "each thread is responsible for one row . at the end", ", the final solution is obtained by a permutation .    to solve the upper triangular problem @xmath31", ", we introduce the following transferring map :    @xmath32 where @xmath14 is the number of rows in @xmath2 . with this map ,", "the upper triangular problem is transferred to a lower triangular problem , and the lower triangular solver is called to solve the problem ."], ["the ilu factorization for a sparse matrix @xmath33 computes a sparse lower triangular matrix @xmath1 and a sparse upper triangular matrix @xmath2 .", "if no fill - in is allowed , we obtain the so - called ilu(0 ) preconditioner .", "if fill - in is allowed , we obtain the ilu(k ) preconditioner , where @xmath34 is the fill - in level .", "another method is ilut(p , tol ) , which drops entries based on the numerical values @xmath35 of the fill - in elements and the maximal number @xmath36 of non - zeros in each row @xcite .", "the performance of parallel triangular solvers for ilu(k ) and ilut is dominated by original problems . in this paper , we implement block ilu(k ) and block ilut preconditioners . when the number of blocks is large enough , we will have sufficient parallel performance", ". if the matrix is not well partitioned and the number of blocks is too large , the effect of ilu(k ) and ilut will be weakened . when we partition a matrix , the graph library metis @xcite is employed .", "as we discussed above , the effect of ilu preconditioners is weakened if the number of blocks is too large .", "the domain decomposition preconditioner is implemented , which was developed by cai et al . @xcite .", "the domain decomposition preconditioner we implement is the so - called restricted additive schwarz ( ras ) method .", "overlap is introduced , and , therefore , this preconditioner is not as sensitive as block ilu preconditioners", ". it can lead to good parallel performance and good preconditioning . in this paper", ", we treat the original matrix as an undirected graph and this graph is partitioned by metis @xcite .", "the subdomain can be extended according to the topology of the graph .", "then each smaller problem can be solved by ilu(k ) or ilut . in this paper", ", we use ilu(0 ) .", "assume that the domain is decomposed into @xmath37 subdomains , and we have @xmath37 smaller problems , @xmath38 , @xmath39 , @xmath40 , and @xmath41 .", "we do not solve these problems one by one but we treat them as one bigger problem : @xmath42 each @xmath43 is factorized by the ilu method , where we have @xmath44 then equation ( [ ddm ] ) is solved by our triangular solvers ."], ["in this section , numerical experiments are presented , which are performed on our workstation with intel xeon x5570 cpus and nvidia tesla c2050/c2070 gpus .", "the operating system is centos 6.2 x86_64 with cuda toolkit 4.1 and gcc 4.4 .", "all cpu codes are compiled with -o2 option .", "the data type of a float point number is double .", "the linear solver is gmres(20 ) .", "bilu , bilut and ras denote the block ilu(0 ) , block ilut and restricted additive schwarz preconditioners , respectively .", "the matrix used in this example is from a three - dimensional poisson equation .", "the dimension is 1,000,000 and the number of non - zeros is 6,940,000 .", "the ilu preconditioners we use in this example are block ilu(0 ) and block ilut(7 , 0.1 ) .", "the performance data is collected in table [ tab - ex1 ] .", ".performance of the matrix from the poisson equation [ cols= \" < , < , < , < , < , < , < , < \" , ]     for the block ilu(0 ) , we can speed it over 3 times faster .", "when the number of blocks increases , our algorithm has better speedup .", "the whole solving part is sped around 8 times faster .", "bilut is a special preconditioner , since it is obtained according to the values of @xmath1 and @xmath2 ; sometimes , the sparse patterns of @xmath1 and @xmath2 are more irregular than those in bilu . from table", "[ tab - ex1 ] , the speedups are a little lower compared to bilu . however , bilut reflects the real data dependence , and its performance is better in general .", "this is demonstrated by table [ tab - ex1 ] .", "the cpu version bilut always takes less time than the cpu bilu .", "but for the gpu versions , their performance is similar .", "the bilut is sped around 2 times faster . the whole solving part", "is sped around 6 times faster .", "the ras preconditioner is always stable .", "it also has better speedup than bilu and bilut .", "the triangular solver is sped around 6 times faster and the average speedup is around 9 .", "matrix atmosmodd is taken from the university of florida sparse matrix collection @xcite and is derived from a computational fluid dynamics ( cfd ) problem .", "the dimension of atmosmodd is 1,270,432 and it has 8,814,880 non - zeros .", "the ilu preconditioners we use in this example are block ilu(0 ) and block ilut(7 , 0.01 ) .", "the performance data is collected in table [ tab - ex2 ] .", "llllllll pre & blocks & cpu ( s ) & gpu ( s ) & speedup & pre cpu ( s)&pre gpu ( s ) & speedup +    bilu & 16 & 20.61 & 2.63 & 7.79 & 0.0248 & 0.0072 & 3.45 + bilu & 128 & 23.94 & 2.80 & 8.50 & 0.0244 & 0.0072 & 3.40 + bilu & 512 & 24.13 & 2.72 & 8.82 & 0.0241 & 0.0070 & 3.46 + bilut & 16 & 14.70 & 2.37 & 6.16 & 0.028669 & 0.0114 & 2.51 + bilut & 128 & 16.58 & 2.43 & 6.79 & 0.028380 & 0.0100 & 2.84 + bilut & 512 & 19.91 & 2.64 & 7.50 & 0.027945 & 0.0113 & 2.47 + ras & 256 & 23.45 & 2.66 & 8.75 & 0.0428 & 0.0072 & 5.96 + ras & 2048 & 25.75 & 3.28 & 7.81 & 0.0546 & 0.0083 & 6.59 +    the performance of bilu , bilut and ras is similar to that of the same preconditioners in example 1 .", "when bilu is applied , the triangular solvers are sped over 3 times faster and the speedup of the whole solving phase is about 7 . because of the irregular non - zero pattern of bilut , the speedup of bilut is around 2 .", "the average speedup of the solving phase is about 6 .", "the speedup increases when the number of blocks grows .", "ras is as stable as in example 1 .", "the triangular solvers are sped around 6 , and the average speedup of the solving phase is around 8 .", "a matrix from spe10 is used @xcite .", "spe10 is a standard benchmark for the black oil simulator .", "the problem is highly heterogenous and it is designed the way so that it is hard to solve .", "the grid size for spe10 is 60x220x85 .", "the number of unknowns is 2,188,851 and the number of non - zeros is 29,915,573 .", "the ilu preconditioners we use in this example are block ilu(0 ) and block ilut(14 , 0.01 ) .", "the performance data is collected in table [ tab - ex3 ] .", "llllllll pre & blocks & cpu ( s ) & gpu ( s ) & speedup & pre cpu ( s)&pre gpu ( s ) & speedup +    bilu & 16 & 92.80 & 12.78 & 7.25 & 0.0421 & 0.0118 & 3.54 + bilu & 128 & 86.22 & 12.05 & 7.14 & 0.0423 & 0.0119 & 3.56 + bilu & 512 & 92.82 & 12.87 & 7.20 & 0.0424 & 0.0119 & 3.56 + bilut & 16 & 32.00 & 7.17 & 4.46 & 0.0645 & 0.0747 & 0.86 + bilut & 128 & 42.51 & 7.82 & 5.42 & 0.0647 & 0.0747 & 0.86 + bilut & 512 & 47.44 & 8.80 & 5.37 & 0.0645 & 0.0747 & 0.86 + ras & 256 & 106.61 & 14.36 & 7.41 & 0.100 & 0.0198 & 5.07 + ras & 1024 & 110.36 & 16.36 & 6.73 & 0.124 & 0.0242 & 5.11 +    from table [ tab - ex3 ] , we can speed the whole solving phase 6.2 times faster when ilu(0 ) is applied .", "the speedup increases if we increase the number of blocks .", "the average speedup of bilu is about 3 and the average speedup for the whole solving stage is about 7 . in this example", ", bilut is the best , which always takes the least running time .", "however , due to its irregular non - zero pattern , we fail to speed the triangular solvers .", "the ras preconditioner is always stable , and the average speedup of ras is about 5 , while the average speedup of the solving phase is around 6.5 ."], ["we have developed a new matrix format and its corresponding triangular solvers . based on them , the block ilu(k ) , block ilut and restricted additive schwarz preconditioners", "have been implemented .", "the block ilu(0 ) is sped over three times faster , the block ilut is sped around 2 times , and the ras preconditioner is sped up to 7 times faster .", "the latter preconditioner is very stable and it can be served as a general preconditioner for parallel platform ."]]}
{"article_id": "1501.06102", "article_text": ["in recent years , there has been rapid growth in research on the human brain , as well as brains of other animals .", "the improvement and wide applicability of non - invasive techniques , such as fmri and new high - resolution electron microscopy , are fueling new research on brain physiology , and brain function . in the past ,", "advanced research on brain physiology was constrained to domain experts from the medical fields , neuroscience and other researchers with access to limited datasets .", "the advent of `` big data '' ecosystems , coupled with new brain research initiatives , large datasets and techniques are becoming more widely available .", "this is opening a new set of problems to a wider audience of professionals from computer science , mathematics and engineering .", "this paper outlines a framework for developing and deploying image - based analytics on high - resolution electron microscopy of animal brains .", "our focus is design and development of a `` full - stack '' prototype for deploying analytics in a hadoop map / reduce ecosystem . by `` full - stack '' ,", "we include interfaces and services that implement a pipeline from data download to distributed analytic processing and results output .", "this task was challenging in a short time - frame , so much more work is required to produce a mature system that can be used broadly .", "this work was completed as part of a graduate course at columbia university , called `` big data analytics '' .", "we plan to continue this research in the follow - on course , `` advanced big data analytics '' .", "big data analytics is a broad area of research . by its nature", ", it encompasses research on large frameworks for processing massive quantities of data .", "often the data is disparate and multi - modal .", "finally , the goal is state - of - the - art application of cutting - edge analytics in a large framework . in recent years", ", there has been tremendous growth in development of breakthrough advanced mathematical and computer science methods for tackling big data .    in computer vision", ", areas such as deep learning are advancing the state - of - the - art in significant ways .", "also , for years , research and development of probabilistic graphical models , belief propagation and graph analytics have pushed state - of - the - art forward . in particular , in areas of face detection and recognition , probabilistic graphical models have been known to produce state - of - the - art accuracy , when coupled with robust feature extraction , and careful training .", "currently , several centers are researching and developing new technologies that address brain understanding problems .", "new insight is being gained by collection massive quantities of high resolution imagery , and advanced analytic processing in large frameworks .", "for this project , we were fortunate to leverage expertise from the open connectome project .", "see @xcite , @xcite and @xcite for further details on this project .    there is a proliferation of big data frameworks , distributed processing and storage architectures , as well as stream processing environments .", "prior to undertaking analytic research and development , we surveyed several publicly - available distributed processing frameworks .", "we wanted to make early decisions on which programming languages and libraries to focus on .", "below we provide a list of some of the frameworks that were considered for this project .", "we do not provide details on each framework here .", "here is a list of publicly available distributed processing frameworks that were considered for this final project :    1 .", "hadoop map / reduce 2 .", "apache spark 3 .", "apache ( twitter ) storm 4 .   yahoo !", "ibm infosphere streams 6 .", "mit csail streamit 7 .", "zeroc ice 8 .", "fastflow ( with zeromq ) 9 .", "muesli 10 .", "rayplatform 12 . caf ( c++ actor framework ) 13 .", "flowvr 14 .", "openmp & mpi 15 .", "intel tbb 16 .", "apache ( facebook ) thrift", "http://cloud.ganita.org/images/beddiagram.png [ ]    developing all the components from figure [ sys_diag ] in a short time frame is very challenging .", "we divided research into the following categories :    1 .", "distributed download service 2 .", "image handling service 3 .   on - demand web service 4 .", "feature extraction service 5 .", "feature matching service 6 .", "graph reconstruction    one or more components have been developed in each of the six categories above . at this time , a complete pipeline is not fully constructed .", "we stood - up a cluster of commodity computers for the purpose of this course .", "we continue to use this cluster for development , testing and giving demos of our services .", "our system is called cloud ganita , and the following section provides more detail . following that", ", we describe data used for this project and then outline various components developed for the course project : brain edge detection .", "all installations , maintenance and system administration were performed on a small cluster , stood - up for this project .", "the table below shows the role of individual nodes in the cloud ganita cluster .", ".1 in     nn = namenode , rm = resourcemanager , dn = datanode , nm = nodemanager , jhs = jobhistoryserver , zk = zookeeper , hm = hbase master , hbm = hbase backup master , rs = hbase regionserver , qp = zk quorumpeer , hrs = hbase restserver    figure [ cloud - ganita1 ] shows a web interface created for this project . from the web interface , it is possible to view namenode , resource manager and job history on the hadoop cluster .", "it is currently password protected .", "log in - username : * ganita * and password : * cloud*.    .1 in    http://cloud.ganita.org [ ]      all of the data used for this project was made available by the open connectome project ( ocp ) .", "we focused on high - resolution electron microscopy collected by bobby kasthuri ( bu & harvard ) .", "it is known as the kasthuri11 dataset .", "here is a description of this data :    * ocp data is available via a restful interface * 3x3x30 cubic nanometer spatial resolution * 660 gb of images * up to 1,000,000,000,000 pixels * focused on 10752 x 13312 x 1850 resolution * @xmath0 265 billion pixels * info @xmath1 http://openconnecto.me/ocp/ca/kasthuri11/info/      the distributed download service has a ui and allows a user to carve out hdf5 voxels for download and indexing into hdfs .", "the ui is built using html5 , javascript and php", ". the php commands partition the em blocks into a manageable number of slices and load a configuration file into hdfs .", "then , a php command launches a hadoop map / reduce job that feeds lines from the configuration file to a ruby script .", "the ruby script uses curl on each configuration line to start the download from any one of the datanodes in our cluster .", "the image handling service allows conversion of hdf5 image data into any of the following formats :    * uncompressed imagery ( pnm / ppm / pgm ) * png compressed images * jpeg compressed images * ogg / theora compressed video * mp4/h.264 compressed video    we used ffmpeg along with the plug - in codecs : libogg , libtheora , libx264 , and the standard codecs .", "the on - demand web service is the user interface for automatically downloading voxels from ocp .", "the feature extraction service includes two main components .", "the first is a newly built c / c++ library for extracting gradient features from em data .", "the second is integration of opencv into the cloud environment and application of standard opencv feature extractors to the converted imagery .", "opencv is the prevailing open package for deploying and running computer vision algorithms on image or video .", "the feature matching service is still in development .", "it will include feature matching from opencv , along with newly created matchers in a c / c++ library .", "we may also continue to explore feature extraction and matching in matlab .", "it enables quick development of new advanced algorithms .", "also , ocp has built a new powerful api ( called cajal3d ) that will work directly with the ocp restful interface .    for graph reconstruction , we have begun development of a new graph processing engine in c / c++ .", "this engine will be tailored to the em brain data .", "plans are to integrate this engine with other existing graphical capabilities such as system g , libdai , etc .", "several tools and algorithms have been researched and developed for the purpose of processing electron microscopy . only recently ,", "have massive volumes of high - res em scans of brains been available .", "typically , in the medical imaging fields , focus has been on lower resolution 2d imagery .", "recently , the open connectome project has built a big data framework ( using sun grid engine ) for deploying computer vision algorithms in a distributed manner .", "also , created by ocp is a matlab api for rapidly developing and testing new algorithms in this framework .", "we found that advanced and robust computer vision algorithms are developed in matlab , c / c++ , or in cuda .", "we have made an early decision to focus on three main languages ( or development environments ) for creating new algorithms . with our eye on future research to include development of deep learning algorithms , probabilistic graphical models and dynamical systems approaches , we will focus on a few different deploy - and - test environments .", "1 .   matlab @xmath1 ocp sun grid engine 2 .", "c / c++ @xmath1 hadoop streaming & infosphere streams 3 .", "cuda @xmath1 gpu & caffe    for this paper , we focus on research and development of c / c++ algorithms and deployment of some of these algorithms in a hadoop map / reduce framework .", "here is a list of the main algorithms developed for this effort :    1 .", "hadoop streaming hdf5 download service 2 .   image handling service ( data conversion ) 3 .", "feature building ( c++ ) 4 .   graph analytic starter engine ( c++ ) 5 .", "web interface for project and basic service ( html5/js )    also , we started testing of basic computer vision feature extractors from opencv on the em data .", "however , since these algorithms are designed for 2-dimensional data ( images or motion frames ) , we have started development of a new c / c++ library for integration of 3-dimensional volumetric processing of em data . initially , we focused on research and development of 3d gradient feature extractors .", "we developed a 3d sobel operator , along with modified versions for tuning on high - res em volumes .", "we developed a standard 3d sobel operator for application on em voxels .", "we used the following kernel : @xmath2    @xmath3    @xmath4    below are two images : the first shows an original em slice , and the second shows the output of the 3d sobel operator .", "we also added the norm as a parameter and tested a modified sobel operator with @xmath5 replaced by @xmath6 for various @xmath7 values .", "finally , we added the ability to binarize the output of the 3d sobel operator using mean and standard deviation - based thresholds .", "notice clear remnants of closed curves , many with circular eccentricity .", "we are working on an algorithm to integrate around points on the closed curves and output a single stable center point .", "our main goal is to detect and extract feature patches for registering regions of the connectome .", "we develop and energy function that can be optimized that outputs many of these center points . from these", ", a sparse graph can be constructed , and then further dense modeling of em blocks and connectome regions .", "we will extract a feature vector based on cell regions , neurons , axons and synapses .", "we first build a graphical model based on statistically stable regions learned from experiments on large volumes of em data .", "the ganita feature extraction module consists of two main c++ classes : ganita3d and ganitaimage .", "ganitaimage handles reading , writing of images , videos and interfacing with external image and video libraries .", "ganita3d implements tailored 3d feature extractors such as the 3d sobel filter .      we have developed a basic graph processing engine in c++ .", "our goal is to use this engine to further analyze em data and integrate with existing probabilistic graphical frameworks .", "currently , our plans are to integrate with libdai which is a free and open source c++ library for discrete approximate inference in graphical models .", "to store a graph , several data structures are created .", "the goal is to avoid allocating @xmath8 locations in memory , since this may be very large .", "we create an array that stores the index of each vertex , and an array that stores the offset into the edge array for each vertex .", "then all edges are packed into a single array of longs in numeric ( vertex order , adjacency vertex order ) .", "total number of memory allocations equals : @xmath9    with each node and edge , a property can be stored . for image processing", ", the property may contain features from a region of the em block data .", "as an example , the graph library can compute the dot product between two nodes ( as the sum of common adjacent vertices ) .", "this can be computed combinatorically efficiently by scanning through the adjacent vertices in unison , while incrementing .", "note , a standard method for computing the dot product would be to store the edges in an adjacency matrix and compute the dot product of two binary vectors .", "we do not wish to allocate this much memory .", "the ganita graph software is composed of the following c++ classes : @xmath10", "a new project was created in github that contains some of the source code used for this project .", "the project name under github is * ganitabrain*. the software is structured as shown in figure [ software - fig ] .", "[ software - fig ] = [ thick , anchor = west , rounded corners , font= , inner sep=2.5pt ] = [ draw = blue , fill = blue!10 ] = [ selected , fill = blue!30 ]    child node [ selected ] build child node at ( 0,-.25 ) [ selected ] doc child node [ selected ] at ( 0,-.5 ) src child node build.sh child node [ selected ] at ( 0,-.2 ) feat child node build.sh child node c++ child node thirdparty child node at ( .4,-3.2 ) [ selected ] child node [ selected ] graph child node build.sh child node c++ child node at ( .4,-4.5 ) [ selected ] child node [ selected ] prep child node build.sh child node c child node cpp child node java child node python child node ruby child node script child node thirdparty child node at ( .4,-8.6 ) [ selected ] child node [ selected ] service child node [ selected ] at ( 0,-.2 ) em - to - video child node dashboard - g child node at ( 0,-.2 ) php ;    the software in prep is used to prepare the em data for analytic processing .", "this includes distributed download from the open connectome project and conversion to various formats .", "the directory feat contains the gradient feature extractor , including a 3-dimensional modified sobel operator . also , included in the same class is a binarization routine .", "the root directory contains three subdirectories : build , doc and src . to build the software , a user must first install all dependencies .", "there is a actually a long list of dependencies , but the most important ones are : libcurl , h5dump , hdf5 libraries and executables , ffmpeg , opencv .", "the c++ software in feat and graph can be built without installing most dependencies . to build executables and jar files ,", "a user should change directory to the root directory and enter the command : @xmath11    .1 in the following figure shows the download of ganitabrain from github and the successful build process .", "several experiments were run using the software developed for this effort .", "below is a screenshot from cloud ganita jobs that perform a distributed download , ingest and indexing of hdf5 imagery .", ".1 in    below are screenshots of cloud ganita namenode and datanodes .", "following that is a screenshot of the hdfs directory showing em data stored in 3 different formats : hdf5 , sequencefile and uncompressed pgm text files .", ".1 in        at this time , we have various analytics running in hadoop / hdfs .", "we have not yet collected ground - truthed data to test results from our query - by - example framework .", "this paper focuses on research to identify components for the big data framework , and development to deploy and integrate many of these components .", "future research will focus on establishing a pipeline for processing data in an on - demand fashion and comparing results to ground - truth .", "an interesting task is identifying synapses in the kasthuri11 dataset .", "for future directions , this author plans to focus on application of 3d feature extraction , labeling and training of classification using deep learning framework ( i.e. caffe ) , and application of probabilistic graphical models .", "focus will be on environments well suited c / c++ algorithm development .", "further utilization of matlab will take place .", "the author acknowledges assistance from several members of the open connectome project , including will gray roncal , jacob vogelstein and mark chevillet ( johns hopkins university ) .", "the author did this research while a student in columbia university s big data analytics course .", "the author thanks teacher , ching - yung lin , for several insightful comments that will guide future research .", "yamamoto , m. and kaneko , k. , parallel image database processing with mapreduce and performance evaluation in pseudo distributed mode , _ international journal of electronic commerce studies _ , * 3:2 * ( 2012 ) ."], "abstract_text": ["<S> this paper outlines research and development of a new hadoop - based architecture for distributed processing and analysis of electron microscopy of brains . </S>", "<S> we show development of a new c++ library for implementation of 3d image analysis techniques , and deployment in a distributed map / reduce framework . </S>", "<S> we demonstrate our new framework on a subset of the kasthuri11 dataset from the open connectome project .    </S>", "<S> brain ; electron microscopy ; connectome ; hadoop ; map / reduce ; service ; </S>"], "labels": null, "section_names": ["introduction", "related works", "system overview", "algorithms", "software package description", "experiment results", "conclusion", "acknowledgment"], "sections": [["in recent years , there has been rapid growth in research on the human brain , as well as brains of other animals .", "the improvement and wide applicability of non - invasive techniques , such as fmri and new high - resolution electron microscopy , are fueling new research on brain physiology , and brain function . in the past ,", "advanced research on brain physiology was constrained to domain experts from the medical fields , neuroscience and other researchers with access to limited datasets .", "the advent of `` big data '' ecosystems , coupled with new brain research initiatives , large datasets and techniques are becoming more widely available .", "this is opening a new set of problems to a wider audience of professionals from computer science , mathematics and engineering .", "this paper outlines a framework for developing and deploying image - based analytics on high - resolution electron microscopy of animal brains .", "our focus is design and development of a `` full - stack '' prototype for deploying analytics in a hadoop map / reduce ecosystem . by `` full - stack '' ,", "we include interfaces and services that implement a pipeline from data download to distributed analytic processing and results output .", "this task was challenging in a short time - frame , so much more work is required to produce a mature system that can be used broadly .", "this work was completed as part of a graduate course at columbia university , called `` big data analytics '' .", "we plan to continue this research in the follow - on course , `` advanced big data analytics '' ."], ["big data analytics is a broad area of research . by its nature", ", it encompasses research on large frameworks for processing massive quantities of data .", "often the data is disparate and multi - modal .", "finally , the goal is state - of - the - art application of cutting - edge analytics in a large framework . in recent years", ", there has been tremendous growth in development of breakthrough advanced mathematical and computer science methods for tackling big data .    in computer vision", ", areas such as deep learning are advancing the state - of - the - art in significant ways .", "also , for years , research and development of probabilistic graphical models , belief propagation and graph analytics have pushed state - of - the - art forward . in particular , in areas of face detection and recognition , probabilistic graphical models have been known to produce state - of - the - art accuracy , when coupled with robust feature extraction , and careful training .", "currently , several centers are researching and developing new technologies that address brain understanding problems .", "new insight is being gained by collection massive quantities of high resolution imagery , and advanced analytic processing in large frameworks .", "for this project , we were fortunate to leverage expertise from the open connectome project .", "see @xcite , @xcite and @xcite for further details on this project .    there is a proliferation of big data frameworks , distributed processing and storage architectures , as well as stream processing environments .", "prior to undertaking analytic research and development , we surveyed several publicly - available distributed processing frameworks .", "we wanted to make early decisions on which programming languages and libraries to focus on .", "below we provide a list of some of the frameworks that were considered for this project .", "we do not provide details on each framework here .", "here is a list of publicly available distributed processing frameworks that were considered for this final project :    1 .", "hadoop map / reduce 2 .", "apache spark 3 .", "apache ( twitter ) storm 4 .   yahoo !", "ibm infosphere streams 6 .", "mit csail streamit 7 .", "zeroc ice 8 .", "fastflow ( with zeromq ) 9 .", "muesli 10 .", "rayplatform 12 . caf ( c++ actor framework ) 13 .", "flowvr 14 .", "openmp & mpi 15 .", "intel tbb 16 .", "apache ( facebook ) thrift"], ["http://cloud.ganita.org/images/beddiagram.png [ ]    developing all the components from figure [ sys_diag ] in a short time frame is very challenging .", "we divided research into the following categories :    1 .", "distributed download service 2 .", "image handling service 3 .   on - demand web service 4 .", "feature extraction service 5 .", "feature matching service 6 .", "graph reconstruction    one or more components have been developed in each of the six categories above . at this time , a complete pipeline is not fully constructed .", "we stood - up a cluster of commodity computers for the purpose of this course .", "we continue to use this cluster for development , testing and giving demos of our services .", "our system is called cloud ganita , and the following section provides more detail . following that", ", we describe data used for this project and then outline various components developed for the course project : brain edge detection .", "all installations , maintenance and system administration were performed on a small cluster , stood - up for this project .", "the table below shows the role of individual nodes in the cloud ganita cluster .", ".1 in     nn = namenode , rm = resourcemanager , dn = datanode , nm = nodemanager , jhs = jobhistoryserver , zk = zookeeper , hm = hbase master , hbm = hbase backup master , rs = hbase regionserver , qp = zk quorumpeer , hrs = hbase restserver    figure [ cloud - ganita1 ] shows a web interface created for this project . from the web interface , it is possible to view namenode , resource manager and job history on the hadoop cluster .", "it is currently password protected .", "log in - username : * ganita * and password : * cloud*.    .1 in    http://cloud.ganita.org [ ]      all of the data used for this project was made available by the open connectome project ( ocp ) .", "we focused on high - resolution electron microscopy collected by bobby kasthuri ( bu & harvard ) .", "it is known as the kasthuri11 dataset .", "here is a description of this data :    * ocp data is available via a restful interface * 3x3x30 cubic nanometer spatial resolution * 660 gb of images * up to 1,000,000,000,000 pixels * focused on 10752 x 13312 x 1850 resolution * @xmath0 265 billion pixels * info @xmath1 http://openconnecto.me/ocp/ca/kasthuri11/info/      the distributed download service has a ui and allows a user to carve out hdf5 voxels for download and indexing into hdfs .", "the ui is built using html5 , javascript and php", ". the php commands partition the em blocks into a manageable number of slices and load a configuration file into hdfs .", "then , a php command launches a hadoop map / reduce job that feeds lines from the configuration file to a ruby script .", "the ruby script uses curl on each configuration line to start the download from any one of the datanodes in our cluster .", "the image handling service allows conversion of hdf5 image data into any of the following formats :    * uncompressed imagery ( pnm / ppm / pgm ) * png compressed images * jpeg compressed images * ogg / theora compressed video * mp4/h.264 compressed video    we used ffmpeg along with the plug - in codecs : libogg , libtheora , libx264 , and the standard codecs .", "the on - demand web service is the user interface for automatically downloading voxels from ocp .", "the feature extraction service includes two main components .", "the first is a newly built c / c++ library for extracting gradient features from em data .", "the second is integration of opencv into the cloud environment and application of standard opencv feature extractors to the converted imagery .", "opencv is the prevailing open package for deploying and running computer vision algorithms on image or video .", "the feature matching service is still in development .", "it will include feature matching from opencv , along with newly created matchers in a c / c++ library .", "we may also continue to explore feature extraction and matching in matlab .", "it enables quick development of new advanced algorithms .", "also , ocp has built a new powerful api ( called cajal3d ) that will work directly with the ocp restful interface .    for graph reconstruction , we have begun development of a new graph processing engine in c / c++ .", "this engine will be tailored to the em brain data .", "plans are to integrate this engine with other existing graphical capabilities such as system g , libdai , etc ."], ["several tools and algorithms have been researched and developed for the purpose of processing electron microscopy . only recently ,", "have massive volumes of high - res em scans of brains been available .", "typically , in the medical imaging fields , focus has been on lower resolution 2d imagery .", "recently , the open connectome project has built a big data framework ( using sun grid engine ) for deploying computer vision algorithms in a distributed manner .", "also , created by ocp is a matlab api for rapidly developing and testing new algorithms in this framework .", "we found that advanced and robust computer vision algorithms are developed in matlab , c / c++ , or in cuda .", "we have made an early decision to focus on three main languages ( or development environments ) for creating new algorithms . with our eye on future research to include development of deep learning algorithms , probabilistic graphical models and dynamical systems approaches , we will focus on a few different deploy - and - test environments .", "1 .   matlab @xmath1 ocp sun grid engine 2 .", "c / c++ @xmath1 hadoop streaming & infosphere streams 3 .", "cuda @xmath1 gpu & caffe    for this paper , we focus on research and development of c / c++ algorithms and deployment of some of these algorithms in a hadoop map / reduce framework .", "here is a list of the main algorithms developed for this effort :    1 .", "hadoop streaming hdf5 download service 2 .   image handling service ( data conversion ) 3 .", "feature building ( c++ ) 4 .   graph analytic starter engine ( c++ ) 5 .", "web interface for project and basic service ( html5/js )    also , we started testing of basic computer vision feature extractors from opencv on the em data .", "however , since these algorithms are designed for 2-dimensional data ( images or motion frames ) , we have started development of a new c / c++ library for integration of 3-dimensional volumetric processing of em data . initially , we focused on research and development of 3d gradient feature extractors .", "we developed a 3d sobel operator , along with modified versions for tuning on high - res em volumes .", "we developed a standard 3d sobel operator for application on em voxels .", "we used the following kernel : @xmath2    @xmath3    @xmath4    below are two images : the first shows an original em slice , and the second shows the output of the 3d sobel operator .", "we also added the norm as a parameter and tested a modified sobel operator with @xmath5 replaced by @xmath6 for various @xmath7 values .", "finally , we added the ability to binarize the output of the 3d sobel operator using mean and standard deviation - based thresholds .", "notice clear remnants of closed curves , many with circular eccentricity .", "we are working on an algorithm to integrate around points on the closed curves and output a single stable center point .", "our main goal is to detect and extract feature patches for registering regions of the connectome .", "we develop and energy function that can be optimized that outputs many of these center points . from these", ", a sparse graph can be constructed , and then further dense modeling of em blocks and connectome regions .", "we will extract a feature vector based on cell regions , neurons , axons and synapses .", "we first build a graphical model based on statistically stable regions learned from experiments on large volumes of em data .", "the ganita feature extraction module consists of two main c++ classes : ganita3d and ganitaimage .", "ganitaimage handles reading , writing of images , videos and interfacing with external image and video libraries .", "ganita3d implements tailored 3d feature extractors such as the 3d sobel filter .      we have developed a basic graph processing engine in c++ .", "our goal is to use this engine to further analyze em data and integrate with existing probabilistic graphical frameworks .", "currently , our plans are to integrate with libdai which is a free and open source c++ library for discrete approximate inference in graphical models .", "to store a graph , several data structures are created .", "the goal is to avoid allocating @xmath8 locations in memory , since this may be very large .", "we create an array that stores the index of each vertex , and an array that stores the offset into the edge array for each vertex .", "then all edges are packed into a single array of longs in numeric ( vertex order , adjacency vertex order ) .", "total number of memory allocations equals : @xmath9    with each node and edge , a property can be stored . for image processing", ", the property may contain features from a region of the em block data .", "as an example , the graph library can compute the dot product between two nodes ( as the sum of common adjacent vertices ) .", "this can be computed combinatorically efficiently by scanning through the adjacent vertices in unison , while incrementing .", "note , a standard method for computing the dot product would be to store the edges in an adjacency matrix and compute the dot product of two binary vectors .", "we do not wish to allocate this much memory .", "the ganita graph software is composed of the following c++ classes : @xmath10"], ["a new project was created in github that contains some of the source code used for this project .", "the project name under github is * ganitabrain*. the software is structured as shown in figure [ software - fig ] .", "[ software - fig ] = [ thick , anchor = west , rounded corners , font= , inner sep=2.5pt ] = [ draw = blue , fill = blue!10 ] = [ selected , fill = blue!30 ]    child node [ selected ] build child node at ( 0,-.25 ) [ selected ] doc child node [ selected ] at ( 0,-.5 ) src child node build.sh child node [ selected ] at ( 0,-.2 ) feat child node build.sh child node c++ child node thirdparty child node at ( .4,-3.2 ) [ selected ] child node [ selected ] graph child node build.sh child node c++ child node at ( .4,-4.5 ) [ selected ] child node [ selected ] prep child node build.sh child node c child node cpp child node java child node python child node ruby child node script child node thirdparty child node at ( .4,-8.6 ) [ selected ] child node [ selected ] service child node [ selected ] at ( 0,-.2 ) em - to - video child node dashboard - g child node at ( 0,-.2 ) php ;    the software in prep is used to prepare the em data for analytic processing .", "this includes distributed download from the open connectome project and conversion to various formats .", "the directory feat contains the gradient feature extractor , including a 3-dimensional modified sobel operator . also , included in the same class is a binarization routine .", "the root directory contains three subdirectories : build , doc and src . to build the software , a user must first install all dependencies .", "there is a actually a long list of dependencies , but the most important ones are : libcurl , h5dump , hdf5 libraries and executables , ffmpeg , opencv .", "the c++ software in feat and graph can be built without installing most dependencies . to build executables and jar files ,", "a user should change directory to the root directory and enter the command : @xmath11    .1 in the following figure shows the download of ganitabrain from github and the successful build process ."], ["several experiments were run using the software developed for this effort .", "below is a screenshot from cloud ganita jobs that perform a distributed download , ingest and indexing of hdf5 imagery .", ".1 in    below are screenshots of cloud ganita namenode and datanodes .", "following that is a screenshot of the hdfs directory showing em data stored in 3 different formats : hdf5 , sequencefile and uncompressed pgm text files .", ".1 in        at this time , we have various analytics running in hadoop / hdfs .", "we have not yet collected ground - truthed data to test results from our query - by - example framework .", "this paper focuses on research to identify components for the big data framework , and development to deploy and integrate many of these components .", "future research will focus on establishing a pipeline for processing data in an on - demand fashion and comparing results to ground - truth .", "an interesting task is identifying synapses in the kasthuri11 dataset ."], ["for future directions , this author plans to focus on application of 3d feature extraction , labeling and training of classification using deep learning framework ( i.e. caffe ) , and application of probabilistic graphical models .", "focus will be on environments well suited c / c++ algorithm development .", "further utilization of matlab will take place ."], ["the author acknowledges assistance from several members of the open connectome project , including will gray roncal , jacob vogelstein and mark chevillet ( johns hopkins university ) .", "the author did this research while a student in columbia university s big data analytics course .", "the author thanks teacher , ching - yung lin , for several insightful comments that will guide future research .", "yamamoto , m. and kaneko , k. , parallel image database processing with mapreduce and performance evaluation in pseudo distributed mode , _ international journal of electronic commerce studies _ , * 3:2 * ( 2012 ) ."]]}
{"article_id": "1604.06581", "article_text": ["infrastructure as a service  ( iaas ) systems @xcite build on virtualisation technologies to allow automated infrastructure provisioning .", "virtual machine ( vm ) based provisioning gives users two major benefits : they do not need to be experts in physical infrastructure maintenance , and they can easily follow their demand patterns and scale their virtual computing infrastructure ( composed of several vms ) with tools built on top of iaas systems .", "these two benefits led to the wide and rapid adoption of such infrastructure offerings .", "unfortunately , their rapid adoption has led to infrastructures that still have plenty of open research issues ( e.g. , energy aware vm placement , service level objective specifications ) .", "however , even iaas systems operated by academia are used in production nowadays . as", "production level systems are used by a multitude of users on a daily basis , changing the internal behaviour of such systems might hinder their user experience ( such as reliability and usability ) .", "thus , research focused on improving the internals of iaas systems ( e.g. , introducing a new experimental virtual machine placement algorithm ) can not be done on such production systems directly .", "consequently , to analyse new and novel ideas for internal behaviour , researchers are either restricted to severely limited iaas deployments ( e.g. , rarely utilising more than a few hosts ) or should resort to theoretical modelling of expected internal behaviour .", "however , results based on such research is often questioned by the operators of production clouds because their applicability to large scale systems is often not proven .", "some researchers use simulators to further evaluate their models  @xcite .", "these simulators allow researchers the evaluation of new ideas in life - like scenarios and as a result such simulators could pave the way for the new research results allowing their wide - spread adoption .", "although a plethora of iaas related simulators exist even today , these simulators have very different focuses .", "some are designed completely from the user s point of view and hide the cloud s internals so users can make decisions on how and what services should be moved to the clouds  @xcite . because of their user orientation , in these simulators it is frequently problematic to introduce changes in iaas behaviour .", "some others  @xcite emphasise the need for precision for such simulations . despite their extensibility , these simulators not only scale very poorly ( making it problematic to evaluate more elaborate iaas scenarios where sometimes thousands of physical machines collaborate ) , but they also require complex setup procedures to be precise ( e.g. , one should model every possible application in the system to receive realistic results ) . finally , there are simulators that introduce some assumptions in the system that reduce the precision of the simulations but reach unprecedented speeds  @xcite . unfortunately , despite having clear advantages , they are too specific to allow investigations on internal iaas changes ( e.g. , groudsim only models external interfaces of clouds , simgrid merely focuses on virtualisation , and cloudsim has conflicting extensions ", "e.g. , power modelling is not available while using networking ) .    in this article ,", "a new versatile simulation framework is presented ( called discrete event based energy consumption simulator for clouds and federations ", "dissect - cf ) .", "compared to the previously mentioned simulators , dissect - cf offers two major benefits : a unified resource sharing model , and a more complete iaas stack simulation ( including for example virtual machine image repositories , storage and in - data - centre networking ) .", "the benefits of the sharing model are threefold : @xmath0 it allows a single framework to model resource bottlenecks ( e.g. , cpu , network ) , @xmath1 generic resource sharing performance optimisations immediately improve entire simulations , @xmath2 it provides a unified view on resource usage counters ( i.e. , allows resource type independent , generic monitoring ) .", "finally , dissect - cf also opens up possibilities for more fine - grained energy consumption modelling by allowing the user to derive energy consumption from multiple resource usage counters . as a result of these new advancements ,", "the new simulator could foster research on schedulers that could either have better insight into internal iaas behaviour or collaborate with internal schedulers of iaas systems in order to achieve previously unprecedented flexibility , adaptability and elasticity in future cloud systems .", "unfortunately , dissect - cf s focus on supporting research on infrastructure cloud schedulers introduces several limitations to its applicability .", "first of all , for performance reasons the simulator represents networks with a simple flow model , which has already been shown by several studies ( e.g. , @xcite ) to be inaccurate for smaller - sized network transfers .", "fortunately , smaller - sized network transfers have a negligible influence on scheduling decisions in most cloud related schedulers .", "also , because scheduler focused research usually uses task or virtual machine instantiation / termination traces for behavioural studies , dissect - cf uses the black box philosophy for applications .", "thus , the simulator will not provide accurate results on resource utilisation if a particular application s behaviour can not be approximated with simple resource consumption metrics ( e.g. , when there is unstable cpu utilisation for extended periods of time ) .", "in fact , these limitations are present in most simulators  ( except those that have packet level network simulations or employ more complex flow models  see @xcite ) . finally ,", "as the new simulator is aimed at providing a framework for researchers to experiment with the internals of infrastructure clouds , the included scheduling mechanisms themselves are present only as examples for future work and they do not extend the scheduling related state - of - the art themselves .", "the behaviour of dissect - cf was analysed by first validating it against the behaviour of a small - scale infrastructure cloud at the university of innsbruck .", "according to the findings of this article , the system s simulated behaviour matches real - life experiments with negligible error ( in terms of application execution time , larger scale network transfers and energy consumption ) . for larger scale experiments ,", "dissect - cf was validated with proven results from two other simulators that are close to the new simulator s functionality ( namely cloudsim @xcite and groudsim @xcite ) .", "then , performance of these two simulators was compared to the newly proposed one .", "comparisons were executed with both real - world  ( using the grid workloads archive  @xcite ) and synthetic traces .", "the use of real - world traces also revealed that dissect - cf based simulations allow 1.5 - 32@xmath3 faster behavioural analysis of simple cloud schedulers or vm placement strategies .", "the performance differences were further investigated through synthetic traces and it is shown that dissect - cf scales significantly better in complex resource sharing situations with the help of its unified resource sharing model ( one can observe an improvement of even over 2800@xmath3 in execution time in some cases ) .    the rest of this article is organised as follows .", "section [ sec - relworks ] presents the related research results .", "then , section [ sec - design ] reveals the architecture of the newly proposed simulator and discusses its internal behaviour and extensibility options .", "section [ sec - eval ] analyses the properties of dissect - cf by comparing its behaviour to real - life systems and by comparing its performance to other simulators .", "finally , section [ sec - conclusion ] concludes the article with a summary and with the identification of future research directions .", "this section first reviews the scheduling scenarios that a cloud simulator might support .", "then an overview is presented on the most popular cloud simulation platforms .", "finally , the section concludes with a problem statement for the new simulator .", "there are seven common kinds of schedulers that could have an influence on the behaviour of a virtual infrastructure created on top of iaas cloud systems . in the following", ", a short overview is given of these kinds of schedulers with special attention on their requirements from a simulated environment .", "the list is presented from the schedulers that have the strongest user - side orientation to the most hidden schedulers in infrastructure systems .", "if a user has large enough resource demands , then its virtual infrastructure might include multiple virtual machines that could host a particular kind of task .", "in such cases , whenever a new task arrives , the user has to decide on which virtual machine it should actually run the task .", "the decision can be automated with a scheduler and a queuing system ( similar to local resource managers ", "e.g. ,  @xcite ) . in order to support research on these kinds of schedulers , _", "simulators should be able to provide past and present vm level performance metrics _", "( e.g. , temporal performance degradation of the vm s computing capabilities ) .    when a user s resource demands are more dynamic and sometimes unpredictable , then he / she would frequently face heavy under- or over - utilisation of his / her virtual infrastructure . to better meet the demands of the newly arriving tasks ,", "the virtual infrastructure should be able to automatically scale .", "this scaling is often achieved with a special scheduler ( e.g. , @xcite ) that decides when to instantiate / terminate a particular kind of vm .", "research on such schedulers need _ simulators that are capable of providing accurate vm management metrics _ ( e.g. , virtual machine instantiation time ) .", "some users have access to multiple cloud infrastructures . for such users ,", "a new scheduler is needed ( e.g. , @xcite ) which can choose between various cloud providers and dispatch vms to them .", "the selection procedure is expected to take into account the availability , reliability and similar metrics of the various providers and also it should consider issues like placing processing close to big data . for such schedulers , _", "simulators are required to offer infrastructure provider level metrics and data locality information . _    inside iaas systems , user requests are no longer represented as tasks but they are only seen as vms . as iaas systems", "are highly automated , decisions to place a particular vm on a physical machine must be also done by a scheduler .", "this kind of scheduler ( e.g. , @xcite ) could have two main tasks : @xmath0 for already existing virtual machines , a new vm to host mapping could be identified which would allow a vm arrangement that considers both the vms actual load and the providers current needs , and @xmath1 for newly requested virtual machines , the scheduler should determine the host where the vm could be run . as these schedulers have diverse tasks , _", "simulators should have the capability to disseminate the load of currently running virtual machines and also the utilisation of physical machines_.    energy conscious iaas systems aim at reducing their energy consumption in several ways .", "a simple way to do so is to consolidate vm load to the most energy efficient machines and switch the rest to a more energy efficient state .", "the automated decisions on which machines should be serving vms and which ones should be waiting in low power states ( e.g. , suspend , switch off ) are done by physical machine schedulers ( e.g. , @xcite ) .", "these schedulers should ensure that , because of their operations , virtual machine creation and quality of service do not degrade below certain levels . to support the development of physical machine ( pm ) schedulers , _", "simulators must necessarily maintain the cost of pm power state changes _", "( e.g. , cold / suspend to ram boot - up procedures ) .", "schedulers are also present in virtual machine monitors  ( like xen or kvm ) in order to allocate physical resources to virtual machines on a time - sharing basis ( e.g. , @xcite ) .", "although , these schedulers are not the main focus of research in cloud computing , they could have a direct impact on the quality of service if the above - mentioned vm placement strategies under - provision some virtual machines .", "for this reason , _", "simulators should be able to correctly handle and report under - provisioning scenarios on physical machines_.    the lowest levels of schedulers that may affect higher - level ( e.g. , task to vm assignment ) decisions are the process schedulers ( e.g. ,  @xcite ) of the operating system in the user s vms . in some cases the user could have an influence on the os scheduler , but in others users must use oss and schedulers that are prepared and accredited by the iaas providers . since these schedulers are generic os level schedulers , they are out of the scope of cloud computing research .", "but since higher - level schedulers might make decisions on how these process schedulers behave , simulators should give their users some information on their behaviour .", "for example , _ simulators should be capable of reporting if a particular vm is under - provisioned _ and tasks have no chance to access resources scheduled for them by the os level scheduler .", "cloudsim  @xcite is amongst the most popular iaas cloud simulators .", "it was initially based on gridsim  ( a widely used grid simulator developed by the same research institute  ", "@xcite ) but , after some performance and reliability issues , it was completely rewritten so it uses only some concepts ( e.g. , cloudlet  gridlet analogy ) from its predecessor  @xcite .", "cloudsim introduced the simulation of virtualised data centres mostly focusing on computational intensive tasks and data interchanges between data centres .", "later , they extended the simulation to better support internal network communications of a data centre with networkcloudsim  @xcite .", "there are also extensions that simulate the energy consumption behaviour of the physical machines in the data centre based on specpower benchmarks and on dynamic voltage and frequency scaling  @xcite .", "cloudsim also formed an ecosystem .", "several third parties offer extensions on top of cloudsim .", "some significantly change cloudsim behaviour", "( e.g. , add performance improvements  @xcite , add better support for inter - cloud operations  @xcite , implement new energy consumption models  @xcite , or introduce sla concepts into the simulation  @xcite ) , while others wrap cloudsim and provide additional functionality ( like graphical user interfaces for teaching  @xcite or for analytics  @xcite ) . despite its wide use , cloudsim has several disadvantages : @xmath4 low performance for scheduling research where thousands of scheduling scenarios should be evaluated in a timely fashion , @xmath5 networking is simulated for tasks only ( e.g. , data centre operations that utilise the same network as user tasks  like virtual machine image transfers  are not simulated even though they could have significant effects on the user perceived network performance ) and @xmath6 using multiple extensions at once is frequently not possible ( e.g. , advanced networking and energy consumption modelling are not usable together since one would need to have virtual machines that inherit behaviour both from ` powervm ` and from ` networkvm ` classes ) .", "the simgrid framework  @xcite is another widely used simulator for analysing the behaviour of distributed systems ( e.g. , grids , peer to peer systems ) .", "its resource sharing simulation is one of the most detailed ; for example , it contains one of the most accurate non - packet oriented network models  @xcite .", "this simulator s focus was not particularly on clouds for a long time but recently its developers introduced extensions for virtualisation ( e.g. , hypervisors or live migration  ", "because of its distributed systems and grid background the simulator is inefficient in iaas cloud related situations . for example , this simulator stops at the virtual machine level , thus it would require significant effort to build a multi data centre / cloud simulation on top of it .    while cloudsim and simgrid were heavily influenced by previous simulators for grids and distributed systems , for performance reasons they also make compromises on networking . to resolve such issues", "there are simulators like icancloud  @xcite and greencloud  @xcite that are built on network simulators ( e.g. , omnet++ or ns2 ) to more accurately simulate network communications in cloud systems .", "their efforts result in great accuracy if all iaas components and applications are modelled correctly network - wise ; otherwise , they just introduce serious performance penalties because of the packet level simulations without the expected accuracy .", "in addition to networking improvements , greencloud  @xcite is also offering precise energy estimations for networking and computing components , while icancloud also offers a user oriented simulation which supports iaas utilisation decision - making  @xcite on top of the regular iaas related simulation functionalities  @xcite .", "next , groudsim  a simulator developed at the university of innsbruck  @xcite  was analysed .", "this simulator aims at performance while it encompasses cloud concepts in a grid simulator environment . the simulator is also integrated with the askalon workflow system  @xcite so it can be used to evaluate behavioural changes of real - life scientific workflows in the case of changes in the computing environment .", "although this simulator supports clouds , it does not provide implementation on the internals of iaas systems ( i.e. , it provides a black box implementation ) , thus it is not suitable for research studies that involve the internals of cloud infrastructures . and", "although groudsim supports both cpu and network resources , the networking implementation of groudsim is one of the least developed ones amongst the reviewed simulators .", "the above simulators focus more on the user related behaviour of data centres , but there is a class of cloud simulators which is more focused on supporting decisions related to data centre operations  ( e.g. ,  @xcite ) .", "so even though these simulators could be used for examining user related behaviour , their detailed implementation of data centre behaviour reduces their usability in this context . on the other hand ,", "these simulators offer some unique features that might be useful for research on iaas related schedulers .", "for example , speci  @xcite is focused on offering a tool to analyse the scalability of iaas toolkits that will support future data centres .", "next , dcsim  @xcite allows the analysis of new virtual machine management operations ( like relocation ) .", "finally , dcworms  @xcite provides a unique view on data centre energy efficiency , including the heating , ventilation and air conditioning ( hvac ) system s airflow and high granularity resource ( e.g. , individual cpu , memory modules , network interfaces ) energy modelling .    [ [ problem - statement ] ] problem statement + + + + + + + + + + + + + + + + +    the analysis of the related work leads to the conclusion that existing simulators have many drawbacks for those who would like to investigate scheduling scenarios in iaas systems . to fulfil the needs of such scheduling scenarios , the rest of the article reveals a new infrastructure simulator that provides better insights on infrastructure behaviour for schedulers while maintaining the scalability of past simulators .", "]    figure  [ fig - arch ] presents the overall architecture of the newly proposed simulator .", "the figure groups the major components with dashed lines into subsystems . each subsystem is implemented as independently from the others as possible . as a result , simulation developers do not need to understand the complexity of the entire simulator if they intend to work on one of its subsystems .", "there are five major subsystems ; they are listed in an order that follows their level of abstraction ( from the most abstract to the more specific to iaas systems ) :    these components provide the time reference for simulations", ".    this subsystem acts as a lightweight and extensible foundation to low - level computing resource sharing ( e.g. , cpu , i / o ) .", "with these components dissect - cf enables simulator developers to monitor and analyse energy usage patterns of each individually simulated resource ( e.g. , network links , disks ) .", "these components handle the behaviour of those iaas system parts ( e.g. , virtual machines ) that are the primary target of iaas related schedulers .", "this subsystem provides the user interface ( the vm management api ) and represents the high level functionalities ( e.g. , virtual machine schedulers ) of infrastructure clouds .    in the following sections ,", "these subsystems are individually discussed .", "the core of the dissect - cf simulator is a simple but high performance event generator  ( reflected as ` timed ` in figure  [ fig - arch ] ) .", "it is used to maintain the time within the simulated system and allow third parties to be notified if a particular time instance has been reached.the simulator is not aware of the applied time granularity  ( i.e. , it is not known in the simulation if a single increase in the maintained time is equivalent to a single millisecond or a full hour ) .", "this enables flexibility in use , and allows simulation developers to have precision only when they assuredly need it ; otherwise , they can benefit from faster simulations . in later sections of the article , the smallest time granularity for the current simulation", "is denoted with @xmath7 and is expressed in seconds .", "thus any given time instance in the simulator can be specified as : @xmath8 , where @xmath9 and @xmath10 .", "here , @xmath11refers to the set of all possible time instances throughout a simulation .", "the simulator also assumes that notifications are recurring .", "thus , subscribing to events means specifying the frequency with which one would like to be notified .", "the simulator contains a construct ( called ` deferredevent ` ) for non - recurring events . creating a subclass of either", "the ` timed ` or the ` deferredevent ` classes allows simulation developers to receive custom time dependent notifications .    finally , the ` timed ` class is also the control point for the simulation time .", "simulations have two distinct ways to influence simulated time :    controls directly influence the timer .", "first , one can _ fire _ the events for the current time instance then advance the timer by one @xmath12 .", "second , it is possible to ask for a time jump that will progress the time with a given interval if within the interval there are no events expected .", "controls let the simulation flow for a given time interval without any intervention  ( e.g. , one can simulate until all events from the queue are cleaned up ) .", "these controls also enable the progression of the timer while dropping irrelevant events that would occur in a given period of time .      directly on top of basic time management", "lies the resource model of the simulator .", "the model is intended to capture low - level resource sharing behaviour ( e.g. , assigning tasks to virtual cpus  of vms  or virtual cpus to physical ones , or balancing network bandwidth utilisation ) .", "dissect - cf applies a provider - consumer scheme to resources where resource consumptions are intermediaries between consumers and providers . in the case of simulated cpus , consumptions represent instructions to be processed , thus cpu computing cycles of a physical machine are provided to virtual machines to consume . in a network analogy ,", "consumptions represent data to be transferred between two network hosts ( where the sender acts as the provider and the receiver as the consumer ) .", "dissect - cf allows the definition of both providers and consumers with the help of the ` resourcespreader ` class  ( see figure  [ fig - arch ] ) .", "the set of all spreaders in a particular simulation will be referred as @xmath13 .", "the simulator uses the concept of resource consumptions as the intermediaries that represent the current processing demands of the actual consumers .", "resource consumptions are denoted with a triplet : @xmath14 , where @xmath15 represents the resource consumption , @xmath16 represents the processing that is currently under way , @xmath17 represents the remaining processing ( i.e. , processing that has not started yet ) and @xmath18 represents the limit for this processing in a single @xmath12  ( e.g. , simulation developers can specify that a resource consumption is single - threaded so it can use the processing power of a single processor of a consumable cpu resource only ) .", "@xmath19 represents all possible resource consumptions in a simulation : @xmath20 . at a given time instance , the function @xmath21 determines which provider offers the resources to be consumed .", "similarly , @xmath22 defines the consumer that utilises the offered resources .", "these functions are time dependent to allow the migration of resource consumptions amongst spreaders .    at a given time", ", a particular resource consumption is processed in its provider by determining how much processing can be considered possible during a single @xmath12 .", "the possible processing has an upper bound of @xmath17 ( i.e. , if more processing could be possible than there is still remaining in @xmath15 , then the provider will have some non - utilised processing capabilities ) .", "also , the possible processing is limited by the provider s maximum processing capability and the processing limit @xmath18 of the consumption . @xmath23 where @xmath24 , @xmath25 and @xmath26 represent the processing under way , the remaining processing and the limit , respectively , for resource consumption @xmath15 at the time instance @xmath27 .", "it must be pointed out that @xmath28 is offering the provider side under processing value only .", "finally , @xmath29 reveals the processing power of a resource spreader ( in this current case the provider for resource consumption @xmath15 : @xmath30 ) at the time instance @xmath27 .", "dissect - cf simulates consumers with similar behaviour , so they remove utilised resources from @xmath16 .", "of course in this case the limit of utilisation is dependent on the consumer and the previously evaluated provider side possible processing value : @xmath31 thus , to determine the state of a particular resource consumption , dissect - cf first evaluates the provider side of resource consumptions , and then it processes the consumer side . after the simulator determined the @xmath16 value for a resource consumption , the remaining consumption @xmath32 can be determined as well by reducing with the increment of the @xmath16 value .", "this behaviour ensures that at the end of the consumer side processing both @xmath16 and @xmath17 will represent the resource consumption s state in the next simulated time instance ( @xmath33 ) .    in order to determine how much processing can be done on resource consumptions at a particular time instance @xmath34", ", resource spreaders apply the lowest level schedulers in dissect - cf based simulations .", "these schedulers share the processing capacities of the resources amongst those resource consumptions that the spreaders are currently dealing with  ( @xmath35 , where the notation of @xmath36 is used to depict a power set ) . as simulation developers", "are expected to run simulations with thousands of resources and millions of resource consumptions , these low - level schedulers must be highly customisable and efficient . to enable simple customisability ,", "dissect - cf provides efficient implementations for most common scheduling related tasks ( e.g. , resource consumption registration , de - registration , event generation for parties interested in resource consumption state ) , allowing providers of new schedulers to just focus on the scheduling logic that calculates the new @xmath34 values .", "+    the scheduling logic is expected to deliver fair resource allocation for simultaneously occurring resource consumptions  denoted as @xmath37 . to simplify the behaviour and complexity of schedulers", ", dissect - cf also introduces the concept of influence groups .", "these groups are formed from all resource spreaders that have a chance to influence each other s resource allocation schedules . with the help of influence groups", "even schedules for complex network structures can be simulated at close - to - real - life behaviour ( e.g. , the simulator can apply fair share algorithms over multiple related network links ) .", "these schedulers can utilise influence groups as their domain in which they have to guarantee a fair resource schedule for the spreader associated resource consumptions .    to determine the membership of an influence group", ", the simulator uses the resource consumptions that link consumers and providers ( see figure  [ fig - inf - gen ] ) . as a practical example ,", "figure  [ fig - inf - cpu ] shows how each simulated physical machine forms independent influence groups with the virtual machines it hosts via their respective cpu spreader implementations .", "formally , an influence group of a resource spreader at the particular time instance is defined as follows ( @xmath38 ) : @xmath39 where @xmath40 is a resource spreader , and the function @xmath41 defines the spreaders available at a particular time instance .", "the equation shows that @xmath42 includes all resource spreaders that are directly or transitively referred by the associated resource consumptions of the spreader @xmath43    @xmath44 . as a result", ", one could find as many influence groups as the number of resource spreaders existing at a given time instance in the simulation . on the other hand ,", "these groups are frequently equivalent because determining the influence group of any member of a particular group will result in the original influence group : @xmath45 this last equation is derived from the definition of @xmath42 and reveals that after the proper calculation of @xmath42 there should not be any members of it ( e.g. , @xmath46 ) that would result in a different influence group than the original @xmath42 .", "although the definition of @xmath47 is quite straightforward , its evaluation in all necessary time instances for all relevant resource spreaders would result in significant simulation performance deterioration .", "therefore , dissect - cf provides an algorithm that significantly reduces the use of @xmath47 but still ensures that the influence groups are available for the scheduling logic in every time instance . to differentiate between the original function s results and the algorithm calculated values , the notation @xmath48 is used for", "the influence groups determined by the algorithm ( see algorithm  [ alg - basopt ] ) .", "@xmath49 @xmath50 [ lin - ext - start ] @xmath51 [ lin - emptyext ] @xmath52 [ lin - identadd ] @xmath53 [ lin - extgadd ] @xmath54 [ lin - realext ] @xmath55 [ lin - igextcomplete ] [ lin - ext - stop ] [ lin - split - start ] @xmath56 @xmath57 [ lin - randsel ] @xmath58 [ lin - correct - group ] @xmath59 [ lin - split - upd - start ] @xmath60 [ lin - split - upd - stop ] [ lin - split - stop ]    in the following few paragraphs , the internal behaviour of the new algorithm is discussed .", "it is built on the assumption that @xmath61 and it is composed of two distinct phases : influence group extension ", "see lines [ lin - ext - start]-[lin - ext - stop ]  and group dissolution ", "see lines [ lin - split - start]-[lin - split - stop ] .", "let us first discuss the extension phase . during this phase ,", "the algorithm first starts with an empty resource spreader set ( see line [ lin - emptyext ] ) that will later on hold the identified extensions of the input influence group  @xmath49 . as a next step , line [ lin - identadd ] determines the resource consumptions that arrived to a particular resource spreader at the time instance @xmath33 .", "afterwards , the following line focuses on those newly added resource consumptions that introduce new resource spreader members into the input influence group .", "these non - member providers or consumers are added to the extension set in line  [ lin - extgadd ] .", "this iteratively created extension set is used to actualise the input influence group in lines [ lin - realext]-[lin - igextcomplete ] .", "the extension phase completes only when there are no newly introduced resource spreader members in the input group .", "otherwise , the current phase is repeated to ensure finding even further group extensions via the resource consumptions associated with the introduced members ( this last step is shown in line [ lin - ext - stop ] ) .", "after there are no new extension possibilities found in the current influence group , the algorithm proceeds to its second phase in which it identifies all splits of the current influence group .", "for example , influence group # 5 in figure  [ fig - inf - gen ] will have to be split when the resource consumption between provider @xmath62 and consumer @xmath63 finishes . to identify the need for splitting", ", the algorithm therefore first determines if there were some resource consumptions dropped from at least one member of the influence group ( see line  [ lin - split - start ] ) .", "if there is a need for splitting , then the algorithm will maintain the not yet split parts of the original influence group in @xmath64 . in order to determine", "which parts have to be split from the not yet split parts , lines [ lin - randsel ] and [ lin - correct - group ] use the original @xmath65 on a randomly selected spreader from @xmath64  resulting in a new influence group called @xmath66 . in the next line ,", "the not yet split parts are updated so that only those resource spreaders will be considered afterwards that are not in @xmath66 .", "finally , the algorithm updates its self - maintained influence group membership so all members of @xmath66 will be exactly the same ( see lines [ lin - split - upd - start]-[lin - split - upd - stop ] ) .    to conclude , the newly introduced algorithm reduces the number of direct @xmath42 function calculations to those cases where there is a chance to have a group to be split . and even in that case", ", it ensures that the number of @xmath42 evaluations is limited by the number of influence groups created after a split .      ]", "the last remaining part of the unified resource model is the simulation developer customisable low - level scheduling logic . to understand the customisation options dissect - cf offers ,", "figure  [ fig - rs ] presents the context of the scheduling logic .", "the figure reveals the role of the low - level scheduler through the illustration of the life of a single resource consumption that can be represented in three phases and denoted with different kinds of arrows : preparation  ", "regular lines ; resource consumption    dashed lines ; and completion", "  dotted lines .", "the next paragraphs provide a brief overview of these three phases .", "first , the preparation phase is initiated by the entity who is responsible for creating ( see _ step 1 _ in the figure ) a particular resource consumption  @xmath67 .", "this entity could be an automated process ( e.g. , a workload generator ) or some higher - level entity of the simulator ( e.g. , the virtual machine representation ) .", "after creation , the registration can be initiated in any time instance @xmath27 after both the consumer ", "@xmath68  and the provider  @xmath69 ", "spreaders are specified .", "the registration is accomplished in _ step 2 _ in the figure .", "the provider nudges the scheduler base in _ step 3 _ after both the consumer and the provider have registered the new resource consumption  @xmath70 and @xmath71 .", "the _ scheduler base _ is implemented in the base class of all resource spreaders and is responsible for interfacing with the event system , the scheduler and the influence group management algorithm . before contacting the event system for subscription ,", "the scheduler base first updates the influence groups with algorithm  [ alg - basopt ] in _ step 4_. after the identification of all distinct influence groups , the scheduler base filters those groups that would need an updated schedule .", "such groups are identified via recently added or dropped resource consumptions to / from one of their member resource spreaders ", "i.e. , @xmath72 . in _ step 5", "_ , the scheduling logic is invoked for each of the filtered groups . during this step", ", it should assign the @xmath34 values for all resource consumptions that are currently taking place in a given influence group . with these assignments ,", "the simulator calculates the earliest completion time of the currently managed resource consumptions .", "then , in _ step 6 _ , it subscribes to a notification from the event system in order to know when the next resource consumption will be removed from the filtered influence groups .    in the following phase , the simulator handles the resource consumptions", "this phase is either done when the event system delivers the notification on a resource consumption completion ( see _ step 7 _ ) or alternatively upon the registration of a new resource consumption . in the second case ,", "the resource consumption handling is automatically executed before influence groups are calculated ( i.e. , steps _ 8 - 12 _ could precede _ step 4 _ of the preparation phase if a resource consumption is registered at a resource spreader that has already had some prior resource consumptions ) . in practice ,", "_ steps 8 - 9 _ evaluate eq .", "[ eq - prov - update ] and _ steps 10 - 11 _ evaluate eq .", "[ eq - cons - update ] for all simultaneously existing resource consumptions ", "@xmath73  in the simulator . resource consumptions ", "e.g. , @xmath74  are automatically marked for removal when they reach their completion ", "i.e. , @xmath75 . as a final step for resource consumption handling , the scheduler base checks for resource consumptions marked for removal  ( see _ step 12 _ ) and on all marked resource consumptions it executes the completion phase .    in the final phase ,", "the resource consumption s completion is simulated .", "consumptions can be considered complete in two cases : either they have no further processing to be done or they were cancelled by the entity using the resource sharing mechanism .", "the scheduler base notifies this entity in both cases  see _ step 13_. then it checks if it has finished all the current resource consumptions ", "if there are still further resource consumptions to process , then the scheduler base resumes operations from _ step 4 _ , otherwise it cancels further notifications from the event system .", "as can be seen , the simulator expects scheduling logic implementations to utilise a fairly narrow and well - defined interface with the scheduler base . through this interface", "the simulator ensures that whenever a new schedule is needed ( i.e. , new @xmath34 values ) , the simulation developer provided scheduling logic is always called .", "dissect - cf also provides two sample implementations for this scheduling logic : a max min fairness algorithm  @xcite implementation with progressive filling , and a simple logic that does not deal with complex bottleneck situations but demonstrates the interfaces with the scheduler base .", "compared to other recently developed simulators , dissect - cf completely decouples energy modelling from resource simulation in order to allow accounting for such energy consumptions that are not in direct relation to the resource utilisation of data centres . with this approach ,", "a more comprehensive energy and power modelling is achievable that enables the analysis of new sophisticated energy aware algorithms in the areas of virtual machine placement , task scheduling , etc .", "these algorithms previously were frequently limited because energy readings from heating , ventilation and air conditioning ( hvac ) units or higher - level iaas components ( like vm schedulers or iaas interfaces ) were scarcely available in past simulators .", "thus , this section presents how energy consumption related information is collected and accumulated so they can support future algorithms .", "later , section  [ sec - hls ] discusses the foundations for physical machine schedulers that are expected to be the primary users of the models overviewed in this section .", "first , in order to enable the decoupling , dissect - cf offers _ resource utilisation counters _ both for producers and consumers .", "these counters allow an aggregated and time dependent view of the consumption of particular resources .", "counters are updated depending on the ` power state ` of a resource spreader ( see figure  [ fig - arch ] ) .", "for example , a physical machine  @xmath77  in suspend to ram ( str ) power state zeroes its processing power : @xmath78 the power states of the various entities in the simulation can be defined as needed ; the simulator only expects these states to define the basic power characteristics ( e.g. , minimum and maximum power draw ) and the resource processing behaviour of the given entity at the specific state .", "the simulator also provides a basic set of power states ( on , off , turning on , turning off ) for which the resource processing behaviour is already defined for the resources incorporated into a physical machine .", "based on this low - level power modelling functionality , the decoupling of energy models is accomplished through ` energy meter`s .", "these meters are organised around four functionalities : @xmath0 monitoring of energy consumption directly related to resource utilisation , @xmath1 indirect energy consumption estimation , @xmath2 aggregation of metering results from multiple meters and @xmath79 presenting up - to - date energy readings to their users .", "the remainder of this subsection reveals how these functions accomplish the decoupling and shows the ways customised , infrastructure specific metering can be achieved in dissect - cf .      ]", "[ [ direct - resource - utilisation - related - energy - consumption - metering ] ] direct resource utilisation related energy consumption metering + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    based on the previously mentioned resource utilisation counters , the simulator can be requested to periodically evaluate the instantaneous utilisation percentage . then , energy ` consumption model`s use these percentages ( see figure  [ fig - arch ] ) to estimate the instantaneous power draw of each monitored resource spreader ( later the resulting estimate and the metering period is used to calculate the direct meter s energy consumption estimate ) .", "consumption models are dependent on the actual power state a resource spreader is in , and the simulator developer can define them . as examples ,", "the simulator provides two simple energy consumption model implementations : @xmath0 a linear interpolation between minimum and maximum power draw depending on current resource utilisation , to allow basic modelling of dynamic power behaviour  or @xmath1 a constant minimum power draw , to allow the effortless modelling of _ off _ or _ str _ power states .", "figure  [ fig - meters ] presents direct meters for each resource spreader in the shown physical machines .", "[ [ indirect - energy - consumption - estimation ] ] indirect energy consumption estimation + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to support more complex energy consumption estimates , the simulator also allows energy consumption to be derived from other properties of the simulated system .", "for example , these properties could include the virtual machine request rate of a particular data centre , or the utilisation of the data centre level storage subsystem ( e.g. , to estimate how many disk drives the currently stored data can occupy ) .", "these meters are expected to periodically evaluate the system state and accumulate their energy consumption estimates for those components of the simulated system that are not directly represented with resource spreaders .", "figure  [ fig - meters ] reveals two indirect metering solutions to represent the internal actions of iaas systems and the energy consumption behaviour of data centre level hvac systems .", "[ [ meter - aggregators ] ] meter aggregators + + + + + + + + + + + + + + + + +    in several cases , the energy consumption values from individual direct or indirect meters are not sufficient for higher - level energy aware decision makers ( e.g. , physical machine state schedulers  see section [ sec - schedulers ] ) .", "for example , a physical machine in dissect - cf is represented with multiple resource spreaders ( e.g. , cpu , disk bandwidth ) , thus to have a complete view of a physical machine s energy consumption , one would need to monitor several direct energy meters .", "meter aggregators allow the automated collection and management of several meters in parallel and provide the higher - level view expected by decision makers .", "figure  [ fig - meters ] shows a complex scenario for the use of aggregators .", "this scenario shows that a single aggregated meter can be constructed for a whole data centre , allowing even the inclusion of indirect metering results such as hvac .", "the figure also shows how a physical machine s resource spreader set can be metered as a single entity .", "as the estimation of energy consumption of the simulated entities might be time consuming , the simulator allows simulation developers to fragment and focus their measurements on those parts of the simulated systems that they are interested in .", "for instance , the simulation developers might be only interested in the energy consumption of a single virtual machine that is deployed on a simulated cloud with multiple data centres .", "in such cases , dissect - cf can limit the number of energy meters that are evaluated with two approaches : independent meters or adjusted aggregations .", "[ [ independent - meters ] ] independent meters + + + + + + + + + + + + + + + + + +    this approach entails that metering results are only dependent on the metered component and the rest of the simulated system can not influence them .", "e.g. , the energy consumption reported for a cpu level resource spreader of a physical machine should not be dependent on the behaviour of the rest of the system .    [", "[ adjusted - aggregations ] ] adjusted aggregations + + + + + + + + + + + + + + + + + + + + +    if there is a dependency between two metered components ( e.g. , a virtual machine that is hosted on a particular physical machine ) , then a special meter aggregator can be created .", "this meter aggregator will not just add the aggregated meters measurements .", "instead it allows simulation developers to define an aggregation function .", "dissect - cf uses the adjusted aggregation technique to handle derived energy consumptions such as vm level energy consumption .", "for example , when applying the linear interpolation based energy consumption model , virtual machine level power draw can be calculated using the power draw of the hosting physical machine , as follows : @xmath80 where @xmath81 is the derived instantaneous power draw of a particular vm while it is running .", "the equation s first part estimates the variable part of the power draw , while the second part provides an estimate for the idle part . the variable part is dependent on @xmath82 , which is the maximum variability of the physical machine s power draw .", "the variable part is proportional to @xmath82 , depending on the resource utilisation of the particular vm compared to the resource utilisation of all vms hosted on the same physical machine .", "the second part of the estimate is the idle part that is derived from the idle power draw of the physical machine  @xmath83 .", "this part is proportional to the number of vms hosted by the physical machine at the given time instance ( this number is one less than the cardinality of the vm s influence group because the group contains also the resource spreader of its hosting physical machine ) .", "[ eq - powermodel ] reveals that the energy consumption model ( that estimates the instantaneous power draw ) of the virtual machine is not independent from the physical machine s behaviour .", "therefore , energy consumption can not be directly accounted to virtual machines .", "when such meters are requested , the simulator identifies them as dependent meters and instead of creating independent meters , it creates an adjusted meter aggregator including those meters that could provide the necessary information to calculate the energy consumption to be attributed to the originally requested meter . it must be noted that dependent meters consider energy consumptions multiple times ( e.g. , energy consumption is accounted to both physical and virtual machines ) .", "thus when meter aggregations are created they must only include meters that are not dependent on each other .      to allow the simplified development of new vm placement algorithms and pm state schedulers , dissect - cf provides an implementation of relevant infrastructure components in iaas systems .", "these components are built on top of the previously discussed resource sharing and energy modelling techniques and provide abstractions for networked entities and for physical / virtual machines ( see figure  [ fig - arch ] ) .", "network activities rarely play a role in scheduling decisions related to tasks or to physical / virtual machines .", "thus , to increase the performance of simulations , by default , dissect - cf offers a limited network model where two networked entities must be always directly connected ( therefore connection properties like bandwidth must be defined between all networked entities that should be able to communicate with each other in the simulation ) .", "this rudimentary behaviour could be sufficient even for some network aware schedulers , but to allow better representation of real networks , the simulator also allows the creation of intermediary network entities ( such as routers ) . the implementation of such entities should alter the processing limit ( @xmath84 ) of all resource consumptions that are directed through them .", "directly connected networked entities ( @xmath85 , where @xmath86 is the set of all possible networking entities ) are simulated with the ` networknode ` component .", "this component encapsulates an incoming and an outgoing network connection simulated with the unified resource sharing foundation ; thus connections are implemented as resource spreaders : @xmath87 , where @xmath88 .", "the processing power of these spreaders represents the network bandwidth ( either incoming or outgoing ) of the given network node .", "when a new network communication must take place , simulation developers are expected to request a resource consumption between the source network node s outgoing resource spreader and the target s incoming resource spreader .", "the component also introduces network latencies ( @xmath89 ) that can be defined between every networked entity for any given time instance .", "the latency values resulting from this function are used as delays preceding the registration of each resource consumption to the incoming or outgoing resource spreader of the node .", "for example , let us see what happens if a network communication ( represented as a resource consumption @xmath90 ) needs to be registered between a source and a target networked entity ( @xmath91 ) at the time instance @xmath92 : @xmath93 where @xmath94 is the network latency between the source and target nodes at the time of registration ( @xmath95 ) , and @xmath96 is the nonperforming spreader that never processes any resource consumption : @xmath97", ". thus the equation shows , that the consumption is registered to the nonperforming spreader for the complete period of the latency , afterwards the simulator switches the consumption s registration to the originally designated spreaders ( which are the network output port @xmath98 of the source node @xmath99 and the input port @xmath100 of the target node @xmath101 ) .", "this last step allows the simulator to utilise its unified resource sharing mechanism after the latency period is over .      in iaas systems ,", "physical machines offer most of the user exploited simulated resources .", "thus , dissect - cf ` physical ` ` machine`s encapsulate a diverse set of resources : local disks ( via repositories ", "see section  [ sec - ui ] ) , network interfaces ( with the help of network nodes  see section  [ sec - nw ] ) , cpus ( using the unified resource sharing model of section  [ sec - ursm ] ) and memory .", "besides the modelling of these resources , dissect - cf s physical machine behaviour also focuses on two additional functionalities : administering resource allocations and vm requests ; and modelling physical machine level power behaviour . as resource sharing and modelling", "has already been discussed in detail , this subsection mainly discusses the latter functionalities .", "[ [ resource - allocator - and - vm - request - handler ] ] resource allocator and vm request handler + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in order to maintain up - to - date information on the available and utilised resources of the physical machine , dissect - cf applies a resource allocator .", "the applied allocator can reserve resources ( e.g. , a given amount of memory or number of cpu cores ) from the physical machine .", "these reservations are represented as ` resource ` ` allocation ` instances that are used to maintain the free resource set of the physical machine .", "allocation instances are also passed as a token of resource availability towards virtual machine schedulers .", "other than the amount of resources associated with them , resource allocations also have the following properties : expiry time if unused or a link to the vm that uses it . when a resource allocation is created , the physical machine automatically initiates a ` deferredevent ` , which automatically cancels the allocation after the expiry time . to avoid this mechanism , the entity that requested", "the allocation must request a vm to use the reserved resources ( i.e. , establishing the link between the vm and the allocation ) .", "the automatic cancellation of the resource allocation is a self - defence mechanism of physical machines to avoid keeping resources out of use just because they received an unused allocation .    for a single vm request ,", "dissect - cf allows multiple ` resource ` ` allocation`s to be made across multiple physical machines .", "the multi - allocation technique can be used by schedulers to optimise for non - functional properties ( e.g. , past availability , expected energy consumption , environmental impact ) of those resources that a vm could bind to at a given moment . after a decision", "is made about the use of a particular allocation for the vm request , the rest of the allocations ( which were non optimal according to the non - functional requirements ) are expected to be cancelled by the schedulers .", "for complex vm instantiation scenarios , schedulers are also allowed to adjust the expiration time upon allocation request . with this mechanism", ", researchers can evaluate advanced reservation - like scenarios regarding vm instances .", "[ [ power - behaviour ] ] power behaviour + + + + + + + + + + + + + + +    as dissect - cf aims at supporting the development of energy aware scheduling strategies in iaas clouds , the energy model of the physical machine is particularly important . in its default configuration ,", "the simulator supports 4 power states ( and the transitions between them ) : off , switching on , running and switching off . although this power state set is fairly limited , the simulator already offers constructs that allow the modelling of more complex operations like : suspend to ram , suspend to disk , dynamic voltage & frequency scaling or core / cpu de- and reactivation .", "suspension related states can be modelled with the introduction of new power states , while the latter two are available because resource spreaders can alter their maximum processing capabilities .", "the modelling of the 4 supported states was done after real - life physical machine behaviour in the clouds of the university of innsbruck and mta sztaki .", "the real - life behaviour of the machines was observed through constantly monitoring their instantaneous power draw while they went through the following cycle : idling @xmath102 shutdown @xmath102 off @xmath102 switch on @xmath102 idling @xmath102 full cpu load .", "a sample measurement with a typical cloud node at innsbruck ( with 80 cpu cores , 128 gb ssd , 132 gb ram , and redundant power supplies ) can be seen in figure  [ fig - pmbehaviour ] . as physical machine state schedulers can be highly influenced by the behaviour in non - running states ( e.g. , their decision on shutting off a machine could be dependent on the time it takes to boot the machine back and the expected power savings because of the completely off machine ) , dissect - cf offers a simplified and a more complex behaviour model .    .simplified power state definition of a physical machine[tab - simpli ] [ cols=\"<,<,<,<,<\",options=\"header \" , ]     during the performance evaluation , all parallel tasks of the trace started up in the first 10 seconds ( _ task spread _ ) and had a length variety between 10 - 90 seconds .", "the necessary amount of tasks for the 10-second - long simulation runtime is shown in table  [ tab - taskdistrs ] .", "each evaluation was ran with different parallel task numbers between ( 1 - 100,000 ) to allow the investigation on how the increasing parallelism changed the performance of the simulators .", "the findings are shown in figure  [ fig - pjruntime ] .    as the figure shows , in the case of cloudsim , both its time - shared and space - shared vm scheduler was experimented on .", "the time - shared scheduler of cloudsim simulates parallelism , while the space - shared scheduler serialises the arrived tasks so at any given time there is only one task that can utilise the cpu , and the rest are queued until this task is finished .", "unfortunately , the time - shared scheduling mechanism of cloudsim is broken ; it lengthens task execution times significantly even in the case of a two - parallel - task setup ( e.g. , a simple setup like the one presented in figure  [ fig - incpartask ] leads to completely different results than one would see in real - life ) on the other hand , the space - shared scheduler provides completely different task completion times ( because of its serialising behaviour ) .", "thus the cloudsim related details of the figure are only valid when considering their simulation runtime ; the simulated tasks do not finish at the expected times in either case .    as depicted in the figure , cloudsim - time - shared and groudsim measurements abruptly finish at 1,000 and 50,000 tasks .", "for higher levels of parallelism , the runtime of the simulation took more than 8 hours and therefore was cancelled . in the case of cloudsim , the performance penalties mainly originate from its centralised design of data centres ( i.e. , most of the logic and event processing reside in the ` datacenter ` class , and the rest of cloudsim s classes are used for state representation only ) .", "groudsim , on the other hand , has a different design issue : it pre calculates all task completion times , puts them into the event queue and thus if a change is needed to them , the whole event queue has to be updated .", "finally , for all simulators , the task processing performance shown in the figure is expected to be faster than what one usually would see , because of the large amount of tasks that were executed for gathering data for the figure .", "the amount of repeated evaluations allowed the jvm to optimise the runtime behaviour of each simulator for the particular performance evaluation scenario . later on this effect", "is excluded from the evaluations .", "[ [ performance - influence - of - load - characteristics ] ] performance influence of load characteristics + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the rest of this sub - section analyses of how the performance of the resource sharing varies depending on changes in task arrival and length characteristics . for this analysis ,", "a baseline measurement was collected with no parallel tasks but varying task lengths . than the assumption", "was made that from this baseline , the simulators would linearly degrade in performance ( i.e. , two times the parallelism would increase the simulation run time of a single task by two times ) . to compare how linearly a particular simulator behaves under particular load characteristics ,", "the following scaling ratio function was designed : @xmath103 where @xmath104 is the scaling ratio , @xmath105 is the kind of simulator , @xmath106 is the chosen range of task length variety ( in the below detailed experiments it was either 10 - 90 s or 200 - 3600 s ) , @xmath107 is the number of parallel tasks , @xmath108 is the task spread ( in particular , 10 s or 200 s were used ) , and @xmath109 is the measurement function that evaluates the particular simulator with the given load characteristics .", "figure  [ fig - jobscalingcompare ] compares dissect - cf with the two selected simulators via the scaling ratio function .", "the labels of the figure are presented in the following format : @xmath110 ( see eq .", "[ eq - scratio ] ) , where @xmath105 is one of ds / gs / cs , which translate to dissect - cf / groudsim / cloudsim , respectively .", "the x - axis of the figure shows the increase in @xmath107 .", "based on the distribution of the measured scaling values for the different load characteristics , the most tolerant to the change in workload is groudsim , while the least tolerant is cloudsim .", "degrading performance in scaling can be also observed for both cloudsim and groudsim . in the case of cloudsim ,", "the degradation starts around 10 parallel tasks , and by the time the simulation reaches 50 parallel tasks the simulator becomes worse than linear ( @xmath111 ) . in the case of groudsim ,", "the degradation starts around 100 parallel tasks , and becomes worse than linear around 20,000 parallel tasks . in both cases", "the early degradation is caused by the data structures and the indirect call structures used ( i.e. often these simulators experience rather deep call stacks during parallel event handling ) .", "finally , although it is not visible in this chart , the degradation starts for dissect - cf at around 10,000 parallel tasks , and based on estimates , its scaling becomes worse than linear around 1.5 million parallel tasks .", "fortunately , this huge amount of parallelism is unlikely for most simulations . but simulations of some highly under - provisioned systems might need levels of parallelism that could lead to degraded performance even in groudsim .", "the figure also reveals that the performance of dissect - cf is more dependent on the task spread , while cloudsim s scaling is limited more by the task length variety . in case of dissect - cf", ", the task spread dependency manifests because the simulator executes its resource sharing mechanism only once per @xmath12 , on new resource consumption arrivals and departures ( see figure  [ fig - rs ] for details ) .", "however , the increased task spread decreases the likelihood that the resource sharing mechanism can be executed on multiple resource consumptions at once .", "therefore , the wider the spread the closer one gets to the worst possible resource sharing performance in dissect - cf .", "in fact , the 200 s long spread was used because this spread is already high enough to reveal close - to - worst - case performance . increasing", "the spread further did not introduce significant resource sharing performance drops , but its impact on cloudsim based simulations rendered the evaluation of the 1,000 parallel task experiment too time consuming .", "as synthetic loads often criticised because of their possible bias , dissect - cf was evaluated and compared using workloads that were collected from real - life computing infrastructures .", "thus , the evaluation required workload traces with the following characteristics : @xmath0 collected for extensive periods of time ( i.e. , at least a few months long ) to ensure the widest variety of observable load characteristics for the particular infrastructure ; @xmath1 tasks should be described in detail including their resource utilisation and submission , start and completion times ( so even if just task definitions are available , one can still translate them to the kinds of virtual machines the tasks would need for their execution in a cloud environment ) ; and @xmath2 if the traces contain virtual machine management logs , then task allocation details are also necessary to enable the analysis of new scheduling techniques that might aim at reallocating tasks or that would change vm management operations .", "based on these requirements , most of the traces ( e.g. , planetlab ) containing virtual machine management logs were not found suitable for the planned comparative study ; the rest of these log based traces are not collected for enough time to be used in large scale experiments .", "thus , although these virtual machine management log based traces would be the best candidates for analysing cloud characteristics , their immaturity necessitates to also search amongst traces collected from other large - scale infrastructures like grids .", "two appropriate sources were identified : the grid workloads archive ( gwa @xcite ) and the parallel workloads archive .", "because both at the university of innsbruck and in mta sztaki , there are earlier good experiences with the processing of gwa traces , this article presents a comparative study of the three selected simulators through the traces downloadable from gwa ( namely : das2 , grid5000 , nordugrid , auvergrid , sharcnet , lcg ) .", "[ [ trace - processing ] ] trace processing + + + + + + + + + + + + + + + +    a trace loader was prepared for all three simulators so events were fired every time a task arrives .", "every time a new task arrival event is fired , the simulators are programmed to create a virtual machine that will host the task .", "once the vm was created the task was instantiated in it according to its definition in the trace file . when the task was completed according to the particular simulator , its hosting vm", "was also terminated .", "because of the known problems with the time - sharing mechanism in cloudsim , single vms did not receive parallel tasks ( e.g. , by requesting a vm that is sized to host multiple tasks first ) . instead ,", "when parallel tasks were needed , multiple vms were created in the simulated cloud infrastructure .", "unfortunately , because of a few conceptual differences between dissect - cf and the other analysed simulators , the above - mentioned trace processing technique has several minor differences in their adaptations to the various simulators .", "first of all , dissect - cf has a queuing first - fit vm scheduler ( see section  [ sec - hls ] ) that allows users to send vm requests right upon task arrival and wait for the vm scheduler s queuing mechanism to notify them about the vm s running state ( and readiness to receive the task ) .", "unfortunately , the rest of the simulators do not offer vm request queuing : they reject vm requests that can not be hosted according to the actual state of the simulated infrastructure . to have a similar behaviour to dissect - cf , either the vm request needs to be resent until it can be fulfilled ( which would introduce an unwanted busy waiting loop ) , or", "alternatively one must rely on user - side information .", "if the vm request is repeatedly resent , then the other simulators are significantly disadvantaged .", "thus , the second approach was used : previously non - servable vms were only re - requested once one of the previous vms have terminated ( i.e. , it was assumed that one can determine the state of all the running vms at a given time instance ) . with this technique", "the cpu share of the performance evaluation code ( as measured by java s embedded cpu sampler ) was kept around the same levels as it was for dissect - cf .", "tasks , which are utilising several cpu cores in parallel , are modelled in two ways .", "first , the simulation tries to request a vm with as many cpu cores as the task needs .", "unfortunately , this can not be achieved in cases when even the biggest possible vm is too small for the task s needs . for these huge tasks ,", "the simulation requests multiple vms , enough to fulfil the parallelism in the task . in groudsim and", "dissect - cf it is possible to request multiple vm instances at once ( e.g. , similarly to how amazon ec2 behaves ) and they will be ensured to be available in parallel . on the other hand , cloudsim does not have such functionality ; therefore it was extended with a technique that only submits the tasks to their virtual machines once all necessary vms are available for the level of parallelism needed for the task .    as groudsim is more focused on the user - side behaviour of cloud infrastructures", ", it has several deficiencies compared to the other two evaluated simulators : @xmath0 it can only handle tasks that occupy a single cpu core , @xmath1 its network sharing mechanism could result in network under utilisation if the communicating parties are using connections with different bandwidths and @xmath2 it does not provide data centre level simulation details ", "e.g. , no vm / pm scheduling and pm level resource sharing is simulated . to overcome the first deficiency ,", "multi - core tasks are simulated as several single core tasks in the same groudsim virtual machine . to avoid the problem with network sharing , the simulated data centres in all three simulators", "were constructed on such a way that every node was connected with the same bandwidth to the others .", "unfortunately , without significantly changing groudsim s code it was not possible to add the missing data centre level simulation details .", "thus , during the performance analysis one should keep in mind that these details are not simulated in groudsim .", "finally , for both groudsim and cloudsim , the instantiation time of a virtual machine is instantaneous , which is not realistic . as the transfer of the vm image is often the most time consuming operation in the vm instantiation procedure  @xcite ,", "this transfer was simulated with a download operation to the vm with both of the simulators . in groudsim", ", a network transfer was initiated right after the vm was created and this transfer delayed the creation of the task on the vm until the transfer s completion . in cloudsim , cloudlets ( computing tasks in cloudsim terminology ) could have input files defined for them .", "thus , tasks in cloudsim were specified so they must transfer an input file with the size of the vm image before they can start their processing .", "unfortunately , even with input files specified , the vms in cloudsim start immediately and execute their tasks right after the vm is created .", "this means that cloudsim based simulations can not be as accurate as the other two simulator s results . in order to reduce the impact of vm image transfer on task execution times  and to allow all three simulations to have a similar simulation completion time  , the size of the vm image was set to 100 mb ( which is the size of a rather small image nowadays ) .    [ [ the - simulated - virtual - infrastructure ] ] the simulated virtual infrastructure + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in all three simulators , a single kind of physical machine acted as the foundation of the simulated infrastructure .", "this physical machine was modelled after a single node in mta sztaki s cloud and had the following properties : @xmath0 64 cpu cores , @xmath1 256 gb ram , @xmath2 5 tb local disk , and @xmath79 two 1 gb / s ethernet connection .", "in the experiments detailed below , it was possible to define how many of these machines should be in the data centre .", "the set up of the machines includes the creation of a network interconnecting all of them with a central switch .", "[ [ the - runtime - comparison - experiment ] ] the runtime comparison experiment + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for the first experiment the simulated infrastructure was set up so it was sufficient to host even the largest parallelism a single task of the traces could request ( i.e. , no task has had to be dropped because there were not enough physical machines to host its level of parallelism ) .", "in particular , this experiment required the simulation of 20 physical machines with the above - mentioned properties . on this infrastructure , the first @xmath112 tasks from a particular trace", "were submitted .", "then , a measurement was initiated for the real time passed between the submission of the first task and the completion of the last one .", "the infrastructure preparation , task submission and time measurement operations were repeated 10 times , each time starting with a new jvm . then , the trace specific average simulation duration for the measurement of @xmath113 tasks was calculated .", "afterwards , the whole measurement procedure was repeated for the remaining traces .", "finally , the _ aggregated simulation duration _ was calculated for @xmath113 tasks by averaging the trace specific averages from all traces .", "later , the aggregated simulation duration is referred as @xmath114 ( where @xmath105 is the measured simulator , @xmath107 is the number of tasks , @xmath115 is the number of physical machines to simulate and @xmath116 ) .", "figure  [ fig - runtimecompare ] presents these aggregated simulation durations while the number of tasks ", "@xmath107  were changed from a hundred to a million . as the measurements ran with a cold jvm , for the first 10,000 tasks one can see that the jvm s effects on the start - up dominate the runtimes .", "the first workload related differences of the simulators can be seen after the jvm s behaviour is no longer a significant contributor to the measurement .", "also , despite dissect - cf s focus on the internals of the infrastructure , its performance is always better than that of the other two simulators .", "this is especially apparent after around 200,000 tasks . in spite of groudsim doing the lightest - weight simulation", "( see the deficiencies listed in the previous paragraphs ) , it is still significantly and consistently slower ( albeit in some cases  like the lcg trace  groudsim performs consistently better ) .", "the figure also shows that groudsim s network simulation code gives around 21% overhead for lower task counts , and for higher task counts the overhead could grow as high as 109% .", "this difference experienced during the network simulation is caused by groudsim s sub - par network modelling ( i.e. , for a single transfer to occur there are several events that needs to be handled in the event system , and in case of parallel transfers these events must be refreshed in the future event queue ) .", "[ [ the - scalability - experiment ] ] the scalability experiment + + + + + + + + + + + + + + + + + + + + + + + + + +    to determine the scalability of the compared simulators , the previous experiment was repeated with varying sizes of infrastructure under them .", "thus , these experiments will not only be able to pinpoint how the various simulators deal with the increasing amounts of tasks and virtual machines , but also how the simulators cope with increasing size of infrastructures .", "infrastructures as small as a single physical machine were used but the experiments were reaching to the size of 500 machines . as even smaller infrastructures were evaluated than 20 machines ( which was the minimum for the level of parallelism found in some of the traces ) , there were tasks that could not fit to the infrastructures .", "these tasks were automatically filtered out by the trace processor and never reached the simulation .", "interestingly , even with the simulation with a single machine the filtered out tasks were never more than 6% of the total number of tasks in any of the traces .", "of course , the single machine still caused a significant serialising effect that lengthened the total execution time of the rest of the tasks .", "similarly to eq .", "[ eq - scratio ] , the scalability of the simulators was calculated compared to linear scaling .", "figure  [ fig - scalingcompare ] presents how the various simulators scale under a particular number of physical machines .", "the figure is composed of twelve sub - charts , each delimited with a black line . between the black lines", "a large bidirectional arrow contains the title of a sub - chart .", "the title shows the number of tasks for which the final runtime measurement values were compared .", "for example : the title `` _ _ 100 - 200 tasks _ _ '' means the sub - chart shows how the final runtime measurement value of 200 tasks compares to the value of 100 tasks .", "the comparison is made according to the following equation : @xmath117 where @xmath118 , @xmath119 is the number of tasks for which the aggregated simulation duration is evaluated , @xmath105 is one of the evaluated simulators and @xmath115 is the physical machine count .", "thus , @xmath120 will become one if the particular simulator scales linearly in the task number range of @xmath118 and @xmath119 .", "if the scaling function above is lower than one then the particular simulator is worse than linear .", "according to the sub - charts in figure  [ fig - scalingcompare ] all simulators scale significantly better than linear with fewer than 10,000 tasks ( again , this is mostly due to the jvm s class loading and start - up mechanism , since practically all measurements under this task count result in sub - second runtimes ) .", "afterwards , one can see that cloudsim is already scaling significantly worse than the other two investigated simulators , and by 50,000 tasks cloudsim already drops below linear scaling .", "it can be also observed that simulators handle the number of parallel machines completely differently . in the case of cloudsim , the bigger the machine number , the more likely that the scaling factor will become lower . in the case of groudsim ,", "the opposite behaviour is observed ; while for dissect - cf , one can see a rather balanced case . in conclusion , groudsim behaves as expected : the missing simulation of physical machine behaviour and vm / pm scheduling techniques allows it to behave practically independently from infrastructure size . in this sense ,", "cloudsim and dissect - cf should behave more closely to each other .", "unfortunately , the problematic implementation of cloudsim s time - shared vm scheduler ( see section  [ sec - syntheval ] ) changes its apparent behaviour and reduces the similarities in the trends between dissect - cf and cloudsim . to conclude , dissect - cf scales comparably to other state - of - the - art simulators ( in fact it never drops below linear scaling , in contrast to the others ) while it offers significantly more detailed infrastructure level behaviour .    this experiment was also used to _ validate the results of dissect - cf _ through a comparison with the other simulators . as the first step of validation ,", "the simulator reported completion time of the last task in every trace specific measurement was collected ( from @xmath114  where @xmath107 was set between one and a million , and @xmath115 between one and five hundred ) .", "then , these task completion times were compared with each other .", "the median of the difference between the simulators was less than 0.001% ( the average difference from the median was 2.15% ) .", "the biggest differences occurred with simulated infrastructures containing the lowest number of physical machines : in such cases the median of difference between the simulators jumped to a little more than 0.29% ( with a sample standard deviation of 7.21% ) .", "the reason behind such a great deviation is the magnified effect of virtual machine instantiation simulation ( i.e. , as vms can only be instantiated on a single pm , they are mostly created and destroyed in a serial fashion ) .", "[ [ the - impact - of - energy - metering ] ] the impact of energy metering + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    so far , all presented measurements were executed without energy meters attached to the resource spreaders of dissect - cf . as energy metering is not available in groudsim", ", the other two simulators would have suffered a disadvantage because of their ongoing metering simulation .", "as dissect - cf was consequently a better performer than the other two simulators , its further evaluation was focused on how one could set up energy metering in order to get comparable performance to the other simulators meter - less operation .", "figure  [ fig - energyperformancedrop ] analyses the performance drop of dissect - cf observed when energy metering was requested for the complete simulated infrastructure with 20 physical machines . just as before ,", "the function @xmath121 was evaluated , but now instead of comparing dissect - cf to other simulators , the comparison was made between the newly received values to values from the previously presented simulations without energy metering . adding energy metering leads to significant performance differences between the various gwa traces . to show what is the possible performance degradation range in the various traces , the two extreme cases ( the best and worst performing ones ) are revealed in the figure .", "the presented cases show that selecting the energy measurement interval is crucial for well performing simulations . in general it", "is not recommended to use energy measurement intervals below one minute for long running simulations ", "i.e. , where the trace s length is over a few thousand tasks .", "if higher precision is needed , then selective metering techniques should be used that evaluate energy meters for only the necessary parts of the infrastructure ; otherwise , one could experience a slowdown of over 50 times compared to meter - less runtimes .", "finally , in figure  [ fig - energyequivalence ] , a comparison is shown about the performance drop caused by the metering to the meter - less setups of other simulators .", "the figure shows what the metering interval is  despite the resulting performance drop  that still provides equivalent performance to other simulators .", "as the figure shows , the performance drop also depends on the number of physical machines in the simulated infrastructure .", "overall , if energy metering is desired on the entire simulated infrastructure , the metering interval must be set to at least five - seconds to receive comparable performance to cloudsim .", "in contrast , groudsim s performance can be matched with a metering interval around 25 - 30 seconds .", "anything above these values will give a performance advantage to dissect - cf .", "on the other hand , metering intervals below these values could still cause a better performing dissect - cf but this is highly dependent on the particular trace or virtual machine request pattern used during simulations .", "this article outlined several iaas related schedulers that could be further improved with the use of cloud simulators .", "then , it has shown that current simulators barely meet the demands of the scheduling oriented researchers : @xmath0 they often limit the accessibility of information , @xmath1 they often hide the internal details of iaas systems , @xmath2 they frequently perform poorly in large - scale simulations , and @xmath79 they provide scarcely any options for introducing new energy management techniques inside iaas clouds . to overcome these issues ,", "the article has proposed a new simulator called dissect - cf .", "the proposed simulator targets information accessibility issues with open apis and monitoring and performance related customisable events .", "the internal details of the cloud systems are also accessible , allowing simulation developers : @xmath0 to construct clouds in novel ways ( e.g. , introduction of new physical machine - virtual machine interaction techniques or cloud organisation topologies ) , and @xmath1 to experiment with new cloud side behaviour ( e.g. , new vm schedulers , power states ) .", "dissect - cf utilises a new unified resource sharing mechanism that allows centralised performance optimisations and ensures scaling independently of the size of the infrastructure and the amount of tasks processed by the simulated system .", "finally , the new simulator deeply integrates energy metering techniques ( ranging from resource usage counters and energy consumption models to meter aggregators ) .", "these techniques not only allow further extensions but they allow selective and composite power metering to ensure minimal performance drops during the metering sessions .", "the new simulator was evaluated by comparing it to small - scale but real - life environments . during this evaluation", ", it was presented how one should model cpu and memory intensive tasks , and it was also shown that the accuracy of the simulator s energy metering technique is also dependent on the new unified resource sharing mechanism .", "experiments shown that the relative error of the new resource sharing technique is around 1% in most cases . after the small - scale evaluation , dissect - cf was compared with two state - of - the - art simulators ( namely cloudsim and groudsim ) .", "the comparisons were focused on the scaling and performance characteristics of the simulators .", "the results revealed that these simulators often produce inaccurate results , while the new simulator not only provided more accurate outputs but also offered better performance .", "future research will consider several areas .", "first , there are plans to look how the simulator s resource sharing accuracy could be improved with stochastic sharing models and new low - level scheduling techniques .", "next , support for more complex network topologies and additional node types ( e.g. , routers and switches ) will also be prepared . as the current simulator merely provides raw data for user - side schedulers ,", "an investigation of the necessary constructs and optimisations to better support the development and analysis of these schedulers also needs to be done .", "finally , memory behaviour is practically neglected in the current simulator , because applications and openly available traces do not offer any details on this . in the future", ", a task level complex memory model is expected to be delivered that not only considers memory bandwidth utilisation but also access patterns . with this model", "the simulator s target could include accurate live migration support .", "this article described the behaviour and features of dissect - cf version 0.9.5 .", "the source code of the simulator is open and available ( under the licensing terms of the gnu lesser general public license 3 ) at the following website :    https://github.com/kecskemeti/dissect-cf .", "this work was partially supported by european union s horizon 2020 research and innovation programme under grant agreement no 644179 ( entice ) as well as by the cost program action ic1305 : network for sustainable ultrascale computing ( nesus ) , finally the research has also received partial funding from the austrian science fund project trp 237-n23 .", "a.  ahmed and a.s .", "cloud computing simulators : a detailed survey and future direction . in _", "advance computing conference ( iacc ) , 2014 ieee international _ , pages 866872 .", "ieee , february 2014 .", "doi:10.1109/iadcc.2014.6779436 .", "w.  zhao , y.  peng , f.  xie , z.  dai , modeling and simulation of cloud computing : a review , in : ieee asia pacific cloud computing congress ( apcloudcc ) , ieee , shenzhen , 2012 , pp . 2024 .", "doi:10.1109/apcloudcc.2012.6486505 .", "t.  hirofuchi , a.  lbre , adding virtual machine abstractions into simgrid : a first step toward the simulation of infrastructure - as - a - service concerns , in : third international conference on cloud and green computing ( cgc ) , ieee , karlsruhe , germany , 2013 , pp . 175180 .", "doi:10.1109/cgc.2013.33 .", "a.  nez , j.  l. vzquez - poletti , a.  c. caminero , j.  carretero , i.  m. llorente , design of a new cloud computing simulation platform , in : computational science and its applications - iccsa 2011 , vol .", "6784 of lecture notes in computer science , springer , santander , spain , 2011 , pp . 582593 .", "doi:10.1007/978 - 3 - 642 - 21931 - 3_45 .", "k.  kurowski , a.  oleksiak , w.  pikatek , t.  piontek , a.  przybyszewski , j.  wkeglarz , dcworms  a tool for simulation of energy efficiency in distributed computing infrastructures , simulation modelling practice and theory 39 ( 2013 ) 135151 .", "doi:10.1016/j.simpat.2013.08.007 .", "d.  kliazovich , p.  bouvry , y.  audzevich , s.  khan , greencloud : a packet - level simulator of energy - aware cloud computing data centers , in : global telecommunications conference ( globecom 2010 ) , 2010 , pp . 15 .", "doi:10.1109/glocom.2010.5683561 .", "r.  n. calheiros , r.  ranjan , a.  beloglazov , c.  a. de  rose , r.  buyya , cloudsim : a toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms , software : practice and experience 41  ( 1 ) ( 2011 ) 2350 .", "doi:10.1002/spe.995 .", "s.  ostermann , k.  plankensteiner , r.  prodan , t.  fahringer , groudsim : an event - based simulation framework for computational grids and clouds , in : euro - par 2010 parallel processing workshops , vol .", "6586 of lecture notes in computer science , springer , 2011 , pp . 305313 .", "doi:10.1007/978 - 3 - 642 - 21878 - 1_38 .", "velho , l.  m. schnorr , h.  casanova , a.  legrand , on the validity of flow - level tcp network models for grid and cloud simulations , acm transactions on modeling and computer simulation ( tomacs ) 23  ( 4 ) ( 2013 ) 23 .", "doi:10.1145/2517448 .", "p.  velho , a.  legrand , accuracy study and improvement of network simulation in the simgrid framework , in : proceedings of the 2nd international conference on simulation tools and techniques , icst ( institute for computer sciences , social - informatics and telecommunications engineering ) , 2009 , p.  13 .", "doi:10.4108/icst.simutools2009.5592 .", "r.  sobie , a.  agarwal , i.  gable , c.  leavett - brown , m.  paterson , r.  taylor , a.  charbonneau , r.  impey , w.  podiama , htc scientific computing in a distributed cloud environment , in : proceedings of the 4th acm workshop on scientific cloud computing , science cloud 13 , acm , new york , ny , usa , 2013 , pp .", "doi:10.1145/2465848.2465850 .", "h.  song , j.  li , x.  liu , idlecached : an idle resource cached dynamic scheduling algorithm in cloud computing , in : 9th international conference on ubiquitous intelligence computing and 9th international conference on autonomic trusted computing ( uic / atc ) , 2012 , pp . 912917 .", "doi:10.1109/uic - atc.2012.24 .", "m.  rodrguez , d.  tapiador , j.  fontn , e.  huedo , r.  montero , i.  llorente , dynamic provisioning of virtual clusters for grid computing , in : e.  csar , m.  alexander , a.  streit , j.  trff , c.  crin , a.  knpfer , d.  kranzlmller , s.  jha ( eds . ) , euro - par 2008 workshops - parallel processing , vol .", "5415 of lecture notes in computer science , springer berlin heidelberg , 2009 , pp .", "doi:10.1007/978 - 3 - 642 - 00955 - 6_4 .", "b.  sotomayor , k.  keahey , i.  foster , combining batch execution and leasing using virtual machines , in : proceedings of the 17th international symposium on high performance distributed computing , hpdc 08 , acm , new york , ny , usa , 2008 , pp .", "doi:10.1145/1383422.1383434 .", "j.  tordsson , r.  s. montero , r.  moreno - vozmediano , i.  m. llorente , cloud brokering mechanisms for optimized placement of virtual machines across multiple providers , future generation computer systems 28  ( 2 ) ( 2012 ) 358  367 .", "doi:10.1016/j.future.2011.07.003 .", "e.  elmroth , l.  larsson , interfaces for placement , migration , and monitoring of virtual machines in federated clouds , in : eighth international conference on grid and cooperative computing .", "gcc 09 . , 2009 ,", "doi:10.1109/gcc.2009.36 .", "k.  tsakalozos , m.  roussopoulos , a.  delis , vm placement in non - homogeneous iaas - clouds , in : g.  kappel , z.  maamar , h.  motahari - nezhad ( eds . ) , service - oriented computing , vol .", "7084 of lecture notes in computer science , springer berlin heidelberg , 2011 , pp . 172187 .", "doi:10.1007/978 - 3 - 642 - 25535 - 9_12 .", "d.  jayasinghe , c.  pu , t.  eilam , m.  steinder , i.  whally , e.  snible , improving performance and availability of services hosted on iaas clouds with structural constraint - aware virtual machine placement , in : ieee international conference on services computing ( scc ) , 2011 , pp .", "doi:10.1109/scc.2011.28 .", "r.  ghosh , v.  naik , k.  trivedi , power - performance trade - offs in iaas cloud : a scalable analytic approach , in : ieee / ifip 41st international conference on dependable systems and networks workshops ( dsn - w ) , 2011 , pp .", "doi:10.1109/dsnw.2011.5958802 .", "e.  feller , c.  rohr , d.  margery , c.  morin , energy management in iaas clouds : a holistic approach , in : ieee 5th international conference on cloud computing ( cloud ) , 2012 , pp .", "doi:10.1109/cloud.2012.50 .", "lizhe wang , samee  u. khan , dan chen , joanna kolodziej , rajiv ranjan , cheng zhong xu , and albert zomaya .", "energy - aware parallel task scheduling in a cluster .", ", 29(7):1661  1670 , september 2013 .", "including special sections : cyber - enabled distributed computing for ubiquitous cloud and network services & amp ; cloud computing and scientific applications  big data , scalable analytics , and beyond .", "doi:10.1016/j.future.2013.02.010 .", "d.  ongaro , a.  l. cox , s.  rixner , scheduling i / o in virtual machine monitors , in : proceedings of the fourth acm sigplan / sigops international conference on virtual execution environments , vee 08 , acm , new york , ny , usa , 2008 , pp .", "doi:10.1145/1346256.1346258 .", "g.  von laszewski , l.  wang , a.  younge , x.  he , power - aware scheduling of virtual machines in dvfs - enabled clusters , in : ieee international conference on cluster computing and workshops .", "cluster 09 .", ", 2009 , pp .", "doi:10.1109/clustr.2009.5289182 .", "j.  yang , x.  zhou , m.  chrobak , y.  zhang , l.  jin , dynamic thermal management through task scheduling , in : ieee international symposium on performance analysis of systems and software .", "ispass 2008 . , 2008 , pp .", "doi:10.1109/ispass.2008.4510751 .", "r.  buyya , r.  ranjan , r.  n. calheiros , modeling and simulation of scalable cloud computing environments and the cloudsim toolkit : challenges and opportunities , in : international conference on high performance computing & simulation ( hpcs09 ) . ,", "ieee , leipzig , 2009 , pp .", "doi:10.1109/hpcsim.2009.5192685 .", "r.  buyya , m.  murshed , gridsim : a toolkit for the modeling and simulation of distributed resource management and scheduling for grid computing , concurrency and computation : practice and experience 14  ( 13 - 15 ) ( 2002 ) 11751220 .", "doi:10.1002/cpe.710 .", "s.  k. garg , r.  buyya , networkcloudsim : modelling parallel applications in cloud simulations , in : fourth ieee international conference on utility and cloud computing ( ucc ) , ieee , victoria , nsw , 2011 , pp .", "doi:10.1109/ucc.2011.24 .", "a.  beloglazov , r.  buyya , optimal online deterministic algorithms and adaptive heuristics for energy and performance efficient dynamic consolidation of virtual machines in cloud data centers , concurrency and computation : practice and experience 24  ( 13 ) ( 2012 ) 13971420 .", "doi:10.1002/cpe.1867 .", "tom gurout , thierry monteil , georges  da costa , rodrigo  neves calheiros , rajkumar buyya , and mihai alexandru .", "energy - aware simulation with \\{dvfs}. , 39:76  91 , december 2013 .", "energy efficiency in grids and clouds .", "doi:10.1016/j.simpat.2013.04.007 .", "x.  li , x.  jiang , p.  huang , k.  ye , dartcsim : an enhanced user - friendly cloud simulation system based on cloudsim with better performance , in : 2nd international conference on cloud computing and intelligent systems ( ccis ) , vol .  1 , ieee , hangzhou , 2012 , pp . 392396", ". doi:10.1109/ccis.2012.6664434 .", "s.  sotiriadis , n.  bessis , n.  antonopoulos , a.  anjum , simic : designing a new inter - cloud simulation platform for integrating large - scale resource management , in : 27th international conference on advanced information networking and applications ( aina ) , 2013 , pp .", "doi:10.1109/aina.2013.123 .", "s.  sotiriadis , n.  bessis , n.  antonopoulos , towards inter - cloud simulation performance analysis : exploring service - oriented benchmarks of clouds in simic , in : 27th international conference on advanced information networking and applications workshops ( waina ) , 2013 , pp .", "doi:10.1109/waina.2013.196 .", "y.  shi , x.  jiang , k.  ye , an energy - efficient scheme for cloud resource provisioning based on cloudsim , in : ieee international conference on cluster computing ( cluster ) , ieee , austin , tx , 2011 , pp .", "doi:10.1109/cluster.2011.63 .", "alexandru - florian antonescu and torsten braun .", "sla - driven simulation of multi - tenant scalable cloud - distributed enterprise information systems . in florin pop and maria potop - butucaru , editors ,", "_ adaptive resource management and scheduling for cloud computing _ , lecture notes in computer science , pages 91102 .", "springer international publishing , november 2014 .", "doi:10.1007/978 - 3 - 319 - 13464 - 2_7 .", "b.  wickremasinghe , r.  n. calheiros , r.  buyya , cloudanalyst : a cloudsim - based visual modeller for analysing cloud computing environments and applications , in : 24th ieee international conference on advanced information networking and applications ( aina ) , ieee , 2010 , pp .", "doi:10.1109/aina.2010.32 .", "thiago teixeira  s , rodrigo  n. calheiros , and danielo  g. gomes . .", "in zaigham mahmood , editor , _ cloud computing _ , computer communications and networks , pages 127142 .", "springer international publishing , october 2014 .", "doi:10.1007/978 - 3 - 319 - 10530 - 7_6 .", "h.  casanova , simgrid : a toolkit for the simulation of application scheduling , in : first ieee / acm international symposium on cluster computing and the grid , ieee , brisbane , qld . , 2001 , pp .", "doi:10.1109/ccgrid.2001.923223 .", "t.  hirofuchi , a.  lbre , l.  pouilloux , et  al . , adding a live migration model into simgrid , one more step toward the simulation of infrastructure - as - a - service concerns , in : 5th ieee international conference on cloud computing technology and science ( ieee cloudcom ) , bristol , uk , 2013 , pp . 96103 .", "doi:10.1109/cloudcom.2013.20 .", "d.  kliazovich , p.  bouvry , s.  u. khan , greencloud : a packet - level simulator of energy - aware cloud computing data centers , the journal of supercomputing 62  ( 3 ) ( 2012 ) 12631283 .", "doi:10.1007/s11227 - 010 - 0504 - 1 .", "a.  nez , g.  castane , j.  vazquez - poletti , a.  caminero , j.  carretero , i.  llorente , design of a flexible and scalable hypervisor module for simulating cloud computing environments , in : international symposium on performance evaluation of computer & telecommunication systems ( spects ) , ieee , 2011 , pp .", "265270 .", "a.  nez , j.  l. vzquez - poletti , a.  c. caminero , g.  g. casta , j.  carretero , i.  m. llorente , icancloud : a flexible and scalable cloud infrastructure simulator , journal of grid computing 10  ( 1 ) ( 2012 ) 185209 .", "doi:10.1007/s10723 - 012 - 9208 - 5 .", "s.  ostermann , k.  plankensteiner , d.  bodner , g.  kraler , r.  prodan , integration of an event - based simulation framework into a scientific workflow execution environment for grids and clouds , in : towards a service - based internet , vol .", "6994 of lecture notes in computer science , springer , poznan , poland , 2011 , pp . 113 .", "doi:10.1007/978 - 3 - 642 - 24755 - 2_1 .", "i.  sriram , speci , a simulation tool exploring cloud - scale data centres , in : cloud computing , vol .", "5931 of lecture notes in computer science , springer , 2009 , pp .", "doi:10.1007/978 - 3 - 642 - 10665 - 1_35 .", "m.  tighe , g.  keller , m.  bauer , h.  lutfiyya , dcsim : a data centre simulation tool for evaluating dynamic virtualized resource management , in : 8th international conference on network and service management ( cnsm ) , ieee , 2012 , pp .", "385392 .", "w.  piatek , dcworms  a tool for simulation of energy efficiency in data centers , in : energy efficiency in large scale distributed systems , vol .", "8046 of lecture notes in computer science , springer , 2013 , pp . 118124 .", "doi:10.1007/978 - 3 - 642 - 40517 - 4_11 .", "m.  tighe , g.  keller , j.  shamy , m.  bauer , h.  lutfiyya , towards an improved data centre simulation with dcsim , in : proceedings of the 9th international conference on network and service management , cnsm 2013 , ieee , zurich , switzerland , 2013 , pp .", "doi:10.1109/cnsm.2013.6727859 .", "g.  kecskemeti , foundations of efficient virtual appliance based service deployments : new techniques for virtual appliance delivery and size optimization in infrastructure as a service clouds , isbn : 978 - 3 - 8484 - 0383 - 7 , lap lambert academic publishing , saarbrcken , 2012 ."], "abstract_text": ["<S> infrastructure as a service ( iaas ) systems offer on demand virtual infrastructures so reliably and flexibly that users expect a high service level . therefore , </S>", "<S> even with regards to internal iaas behaviour , production clouds only adopt novel ideas that are proven not to hinder established service levels . to analyse their expected behaviour , </S>", "<S> new ideas are often evaluated with simulators in production iaas system - like scenarios . </S>", "<S> for instance , new research could enable collaboration amongst several layers of schedulers or could consider new optimisation objectives such as energy consumption . </S>", "<S> unfortunately , current cloud simulators are hard to employ and they often have performance issues when several layers of schedulers interact in them . to target these issues , a new iaas simulation framework ( called dissect - cf ) was designed . </S>", "<S> the new simulator s foundation has the following goals : easy extensibility , support energy evaluation of iaass and to enable fast evaluation of many scheduling and iaas internal behaviour related scenarios . in response to the requirements of such scenarios , </S>", "<S> the new simulator introduces concepts such as : a unified model for resource sharing and a new energy metering framework with hierarchical and indirect metering options . </S>", "<S> then , the comparison of several simulated situations to real - life iaas behaviour is used to validate the simulator s functionality . </S>", "<S> finally , a performance comparison is presented between dissect - cf and some currently available simulators .    </S>", "<S> cloud computing , infrastructure as a service , energy - awareness , resource management , simulation </S>"], "labels": null, "section_names": ["introduction", "research background", "design and internals of the simulator", "conclusion", "software availability", "acknowledgements"], "sections": [["infrastructure as a service  ( iaas ) systems @xcite build on virtualisation technologies to allow automated infrastructure provisioning .", "virtual machine ( vm ) based provisioning gives users two major benefits : they do not need to be experts in physical infrastructure maintenance , and they can easily follow their demand patterns and scale their virtual computing infrastructure ( composed of several vms ) with tools built on top of iaas systems .", "these two benefits led to the wide and rapid adoption of such infrastructure offerings .", "unfortunately , their rapid adoption has led to infrastructures that still have plenty of open research issues ( e.g. , energy aware vm placement , service level objective specifications ) .", "however , even iaas systems operated by academia are used in production nowadays . as", "production level systems are used by a multitude of users on a daily basis , changing the internal behaviour of such systems might hinder their user experience ( such as reliability and usability ) .", "thus , research focused on improving the internals of iaas systems ( e.g. , introducing a new experimental virtual machine placement algorithm ) can not be done on such production systems directly .", "consequently , to analyse new and novel ideas for internal behaviour , researchers are either restricted to severely limited iaas deployments ( e.g. , rarely utilising more than a few hosts ) or should resort to theoretical modelling of expected internal behaviour .", "however , results based on such research is often questioned by the operators of production clouds because their applicability to large scale systems is often not proven .", "some researchers use simulators to further evaluate their models  @xcite .", "these simulators allow researchers the evaluation of new ideas in life - like scenarios and as a result such simulators could pave the way for the new research results allowing their wide - spread adoption .", "although a plethora of iaas related simulators exist even today , these simulators have very different focuses .", "some are designed completely from the user s point of view and hide the cloud s internals so users can make decisions on how and what services should be moved to the clouds  @xcite . because of their user orientation , in these simulators it is frequently problematic to introduce changes in iaas behaviour .", "some others  @xcite emphasise the need for precision for such simulations . despite their extensibility , these simulators not only scale very poorly ( making it problematic to evaluate more elaborate iaas scenarios where sometimes thousands of physical machines collaborate ) , but they also require complex setup procedures to be precise ( e.g. , one should model every possible application in the system to receive realistic results ) . finally , there are simulators that introduce some assumptions in the system that reduce the precision of the simulations but reach unprecedented speeds  @xcite . unfortunately , despite having clear advantages , they are too specific to allow investigations on internal iaas changes ( e.g. , groudsim only models external interfaces of clouds , simgrid merely focuses on virtualisation , and cloudsim has conflicting extensions ", "e.g. , power modelling is not available while using networking ) .    in this article ,", "a new versatile simulation framework is presented ( called discrete event based energy consumption simulator for clouds and federations ", "dissect - cf ) .", "compared to the previously mentioned simulators , dissect - cf offers two major benefits : a unified resource sharing model , and a more complete iaas stack simulation ( including for example virtual machine image repositories , storage and in - data - centre networking ) .", "the benefits of the sharing model are threefold : @xmath0 it allows a single framework to model resource bottlenecks ( e.g. , cpu , network ) , @xmath1 generic resource sharing performance optimisations immediately improve entire simulations , @xmath2 it provides a unified view on resource usage counters ( i.e. , allows resource type independent , generic monitoring ) .", "finally , dissect - cf also opens up possibilities for more fine - grained energy consumption modelling by allowing the user to derive energy consumption from multiple resource usage counters . as a result of these new advancements ,", "the new simulator could foster research on schedulers that could either have better insight into internal iaas behaviour or collaborate with internal schedulers of iaas systems in order to achieve previously unprecedented flexibility , adaptability and elasticity in future cloud systems .", "unfortunately , dissect - cf s focus on supporting research on infrastructure cloud schedulers introduces several limitations to its applicability .", "first of all , for performance reasons the simulator represents networks with a simple flow model , which has already been shown by several studies ( e.g. , @xcite ) to be inaccurate for smaller - sized network transfers .", "fortunately , smaller - sized network transfers have a negligible influence on scheduling decisions in most cloud related schedulers .", "also , because scheduler focused research usually uses task or virtual machine instantiation / termination traces for behavioural studies , dissect - cf uses the black box philosophy for applications .", "thus , the simulator will not provide accurate results on resource utilisation if a particular application s behaviour can not be approximated with simple resource consumption metrics ( e.g. , when there is unstable cpu utilisation for extended periods of time ) .", "in fact , these limitations are present in most simulators  ( except those that have packet level network simulations or employ more complex flow models  see @xcite ) . finally ,", "as the new simulator is aimed at providing a framework for researchers to experiment with the internals of infrastructure clouds , the included scheduling mechanisms themselves are present only as examples for future work and they do not extend the scheduling related state - of - the art themselves .", "the behaviour of dissect - cf was analysed by first validating it against the behaviour of a small - scale infrastructure cloud at the university of innsbruck .", "according to the findings of this article , the system s simulated behaviour matches real - life experiments with negligible error ( in terms of application execution time , larger scale network transfers and energy consumption ) . for larger scale experiments ,", "dissect - cf was validated with proven results from two other simulators that are close to the new simulator s functionality ( namely cloudsim @xcite and groudsim @xcite ) .", "then , performance of these two simulators was compared to the newly proposed one .", "comparisons were executed with both real - world  ( using the grid workloads archive  @xcite ) and synthetic traces .", "the use of real - world traces also revealed that dissect - cf based simulations allow 1.5 - 32@xmath3 faster behavioural analysis of simple cloud schedulers or vm placement strategies .", "the performance differences were further investigated through synthetic traces and it is shown that dissect - cf scales significantly better in complex resource sharing situations with the help of its unified resource sharing model ( one can observe an improvement of even over 2800@xmath3 in execution time in some cases ) .    the rest of this article is organised as follows .", "section [ sec - relworks ] presents the related research results .", "then , section [ sec - design ] reveals the architecture of the newly proposed simulator and discusses its internal behaviour and extensibility options .", "section [ sec - eval ] analyses the properties of dissect - cf by comparing its behaviour to real - life systems and by comparing its performance to other simulators .", "finally , section [ sec - conclusion ] concludes the article with a summary and with the identification of future research directions ."], ["this section first reviews the scheduling scenarios that a cloud simulator might support .", "then an overview is presented on the most popular cloud simulation platforms .", "finally , the section concludes with a problem statement for the new simulator .", "there are seven common kinds of schedulers that could have an influence on the behaviour of a virtual infrastructure created on top of iaas cloud systems . in the following", ", a short overview is given of these kinds of schedulers with special attention on their requirements from a simulated environment .", "the list is presented from the schedulers that have the strongest user - side orientation to the most hidden schedulers in infrastructure systems .", "if a user has large enough resource demands , then its virtual infrastructure might include multiple virtual machines that could host a particular kind of task .", "in such cases , whenever a new task arrives , the user has to decide on which virtual machine it should actually run the task .", "the decision can be automated with a scheduler and a queuing system ( similar to local resource managers ", "e.g. ,  @xcite ) . in order to support research on these kinds of schedulers , _", "simulators should be able to provide past and present vm level performance metrics _", "( e.g. , temporal performance degradation of the vm s computing capabilities ) .    when a user s resource demands are more dynamic and sometimes unpredictable , then he / she would frequently face heavy under- or over - utilisation of his / her virtual infrastructure . to better meet the demands of the newly arriving tasks ,", "the virtual infrastructure should be able to automatically scale .", "this scaling is often achieved with a special scheduler ( e.g. , @xcite ) that decides when to instantiate / terminate a particular kind of vm .", "research on such schedulers need _ simulators that are capable of providing accurate vm management metrics _ ( e.g. , virtual machine instantiation time ) .", "some users have access to multiple cloud infrastructures . for such users ,", "a new scheduler is needed ( e.g. , @xcite ) which can choose between various cloud providers and dispatch vms to them .", "the selection procedure is expected to take into account the availability , reliability and similar metrics of the various providers and also it should consider issues like placing processing close to big data . for such schedulers , _", "simulators are required to offer infrastructure provider level metrics and data locality information . _    inside iaas systems , user requests are no longer represented as tasks but they are only seen as vms . as iaas systems", "are highly automated , decisions to place a particular vm on a physical machine must be also done by a scheduler .", "this kind of scheduler ( e.g. , @xcite ) could have two main tasks : @xmath0 for already existing virtual machines , a new vm to host mapping could be identified which would allow a vm arrangement that considers both the vms actual load and the providers current needs , and @xmath1 for newly requested virtual machines , the scheduler should determine the host where the vm could be run . as these schedulers have diverse tasks , _", "simulators should have the capability to disseminate the load of currently running virtual machines and also the utilisation of physical machines_.    energy conscious iaas systems aim at reducing their energy consumption in several ways .", "a simple way to do so is to consolidate vm load to the most energy efficient machines and switch the rest to a more energy efficient state .", "the automated decisions on which machines should be serving vms and which ones should be waiting in low power states ( e.g. , suspend , switch off ) are done by physical machine schedulers ( e.g. , @xcite ) .", "these schedulers should ensure that , because of their operations , virtual machine creation and quality of service do not degrade below certain levels . to support the development of physical machine ( pm ) schedulers , _", "simulators must necessarily maintain the cost of pm power state changes _", "( e.g. , cold / suspend to ram boot - up procedures ) .", "schedulers are also present in virtual machine monitors  ( like xen or kvm ) in order to allocate physical resources to virtual machines on a time - sharing basis ( e.g. , @xcite ) .", "although , these schedulers are not the main focus of research in cloud computing , they could have a direct impact on the quality of service if the above - mentioned vm placement strategies under - provision some virtual machines .", "for this reason , _", "simulators should be able to correctly handle and report under - provisioning scenarios on physical machines_.    the lowest levels of schedulers that may affect higher - level ( e.g. , task to vm assignment ) decisions are the process schedulers ( e.g. ,  @xcite ) of the operating system in the user s vms . in some cases the user could have an influence on the os scheduler , but in others users must use oss and schedulers that are prepared and accredited by the iaas providers . since these schedulers are generic os level schedulers , they are out of the scope of cloud computing research .", "but since higher - level schedulers might make decisions on how these process schedulers behave , simulators should give their users some information on their behaviour .", "for example , _ simulators should be capable of reporting if a particular vm is under - provisioned _ and tasks have no chance to access resources scheduled for them by the os level scheduler .", "cloudsim  @xcite is amongst the most popular iaas cloud simulators .", "it was initially based on gridsim  ( a widely used grid simulator developed by the same research institute  ", "@xcite ) but , after some performance and reliability issues , it was completely rewritten so it uses only some concepts ( e.g. , cloudlet  gridlet analogy ) from its predecessor  @xcite .", "cloudsim introduced the simulation of virtualised data centres mostly focusing on computational intensive tasks and data interchanges between data centres .", "later , they extended the simulation to better support internal network communications of a data centre with networkcloudsim  @xcite .", "there are also extensions that simulate the energy consumption behaviour of the physical machines in the data centre based on specpower benchmarks and on dynamic voltage and frequency scaling  @xcite .", "cloudsim also formed an ecosystem .", "several third parties offer extensions on top of cloudsim .", "some significantly change cloudsim behaviour", "( e.g. , add performance improvements  @xcite , add better support for inter - cloud operations  @xcite , implement new energy consumption models  @xcite , or introduce sla concepts into the simulation  @xcite ) , while others wrap cloudsim and provide additional functionality ( like graphical user interfaces for teaching  @xcite or for analytics  @xcite ) . despite its wide use , cloudsim has several disadvantages : @xmath4 low performance for scheduling research where thousands of scheduling scenarios should be evaluated in a timely fashion , @xmath5 networking is simulated for tasks only ( e.g. , data centre operations that utilise the same network as user tasks  like virtual machine image transfers  are not simulated even though they could have significant effects on the user perceived network performance ) and @xmath6 using multiple extensions at once is frequently not possible ( e.g. , advanced networking and energy consumption modelling are not usable together since one would need to have virtual machines that inherit behaviour both from ` powervm ` and from ` networkvm ` classes ) .", "the simgrid framework  @xcite is another widely used simulator for analysing the behaviour of distributed systems ( e.g. , grids , peer to peer systems ) .", "its resource sharing simulation is one of the most detailed ; for example , it contains one of the most accurate non - packet oriented network models  @xcite .", "this simulator s focus was not particularly on clouds for a long time but recently its developers introduced extensions for virtualisation ( e.g. , hypervisors or live migration  ", "because of its distributed systems and grid background the simulator is inefficient in iaas cloud related situations . for example , this simulator stops at the virtual machine level , thus it would require significant effort to build a multi data centre / cloud simulation on top of it .    while cloudsim and simgrid were heavily influenced by previous simulators for grids and distributed systems , for performance reasons they also make compromises on networking . to resolve such issues", "there are simulators like icancloud  @xcite and greencloud  @xcite that are built on network simulators ( e.g. , omnet++ or ns2 ) to more accurately simulate network communications in cloud systems .", "their efforts result in great accuracy if all iaas components and applications are modelled correctly network - wise ; otherwise , they just introduce serious performance penalties because of the packet level simulations without the expected accuracy .", "in addition to networking improvements , greencloud  @xcite is also offering precise energy estimations for networking and computing components , while icancloud also offers a user oriented simulation which supports iaas utilisation decision - making  @xcite on top of the regular iaas related simulation functionalities  @xcite .", "next , groudsim  a simulator developed at the university of innsbruck  @xcite  was analysed .", "this simulator aims at performance while it encompasses cloud concepts in a grid simulator environment . the simulator is also integrated with the askalon workflow system  @xcite so it can be used to evaluate behavioural changes of real - life scientific workflows in the case of changes in the computing environment .", "although this simulator supports clouds , it does not provide implementation on the internals of iaas systems ( i.e. , it provides a black box implementation ) , thus it is not suitable for research studies that involve the internals of cloud infrastructures . and", "although groudsim supports both cpu and network resources , the networking implementation of groudsim is one of the least developed ones amongst the reviewed simulators .", "the above simulators focus more on the user related behaviour of data centres , but there is a class of cloud simulators which is more focused on supporting decisions related to data centre operations  ( e.g. ,  @xcite ) .", "so even though these simulators could be used for examining user related behaviour , their detailed implementation of data centre behaviour reduces their usability in this context . on the other hand ,", "these simulators offer some unique features that might be useful for research on iaas related schedulers .", "for example , speci  @xcite is focused on offering a tool to analyse the scalability of iaas toolkits that will support future data centres .", "next , dcsim  @xcite allows the analysis of new virtual machine management operations ( like relocation ) .", "finally , dcworms  @xcite provides a unique view on data centre energy efficiency , including the heating , ventilation and air conditioning ( hvac ) system s airflow and high granularity resource ( e.g. , individual cpu , memory modules , network interfaces ) energy modelling .    [ [ problem - statement ] ] problem statement + + + + + + + + + + + + + + + + +    the analysis of the related work leads to the conclusion that existing simulators have many drawbacks for those who would like to investigate scheduling scenarios in iaas systems . to fulfil the needs of such scheduling scenarios , the rest of the article reveals a new infrastructure simulator that provides better insights on infrastructure behaviour for schedulers while maintaining the scalability of past simulators ."], ["]    figure  [ fig - arch ] presents the overall architecture of the newly proposed simulator .", "the figure groups the major components with dashed lines into subsystems . each subsystem is implemented as independently from the others as possible . as a result , simulation developers do not need to understand the complexity of the entire simulator if they intend to work on one of its subsystems .", "there are five major subsystems ; they are listed in an order that follows their level of abstraction ( from the most abstract to the more specific to iaas systems ) :    these components provide the time reference for simulations", ".    this subsystem acts as a lightweight and extensible foundation to low - level computing resource sharing ( e.g. , cpu , i / o ) .", "with these components dissect - cf enables simulator developers to monitor and analyse energy usage patterns of each individually simulated resource ( e.g. , network links , disks ) .", "these components handle the behaviour of those iaas system parts ( e.g. , virtual machines ) that are the primary target of iaas related schedulers .", "this subsystem provides the user interface ( the vm management api ) and represents the high level functionalities ( e.g. , virtual machine schedulers ) of infrastructure clouds .    in the following sections ,", "these subsystems are individually discussed .", "the core of the dissect - cf simulator is a simple but high performance event generator  ( reflected as ` timed ` in figure  [ fig - arch ] ) .", "it is used to maintain the time within the simulated system and allow third parties to be notified if a particular time instance has been reached.the simulator is not aware of the applied time granularity  ( i.e. , it is not known in the simulation if a single increase in the maintained time is equivalent to a single millisecond or a full hour ) .", "this enables flexibility in use , and allows simulation developers to have precision only when they assuredly need it ; otherwise , they can benefit from faster simulations . in later sections of the article , the smallest time granularity for the current simulation", "is denoted with @xmath7 and is expressed in seconds .", "thus any given time instance in the simulator can be specified as : @xmath8 , where @xmath9 and @xmath10 .", "here , @xmath11refers to the set of all possible time instances throughout a simulation .", "the simulator also assumes that notifications are recurring .", "thus , subscribing to events means specifying the frequency with which one would like to be notified .", "the simulator contains a construct ( called ` deferredevent ` ) for non - recurring events . creating a subclass of either", "the ` timed ` or the ` deferredevent ` classes allows simulation developers to receive custom time dependent notifications .    finally , the ` timed ` class is also the control point for the simulation time .", "simulations have two distinct ways to influence simulated time :    controls directly influence the timer .", "first , one can _ fire _ the events for the current time instance then advance the timer by one @xmath12 .", "second , it is possible to ask for a time jump that will progress the time with a given interval if within the interval there are no events expected .", "controls let the simulation flow for a given time interval without any intervention  ( e.g. , one can simulate until all events from the queue are cleaned up ) .", "these controls also enable the progression of the timer while dropping irrelevant events that would occur in a given period of time .      directly on top of basic time management", "lies the resource model of the simulator .", "the model is intended to capture low - level resource sharing behaviour ( e.g. , assigning tasks to virtual cpus  of vms  or virtual cpus to physical ones , or balancing network bandwidth utilisation ) .", "dissect - cf applies a provider - consumer scheme to resources where resource consumptions are intermediaries between consumers and providers . in the case of simulated cpus , consumptions represent instructions to be processed , thus cpu computing cycles of a physical machine are provided to virtual machines to consume . in a network analogy ,", "consumptions represent data to be transferred between two network hosts ( where the sender acts as the provider and the receiver as the consumer ) .", "dissect - cf allows the definition of both providers and consumers with the help of the ` resourcespreader ` class  ( see figure  [ fig - arch ] ) .", "the set of all spreaders in a particular simulation will be referred as @xmath13 .", "the simulator uses the concept of resource consumptions as the intermediaries that represent the current processing demands of the actual consumers .", "resource consumptions are denoted with a triplet : @xmath14 , where @xmath15 represents the resource consumption , @xmath16 represents the processing that is currently under way , @xmath17 represents the remaining processing ( i.e. , processing that has not started yet ) and @xmath18 represents the limit for this processing in a single @xmath12  ( e.g. , simulation developers can specify that a resource consumption is single - threaded so it can use the processing power of a single processor of a consumable cpu resource only ) .", "@xmath19 represents all possible resource consumptions in a simulation : @xmath20 . at a given time instance , the function @xmath21 determines which provider offers the resources to be consumed .", "similarly , @xmath22 defines the consumer that utilises the offered resources .", "these functions are time dependent to allow the migration of resource consumptions amongst spreaders .    at a given time", ", a particular resource consumption is processed in its provider by determining how much processing can be considered possible during a single @xmath12 .", "the possible processing has an upper bound of @xmath17 ( i.e. , if more processing could be possible than there is still remaining in @xmath15 , then the provider will have some non - utilised processing capabilities ) .", "also , the possible processing is limited by the provider s maximum processing capability and the processing limit @xmath18 of the consumption . @xmath23 where @xmath24 , @xmath25 and @xmath26 represent the processing under way , the remaining processing and the limit , respectively , for resource consumption @xmath15 at the time instance @xmath27 .", "it must be pointed out that @xmath28 is offering the provider side under processing value only .", "finally , @xmath29 reveals the processing power of a resource spreader ( in this current case the provider for resource consumption @xmath15 : @xmath30 ) at the time instance @xmath27 .", "dissect - cf simulates consumers with similar behaviour , so they remove utilised resources from @xmath16 .", "of course in this case the limit of utilisation is dependent on the consumer and the previously evaluated provider side possible processing value : @xmath31 thus , to determine the state of a particular resource consumption , dissect - cf first evaluates the provider side of resource consumptions , and then it processes the consumer side . after the simulator determined the @xmath16 value for a resource consumption , the remaining consumption @xmath32 can be determined as well by reducing with the increment of the @xmath16 value .", "this behaviour ensures that at the end of the consumer side processing both @xmath16 and @xmath17 will represent the resource consumption s state in the next simulated time instance ( @xmath33 ) .    in order to determine how much processing can be done on resource consumptions at a particular time instance @xmath34", ", resource spreaders apply the lowest level schedulers in dissect - cf based simulations .", "these schedulers share the processing capacities of the resources amongst those resource consumptions that the spreaders are currently dealing with  ( @xmath35 , where the notation of @xmath36 is used to depict a power set ) . as simulation developers", "are expected to run simulations with thousands of resources and millions of resource consumptions , these low - level schedulers must be highly customisable and efficient . to enable simple customisability ,", "dissect - cf provides efficient implementations for most common scheduling related tasks ( e.g. , resource consumption registration , de - registration , event generation for parties interested in resource consumption state ) , allowing providers of new schedulers to just focus on the scheduling logic that calculates the new @xmath34 values .", "+    the scheduling logic is expected to deliver fair resource allocation for simultaneously occurring resource consumptions  denoted as @xmath37 . to simplify the behaviour and complexity of schedulers", ", dissect - cf also introduces the concept of influence groups .", "these groups are formed from all resource spreaders that have a chance to influence each other s resource allocation schedules . with the help of influence groups", "even schedules for complex network structures can be simulated at close - to - real - life behaviour ( e.g. , the simulator can apply fair share algorithms over multiple related network links ) .", "these schedulers can utilise influence groups as their domain in which they have to guarantee a fair resource schedule for the spreader associated resource consumptions .    to determine the membership of an influence group", ", the simulator uses the resource consumptions that link consumers and providers ( see figure  [ fig - inf - gen ] ) . as a practical example ,", "figure  [ fig - inf - cpu ] shows how each simulated physical machine forms independent influence groups with the virtual machines it hosts via their respective cpu spreader implementations .", "formally , an influence group of a resource spreader at the particular time instance is defined as follows ( @xmath38 ) : @xmath39 where @xmath40 is a resource spreader , and the function @xmath41 defines the spreaders available at a particular time instance .", "the equation shows that @xmath42 includes all resource spreaders that are directly or transitively referred by the associated resource consumptions of the spreader @xmath43    @xmath44 . as a result", ", one could find as many influence groups as the number of resource spreaders existing at a given time instance in the simulation . on the other hand ,", "these groups are frequently equivalent because determining the influence group of any member of a particular group will result in the original influence group : @xmath45 this last equation is derived from the definition of @xmath42 and reveals that after the proper calculation of @xmath42 there should not be any members of it ( e.g. , @xmath46 ) that would result in a different influence group than the original @xmath42 .", "although the definition of @xmath47 is quite straightforward , its evaluation in all necessary time instances for all relevant resource spreaders would result in significant simulation performance deterioration .", "therefore , dissect - cf provides an algorithm that significantly reduces the use of @xmath47 but still ensures that the influence groups are available for the scheduling logic in every time instance . to differentiate between the original function s results and the algorithm calculated values , the notation @xmath48 is used for", "the influence groups determined by the algorithm ( see algorithm  [ alg - basopt ] ) .", "@xmath49 @xmath50 [ lin - ext - start ] @xmath51 [ lin - emptyext ] @xmath52 [ lin - identadd ] @xmath53 [ lin - extgadd ] @xmath54 [ lin - realext ] @xmath55 [ lin - igextcomplete ] [ lin - ext - stop ] [ lin - split - start ] @xmath56 @xmath57 [ lin - randsel ] @xmath58 [ lin - correct - group ] @xmath59 [ lin - split - upd - start ] @xmath60 [ lin - split - upd - stop ] [ lin - split - stop ]    in the following few paragraphs , the internal behaviour of the new algorithm is discussed .", "it is built on the assumption that @xmath61 and it is composed of two distinct phases : influence group extension ", "see lines [ lin - ext - start]-[lin - ext - stop ]  and group dissolution ", "see lines [ lin - split - start]-[lin - split - stop ] .", "let us first discuss the extension phase . during this phase ,", "the algorithm first starts with an empty resource spreader set ( see line [ lin - emptyext ] ) that will later on hold the identified extensions of the input influence group  @xmath49 . as a next step , line [ lin - identadd ] determines the resource consumptions that arrived to a particular resource spreader at the time instance @xmath33 .", "afterwards , the following line focuses on those newly added resource consumptions that introduce new resource spreader members into the input influence group .", "these non - member providers or consumers are added to the extension set in line  [ lin - extgadd ] .", "this iteratively created extension set is used to actualise the input influence group in lines [ lin - realext]-[lin - igextcomplete ] .", "the extension phase completes only when there are no newly introduced resource spreader members in the input group .", "otherwise , the current phase is repeated to ensure finding even further group extensions via the resource consumptions associated with the introduced members ( this last step is shown in line [ lin - ext - stop ] ) .", "after there are no new extension possibilities found in the current influence group , the algorithm proceeds to its second phase in which it identifies all splits of the current influence group .", "for example , influence group # 5 in figure  [ fig - inf - gen ] will have to be split when the resource consumption between provider @xmath62 and consumer @xmath63 finishes . to identify the need for splitting", ", the algorithm therefore first determines if there were some resource consumptions dropped from at least one member of the influence group ( see line  [ lin - split - start ] ) .", "if there is a need for splitting , then the algorithm will maintain the not yet split parts of the original influence group in @xmath64 . in order to determine", "which parts have to be split from the not yet split parts , lines [ lin - randsel ] and [ lin - correct - group ] use the original @xmath65 on a randomly selected spreader from @xmath64  resulting in a new influence group called @xmath66 . in the next line ,", "the not yet split parts are updated so that only those resource spreaders will be considered afterwards that are not in @xmath66 .", "finally , the algorithm updates its self - maintained influence group membership so all members of @xmath66 will be exactly the same ( see lines [ lin - split - upd - start]-[lin - split - upd - stop ] ) .    to conclude , the newly introduced algorithm reduces the number of direct @xmath42 function calculations to those cases where there is a chance to have a group to be split . and even in that case", ", it ensures that the number of @xmath42 evaluations is limited by the number of influence groups created after a split .      ]", "the last remaining part of the unified resource model is the simulation developer customisable low - level scheduling logic . to understand the customisation options dissect - cf offers ,", "figure  [ fig - rs ] presents the context of the scheduling logic .", "the figure reveals the role of the low - level scheduler through the illustration of the life of a single resource consumption that can be represented in three phases and denoted with different kinds of arrows : preparation  ", "regular lines ; resource consumption    dashed lines ; and completion", "  dotted lines .", "the next paragraphs provide a brief overview of these three phases .", "first , the preparation phase is initiated by the entity who is responsible for creating ( see _ step 1 _ in the figure ) a particular resource consumption  @xmath67 .", "this entity could be an automated process ( e.g. , a workload generator ) or some higher - level entity of the simulator ( e.g. , the virtual machine representation ) .", "after creation , the registration can be initiated in any time instance @xmath27 after both the consumer ", "@xmath68  and the provider  @xmath69 ", "spreaders are specified .", "the registration is accomplished in _ step 2 _ in the figure .", "the provider nudges the scheduler base in _ step 3 _ after both the consumer and the provider have registered the new resource consumption  @xmath70 and @xmath71 .", "the _ scheduler base _ is implemented in the base class of all resource spreaders and is responsible for interfacing with the event system , the scheduler and the influence group management algorithm . before contacting the event system for subscription ,", "the scheduler base first updates the influence groups with algorithm  [ alg - basopt ] in _ step 4_. after the identification of all distinct influence groups , the scheduler base filters those groups that would need an updated schedule .", "such groups are identified via recently added or dropped resource consumptions to / from one of their member resource spreaders ", "i.e. , @xmath72 . in _ step 5", "_ , the scheduling logic is invoked for each of the filtered groups . during this step", ", it should assign the @xmath34 values for all resource consumptions that are currently taking place in a given influence group . with these assignments ,", "the simulator calculates the earliest completion time of the currently managed resource consumptions .", "then , in _ step 6 _ , it subscribes to a notification from the event system in order to know when the next resource consumption will be removed from the filtered influence groups .    in the following phase , the simulator handles the resource consumptions", "this phase is either done when the event system delivers the notification on a resource consumption completion ( see _ step 7 _ ) or alternatively upon the registration of a new resource consumption . in the second case ,", "the resource consumption handling is automatically executed before influence groups are calculated ( i.e. , steps _ 8 - 12 _ could precede _ step 4 _ of the preparation phase if a resource consumption is registered at a resource spreader that has already had some prior resource consumptions ) . in practice ,", "_ steps 8 - 9 _ evaluate eq .", "[ eq - prov - update ] and _ steps 10 - 11 _ evaluate eq .", "[ eq - cons - update ] for all simultaneously existing resource consumptions ", "@xmath73  in the simulator . resource consumptions ", "e.g. , @xmath74  are automatically marked for removal when they reach their completion ", "i.e. , @xmath75 . as a final step for resource consumption handling , the scheduler base checks for resource consumptions marked for removal  ( see _ step 12 _ ) and on all marked resource consumptions it executes the completion phase .    in the final phase ,", "the resource consumption s completion is simulated .", "consumptions can be considered complete in two cases : either they have no further processing to be done or they were cancelled by the entity using the resource sharing mechanism .", "the scheduler base notifies this entity in both cases  see _ step 13_. then it checks if it has finished all the current resource consumptions ", "if there are still further resource consumptions to process , then the scheduler base resumes operations from _ step 4 _ , otherwise it cancels further notifications from the event system .", "as can be seen , the simulator expects scheduling logic implementations to utilise a fairly narrow and well - defined interface with the scheduler base . through this interface", "the simulator ensures that whenever a new schedule is needed ( i.e. , new @xmath34 values ) , the simulation developer provided scheduling logic is always called .", "dissect - cf also provides two sample implementations for this scheduling logic : a max min fairness algorithm  @xcite implementation with progressive filling , and a simple logic that does not deal with complex bottleneck situations but demonstrates the interfaces with the scheduler base .", "compared to other recently developed simulators , dissect - cf completely decouples energy modelling from resource simulation in order to allow accounting for such energy consumptions that are not in direct relation to the resource utilisation of data centres . with this approach ,", "a more comprehensive energy and power modelling is achievable that enables the analysis of new sophisticated energy aware algorithms in the areas of virtual machine placement , task scheduling , etc .", "these algorithms previously were frequently limited because energy readings from heating , ventilation and air conditioning ( hvac ) units or higher - level iaas components ( like vm schedulers or iaas interfaces ) were scarcely available in past simulators .", "thus , this section presents how energy consumption related information is collected and accumulated so they can support future algorithms .", "later , section  [ sec - hls ] discusses the foundations for physical machine schedulers that are expected to be the primary users of the models overviewed in this section .", "first , in order to enable the decoupling , dissect - cf offers _ resource utilisation counters _ both for producers and consumers .", "these counters allow an aggregated and time dependent view of the consumption of particular resources .", "counters are updated depending on the ` power state ` of a resource spreader ( see figure  [ fig - arch ] ) .", "for example , a physical machine  @xmath77  in suspend to ram ( str ) power state zeroes its processing power : @xmath78 the power states of the various entities in the simulation can be defined as needed ; the simulator only expects these states to define the basic power characteristics ( e.g. , minimum and maximum power draw ) and the resource processing behaviour of the given entity at the specific state .", "the simulator also provides a basic set of power states ( on , off , turning on , turning off ) for which the resource processing behaviour is already defined for the resources incorporated into a physical machine .", "based on this low - level power modelling functionality , the decoupling of energy models is accomplished through ` energy meter`s .", "these meters are organised around four functionalities : @xmath0 monitoring of energy consumption directly related to resource utilisation , @xmath1 indirect energy consumption estimation , @xmath2 aggregation of metering results from multiple meters and @xmath79 presenting up - to - date energy readings to their users .", "the remainder of this subsection reveals how these functions accomplish the decoupling and shows the ways customised , infrastructure specific metering can be achieved in dissect - cf .      ]", "[ [ direct - resource - utilisation - related - energy - consumption - metering ] ] direct resource utilisation related energy consumption metering + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    based on the previously mentioned resource utilisation counters , the simulator can be requested to periodically evaluate the instantaneous utilisation percentage . then , energy ` consumption model`s use these percentages ( see figure  [ fig - arch ] ) to estimate the instantaneous power draw of each monitored resource spreader ( later the resulting estimate and the metering period is used to calculate the direct meter s energy consumption estimate ) .", "consumption models are dependent on the actual power state a resource spreader is in , and the simulator developer can define them . as examples ,", "the simulator provides two simple energy consumption model implementations : @xmath0 a linear interpolation between minimum and maximum power draw depending on current resource utilisation , to allow basic modelling of dynamic power behaviour  or @xmath1 a constant minimum power draw , to allow the effortless modelling of _ off _ or _ str _ power states .", "figure  [ fig - meters ] presents direct meters for each resource spreader in the shown physical machines .", "[ [ indirect - energy - consumption - estimation ] ] indirect energy consumption estimation + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to support more complex energy consumption estimates , the simulator also allows energy consumption to be derived from other properties of the simulated system .", "for example , these properties could include the virtual machine request rate of a particular data centre , or the utilisation of the data centre level storage subsystem ( e.g. , to estimate how many disk drives the currently stored data can occupy ) .", "these meters are expected to periodically evaluate the system state and accumulate their energy consumption estimates for those components of the simulated system that are not directly represented with resource spreaders .", "figure  [ fig - meters ] reveals two indirect metering solutions to represent the internal actions of iaas systems and the energy consumption behaviour of data centre level hvac systems .", "[ [ meter - aggregators ] ] meter aggregators + + + + + + + + + + + + + + + + +    in several cases , the energy consumption values from individual direct or indirect meters are not sufficient for higher - level energy aware decision makers ( e.g. , physical machine state schedulers  see section [ sec - schedulers ] ) .", "for example , a physical machine in dissect - cf is represented with multiple resource spreaders ( e.g. , cpu , disk bandwidth ) , thus to have a complete view of a physical machine s energy consumption , one would need to monitor several direct energy meters .", "meter aggregators allow the automated collection and management of several meters in parallel and provide the higher - level view expected by decision makers .", "figure  [ fig - meters ] shows a complex scenario for the use of aggregators .", "this scenario shows that a single aggregated meter can be constructed for a whole data centre , allowing even the inclusion of indirect metering results such as hvac .", "the figure also shows how a physical machine s resource spreader set can be metered as a single entity .", "as the estimation of energy consumption of the simulated entities might be time consuming , the simulator allows simulation developers to fragment and focus their measurements on those parts of the simulated systems that they are interested in .", "for instance , the simulation developers might be only interested in the energy consumption of a single virtual machine that is deployed on a simulated cloud with multiple data centres .", "in such cases , dissect - cf can limit the number of energy meters that are evaluated with two approaches : independent meters or adjusted aggregations .", "[ [ independent - meters ] ] independent meters + + + + + + + + + + + + + + + + + +    this approach entails that metering results are only dependent on the metered component and the rest of the simulated system can not influence them .", "e.g. , the energy consumption reported for a cpu level resource spreader of a physical machine should not be dependent on the behaviour of the rest of the system .    [", "[ adjusted - aggregations ] ] adjusted aggregations + + + + + + + + + + + + + + + + + + + + +    if there is a dependency between two metered components ( e.g. , a virtual machine that is hosted on a particular physical machine ) , then a special meter aggregator can be created .", "this meter aggregator will not just add the aggregated meters measurements .", "instead it allows simulation developers to define an aggregation function .", "dissect - cf uses the adjusted aggregation technique to handle derived energy consumptions such as vm level energy consumption .", "for example , when applying the linear interpolation based energy consumption model , virtual machine level power draw can be calculated using the power draw of the hosting physical machine , as follows : @xmath80 where @xmath81 is the derived instantaneous power draw of a particular vm while it is running .", "the equation s first part estimates the variable part of the power draw , while the second part provides an estimate for the idle part . the variable part is dependent on @xmath82 , which is the maximum variability of the physical machine s power draw .", "the variable part is proportional to @xmath82 , depending on the resource utilisation of the particular vm compared to the resource utilisation of all vms hosted on the same physical machine .", "the second part of the estimate is the idle part that is derived from the idle power draw of the physical machine  @xmath83 .", "this part is proportional to the number of vms hosted by the physical machine at the given time instance ( this number is one less than the cardinality of the vm s influence group because the group contains also the resource spreader of its hosting physical machine ) .", "[ eq - powermodel ] reveals that the energy consumption model ( that estimates the instantaneous power draw ) of the virtual machine is not independent from the physical machine s behaviour .", "therefore , energy consumption can not be directly accounted to virtual machines .", "when such meters are requested , the simulator identifies them as dependent meters and instead of creating independent meters , it creates an adjusted meter aggregator including those meters that could provide the necessary information to calculate the energy consumption to be attributed to the originally requested meter . it must be noted that dependent meters consider energy consumptions multiple times ( e.g. , energy consumption is accounted to both physical and virtual machines ) .", "thus when meter aggregations are created they must only include meters that are not dependent on each other .      to allow the simplified development of new vm placement algorithms and pm state schedulers , dissect - cf provides an implementation of relevant infrastructure components in iaas systems .", "these components are built on top of the previously discussed resource sharing and energy modelling techniques and provide abstractions for networked entities and for physical / virtual machines ( see figure  [ fig - arch ] ) .", "network activities rarely play a role in scheduling decisions related to tasks or to physical / virtual machines .", "thus , to increase the performance of simulations , by default , dissect - cf offers a limited network model where two networked entities must be always directly connected ( therefore connection properties like bandwidth must be defined between all networked entities that should be able to communicate with each other in the simulation ) .", "this rudimentary behaviour could be sufficient even for some network aware schedulers , but to allow better representation of real networks , the simulator also allows the creation of intermediary network entities ( such as routers ) . the implementation of such entities should alter the processing limit ( @xmath84 ) of all resource consumptions that are directed through them .", "directly connected networked entities ( @xmath85 , where @xmath86 is the set of all possible networking entities ) are simulated with the ` networknode ` component .", "this component encapsulates an incoming and an outgoing network connection simulated with the unified resource sharing foundation ; thus connections are implemented as resource spreaders : @xmath87 , where @xmath88 .", "the processing power of these spreaders represents the network bandwidth ( either incoming or outgoing ) of the given network node .", "when a new network communication must take place , simulation developers are expected to request a resource consumption between the source network node s outgoing resource spreader and the target s incoming resource spreader .", "the component also introduces network latencies ( @xmath89 ) that can be defined between every networked entity for any given time instance .", "the latency values resulting from this function are used as delays preceding the registration of each resource consumption to the incoming or outgoing resource spreader of the node .", "for example , let us see what happens if a network communication ( represented as a resource consumption @xmath90 ) needs to be registered between a source and a target networked entity ( @xmath91 ) at the time instance @xmath92 : @xmath93 where @xmath94 is the network latency between the source and target nodes at the time of registration ( @xmath95 ) , and @xmath96 is the nonperforming spreader that never processes any resource consumption : @xmath97", ". thus the equation shows , that the consumption is registered to the nonperforming spreader for the complete period of the latency , afterwards the simulator switches the consumption s registration to the originally designated spreaders ( which are the network output port @xmath98 of the source node @xmath99 and the input port @xmath100 of the target node @xmath101 ) .", "this last step allows the simulator to utilise its unified resource sharing mechanism after the latency period is over .      in iaas systems ,", "physical machines offer most of the user exploited simulated resources .", "thus , dissect - cf ` physical ` ` machine`s encapsulate a diverse set of resources : local disks ( via repositories ", "see section  [ sec - ui ] ) , network interfaces ( with the help of network nodes  see section  [ sec - nw ] ) , cpus ( using the unified resource sharing model of section  [ sec - ursm ] ) and memory .", "besides the modelling of these resources , dissect - cf s physical machine behaviour also focuses on two additional functionalities : administering resource allocations and vm requests ; and modelling physical machine level power behaviour . as resource sharing and modelling", "has already been discussed in detail , this subsection mainly discusses the latter functionalities .", "[ [ resource - allocator - and - vm - request - handler ] ] resource allocator and vm request handler + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in order to maintain up - to - date information on the available and utilised resources of the physical machine , dissect - cf applies a resource allocator .", "the applied allocator can reserve resources ( e.g. , a given amount of memory or number of cpu cores ) from the physical machine .", "these reservations are represented as ` resource ` ` allocation ` instances that are used to maintain the free resource set of the physical machine .", "allocation instances are also passed as a token of resource availability towards virtual machine schedulers .", "other than the amount of resources associated with them , resource allocations also have the following properties : expiry time if unused or a link to the vm that uses it . when a resource allocation is created , the physical machine automatically initiates a ` deferredevent ` , which automatically cancels the allocation after the expiry time . to avoid this mechanism , the entity that requested", "the allocation must request a vm to use the reserved resources ( i.e. , establishing the link between the vm and the allocation ) .", "the automatic cancellation of the resource allocation is a self - defence mechanism of physical machines to avoid keeping resources out of use just because they received an unused allocation .    for a single vm request ,", "dissect - cf allows multiple ` resource ` ` allocation`s to be made across multiple physical machines .", "the multi - allocation technique can be used by schedulers to optimise for non - functional properties ( e.g. , past availability , expected energy consumption , environmental impact ) of those resources that a vm could bind to at a given moment . after a decision", "is made about the use of a particular allocation for the vm request , the rest of the allocations ( which were non optimal according to the non - functional requirements ) are expected to be cancelled by the schedulers .", "for complex vm instantiation scenarios , schedulers are also allowed to adjust the expiration time upon allocation request . with this mechanism", ", researchers can evaluate advanced reservation - like scenarios regarding vm instances .", "[ [ power - behaviour ] ] power behaviour + + + + + + + + + + + + + + +    as dissect - cf aims at supporting the development of energy aware scheduling strategies in iaas clouds , the energy model of the physical machine is particularly important . in its default configuration ,", "the simulator supports 4 power states ( and the transitions between them ) : off , switching on , running and switching off . although this power state set is fairly limited , the simulator already offers constructs that allow the modelling of more complex operations like : suspend to ram , suspend to disk , dynamic voltage & frequency scaling or core / cpu de- and reactivation .", "suspension related states can be modelled with the introduction of new power states , while the latter two are available because resource spreaders can alter their maximum processing capabilities .", "the modelling of the 4 supported states was done after real - life physical machine behaviour in the clouds of the university of innsbruck and mta sztaki .", "the real - life behaviour of the machines was observed through constantly monitoring their instantaneous power draw while they went through the following cycle : idling @xmath102 shutdown @xmath102 off @xmath102 switch on @xmath102 idling @xmath102 full cpu load .", "a sample measurement with a typical cloud node at innsbruck ( with 80 cpu cores , 128 gb ssd , 132 gb ram , and redundant power supplies ) can be seen in figure  [ fig - pmbehaviour ] . as physical machine state schedulers can be highly influenced by the behaviour in non - running states ( e.g. , their decision on shutting off a machine could be dependent on the time it takes to boot the machine back and the expected power savings because of the completely off machine ) , dissect - cf offers a simplified and a more complex behaviour model .    .simplified power state definition of a physical machine[tab - simpli ] [ cols=\"<,<,<,<,<\",options=\"header \" , ]     during the performance evaluation , all parallel tasks of the trace started up in the first 10 seconds ( _ task spread _ ) and had a length variety between 10 - 90 seconds .", "the necessary amount of tasks for the 10-second - long simulation runtime is shown in table  [ tab - taskdistrs ] .", "each evaluation was ran with different parallel task numbers between ( 1 - 100,000 ) to allow the investigation on how the increasing parallelism changed the performance of the simulators .", "the findings are shown in figure  [ fig - pjruntime ] .    as the figure shows , in the case of cloudsim , both its time - shared and space - shared vm scheduler was experimented on .", "the time - shared scheduler of cloudsim simulates parallelism , while the space - shared scheduler serialises the arrived tasks so at any given time there is only one task that can utilise the cpu , and the rest are queued until this task is finished .", "unfortunately , the time - shared scheduling mechanism of cloudsim is broken ; it lengthens task execution times significantly even in the case of a two - parallel - task setup ( e.g. , a simple setup like the one presented in figure  [ fig - incpartask ] leads to completely different results than one would see in real - life ) on the other hand , the space - shared scheduler provides completely different task completion times ( because of its serialising behaviour ) .", "thus the cloudsim related details of the figure are only valid when considering their simulation runtime ; the simulated tasks do not finish at the expected times in either case .    as depicted in the figure , cloudsim - time - shared and groudsim measurements abruptly finish at 1,000 and 50,000 tasks .", "for higher levels of parallelism , the runtime of the simulation took more than 8 hours and therefore was cancelled . in the case of cloudsim , the performance penalties mainly originate from its centralised design of data centres ( i.e. , most of the logic and event processing reside in the ` datacenter ` class , and the rest of cloudsim s classes are used for state representation only ) .", "groudsim , on the other hand , has a different design issue : it pre calculates all task completion times , puts them into the event queue and thus if a change is needed to them , the whole event queue has to be updated .", "finally , for all simulators , the task processing performance shown in the figure is expected to be faster than what one usually would see , because of the large amount of tasks that were executed for gathering data for the figure .", "the amount of repeated evaluations allowed the jvm to optimise the runtime behaviour of each simulator for the particular performance evaluation scenario . later on this effect", "is excluded from the evaluations .", "[ [ performance - influence - of - load - characteristics ] ] performance influence of load characteristics + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the rest of this sub - section analyses of how the performance of the resource sharing varies depending on changes in task arrival and length characteristics . for this analysis ,", "a baseline measurement was collected with no parallel tasks but varying task lengths . than the assumption", "was made that from this baseline , the simulators would linearly degrade in performance ( i.e. , two times the parallelism would increase the simulation run time of a single task by two times ) . to compare how linearly a particular simulator behaves under particular load characteristics ,", "the following scaling ratio function was designed : @xmath103 where @xmath104 is the scaling ratio , @xmath105 is the kind of simulator , @xmath106 is the chosen range of task length variety ( in the below detailed experiments it was either 10 - 90 s or 200 - 3600 s ) , @xmath107 is the number of parallel tasks , @xmath108 is the task spread ( in particular , 10 s or 200 s were used ) , and @xmath109 is the measurement function that evaluates the particular simulator with the given load characteristics .", "figure  [ fig - jobscalingcompare ] compares dissect - cf with the two selected simulators via the scaling ratio function .", "the labels of the figure are presented in the following format : @xmath110 ( see eq .", "[ eq - scratio ] ) , where @xmath105 is one of ds / gs / cs , which translate to dissect - cf / groudsim / cloudsim , respectively .", "the x - axis of the figure shows the increase in @xmath107 .", "based on the distribution of the measured scaling values for the different load characteristics , the most tolerant to the change in workload is groudsim , while the least tolerant is cloudsim .", "degrading performance in scaling can be also observed for both cloudsim and groudsim . in the case of cloudsim ,", "the degradation starts around 10 parallel tasks , and by the time the simulation reaches 50 parallel tasks the simulator becomes worse than linear ( @xmath111 ) . in the case of groudsim ,", "the degradation starts around 100 parallel tasks , and becomes worse than linear around 20,000 parallel tasks . in both cases", "the early degradation is caused by the data structures and the indirect call structures used ( i.e. often these simulators experience rather deep call stacks during parallel event handling ) .", "finally , although it is not visible in this chart , the degradation starts for dissect - cf at around 10,000 parallel tasks , and based on estimates , its scaling becomes worse than linear around 1.5 million parallel tasks .", "fortunately , this huge amount of parallelism is unlikely for most simulations . but simulations of some highly under - provisioned systems might need levels of parallelism that could lead to degraded performance even in groudsim .", "the figure also reveals that the performance of dissect - cf is more dependent on the task spread , while cloudsim s scaling is limited more by the task length variety . in case of dissect - cf", ", the task spread dependency manifests because the simulator executes its resource sharing mechanism only once per @xmath12 , on new resource consumption arrivals and departures ( see figure  [ fig - rs ] for details ) .", "however , the increased task spread decreases the likelihood that the resource sharing mechanism can be executed on multiple resource consumptions at once .", "therefore , the wider the spread the closer one gets to the worst possible resource sharing performance in dissect - cf .", "in fact , the 200 s long spread was used because this spread is already high enough to reveal close - to - worst - case performance . increasing", "the spread further did not introduce significant resource sharing performance drops , but its impact on cloudsim based simulations rendered the evaluation of the 1,000 parallel task experiment too time consuming .", "as synthetic loads often criticised because of their possible bias , dissect - cf was evaluated and compared using workloads that were collected from real - life computing infrastructures .", "thus , the evaluation required workload traces with the following characteristics : @xmath0 collected for extensive periods of time ( i.e. , at least a few months long ) to ensure the widest variety of observable load characteristics for the particular infrastructure ; @xmath1 tasks should be described in detail including their resource utilisation and submission , start and completion times ( so even if just task definitions are available , one can still translate them to the kinds of virtual machines the tasks would need for their execution in a cloud environment ) ; and @xmath2 if the traces contain virtual machine management logs , then task allocation details are also necessary to enable the analysis of new scheduling techniques that might aim at reallocating tasks or that would change vm management operations .", "based on these requirements , most of the traces ( e.g. , planetlab ) containing virtual machine management logs were not found suitable for the planned comparative study ; the rest of these log based traces are not collected for enough time to be used in large scale experiments .", "thus , although these virtual machine management log based traces would be the best candidates for analysing cloud characteristics , their immaturity necessitates to also search amongst traces collected from other large - scale infrastructures like grids .", "two appropriate sources were identified : the grid workloads archive ( gwa @xcite ) and the parallel workloads archive .", "because both at the university of innsbruck and in mta sztaki , there are earlier good experiences with the processing of gwa traces , this article presents a comparative study of the three selected simulators through the traces downloadable from gwa ( namely : das2 , grid5000 , nordugrid , auvergrid , sharcnet , lcg ) .", "[ [ trace - processing ] ] trace processing + + + + + + + + + + + + + + + +    a trace loader was prepared for all three simulators so events were fired every time a task arrives .", "every time a new task arrival event is fired , the simulators are programmed to create a virtual machine that will host the task .", "once the vm was created the task was instantiated in it according to its definition in the trace file . when the task was completed according to the particular simulator , its hosting vm", "was also terminated .", "because of the known problems with the time - sharing mechanism in cloudsim , single vms did not receive parallel tasks ( e.g. , by requesting a vm that is sized to host multiple tasks first ) . instead ,", "when parallel tasks were needed , multiple vms were created in the simulated cloud infrastructure .", "unfortunately , because of a few conceptual differences between dissect - cf and the other analysed simulators , the above - mentioned trace processing technique has several minor differences in their adaptations to the various simulators .", "first of all , dissect - cf has a queuing first - fit vm scheduler ( see section  [ sec - hls ] ) that allows users to send vm requests right upon task arrival and wait for the vm scheduler s queuing mechanism to notify them about the vm s running state ( and readiness to receive the task ) .", "unfortunately , the rest of the simulators do not offer vm request queuing : they reject vm requests that can not be hosted according to the actual state of the simulated infrastructure . to have a similar behaviour to dissect - cf , either the vm request needs to be resent until it can be fulfilled ( which would introduce an unwanted busy waiting loop ) , or", "alternatively one must rely on user - side information .", "if the vm request is repeatedly resent , then the other simulators are significantly disadvantaged .", "thus , the second approach was used : previously non - servable vms were only re - requested once one of the previous vms have terminated ( i.e. , it was assumed that one can determine the state of all the running vms at a given time instance ) . with this technique", "the cpu share of the performance evaluation code ( as measured by java s embedded cpu sampler ) was kept around the same levels as it was for dissect - cf .", "tasks , which are utilising several cpu cores in parallel , are modelled in two ways .", "first , the simulation tries to request a vm with as many cpu cores as the task needs .", "unfortunately , this can not be achieved in cases when even the biggest possible vm is too small for the task s needs . for these huge tasks ,", "the simulation requests multiple vms , enough to fulfil the parallelism in the task . in groudsim and", "dissect - cf it is possible to request multiple vm instances at once ( e.g. , similarly to how amazon ec2 behaves ) and they will be ensured to be available in parallel . on the other hand , cloudsim does not have such functionality ; therefore it was extended with a technique that only submits the tasks to their virtual machines once all necessary vms are available for the level of parallelism needed for the task .    as groudsim is more focused on the user - side behaviour of cloud infrastructures", ", it has several deficiencies compared to the other two evaluated simulators : @xmath0 it can only handle tasks that occupy a single cpu core , @xmath1 its network sharing mechanism could result in network under utilisation if the communicating parties are using connections with different bandwidths and @xmath2 it does not provide data centre level simulation details ", "e.g. , no vm / pm scheduling and pm level resource sharing is simulated . to overcome the first deficiency ,", "multi - core tasks are simulated as several single core tasks in the same groudsim virtual machine . to avoid the problem with network sharing , the simulated data centres in all three simulators", "were constructed on such a way that every node was connected with the same bandwidth to the others .", "unfortunately , without significantly changing groudsim s code it was not possible to add the missing data centre level simulation details .", "thus , during the performance analysis one should keep in mind that these details are not simulated in groudsim .", "finally , for both groudsim and cloudsim , the instantiation time of a virtual machine is instantaneous , which is not realistic . as the transfer of the vm image is often the most time consuming operation in the vm instantiation procedure  @xcite ,", "this transfer was simulated with a download operation to the vm with both of the simulators . in groudsim", ", a network transfer was initiated right after the vm was created and this transfer delayed the creation of the task on the vm until the transfer s completion . in cloudsim , cloudlets ( computing tasks in cloudsim terminology ) could have input files defined for them .", "thus , tasks in cloudsim were specified so they must transfer an input file with the size of the vm image before they can start their processing .", "unfortunately , even with input files specified , the vms in cloudsim start immediately and execute their tasks right after the vm is created .", "this means that cloudsim based simulations can not be as accurate as the other two simulator s results . in order to reduce the impact of vm image transfer on task execution times  and to allow all three simulations to have a similar simulation completion time  , the size of the vm image was set to 100 mb ( which is the size of a rather small image nowadays ) .    [ [ the - simulated - virtual - infrastructure ] ] the simulated virtual infrastructure + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in all three simulators , a single kind of physical machine acted as the foundation of the simulated infrastructure .", "this physical machine was modelled after a single node in mta sztaki s cloud and had the following properties : @xmath0 64 cpu cores , @xmath1 256 gb ram , @xmath2 5 tb local disk , and @xmath79 two 1 gb / s ethernet connection .", "in the experiments detailed below , it was possible to define how many of these machines should be in the data centre .", "the set up of the machines includes the creation of a network interconnecting all of them with a central switch .", "[ [ the - runtime - comparison - experiment ] ] the runtime comparison experiment + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for the first experiment the simulated infrastructure was set up so it was sufficient to host even the largest parallelism a single task of the traces could request ( i.e. , no task has had to be dropped because there were not enough physical machines to host its level of parallelism ) .", "in particular , this experiment required the simulation of 20 physical machines with the above - mentioned properties . on this infrastructure , the first @xmath112 tasks from a particular trace", "were submitted .", "then , a measurement was initiated for the real time passed between the submission of the first task and the completion of the last one .", "the infrastructure preparation , task submission and time measurement operations were repeated 10 times , each time starting with a new jvm . then , the trace specific average simulation duration for the measurement of @xmath113 tasks was calculated .", "afterwards , the whole measurement procedure was repeated for the remaining traces .", "finally , the _ aggregated simulation duration _ was calculated for @xmath113 tasks by averaging the trace specific averages from all traces .", "later , the aggregated simulation duration is referred as @xmath114 ( where @xmath105 is the measured simulator , @xmath107 is the number of tasks , @xmath115 is the number of physical machines to simulate and @xmath116 ) .", "figure  [ fig - runtimecompare ] presents these aggregated simulation durations while the number of tasks ", "@xmath107  were changed from a hundred to a million . as the measurements ran with a cold jvm , for the first 10,000 tasks one can see that the jvm s effects on the start - up dominate the runtimes .", "the first workload related differences of the simulators can be seen after the jvm s behaviour is no longer a significant contributor to the measurement .", "also , despite dissect - cf s focus on the internals of the infrastructure , its performance is always better than that of the other two simulators .", "this is especially apparent after around 200,000 tasks . in spite of groudsim doing the lightest - weight simulation", "( see the deficiencies listed in the previous paragraphs ) , it is still significantly and consistently slower ( albeit in some cases  like the lcg trace  groudsim performs consistently better ) .", "the figure also shows that groudsim s network simulation code gives around 21% overhead for lower task counts , and for higher task counts the overhead could grow as high as 109% .", "this difference experienced during the network simulation is caused by groudsim s sub - par network modelling ( i.e. , for a single transfer to occur there are several events that needs to be handled in the event system , and in case of parallel transfers these events must be refreshed in the future event queue ) .", "[ [ the - scalability - experiment ] ] the scalability experiment + + + + + + + + + + + + + + + + + + + + + + + + + +    to determine the scalability of the compared simulators , the previous experiment was repeated with varying sizes of infrastructure under them .", "thus , these experiments will not only be able to pinpoint how the various simulators deal with the increasing amounts of tasks and virtual machines , but also how the simulators cope with increasing size of infrastructures .", "infrastructures as small as a single physical machine were used but the experiments were reaching to the size of 500 machines . as even smaller infrastructures were evaluated than 20 machines ( which was the minimum for the level of parallelism found in some of the traces ) , there were tasks that could not fit to the infrastructures .", "these tasks were automatically filtered out by the trace processor and never reached the simulation .", "interestingly , even with the simulation with a single machine the filtered out tasks were never more than 6% of the total number of tasks in any of the traces .", "of course , the single machine still caused a significant serialising effect that lengthened the total execution time of the rest of the tasks .", "similarly to eq .", "[ eq - scratio ] , the scalability of the simulators was calculated compared to linear scaling .", "figure  [ fig - scalingcompare ] presents how the various simulators scale under a particular number of physical machines .", "the figure is composed of twelve sub - charts , each delimited with a black line . between the black lines", "a large bidirectional arrow contains the title of a sub - chart .", "the title shows the number of tasks for which the final runtime measurement values were compared .", "for example : the title `` _ _ 100 - 200 tasks _ _ '' means the sub - chart shows how the final runtime measurement value of 200 tasks compares to the value of 100 tasks .", "the comparison is made according to the following equation : @xmath117 where @xmath118 , @xmath119 is the number of tasks for which the aggregated simulation duration is evaluated , @xmath105 is one of the evaluated simulators and @xmath115 is the physical machine count .", "thus , @xmath120 will become one if the particular simulator scales linearly in the task number range of @xmath118 and @xmath119 .", "if the scaling function above is lower than one then the particular simulator is worse than linear .", "according to the sub - charts in figure  [ fig - scalingcompare ] all simulators scale significantly better than linear with fewer than 10,000 tasks ( again , this is mostly due to the jvm s class loading and start - up mechanism , since practically all measurements under this task count result in sub - second runtimes ) .", "afterwards , one can see that cloudsim is already scaling significantly worse than the other two investigated simulators , and by 50,000 tasks cloudsim already drops below linear scaling .", "it can be also observed that simulators handle the number of parallel machines completely differently . in the case of cloudsim , the bigger the machine number , the more likely that the scaling factor will become lower . in the case of groudsim ,", "the opposite behaviour is observed ; while for dissect - cf , one can see a rather balanced case . in conclusion , groudsim behaves as expected : the missing simulation of physical machine behaviour and vm / pm scheduling techniques allows it to behave practically independently from infrastructure size . in this sense ,", "cloudsim and dissect - cf should behave more closely to each other .", "unfortunately , the problematic implementation of cloudsim s time - shared vm scheduler ( see section  [ sec - syntheval ] ) changes its apparent behaviour and reduces the similarities in the trends between dissect - cf and cloudsim . to conclude , dissect - cf scales comparably to other state - of - the - art simulators ( in fact it never drops below linear scaling , in contrast to the others ) while it offers significantly more detailed infrastructure level behaviour .    this experiment was also used to _ validate the results of dissect - cf _ through a comparison with the other simulators . as the first step of validation ,", "the simulator reported completion time of the last task in every trace specific measurement was collected ( from @xmath114  where @xmath107 was set between one and a million , and @xmath115 between one and five hundred ) .", "then , these task completion times were compared with each other .", "the median of the difference between the simulators was less than 0.001% ( the average difference from the median was 2.15% ) .", "the biggest differences occurred with simulated infrastructures containing the lowest number of physical machines : in such cases the median of difference between the simulators jumped to a little more than 0.29% ( with a sample standard deviation of 7.21% ) .", "the reason behind such a great deviation is the magnified effect of virtual machine instantiation simulation ( i.e. , as vms can only be instantiated on a single pm , they are mostly created and destroyed in a serial fashion ) .", "[ [ the - impact - of - energy - metering ] ] the impact of energy metering + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    so far , all presented measurements were executed without energy meters attached to the resource spreaders of dissect - cf . as energy metering is not available in groudsim", ", the other two simulators would have suffered a disadvantage because of their ongoing metering simulation .", "as dissect - cf was consequently a better performer than the other two simulators , its further evaluation was focused on how one could set up energy metering in order to get comparable performance to the other simulators meter - less operation .", "figure  [ fig - energyperformancedrop ] analyses the performance drop of dissect - cf observed when energy metering was requested for the complete simulated infrastructure with 20 physical machines . just as before ,", "the function @xmath121 was evaluated , but now instead of comparing dissect - cf to other simulators , the comparison was made between the newly received values to values from the previously presented simulations without energy metering . adding energy metering leads to significant performance differences between the various gwa traces . to show what is the possible performance degradation range in the various traces , the two extreme cases ( the best and worst performing ones ) are revealed in the figure .", "the presented cases show that selecting the energy measurement interval is crucial for well performing simulations . in general it", "is not recommended to use energy measurement intervals below one minute for long running simulations ", "i.e. , where the trace s length is over a few thousand tasks .", "if higher precision is needed , then selective metering techniques should be used that evaluate energy meters for only the necessary parts of the infrastructure ; otherwise , one could experience a slowdown of over 50 times compared to meter - less runtimes .", "finally , in figure  [ fig - energyequivalence ] , a comparison is shown about the performance drop caused by the metering to the meter - less setups of other simulators .", "the figure shows what the metering interval is  despite the resulting performance drop  that still provides equivalent performance to other simulators .", "as the figure shows , the performance drop also depends on the number of physical machines in the simulated infrastructure .", "overall , if energy metering is desired on the entire simulated infrastructure , the metering interval must be set to at least five - seconds to receive comparable performance to cloudsim .", "in contrast , groudsim s performance can be matched with a metering interval around 25 - 30 seconds .", "anything above these values will give a performance advantage to dissect - cf .", "on the other hand , metering intervals below these values could still cause a better performing dissect - cf but this is highly dependent on the particular trace or virtual machine request pattern used during simulations ."], ["this article outlined several iaas related schedulers that could be further improved with the use of cloud simulators .", "then , it has shown that current simulators barely meet the demands of the scheduling oriented researchers : @xmath0 they often limit the accessibility of information , @xmath1 they often hide the internal details of iaas systems , @xmath2 they frequently perform poorly in large - scale simulations , and @xmath79 they provide scarcely any options for introducing new energy management techniques inside iaas clouds . to overcome these issues ,", "the article has proposed a new simulator called dissect - cf .", "the proposed simulator targets information accessibility issues with open apis and monitoring and performance related customisable events .", "the internal details of the cloud systems are also accessible , allowing simulation developers : @xmath0 to construct clouds in novel ways ( e.g. , introduction of new physical machine - virtual machine interaction techniques or cloud organisation topologies ) , and @xmath1 to experiment with new cloud side behaviour ( e.g. , new vm schedulers , power states ) .", "dissect - cf utilises a new unified resource sharing mechanism that allows centralised performance optimisations and ensures scaling independently of the size of the infrastructure and the amount of tasks processed by the simulated system .", "finally , the new simulator deeply integrates energy metering techniques ( ranging from resource usage counters and energy consumption models to meter aggregators ) .", "these techniques not only allow further extensions but they allow selective and composite power metering to ensure minimal performance drops during the metering sessions .", "the new simulator was evaluated by comparing it to small - scale but real - life environments . during this evaluation", ", it was presented how one should model cpu and memory intensive tasks , and it was also shown that the accuracy of the simulator s energy metering technique is also dependent on the new unified resource sharing mechanism .", "experiments shown that the relative error of the new resource sharing technique is around 1% in most cases . after the small - scale evaluation , dissect - cf was compared with two state - of - the - art simulators ( namely cloudsim and groudsim ) .", "the comparisons were focused on the scaling and performance characteristics of the simulators .", "the results revealed that these simulators often produce inaccurate results , while the new simulator not only provided more accurate outputs but also offered better performance .", "future research will consider several areas .", "first , there are plans to look how the simulator s resource sharing accuracy could be improved with stochastic sharing models and new low - level scheduling techniques .", "next , support for more complex network topologies and additional node types ( e.g. , routers and switches ) will also be prepared . as the current simulator merely provides raw data for user - side schedulers ,", "an investigation of the necessary constructs and optimisations to better support the development and analysis of these schedulers also needs to be done .", "finally , memory behaviour is practically neglected in the current simulator , because applications and openly available traces do not offer any details on this . in the future", ", a task level complex memory model is expected to be delivered that not only considers memory bandwidth utilisation but also access patterns . with this model", "the simulator s target could include accurate live migration support ."], ["this article described the behaviour and features of dissect - cf version 0.9.5 .", "the source code of the simulator is open and available ( under the licensing terms of the gnu lesser general public license 3 ) at the following website :    https://github.com/kecskemeti/dissect-cf ."], ["this work was partially supported by european union s horizon 2020 research and innovation programme under grant agreement no 644179 ( entice ) as well as by the cost program action ic1305 : network for sustainable ultrascale computing ( nesus ) , finally the research has also received partial funding from the austrian science fund project trp 237-n23 .", "a.  ahmed and a.s .", "cloud computing simulators : a detailed survey and future direction . in _", "advance computing conference ( iacc ) , 2014 ieee international _ , pages 866872 .", "ieee , february 2014 .", "doi:10.1109/iadcc.2014.6779436 .", "w.  zhao , y.  peng , f.  xie , z.  dai , modeling and simulation of cloud computing : a review , in : ieee asia pacific cloud computing congress ( apcloudcc ) , ieee , shenzhen , 2012 , pp . 2024 .", "doi:10.1109/apcloudcc.2012.6486505 .", "t.  hirofuchi , a.  lbre , adding virtual machine abstractions into simgrid : a first step toward the simulation of infrastructure - as - a - service concerns , in : third international conference on cloud and green computing ( cgc ) , ieee , karlsruhe , germany , 2013 , pp . 175180 .", "doi:10.1109/cgc.2013.33 .", "a.  nez , j.  l. vzquez - poletti , a.  c. caminero , j.  carretero , i.  m. llorente , design of a new cloud computing simulation platform , in : computational science and its applications - iccsa 2011 , vol .", "6784 of lecture notes in computer science , springer , santander , spain , 2011 , pp . 582593 .", "doi:10.1007/978 - 3 - 642 - 21931 - 3_45 .", "k.  kurowski , a.  oleksiak , w.  pikatek , t.  piontek , a.  przybyszewski , j.  wkeglarz , dcworms  a tool for simulation of energy efficiency in distributed computing infrastructures , simulation modelling practice and theory 39 ( 2013 ) 135151 .", "doi:10.1016/j.simpat.2013.08.007 .", "d.  kliazovich , p.  bouvry , y.  audzevich , s.  khan , greencloud : a packet - level simulator of energy - aware cloud computing data centers , in : global telecommunications conference ( globecom 2010 ) , 2010 , pp . 15 .", "doi:10.1109/glocom.2010.5683561 .", "r.  n. calheiros , r.  ranjan , a.  beloglazov , c.  a. de  rose , r.  buyya , cloudsim : a toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms , software : practice and experience 41  ( 1 ) ( 2011 ) 2350 .", "doi:10.1002/spe.995 .", "s.  ostermann , k.  plankensteiner , r.  prodan , t.  fahringer , groudsim : an event - based simulation framework for computational grids and clouds , in : euro - par 2010 parallel processing workshops , vol .", "6586 of lecture notes in computer science , springer , 2011 , pp . 305313 .", "doi:10.1007/978 - 3 - 642 - 21878 - 1_38 .", "velho , l.  m. schnorr , h.  casanova , a.  legrand , on the validity of flow - level tcp network models for grid and cloud simulations , acm transactions on modeling and computer simulation ( tomacs ) 23  ( 4 ) ( 2013 ) 23 .", "doi:10.1145/2517448 .", "p.  velho , a.  legrand , accuracy study and improvement of network simulation in the simgrid framework , in : proceedings of the 2nd international conference on simulation tools and techniques , icst ( institute for computer sciences , social - informatics and telecommunications engineering ) , 2009 , p.  13 .", "doi:10.4108/icst.simutools2009.5592 .", "r.  sobie , a.  agarwal , i.  gable , c.  leavett - brown , m.  paterson , r.  taylor , a.  charbonneau , r.  impey , w.  podiama , htc scientific computing in a distributed cloud environment , in : proceedings of the 4th acm workshop on scientific cloud computing , science cloud 13 , acm , new york , ny , usa , 2013 , pp .", "doi:10.1145/2465848.2465850 .", "h.  song , j.  li , x.  liu , idlecached : an idle resource cached dynamic scheduling algorithm in cloud computing , in : 9th international conference on ubiquitous intelligence computing and 9th international conference on autonomic trusted computing ( uic / atc ) , 2012 , pp . 912917 .", "doi:10.1109/uic - atc.2012.24 .", "m.  rodrguez , d.  tapiador , j.  fontn , e.  huedo , r.  montero , i.  llorente , dynamic provisioning of virtual clusters for grid computing , in : e.  csar , m.  alexander , a.  streit , j.  trff , c.  crin , a.  knpfer , d.  kranzlmller , s.  jha ( eds . ) , euro - par 2008 workshops - parallel processing , vol .", "5415 of lecture notes in computer science , springer berlin heidelberg , 2009 , pp .", "doi:10.1007/978 - 3 - 642 - 00955 - 6_4 .", "b.  sotomayor , k.  keahey , i.  foster , combining batch execution and leasing using virtual machines , in : proceedings of the 17th international symposium on high performance distributed computing , hpdc 08 , acm , new york , ny , usa , 2008 , pp .", "doi:10.1145/1383422.1383434 .", "j.  tordsson , r.  s. montero , r.  moreno - vozmediano , i.  m. llorente , cloud brokering mechanisms for optimized placement of virtual machines across multiple providers , future generation computer systems 28  ( 2 ) ( 2012 ) 358  367 .", "doi:10.1016/j.future.2011.07.003 .", "e.  elmroth , l.  larsson , interfaces for placement , migration , and monitoring of virtual machines in federated clouds , in : eighth international conference on grid and cooperative computing .", "gcc 09 . , 2009 ,", "doi:10.1109/gcc.2009.36 .", "k.  tsakalozos , m.  roussopoulos , a.  delis , vm placement in non - homogeneous iaas - clouds , in : g.  kappel , z.  maamar , h.  motahari - nezhad ( eds . ) , service - oriented computing , vol .", "7084 of lecture notes in computer science , springer berlin heidelberg , 2011 , pp . 172187 .", "doi:10.1007/978 - 3 - 642 - 25535 - 9_12 .", "d.  jayasinghe , c.  pu , t.  eilam , m.  steinder , i.  whally , e.  snible , improving performance and availability of services hosted on iaas clouds with structural constraint - aware virtual machine placement , in : ieee international conference on services computing ( scc ) , 2011 , pp .", "doi:10.1109/scc.2011.28 .", "r.  ghosh , v.  naik , k.  trivedi , power - performance trade - offs in iaas cloud : a scalable analytic approach , in : ieee / ifip 41st international conference on dependable systems and networks workshops ( dsn - w ) , 2011 , pp .", "doi:10.1109/dsnw.2011.5958802 .", "e.  feller , c.  rohr , d.  margery , c.  morin , energy management in iaas clouds : a holistic approach , in : ieee 5th international conference on cloud computing ( cloud ) , 2012 , pp .", "doi:10.1109/cloud.2012.50 .", "lizhe wang , samee  u. khan , dan chen , joanna kolodziej , rajiv ranjan , cheng zhong xu , and albert zomaya .", "energy - aware parallel task scheduling in a cluster .", ", 29(7):1661  1670 , september 2013 .", "including special sections : cyber - enabled distributed computing for ubiquitous cloud and network services & amp ; cloud computing and scientific applications  big data , scalable analytics , and beyond .", "doi:10.1016/j.future.2013.02.010 .", "d.  ongaro , a.  l. cox , s.  rixner , scheduling i / o in virtual machine monitors , in : proceedings of the fourth acm sigplan / sigops international conference on virtual execution environments , vee 08 , acm , new york , ny , usa , 2008 , pp .", "doi:10.1145/1346256.1346258 .", "g.  von laszewski , l.  wang , a.  younge , x.  he , power - aware scheduling of virtual machines in dvfs - enabled clusters , in : ieee international conference on cluster computing and workshops .", "cluster 09 .", ", 2009 , pp .", "doi:10.1109/clustr.2009.5289182 .", "j.  yang , x.  zhou , m.  chrobak , y.  zhang , l.  jin , dynamic thermal management through task scheduling , in : ieee international symposium on performance analysis of systems and software .", "ispass 2008 . , 2008 , pp .", "doi:10.1109/ispass.2008.4510751 .", "r.  buyya , r.  ranjan , r.  n. calheiros , modeling and simulation of scalable cloud computing environments and the cloudsim toolkit : challenges and opportunities , in : international conference on high performance computing & simulation ( hpcs09 ) . ,", "ieee , leipzig , 2009 , pp .", "doi:10.1109/hpcsim.2009.5192685 .", "r.  buyya , m.  murshed , gridsim : a toolkit for the modeling and simulation of distributed resource management and scheduling for grid computing , concurrency and computation : practice and experience 14  ( 13 - 15 ) ( 2002 ) 11751220 .", "doi:10.1002/cpe.710 .", "s.  k. garg , r.  buyya , networkcloudsim : modelling parallel applications in cloud simulations , in : fourth ieee international conference on utility and cloud computing ( ucc ) , ieee , victoria , nsw , 2011 , pp .", "doi:10.1109/ucc.2011.24 .", "a.  beloglazov , r.  buyya , optimal online deterministic algorithms and adaptive heuristics for energy and performance efficient dynamic consolidation of virtual machines in cloud data centers , concurrency and computation : practice and experience 24  ( 13 ) ( 2012 ) 13971420 .", "doi:10.1002/cpe.1867 .", "tom gurout , thierry monteil , georges  da costa , rodrigo  neves calheiros , rajkumar buyya , and mihai alexandru .", "energy - aware simulation with \\{dvfs}. , 39:76  91 , december 2013 .", "energy efficiency in grids and clouds .", "doi:10.1016/j.simpat.2013.04.007 .", "x.  li , x.  jiang , p.  huang , k.  ye , dartcsim : an enhanced user - friendly cloud simulation system based on cloudsim with better performance , in : 2nd international conference on cloud computing and intelligent systems ( ccis ) , vol .  1 , ieee , hangzhou , 2012 , pp . 392396", ". doi:10.1109/ccis.2012.6664434 .", "s.  sotiriadis , n.  bessis , n.  antonopoulos , a.  anjum , simic : designing a new inter - cloud simulation platform for integrating large - scale resource management , in : 27th international conference on advanced information networking and applications ( aina ) , 2013 , pp .", "doi:10.1109/aina.2013.123 .", "s.  sotiriadis , n.  bessis , n.  antonopoulos , towards inter - cloud simulation performance analysis : exploring service - oriented benchmarks of clouds in simic , in : 27th international conference on advanced information networking and applications workshops ( waina ) , 2013 , pp .", "doi:10.1109/waina.2013.196 .", "y.  shi , x.  jiang , k.  ye , an energy - efficient scheme for cloud resource provisioning based on cloudsim , in : ieee international conference on cluster computing ( cluster ) , ieee , austin , tx , 2011 , pp .", "doi:10.1109/cluster.2011.63 .", "alexandru - florian antonescu and torsten braun .", "sla - driven simulation of multi - tenant scalable cloud - distributed enterprise information systems . in florin pop and maria potop - butucaru , editors ,", "_ adaptive resource management and scheduling for cloud computing _ , lecture notes in computer science , pages 91102 .", "springer international publishing , november 2014 .", "doi:10.1007/978 - 3 - 319 - 13464 - 2_7 .", "b.  wickremasinghe , r.  n. calheiros , r.  buyya , cloudanalyst : a cloudsim - based visual modeller for analysing cloud computing environments and applications , in : 24th ieee international conference on advanced information networking and applications ( aina ) , ieee , 2010 , pp .", "doi:10.1109/aina.2010.32 .", "thiago teixeira  s , rodrigo  n. calheiros , and danielo  g. gomes . .", "in zaigham mahmood , editor , _ cloud computing _ , computer communications and networks , pages 127142 .", "springer international publishing , october 2014 .", "doi:10.1007/978 - 3 - 319 - 10530 - 7_6 .", "h.  casanova , simgrid : a toolkit for the simulation of application scheduling , in : first ieee / acm international symposium on cluster computing and the grid , ieee , brisbane , qld . , 2001 , pp .", "doi:10.1109/ccgrid.2001.923223 .", "t.  hirofuchi , a.  lbre , l.  pouilloux , et  al . , adding a live migration model into simgrid , one more step toward the simulation of infrastructure - as - a - service concerns , in : 5th ieee international conference on cloud computing technology and science ( ieee cloudcom ) , bristol , uk , 2013 , pp . 96103 .", "doi:10.1109/cloudcom.2013.20 .", "d.  kliazovich , p.  bouvry , s.  u. khan , greencloud : a packet - level simulator of energy - aware cloud computing data centers , the journal of supercomputing 62  ( 3 ) ( 2012 ) 12631283 .", "doi:10.1007/s11227 - 010 - 0504 - 1 .", "a.  nez , g.  castane , j.  vazquez - poletti , a.  caminero , j.  carretero , i.  llorente , design of a flexible and scalable hypervisor module for simulating cloud computing environments , in : international symposium on performance evaluation of computer & telecommunication systems ( spects ) , ieee , 2011 , pp .", "265270 .", "a.  nez , j.  l. vzquez - poletti , a.  c. caminero , g.  g. casta , j.  carretero , i.  m. llorente , icancloud : a flexible and scalable cloud infrastructure simulator , journal of grid computing 10  ( 1 ) ( 2012 ) 185209 .", "doi:10.1007/s10723 - 012 - 9208 - 5 .", "s.  ostermann , k.  plankensteiner , d.  bodner , g.  kraler , r.  prodan , integration of an event - based simulation framework into a scientific workflow execution environment for grids and clouds , in : towards a service - based internet , vol .", "6994 of lecture notes in computer science , springer , poznan , poland , 2011 , pp . 113 .", "doi:10.1007/978 - 3 - 642 - 24755 - 2_1 .", "i.  sriram , speci , a simulation tool exploring cloud - scale data centres , in : cloud computing , vol .", "5931 of lecture notes in computer science , springer , 2009 , pp .", "doi:10.1007/978 - 3 - 642 - 10665 - 1_35 .", "m.  tighe , g.  keller , m.  bauer , h.  lutfiyya , dcsim : a data centre simulation tool for evaluating dynamic virtualized resource management , in : 8th international conference on network and service management ( cnsm ) , ieee , 2012 , pp .", "385392 .", "w.  piatek , dcworms  a tool for simulation of energy efficiency in data centers , in : energy efficiency in large scale distributed systems , vol .", "8046 of lecture notes in computer science , springer , 2013 , pp . 118124 .", "doi:10.1007/978 - 3 - 642 - 40517 - 4_11 .", "m.  tighe , g.  keller , j.  shamy , m.  bauer , h.  lutfiyya , towards an improved data centre simulation with dcsim , in : proceedings of the 9th international conference on network and service management , cnsm 2013 , ieee , zurich , switzerland , 2013 , pp .", "doi:10.1109/cnsm.2013.6727859 .", "g.  kecskemeti , foundations of efficient virtual appliance based service deployments : new techniques for virtual appliance delivery and size optimization in infrastructure as a service clouds , isbn : 978 - 3 - 8484 - 0383 - 7 , lap lambert academic publishing , saarbrcken , 2012 ."]]}
{"article_id": "1207.5446", "article_text": ["_ public key cryptography _ is based on asymmetric cryptographic algorithms that use two related keys , a _", "public key _ and a _ private key _ ; the two keys have the property that , given the public key , it is computationally infeasible to derive the private key .", "a user publishes his / her public key in a public directory such as an ldap directory and keeps his / her private key to himself / herself .", "according to the purpose of the algorithm , there are public - key encryption / decryption algorithms and signature algorithms .", "an encryption algorithm could be used to encrypt a data ( for example , a symmetric key ) using the public key so that only the recipient who has the corresponding private key could decrypt the data .", "typical public key encryption algorithms are rsa and ecies ( elliptic curve integrated encryption scheme , see , secg 2000 ) . a signature algorithm together with a message digest algorithm could be used to transform a message of any length using the private key to a _ signature _ in such a way that , without the knowledge of the private key , it is computationally infeasible to find two messages with the same signature , to find a message for a pre - determined signature , or to find a signature for a given message .", "anyone who has the corresponding public key could verify the validity of the signature .", "typical public key digital signature algorithms are rsa , dsa , and ecdsa .    there have been extensive standardization efforts for public key cryptographic techniques .", "the major standards organizations that have been involved in public key cryptographic techniques are :    * iso / iec . the international organization for standardization ( iso ) and the international electrotechnical commission ( iec ) ( individually and jointly )", "have been developing a series of standards for application - independent cryptographic techniques .", "iso has also been developing bank security standards under the iso technical committee tc86banking and related financial services . *", "the american national standards institute ( ansi ) have been developing public key cryptographic technique standards for financial services under accredited standards committee ( asc ) x9 .", "for example , they have developed the standards ansi x9.42 ( key management using diffie - hellman ) , ansi x9.44 ( key establishment using factoring - based public key cryptography ) , and ansi x9.63 ( key agreement and key management using ecc ) .", "the national institute of standards and technology ( nist ) has been developing public key cryptography standards for use by us federal government departments .", "these standards are released in federal information processing standards ( fips ) publications . *", "the internet engineering task force has been developing public key cryptography standards for use by the internet community .", "these standards are published in requests for comments ( rfcs ) . *", "the ieee 1363 working group has been publishing standards for public key cryptography , including ieee 1363 - 2000 , ieee 1363a , ieee p1363.1 , and ieee p1363.2 . * vendor - specific standards .", "this category includes pkcs standards that we will describe , sec standards , and others .", "standards for efficient cryptography ( sec ) # 1 and # 2 are elliptic curve public key cryptography standards that have been developed by certicom corp .  in cooperation with secure systems developers world - wide .", "the pkcs standards , developed by rsa laboratories ( a division of rsa data security inc . ) in cooperation with secure systems developers worldwide for the purpose of accelerating the deployment of public - key cryptography , are widely implemented in practice , and periodically updated .", "contributions from the pkcs standards have become part of many formal and de facto standards , including ansi x9 documents , ietf documents , and ssl / tls ( secure socket layer / transport layer security ) .", "the parts and status of pkcs standards are listed in table [ pkcstable ] and are discussed in details in the following sections .", "the descriptions are largely adapted from the pkcs documents themselves . in section", "[ conclusion ] , we give an example application which uses all these pkcs standards .", ".pkcs specifications [ cols=\"^,<,<\",options=\"header \" , ]     pkcs # 15 compliant ic cards should support direct application selection as defined in iso / iec 7816 - 4 section 9 and iso - iec 7816 - 5 section 6 ( the full aid is to be used as parameter for a `` select file '' command ) .", "the operating system of the card must keep track of the currently selected application and only allow the commands applicable to that particular application while it is selected . the application identifier ( aid )", "data element consists 12 bytes and its contents is defined in pkcs # 15 .", "objects could be created , modified , and removed from the object directory file on a card .", "asn.1 syntax for these objects have also been specified in pkcs # 15 .", "we conclude this chapter with an example application of different pkcs standards .", "assume that we want to implement a smart card authentication system based on public key cryptography technology .", "each user will be issued a smart card containing user s private key , public key certificate , and other personal information .", "users can authenticate themselves to different computing systems ( or banking systems ) by inserting their smart cards into card readers attached to these computing systems and typing the password ( or pin ) .", "rsa cryptographic primitives specified in pkcs # 1 could be chosen as the underlying cryptographic mechanisms .", "first , user alice needs to register herself to the system to get her smart card . in the registration process , the system first generates a public - key / private - key pair for alice . using pkcs # 9 ,", "the system may create a naturalperson object or a few attributes containing alice s personal information .", "these information can then be used to generate a certificaterequest object according to pkcs#10 .", "the system can then send the certificaterequest object to the certificate authorities ( ca ) enveloped using cms ( pkcs # 7 ) .", "after the identity information verification , the ca signs alice s public key to generate a certificate for alice and sends it back to the system . after receiving alice s certificate from the ca , the system can now build a smart card for alice . using alice s password ( pin )", ", the system generates an encryptedprivatekeyinfo object for alice according to pkcs # 8 and pkcs # 9 ( pkcs # 5 is also used in this procedure ) .", "pkcs # 12 may then be used to transfer alice s encrypted private key and personal information from one computer to another computer ( e.g. , from a server machine to the smart card making machine ) . using the dedicated file format df(pkcs#15 )", ", alice s encrypted private key object encryptedprivatekeyinfo , certificate , and other personal information could be stored on the smart card .", "the card is now ready for alice to use ! at the same time , alice may also get a copy of these private information on a usb memory stick . these personal information is stored on the memory stick according to pkcs # 12 .", "since all computing systems ( e.g. , different platforms from different vendors ) support pkcs # 11 api , when alice insert her card into an attached card reader , applications on these computing systems can communicate smoothly with alice s smart card .", "in particular , after typing password ( pin ) , alice s smart card can digitally sign challenges from these computing systems and these computing systems can verify alice s signature using the certificate presented by alice s smart card .", "thus alice can authenticate herself to these systems .", "* acknowledgements*. the author would like to thank anonymous referees for the constructive comments on improving the presentation of this chapter . the author would also like to thank dennis hamilton ( uol kit elearning division ) for some comments on pkcs#5v2.0 .    1 .", "* aes * a secret key cipher , as defined in fips pub 197 ( 2001 ) 2 .   * asn.1 * abstract syntax notation one , as defined in iso / iec 8824 - 1,2,3,4 ( 1995 ) 3 .   *", "attribute * an asn.1 type that identifies an attribute type ( by an object identifier ) and an associated attribute value 4 .", "* ber * basic encoding rules , as defined in x.690 ( 1994 ) 5 .", "* cryptoki * short for `` cryptographic token interface '' 6 .   * des and triple des * secret key ciphers , as defined in fips pub 46 - 3 ( 1999 ) 7 .", "* ecc * elliptic curve cryptography 8 .", "* key derivation function * a function that produces a derived key from a base key and other parameters 9 .", "* ldap * lightweight directory access protocol , as defined in hodges and morgan ( 2002 ) 10 . * mac scheme * a mac scheme is a cryptographic scheme consisting of a message tagging operation and a tag checking operation which is capable of providing data origin authentication and data integrity 11 .", "* md5 * a cryptographic hash function , as defined in rivest ( 1992 ) .", "md5 reduces messages of any length to message digests of 128 bits 12 . * oaep * optimal asymmetric encryption padding 13 . *", "octet * an octet is a bit string of length 8 .", "an octet is represented by a hexadecimal string of length 2 .", "for example 0x9d represents the bit string 10011101 14 . * octet string * an octet string is an ordered sequence of octets 15 . * pdu * protocol data unit , which is a sequence of bits in machine - independent format constituting a message in a protocol 16 . * personal identity information * personal information such as private keys , certificates , and miscellaneous secrets 17 .", "* pkcs # 11 token * the logical view of a cryptographic device defined by cryptoki 18 . * pkcs # 15 elementary file * set of data units or records that share the same file identifier , and which can not be a parent of another file 19 . * pkcs # 15 directory ( dir ) file * elementary file containing a list of applications supported by the card and optional related data elements 20 . * sha-1 , sha-256 , sha-384 , and sha-512 * cryptographic hash function functions , as defined in fips pub 180 - 2 , ( 2002 ) .", "sha-1 ( sha-256 , sha-384 , and sha-512 , respectively ) reduces messages of any length to message digests of 160 bits ( 256 bits , 384 bits , and 512 bits , respectively )          bellare , m. and rogaway , p. ( 1995 ) .", "optimal asymmetric encryption  how to encrypt with rsa . in a.", "de santis , editor , advances in cryptology , eurocrypt 94 , volume 950 of lecture notes in computer science , pp .", "springer verlag .", "bellare , m. and rogaway , p. ( 1996 ) .", "the exact security of digital signatures  how to sign with rsa and rabin . in u. maurer , editor , advances in cryptology ,", "eurocrypt 96 , volume 1070 of lecture notes in computer science , pp .", "399 - 416 .", "springer verlag .", "x.690 ( 1994 ) .", "itu - t recommendation x.690 : information technology - asn.1 encoding rules : specification of basic encoding rules ( ber ) , canonical encoding rules ( cer ) , and distinguished encoding rules ( der ) ."], "abstract_text": ["<S> cryptographic standards serve two important goals : making different implementations interoperable and avoiding various known pitfalls in commonly used schemes . </S>", "<S> this chapter discusses public - key cryptography standards ( pkcs ) which have significant impact on the use of public key cryptography in practice . </S>", "<S> pkcs standards are a set of standards , called pkcs # 1 through # 15 . </S>", "<S> these standards cover rsa encryption , rsa signature , password - based encryption , cryptographic message syntax , private - key information syntax , selected object classes and attribute types , certification request syntax , cryptographic token interface , personal information exchange syntax , and cryptographic token information syntax . </S>", "<S> the pkcs standards are published by rsa laboratories . </S>", "<S> though rsa laboratories solicits public opinions and advice for pkcs standards , rsa laboratories retain sole decision - making authority on all aspects of pkcs standards . </S>", "<S> pkcs has been the basis for many other standards such as s / mime .    </S>", "<S> * key works*. asn.1 , public key cryptography , digital signature , encryption , key establishment scheme , public key certificate , cryptographic message syntax , cryptographic token interface ( cryptoki ) . </S>"], "labels": null, "section_names": ["introduction", "an example"], "sections": [["_ public key cryptography _ is based on asymmetric cryptographic algorithms that use two related keys , a _", "public key _ and a _ private key _ ; the two keys have the property that , given the public key , it is computationally infeasible to derive the private key .", "a user publishes his / her public key in a public directory such as an ldap directory and keeps his / her private key to himself / herself .", "according to the purpose of the algorithm , there are public - key encryption / decryption algorithms and signature algorithms .", "an encryption algorithm could be used to encrypt a data ( for example , a symmetric key ) using the public key so that only the recipient who has the corresponding private key could decrypt the data .", "typical public key encryption algorithms are rsa and ecies ( elliptic curve integrated encryption scheme , see , secg 2000 ) . a signature algorithm together with a message digest algorithm could be used to transform a message of any length using the private key to a _ signature _ in such a way that , without the knowledge of the private key , it is computationally infeasible to find two messages with the same signature , to find a message for a pre - determined signature , or to find a signature for a given message .", "anyone who has the corresponding public key could verify the validity of the signature .", "typical public key digital signature algorithms are rsa , dsa , and ecdsa .    there have been extensive standardization efforts for public key cryptographic techniques .", "the major standards organizations that have been involved in public key cryptographic techniques are :    * iso / iec . the international organization for standardization ( iso ) and the international electrotechnical commission ( iec ) ( individually and jointly )", "have been developing a series of standards for application - independent cryptographic techniques .", "iso has also been developing bank security standards under the iso technical committee tc86banking and related financial services . *", "the american national standards institute ( ansi ) have been developing public key cryptographic technique standards for financial services under accredited standards committee ( asc ) x9 .", "for example , they have developed the standards ansi x9.42 ( key management using diffie - hellman ) , ansi x9.44 ( key establishment using factoring - based public key cryptography ) , and ansi x9.63 ( key agreement and key management using ecc ) .", "the national institute of standards and technology ( nist ) has been developing public key cryptography standards for use by us federal government departments .", "these standards are released in federal information processing standards ( fips ) publications . *", "the internet engineering task force has been developing public key cryptography standards for use by the internet community .", "these standards are published in requests for comments ( rfcs ) . *", "the ieee 1363 working group has been publishing standards for public key cryptography , including ieee 1363 - 2000 , ieee 1363a , ieee p1363.1 , and ieee p1363.2 . * vendor - specific standards .", "this category includes pkcs standards that we will describe , sec standards , and others .", "standards for efficient cryptography ( sec ) # 1 and # 2 are elliptic curve public key cryptography standards that have been developed by certicom corp .  in cooperation with secure systems developers world - wide .", "the pkcs standards , developed by rsa laboratories ( a division of rsa data security inc . ) in cooperation with secure systems developers worldwide for the purpose of accelerating the deployment of public - key cryptography , are widely implemented in practice , and periodically updated .", "contributions from the pkcs standards have become part of many formal and de facto standards , including ansi x9 documents , ietf documents , and ssl / tls ( secure socket layer / transport layer security ) .", "the parts and status of pkcs standards are listed in table [ pkcstable ] and are discussed in details in the following sections .", "the descriptions are largely adapted from the pkcs documents themselves . in section", "[ conclusion ] , we give an example application which uses all these pkcs standards .", ".pkcs specifications [ cols=\"^,<,<\",options=\"header \" , ]     pkcs # 15 compliant ic cards should support direct application selection as defined in iso / iec 7816 - 4 section 9 and iso - iec 7816 - 5 section 6 ( the full aid is to be used as parameter for a `` select file '' command ) .", "the operating system of the card must keep track of the currently selected application and only allow the commands applicable to that particular application while it is selected . the application identifier ( aid )", "data element consists 12 bytes and its contents is defined in pkcs # 15 .", "objects could be created , modified , and removed from the object directory file on a card .", "asn.1 syntax for these objects have also been specified in pkcs # 15 ."], ["we conclude this chapter with an example application of different pkcs standards .", "assume that we want to implement a smart card authentication system based on public key cryptography technology .", "each user will be issued a smart card containing user s private key , public key certificate , and other personal information .", "users can authenticate themselves to different computing systems ( or banking systems ) by inserting their smart cards into card readers attached to these computing systems and typing the password ( or pin ) .", "rsa cryptographic primitives specified in pkcs # 1 could be chosen as the underlying cryptographic mechanisms .", "first , user alice needs to register herself to the system to get her smart card . in the registration process , the system first generates a public - key / private - key pair for alice . using pkcs # 9 ,", "the system may create a naturalperson object or a few attributes containing alice s personal information .", "these information can then be used to generate a certificaterequest object according to pkcs#10 .", "the system can then send the certificaterequest object to the certificate authorities ( ca ) enveloped using cms ( pkcs # 7 ) .", "after the identity information verification , the ca signs alice s public key to generate a certificate for alice and sends it back to the system . after receiving alice s certificate from the ca , the system can now build a smart card for alice . using alice s password ( pin )", ", the system generates an encryptedprivatekeyinfo object for alice according to pkcs # 8 and pkcs # 9 ( pkcs # 5 is also used in this procedure ) .", "pkcs # 12 may then be used to transfer alice s encrypted private key and personal information from one computer to another computer ( e.g. , from a server machine to the smart card making machine ) . using the dedicated file format df(pkcs#15 )", ", alice s encrypted private key object encryptedprivatekeyinfo , certificate , and other personal information could be stored on the smart card .", "the card is now ready for alice to use ! at the same time , alice may also get a copy of these private information on a usb memory stick . these personal information is stored on the memory stick according to pkcs # 12 .", "since all computing systems ( e.g. , different platforms from different vendors ) support pkcs # 11 api , when alice insert her card into an attached card reader , applications on these computing systems can communicate smoothly with alice s smart card .", "in particular , after typing password ( pin ) , alice s smart card can digitally sign challenges from these computing systems and these computing systems can verify alice s signature using the certificate presented by alice s smart card .", "thus alice can authenticate herself to these systems .", "* acknowledgements*. the author would like to thank anonymous referees for the constructive comments on improving the presentation of this chapter . the author would also like to thank dennis hamilton ( uol kit elearning division ) for some comments on pkcs#5v2.0 .    1 .", "* aes * a secret key cipher , as defined in fips pub 197 ( 2001 ) 2 .   * asn.1 * abstract syntax notation one , as defined in iso / iec 8824 - 1,2,3,4 ( 1995 ) 3 .   *", "attribute * an asn.1 type that identifies an attribute type ( by an object identifier ) and an associated attribute value 4 .", "* ber * basic encoding rules , as defined in x.690 ( 1994 ) 5 .", "* cryptoki * short for `` cryptographic token interface '' 6 .   * des and triple des * secret key ciphers , as defined in fips pub 46 - 3 ( 1999 ) 7 .", "* ecc * elliptic curve cryptography 8 .", "* key derivation function * a function that produces a derived key from a base key and other parameters 9 .", "* ldap * lightweight directory access protocol , as defined in hodges and morgan ( 2002 ) 10 . * mac scheme * a mac scheme is a cryptographic scheme consisting of a message tagging operation and a tag checking operation which is capable of providing data origin authentication and data integrity 11 .", "* md5 * a cryptographic hash function , as defined in rivest ( 1992 ) .", "md5 reduces messages of any length to message digests of 128 bits 12 . * oaep * optimal asymmetric encryption padding 13 . *", "octet * an octet is a bit string of length 8 .", "an octet is represented by a hexadecimal string of length 2 .", "for example 0x9d represents the bit string 10011101 14 . * octet string * an octet string is an ordered sequence of octets 15 . * pdu * protocol data unit , which is a sequence of bits in machine - independent format constituting a message in a protocol 16 . * personal identity information * personal information such as private keys , certificates , and miscellaneous secrets 17 .", "* pkcs # 11 token * the logical view of a cryptographic device defined by cryptoki 18 . * pkcs # 15 elementary file * set of data units or records that share the same file identifier , and which can not be a parent of another file 19 . * pkcs # 15 directory ( dir ) file * elementary file containing a list of applications supported by the card and optional related data elements 20 . * sha-1 , sha-256 , sha-384 , and sha-512 * cryptographic hash function functions , as defined in fips pub 180 - 2 , ( 2002 ) .", "sha-1 ( sha-256 , sha-384 , and sha-512 , respectively ) reduces messages of any length to message digests of 160 bits ( 256 bits , 384 bits , and 512 bits , respectively )          bellare , m. and rogaway , p. ( 1995 ) .", "optimal asymmetric encryption  how to encrypt with rsa . in a.", "de santis , editor , advances in cryptology , eurocrypt 94 , volume 950 of lecture notes in computer science , pp .", "springer verlag .", "bellare , m. and rogaway , p. ( 1996 ) .", "the exact security of digital signatures  how to sign with rsa and rabin . in u. maurer , editor , advances in cryptology ,", "eurocrypt 96 , volume 1070 of lecture notes in computer science , pp .", "399 - 416 .", "springer verlag .", "x.690 ( 1994 ) .", "itu - t recommendation x.690 : information technology - asn.1 encoding rules : specification of basic encoding rules ( ber ) , canonical encoding rules ( cer ) , and distinguished encoding rules ( der ) ."]]}
{"article_id": "1611.03006", "article_text": ["over the past few years , advances in genomics and genome sequencing are not only enabling progress in medicine and healthcare , but are also bringing genetic testing to the masses , as an increasing number of `` direct to consumer '' ( dtc ) companies have entered , and sometimes disrupted , the market .", "for instance , 23andme.com offers a $ 99/129 assessment of inherited traits , genealogy , and congenital risk factors , through genotyping of a saliva sample posted via mail , and ancestry.com also offers low - cost genotyping - based dna tests .    in this paper", ", we focus a popular test offered by many dtc companies , namely , genetic relatedness test ( grt ) .", "this is used to identify whether or not a pair of individuals are closely related , genetically speaking . the standard approach to relative identification is to detect the identity - by - descent ( ibd ) segments between the individuals , and further identify the degree of relatedness via the amount of shared ibd segments  @xcite .", "the quantity of shared ibd segments is then detected from the phased haplotypes ( note a haplotype is a group of genes within an organism that was inherited together from a single parent ) of ( a pair of ) individuals , which are the specific groups of genes that a progeny inherits from one parent and consists of a fixed number of single nucleotide polymorphisms ( snps are the most common form of dna variation occurring when a single nucleotide differs between members of the same species or paired chromosomes of an individual  @xcite ) .", "nevertheless , grt services available today require individuals to send their genetic data ( in plaintext ) to possibly untrusted dtc companies . furthermore , collected genetic data is often impossible to anonymize  @xcite and hard to protect from intentional or accidental leakage .", "privacy risks from individual genetic exposure have been studied extensively  @xcite , thus motivating the need to design a grt algorithms that can operate without accessing genetic data in the clear and violating individuals privacy .", "we focus on genomic data that has already been phased into haplotypes and assume that the haplotypes of a pair of individuals with the same length are both interpreted by letters `` a , g , c , t '' , so that the problem of detecting ibd shared segments is reduced to the identification of the shared positions of two equal - length strings .", "dynamic programming can then be used to calculate shared positions , e.g. , edit distance , hamming distance and longest common subsequence ( lcs ) algorithms .", "overall , the research community has dedicated a lot of attention to genomic privacy , and proposed cryptographic techniques for privacy - preserving genetic testing  @xcite .", "prior work on secure ( two - party ) computation could also be used to protect privacy in relatedness test protocol building on traditional cryptographic primitives like homomorphic encryption ( e.g.  @xcite ) , private set intersection ( e.g.  @xcite ) , and garbled circuits based secure computation ( e.g.  @xcite ) .", "some existing two - party secure distance computation protocols could also come to help here , for example , private set intersection  @xcite , privacy - preserving approximating edit distance  @xcite based on garbled circuit and oblivious transfer , oblivious transfer based hamming distance system  @xcite , or a homomorphic computation of edit distance  @xcite .", "these two - party computation tools can be extended into a cloud - based context involving a trusted cloud server ( with fully access to an online database consisting of a number of individuals genetic data ) and a relatedness test service ( client ) with its own genetic information .", "the client could use a additively homomorphic encryption scheme , e.g. , paillier cryptosystem  @xcite , to encrypt a haplotype and send it ( along with the public key ) to the server .", "the server would then encrypt each haplotype in its database using the same encryption algorithm , `` subtract '' client s encryption by the encrypted haplotype , and return the results to the client .", "the latter could then decrypt and identify the quantity of shared positions by checking the number of 0 s decrypted .", "two recent works  @xcite open a new perspective for privacy - friendly grt by using fuzzy encryption technique . in these systems ,", "each individual first compresses its haplotype into a 0/1 string , called private genome sketch , and then `` encrypts '' the sketch by using a random row of a given error correct code matrix .", "one may detect if user @xmath0 is relative by downloading @xmath0 s encrypted sketch , and next `` decrypting '' the sketch with its own private genome sketch .", "if the decryption closely leads to a row in the matrix , the haplotypes of both individuals are approximately matched . however ,", "all the aforementioned computation approaches do not really scale well in practice , as they all suffer from an important limitation , i.e. , the client is burdened with heavy computation and communication overhead , as it has to download all related `` encrypted '' results from the server and perform a huge numbers of decryption to identify the relatedness .", "how to design scalable privacy - preserving grt that can scale on both server and client side constitutes the main motivation for our work .", "this paper presents a novel , efficient privacy - preserving genetic relatedness test ( ppgrt ) protocol that relies on the cloud s computational power . generally speaking", ", we allow the cloud to perform grt by only given an encrypted genetic database and an encrypted personal haplotype , as opposed to the traditional context where the cloud is able to fully access the database . before instantiating the protocol", ", we first discuss how ppgrt protocol can be generically constructed on top of a public key searchable encryption ( e.g.  @xcite ) and a symmetric key encryption cryptosystems .", "next , we propose a concrete construction building on searchable encryption technique  @xcite . specifically , we encode the haplotypes stored on the cloud server as a bunch of search trapdoors , and the haplotype uploaded by a test issuer ( i.e. , the client ) is interpreted as a set of search indices .", "an ibd shared segment detection is then reduced to a searchable matching problem , i.e. , finding a match indicating a shared segment .", "note that the method does not leak information to cloud server even if the server knows the length of shared segment as long as the secrecy of haplotypes is guaranteed .", "it is worth mentioning that the paper is the first to explore searchable encryption technique into relatedness test .", "we hope this work will inspire a new perspective for the future genomic privacy research .", "we assume a genetic data sequence is represented as a haplotype consisting of five basic letters @xmath1 , where @xmath2 denotes a piece of unknown / unmarked information , and assume the length of each letter is @xmath3 . as illustrated in fig .", "[ fig:1 ] , there are four parties in a genetic relatedness test ( grt ) system , which mirrors the one introduced by ayday et al . in  @xcite , specifically :    1 .", "* system users ( sus ) . * system users plain genetic information are collected and phased into haplotypes by a trusted and certified institution , e.g. local health / medical center .", "* certified institution ( ci ) . *", "a trusted ci collects system users plain genetic data to form a genome database ( @xmath4 ) , and encrypts @xmath4 to an @xmath5 .", "later , ci outsources @xmath5 to a storage and processing unit in which genetic relatedness testing is performed .", "3 .   * storage and processing unit ( spu ) . *", "a spu stores @xmath5 locally and meanwhile , it is mainly responsible for running relatedness test as well as returning the test result to a test issuer .", "note that spu knows nothing about the underlying data stored in @xmath5 .", "* test issuer ( ti ) . *", "a ti encrypts its phased haplotype via an _ asymmetric _ key encryption mechanism , and sends the encryption to the spu . after the test", ", the spu returns the result to the ti without learning any ( underlying haplotype ) information .        * algorithms . *", "the system consists of the following algorithms :    1 .", "@xmath6 : on input a security parameter @xmath7 , output public parameter and secret key for the system .", "@xmath8 : on input secret key @xmath9 , public parameter @xmath10 and a plain genome database , output an encrypted genome database @xmath5 .", "we assume that @xmath4 consists of a list of plain haplotypes , i.e. @xmath11 , and @xmath5 is a list of index / encrypted haplotype pairs , i.e. @xmath12 , where @xmath13 , and @xmath14 is the total number of haplotypes .", "@xmath15 : on input public parameter @xmath10 and a plain haplotype information , output an encrypted haplotype @xmath16 .", "@xmath17 : on input public parameter @xmath10 , @xmath5 and an encrypted query , output a set @xmath18 whereby the set includes @xmath14 test results between an encrypted query @xmath16 and each encrypted haplotype in @xmath5 .", "we highlight that privacy - preserving genetic relatedness testing ( ppgrt ) can be generically constructed from a public key based searchable encryption ( pkse ) and a symmetric encryption scheme . using  @xcite s definition for pkse , and given a pkse ( @xmath19 , @xmath20 , @xmath21 ( note we assume that randomness is already taken in algorithms @xmath22 and @xmath23 , so that each trapdoor and each index `` look '' uniformly random in the view of spu . ) , @xmath24 ) , a symmetric encryption ( @xmath25 , @xmath26 , @xmath27 ) , we can build a generic ppgrt construction as follows :    1 .", "@xmath28 : run @xmath29 algorithm to generate ( @xmath30 , @xmath31 ) , and @xmath32 algorithm to generate @xmath33 , and set @xmath34 and @xmath31 to be @xmath9 and @xmath10 , respectively .", "@xmath35 : run @xmath36 algorithm @xmath14 times to encrypt each plaintext haplotype @xmath37 to become an encrypted value @xmath38 , and further run @xmath39 algorithm to build a searchable trapdoor @xmath40 for each letter @xmath41 in a haplotype @xmath37 , where @xmath42 $ ] .", "we have @xmath43 , @xmath44 .", "@xmath45 : run @xmath46 to generate a search index @xmath47 for each letter @xmath48 in the haplotype @xmath49 , and further set @xmath50 , where @xmath51 $ ] .", "@xmath52 : given two - equal length `` sequences '' @xmath53 and @xmath54 , the calculation of shared ( positions ) length is reduced to the counting of the number of `` 0 '' output by the algorithm @xmath55 .", "some dynamic distance programming algorithms ( e.g. hamming / edit distance ) can be used here . by intaking an index sequence @xmath16 and a trapdoor sequence @xmath56 ( with equal length ) , for @xmath57 to @xmath58 , the algorithm checks the output of @xmath59 . if the algorithm outputs 0 , indicating there is a mismatch for the corresponding @xmath60-th position ( of both sequences ) , we add 1 to the distance . we calculate the shared length ( between @xmath16 and @xmath38 ) by subtracting @xmath58 with the distance , and store the result as an @xmath61-th tuple in @xmath18 .", "after @xmath14 rounds of the test , output @xmath18 .    * ppgrt security .", "* a secure ppgrt system needs to guarantee that : ( 1 ) @xmath5 must not leak the underlying encrypted sensitive haplotype data to spu , ( 2 ) spu can not learn the underlying haplotype embedded in the search index @xmath56 , and ( 3 ) the underlying haplotype can not be compromised from the test result @xmath18 by spu .    under the assumption that the underlying symmetric key encryption is chosen plaintext secure , and the underlying pkse is secure against chosen keyword attacks with trapdoor privacy  @xcite , the above generic construction is secure .", "since the generic construction ppgrt consists of two parts , namely encrypted data and encrypted search index structure , its security mainly depends on two aspects : one is the security of the underlying symmetric key encryption , and the other one is the security of the pkse .", "the chosen plaintext security of the symmetric encryption guarantees the secrecy of the underlying haplotype .", "the security holding against chosen keyword attacks is used to ensure the query haplotype search index ( issued by ti ) to be hidden from the view of search server .", "the trapdoor privacy of the pkse is to protect the haplotype embedded in the searchable trapdoor from being known by the `` curious '' server .", "the security of the whole ppgrt construction can be proved by following the universally composable model ( introduced in  @xcite ) with the above security assumption .", "below we review the paillier encryption .", "please refer to @xcite for more definition and technical details .", "we let @xmath62 be a safe number , @xmath63 be the set of elements of order @xmath64 , @xmath65 be their disjoint union for @xmath66 , where @xmath67 , @xmath68 , @xmath69 are large primes , carmichael s function @xmath70 , and set @xmath71 to be @xmath72 .", "suppose @xmath73 is a base of @xmath65 , and set @xmath74 .", "in paillier encryption system , we set @xmath75 as public parameters , and @xmath72 as secret key .", "the encryption algorithms works as @xmath76 where @xmath77 is the ciphertext , @xmath78 is the message , @xmath79 is a random value , and @xmath80 , @xmath81 .", "the decryption algorithm runs as @xmath82 where the ciphertext @xmath83 .", "the paillier encryption supports the following homomorphic properties : @xmath84      following the generic construction , we now build a concrete system for ppgrt ( via the usage of lcs ) on top of a single keyword equality variant of the searchable encryption scheme  @xcite . in the construction ,", "we revise the lcs algorithm ( note our lcs algorithm is revised from the one given in http://introcs.cs.princeton.edu/java/96optimization/lcs.java . ) so that it only outputs the shared length between a pair of encrypted haplotypes but not the lcs sequence .", "* * setup(@xmath85 ) : * the ci chooses two target collision resistant hash functions @xmath86 ,", "@xmath87 , @xmath88 , a cyclic group @xmath89 with order @xmath90 ( @xmath91 is a base of the group , the computation is based on the modulus @xmath90 ) , sets @xmath92 , @xmath93 , where @xmath94 are parameters in paillier encryption ( please refer to the previous section and  @xcite for more details ) , and euler s totient function @xmath95 , and @xmath96 . the ci sets system public key as @xmath97 , sets its secret key as @xmath98 , and sends spu a public key tuple @xmath99 . * * genedb(@xmath9 , @xmath10 , @xmath4 ) : * before delivering a @xmath4 to an spu , the ci encrypts it as follows .", "below we use @xmath100 to denote a plain haplotype .", "1 .   for @xmath101 to @xmath102 ,", "the ci runs as 1 .", "given @xmath10 and a haplotype @xmath100 with length @xmath103 , the ci works as follows .", "to randomize each letter , for each @xmath104 ( @xmath105 $ ] ) , the ci randomly chooses new random @xmath106 , and sets @xmath107 and @xmath108 .", "the haplotype s encrypted index sequence @xmath56 now is represented by a set of @xmath109 in which @xmath110 . for convenience , we use a notation @xmath111 later , where @xmath112 , @xmath113 stores random factors @xmath114 and @xmath115 is with a set of random @xmath116 .", "note that ci will choose a pair of distinct random factors ( @xmath117 , @xmath118 ) for each letter so as to avoid the case where the repeated letters share with the same random pair .", "for example , for a two - bit string @xmath119 , ci chooses random factors @xmath120 and @xmath121 for the first and second @xmath0 , respectively .", "the ci also encrypts @xmath100 by using a symmetric key encryption system @xmath122 , @xmath123 , @xmath124 .", "it sets @xmath125 , and computes @xmath126 .", "we suppose the length of @xmath33 is identical to that of @xmath100 , such that the symmetric encryption is a perfect one - time pad to @xmath100 .", "the ci finally outputs an @xmath127 . *", "* genquery(@xmath10 , @xmath49 ) : * before sending its haplotype sequence @xmath49 to the spu for relatedness test , the ti encrypts the sequence as follows .", "suppose @xmath128 , and @xmath129 .", "the ti sets @xmath130 , and interprets the sequence as @xmath131 , where @xmath132 , @xmath133 is a paillier encryption for @xmath134 , and @xmath135 .", "each @xmath136 is hidden ( with uniformly random value @xmath137 ) by the paillier probabilistic encryption . *", "* test(@xmath138 , @xmath5 , @xmath16 ) : * given an encrypted @xmath5 and an encrypted haplotype sequence @xmath16 , the spu first extracts a tuple @xmath139 of an encrypted haplotype @xmath140 from the @xmath5 , where @xmath141 $ ] and @xmath142 . from @xmath143 to @xmath14 , the spu runs the algorithm @xmath144 ( see the privacy - preserving longest common subsequence algorithm  [ a1 ] ) , and finally outputs the length of the shared positions between @xmath140 and @xmath16 . note that we use @xmath145 $ ] and @xmath146 $ ] to denote the @xmath61-th element in array @xmath147 and @xmath148 , respectively .", "note that the lcs algorithm here is revised from the one shared in princeton java library ( http://introcs.cs.princeton.edu/java/96optimization/lcs.java . ) .", "+ int @xmath149 $ ] ; + for @xmath150 to @xmath103 do @xmath151 @xmath152=0 $ ] ; @xmath153 + for @xmath154 to @xmath155 do @xmath151 @xmath156=0 $ ] ; @xmath153 + for @xmath101 to @xmath103 do @xmath151 for @xmath57 to @xmath155 do @xmath151 + if ( ) @xmath151 @xmath157 = 1 + w[i-1 , j-1]$ ] ; @xmath153 + else if @xmath158 \\ge w[i , j-1 ] ) $ ] @xmath151 @xmath157= w[i-1 , j]$ ] ; @xmath153 + else @xmath151 @xmath157= w[i , j-1]$ ] ; @xmath153 + @xmath153 + @xmath153 + return @xmath159 $ ] ;    since the correctness mainly follows that of  @xcite , we can have the check below .", "one may verify the equality of @xmath160^{\\beta \\cdot b_z[i]}\\ mod\\ n^2)==s_z[i]\\ ( mod\\ n)\\ ) $ ] to see if the letter @xmath161 in @xmath146 $ ] is equal to the letter @xmath136 in the encryption @xmath162 $ ] as in  @xcite . since the decryption algorithm of paillier encryption system can be represented as @xmath163^\\xi)^{\\lambda}\\ mod\\ n^2)}{l(g^{\\lambda } \\", "mod\\ n^2 ) } \\", "( mod\\ n)$ ] , we have @xmath164^\\xi)^{\\lambda}\\ mod\\ n^2)\\ ( mod\\ n)$ ] . set @xmath165 , we have @xmath166^{\\rho_i   \\sigma   h_0(\\eta_i p)})^{\\lambda}\\ mod\\ n^2)\\ ( mod\\ n)\\\\ & \\rho_i   h(x_j ) h_0(\\eta_i p ) \\sigma", "l(g^{\\lambda } \\", "mod\\ n^2 )   =   l(c[j]^{\\rho_i \\sigma   h_0(\\eta_i p ) \\lambda}\\ mod\\ n^2)\\ ( mod\\ n)\\\\ & \\rho_i   h(x_j ) h_0(\\eta_i p )", "\\gamma   =   l(c[j]^{b[i ] \\beta}\\ mod\\ n^2)\\ ( mod\\ n)\\end{split}\\ ] ] if the letter @xmath167 ( on the left hand side of the equation ) is equal to the letter @xmath136 embedded in the encryption @xmath162 $ ] , we then definitely have @xmath168^{b[i ] \\beta}\\ mod\\ n^2)\\ ( mod\\ n)$ ] , so that @xmath169   =   l(c[j]^{b[i ] \\beta}\\ mod\\ n^2)$ ] @xmath170 .", "we note that the equation can be checked by spu without knowing @xmath9 .", "the concrete construction above is secure assuming the underlying symmetric encryption is secure , the paillier encryption is secure and @xcite is secure with trapdoor privacy in the indistinguishability of ciphertext from random game .", "we note that the details of the proof follow that of  @xcite .      in the previous section ,", "a basic concrete construction for our generic ppgrt is built based on the searchable encryption scheme  @xcite .", "it is not fully secure yet because of suffering from deterministic identifier and offline keyword guessing attacks .", "below we present some solutions to tackle the attacks and meanwhile , we show that the construction can be extended to friendly support edit and hamming distance algorithms .      from the construction of the haplotype encrypted index sequence", ", we can see that @xmath171 and @xmath172 are constructed as @xmath173 and @xmath174 , respectively .", "recall that the tuple ( @xmath171 , @xmath172 ) is given to the spu . here , a malicious spu can easily obtain @xmath175 . taking two distinct @xmath176 and @xmath177 ,", "the malicious spu can compute @xmath178 to recover @xmath179 .", "with knowledge of @xmath179 , it may correctly guess the haplotype sequence , since the message space ( @xmath180 ) is relatively small in our context . to prevent the attack ,", "the ci may choose to add random factor into the hash function .", "it can choose a random value , say @xmath181 , so that @xmath182 , where @xmath183 is a new target collision resistance hash function so that @xmath184 .", "now , even being able to achieve @xmath185 , the malicious spu may not easily guess what is the input of the hash function .", "this naive solution , however , incurs another deterministic issue .", "while the input of the hash function is identical - meaning that a snps letter is repeated , the hash output leads to the same value , for example , @xmath186 .", "in such a case , the malicious spu may make use of some statistical analysis ( e.g. , 70% of the repetition occurs for the letter @xmath0 or @xmath16 ) to reveal the whole haplotype sequence .", "a better countermeasure is to bring more random factors into the construction of @xmath172 .", "specifically , the ci may set @xmath187 , @xmath188 , where @xmath189 , @xmath190 and @xmath191 .", "note the @xmath56 now includes ( @xmath113 , @xmath115 , @xmath192 ) , where @xmath192 is the set of all @xmath193 .", "accordingly , the equality check needs to be revised as @xmath160^{\\beta \\cdot b_z[i]+ k_z[i]}\\ mod\\ n^2)==s_z[i]\\ ( mod\\ n)\\ ) $ ] .", "one may check the correctness by setting @xmath194 .", "note we will talk about the random factor @xmath172 later .", "since the message space of haplotype is relative small , @xmath1 , it is extremely important for ci to produce randomly indistinguishable index sequence in the privacy point of view .", "recall that this index sequence can be exactly seen as a searchable trapdoor ( in our generic ppgrt construction ) .", "therefore , the privacy of the index sequence is now reduced to the trapdoor privacy .", "there have been some research works that introduce effective methods to tackle the trapdoor privacy issue , e.g.  @xcite . the crucial idea behind trapdoor privacy is to disable spu s ability of telling / identifying the relationship between ( any ) two given trapdoors ( with respective keywords inside ) .", "it is much like the anonymity feature in the context of identity - based encryption .", "the premise of doing so is to ensure a fresh randomness to each trapdoor .", "we here recommend readers to take a public key - based searchable encryption system with trapdoor privacy as an input building block to our generic ppgrt `` compiler '' , while attempting to construct a concrete scheme , so as to guarantee the privacy of haplotype encrypted index sequence .", "int @xmath195 ;    if @xmath196 @xmath197 @xmath198 @xmath151 _ please input equal length segments .", "_ @xmath153 //recall that @xmath199 which is the test issuer s encrypted sequence , while @xmath142 which is the genomic index sequence , and @xmath147 is the random factor sequence .", "else for @xmath101 to @xmath103 do @xmath151    if ( ) @xmath151 @xmath200 ; @xmath153    @xmath153    return @xmath201 ;      since the paillier encryption algorithm is publicly known , the spu may launch offline keyword dictionary attack to guess the information of the haplotypes stored in its database . specifically , the spu can randomly choose a haplotype sequence @xmath202 , and further runs the algorithm @xmath203 to generate an encrypted sequence @xmath204 . by running the algorithm @xmath55 with input @xmath204 and a client s haplotype - hidden index @xmath56 stored in @xmath5 , the spu may learn how similarity the two sequences share with .", "accordingly , the client s haplotype information definitely suffers from a leakage risk . to thwart this possible attack ,", "our system allows the ci to share a secret information @xmath205 , being seen as an extra input for the hash function , with the ti via a well - studied and efficient diffie - hellman key exchange protocol ( in des way ) with ebc mode , so that the ti will construct each encrypted element @xmath133 ( of the corresponding letter @xmath136 ) as @xmath206 .", "accordingly , the secret information @xmath205 has to be put in the index element @xmath172 by the ci as well . the spu will fail to launch effective offline keyword dictionary attack , since it can not compute a valid hash value without knowledge of the shared secret @xmath205 .", "the key exchange protocol above can be seen as a permission granted from the ci , so that the ti can proceed to the test phase .", "imagine that in practice , a public tester usually needs to request a test permission from the edb owner , i.e. the ci , even the edb is outsourced to the spu .", "if the ci grants the permission , it will share the secret information with the ti ; otherwise , nothing will be shared .    to keep consistency with the revision introduced in the previous subsection , a random factor @xmath205 is replaced with a @xmath207 , where @xmath208 and @xmath209 $ ] .", "we note that there is no need for the ci and the ti to share with the whole vector @xmath210 in the exchange protocol .", "for example , the ci may leverage the equation @xmath211 to calculate the value @xmath172 , where @xmath212 and @xmath167 is some `` common '' information that the ti knows as well ( e.g. @xmath213 , where @xmath214 is the current position of the letter @xmath136 on the sequence ) . in this case", ", the ci will only need to share @xmath215 with the ti in the key exchange protocol .", "int @xmath216 ; int @xmath217 ; int @xmath218 ;    if ( @xmath219 ) @xmath151 @xmath220 ; @xmath153    else @xmath151 @xmath221 ; @xmath153    int [ ] [ ] @xmath222 = new int [ @xmath223 [ @xmath224 ;    for @xmath150 to @xmath225 do @xmath151 @xmath226[0]=i$ ] ; @xmath153    for @xmath154 to @xmath227 do @xmath151 @xmath228[j]=j$ ] ; @xmath153    for @xmath150 to @xmath229 do    for @xmath154 to @xmath230 do @xmath151    if ( ) @xmath151 @xmath231[j+1]=dp[i][j]$ ] ; @xmath153    else @xmath151    int @xmath232[j]+1 $ ] ;    int @xmath233[j+1]+1 $ ] ;    int @xmath234[j]+1 $ ] ;    int @xmath235 ;    @xmath236 ;    @xmath231[j+1 ] = min$ ] ;    @xmath153    @xmath153    return @xmath237[len2]$ ] ;      we state that our concrete construction is naturally compatible with edit distance and hamming distance algorithms .", "the intuition is to see the equation highlighted in the box in algorithm  [ a1 ] as a condition for distance counting  that is , if the equation does not hold ( indicating a mismatch ) , we proceed to a distance adding operation  and , meanwhile , the final output is set to only show the shared length of two given strings .", "the privacy - preserving hamming distance algorithm is shown in  algorithm  [ a3 ] , in which we assume that each segment of the test issuer s encrypted sequence ( being taken into the algorithm ) is with the same length of the genomic index .", "the privacy - preserving variant for edit distance algorithm is presented in  algorithm  [ a2 ] .", "we note that the output of our privacy - preserving edit distance algorithm is somewhat different from that of the original edit distance algorithm  outputting the number of the `` difference '' of two strings .    [", "cols=\"^,^\",options=\"header \" , ]     we further assume that there is a pair of haplotypes , one encrypted by ci , and the other encrypted by ti : both encryptions will be sent to spu for two groups of test - one group of test is for original ( non - privacy - preserving ) lcs , edit and hamming distance algorithms , and the other is for privacy - preserving ones .", "note that both test groups will intake the identical haplotype sample .", "our pair of haplotypes are in the `` a , g , c , t '' format with the length 36@xmath238500 letter bits ( note in this paper we refer one letter to as a letter bit , e.g. `` ag '' are two letter bits ) . for matching 1000 genomes data format , we cut a total 18,000 letter bits haplotype into 500 segments of which contains 36 letter bits .", "note that the running times of each algorithm are averaged over 100 executions .", "the running time of non - privacy - preserving distance algorithms is depicted in figure  [ fg2 ] .", "it can be seen that hamming distance algorithm significantly outperforms edit distance algorithm , while the lcs suffers from the worst performance .", "the running time test here appropriately show the efficiency of the three algorithms .", "the running time of privacy - preserving hamming distance algorithm ( in figure  [ fg3 ] ) only takes approximately 900s to finish the test by intaking two encrypted haplotypes with 36@xmath238500 snps letter bits , while the privacy - preserving lcs and edit distance ( nearly overlapped ) requires over @xmath239s .", "accordingly , dealing with phased haplotype , it is better to leverage hamming distance algorithm as a secure building block for our ppgrt .", "we here state that the matching result of using privacy - preserving algorithms is identical to that of using original algorithms by inputing the same sample , 36@xmath238500 snps .", "note that please refer to our supplement test materials .", "we therefore conclude that the privacy - preserving algorithms maintain the exact test accuracy compared to the ones without any privacy protection .    however , the privacy - preserving algorithms , specially for edit distance and lcs , suffer from huge efficiency loss as a price to maintain privacy .", "this research work creates an interesting open problem on how to improve the test efficiency without loss of privacy .", "this research work introduced an interesting observation about how to generically construct a privacy preserving relatedness test protocol based on public key searchable encryption , and next proposed a concrete construction as well as its performance evaluation .", "the evaluation showed that our construction built on top of hamming distance algorithm is very efficient , while the designs for edit distance and lcs suffer from efficiency loss .", "the simulation also leverages real - world genetic database ( 1000 genomes ) to show the test accuracy of the system .", "the efficiency improvement of the construction will be seen as a future work .", "this paper also leave the academic and industrial communities followings interesting open problems - to improve the efficiency , can we make use of a symmetric searchable encryption to achieve the same grt test function without loss of privacy ; is it possible to directly use a normal but not reversed searchable encryption technique in our context ; how can we protect the haplotype ( encrypted and outsourced to spu ) even in the case where spu colludes with a group of tis ; how does ci `` update '' edb ( which is outsourced to spu ) but also `` control '' which parts of the edb to be `` performed '' test tasks by the spu ; what if there are different encryption formats in the back - end of the spu , how does it perform efficient and effective test .", "* acknowledgments . *", "the authors wish to thank alexandros mittos for presenting the paper at the genopri workshop .", "this work is supported by a google faculty award grant and eu project h2020-msca - itn `` privacy & us '' ( grant no ."], "abstract_text": ["<S> an increasing number of individuals are turning to direct - to - consumer ( dtc ) genetic testing to learn about their predisposition to diseases , traits , and/or ancestry . </S>", "<S> dtc companies like 23andme and ancestry.com have started to offer popular and affordable ancestry and genealogy tests , with services allowing users to find unknown relatives and long - distant cousins . </S>", "<S> naturally , access and possible dissemination of genetic data prompts serious privacy concerns , thus motivating the need to design efficient primitives supporting private genetic tests . in this paper , we present an effective protocol for privacy - preserving genetic relatedness test ( ppgrt ) , enabling a cloud server to run relatedness tests on input an encrypted genetic database and a test facility s encrypted genetic sample . </S>", "<S> we reduce the test to a data matching problem and perform it , `` privately '' , using searchable encryption . </S>", "<S> finally , a performance evaluation of hamming distance based pp - grt attests to the practicality of our proposals . </S>"], "labels": null, "section_names": ["introduction", "privacy-preserving genetic relatedness testing (ppgrt)", "conclusion"], "sections": [["over the past few years , advances in genomics and genome sequencing are not only enabling progress in medicine and healthcare , but are also bringing genetic testing to the masses , as an increasing number of `` direct to consumer '' ( dtc ) companies have entered , and sometimes disrupted , the market .", "for instance , 23andme.com offers a $ 99/129 assessment of inherited traits , genealogy , and congenital risk factors , through genotyping of a saliva sample posted via mail , and ancestry.com also offers low - cost genotyping - based dna tests .    in this paper", ", we focus a popular test offered by many dtc companies , namely , genetic relatedness test ( grt ) .", "this is used to identify whether or not a pair of individuals are closely related , genetically speaking . the standard approach to relative identification is to detect the identity - by - descent ( ibd ) segments between the individuals , and further identify the degree of relatedness via the amount of shared ibd segments  @xcite .", "the quantity of shared ibd segments is then detected from the phased haplotypes ( note a haplotype is a group of genes within an organism that was inherited together from a single parent ) of ( a pair of ) individuals , which are the specific groups of genes that a progeny inherits from one parent and consists of a fixed number of single nucleotide polymorphisms ( snps are the most common form of dna variation occurring when a single nucleotide differs between members of the same species or paired chromosomes of an individual  @xcite ) .", "nevertheless , grt services available today require individuals to send their genetic data ( in plaintext ) to possibly untrusted dtc companies . furthermore , collected genetic data is often impossible to anonymize  @xcite and hard to protect from intentional or accidental leakage .", "privacy risks from individual genetic exposure have been studied extensively  @xcite , thus motivating the need to design a grt algorithms that can operate without accessing genetic data in the clear and violating individuals privacy .", "we focus on genomic data that has already been phased into haplotypes and assume that the haplotypes of a pair of individuals with the same length are both interpreted by letters `` a , g , c , t '' , so that the problem of detecting ibd shared segments is reduced to the identification of the shared positions of two equal - length strings .", "dynamic programming can then be used to calculate shared positions , e.g. , edit distance , hamming distance and longest common subsequence ( lcs ) algorithms .", "overall , the research community has dedicated a lot of attention to genomic privacy , and proposed cryptographic techniques for privacy - preserving genetic testing  @xcite .", "prior work on secure ( two - party ) computation could also be used to protect privacy in relatedness test protocol building on traditional cryptographic primitives like homomorphic encryption ( e.g.  @xcite ) , private set intersection ( e.g.  @xcite ) , and garbled circuits based secure computation ( e.g.  @xcite ) .", "some existing two - party secure distance computation protocols could also come to help here , for example , private set intersection  @xcite , privacy - preserving approximating edit distance  @xcite based on garbled circuit and oblivious transfer , oblivious transfer based hamming distance system  @xcite , or a homomorphic computation of edit distance  @xcite .", "these two - party computation tools can be extended into a cloud - based context involving a trusted cloud server ( with fully access to an online database consisting of a number of individuals genetic data ) and a relatedness test service ( client ) with its own genetic information .", "the client could use a additively homomorphic encryption scheme , e.g. , paillier cryptosystem  @xcite , to encrypt a haplotype and send it ( along with the public key ) to the server .", "the server would then encrypt each haplotype in its database using the same encryption algorithm , `` subtract '' client s encryption by the encrypted haplotype , and return the results to the client .", "the latter could then decrypt and identify the quantity of shared positions by checking the number of 0 s decrypted .", "two recent works  @xcite open a new perspective for privacy - friendly grt by using fuzzy encryption technique . in these systems ,", "each individual first compresses its haplotype into a 0/1 string , called private genome sketch , and then `` encrypts '' the sketch by using a random row of a given error correct code matrix .", "one may detect if user @xmath0 is relative by downloading @xmath0 s encrypted sketch , and next `` decrypting '' the sketch with its own private genome sketch .", "if the decryption closely leads to a row in the matrix , the haplotypes of both individuals are approximately matched . however ,", "all the aforementioned computation approaches do not really scale well in practice , as they all suffer from an important limitation , i.e. , the client is burdened with heavy computation and communication overhead , as it has to download all related `` encrypted '' results from the server and perform a huge numbers of decryption to identify the relatedness .", "how to design scalable privacy - preserving grt that can scale on both server and client side constitutes the main motivation for our work .", "this paper presents a novel , efficient privacy - preserving genetic relatedness test ( ppgrt ) protocol that relies on the cloud s computational power . generally speaking", ", we allow the cloud to perform grt by only given an encrypted genetic database and an encrypted personal haplotype , as opposed to the traditional context where the cloud is able to fully access the database . before instantiating the protocol", ", we first discuss how ppgrt protocol can be generically constructed on top of a public key searchable encryption ( e.g.  @xcite ) and a symmetric key encryption cryptosystems .", "next , we propose a concrete construction building on searchable encryption technique  @xcite . specifically , we encode the haplotypes stored on the cloud server as a bunch of search trapdoors , and the haplotype uploaded by a test issuer ( i.e. , the client ) is interpreted as a set of search indices .", "an ibd shared segment detection is then reduced to a searchable matching problem , i.e. , finding a match indicating a shared segment .", "note that the method does not leak information to cloud server even if the server knows the length of shared segment as long as the secrecy of haplotypes is guaranteed .", "it is worth mentioning that the paper is the first to explore searchable encryption technique into relatedness test .", "we hope this work will inspire a new perspective for the future genomic privacy research ."], ["we assume a genetic data sequence is represented as a haplotype consisting of five basic letters @xmath1 , where @xmath2 denotes a piece of unknown / unmarked information , and assume the length of each letter is @xmath3 . as illustrated in fig .", "[ fig:1 ] , there are four parties in a genetic relatedness test ( grt ) system , which mirrors the one introduced by ayday et al . in  @xcite , specifically :    1 .", "* system users ( sus ) . * system users plain genetic information are collected and phased into haplotypes by a trusted and certified institution , e.g. local health / medical center .", "* certified institution ( ci ) . *", "a trusted ci collects system users plain genetic data to form a genome database ( @xmath4 ) , and encrypts @xmath4 to an @xmath5 .", "later , ci outsources @xmath5 to a storage and processing unit in which genetic relatedness testing is performed .", "3 .   * storage and processing unit ( spu ) . *", "a spu stores @xmath5 locally and meanwhile , it is mainly responsible for running relatedness test as well as returning the test result to a test issuer .", "note that spu knows nothing about the underlying data stored in @xmath5 .", "* test issuer ( ti ) . *", "a ti encrypts its phased haplotype via an _ asymmetric _ key encryption mechanism , and sends the encryption to the spu . after the test", ", the spu returns the result to the ti without learning any ( underlying haplotype ) information .        * algorithms . *", "the system consists of the following algorithms :    1 .", "@xmath6 : on input a security parameter @xmath7 , output public parameter and secret key for the system .", "@xmath8 : on input secret key @xmath9 , public parameter @xmath10 and a plain genome database , output an encrypted genome database @xmath5 .", "we assume that @xmath4 consists of a list of plain haplotypes , i.e. @xmath11 , and @xmath5 is a list of index / encrypted haplotype pairs , i.e. @xmath12 , where @xmath13 , and @xmath14 is the total number of haplotypes .", "@xmath15 : on input public parameter @xmath10 and a plain haplotype information , output an encrypted haplotype @xmath16 .", "@xmath17 : on input public parameter @xmath10 , @xmath5 and an encrypted query , output a set @xmath18 whereby the set includes @xmath14 test results between an encrypted query @xmath16 and each encrypted haplotype in @xmath5 .", "we highlight that privacy - preserving genetic relatedness testing ( ppgrt ) can be generically constructed from a public key based searchable encryption ( pkse ) and a symmetric encryption scheme . using  @xcite s definition for pkse , and given a pkse ( @xmath19 , @xmath20 , @xmath21 ( note we assume that randomness is already taken in algorithms @xmath22 and @xmath23 , so that each trapdoor and each index `` look '' uniformly random in the view of spu . ) , @xmath24 ) , a symmetric encryption ( @xmath25 , @xmath26 , @xmath27 ) , we can build a generic ppgrt construction as follows :    1 .", "@xmath28 : run @xmath29 algorithm to generate ( @xmath30 , @xmath31 ) , and @xmath32 algorithm to generate @xmath33 , and set @xmath34 and @xmath31 to be @xmath9 and @xmath10 , respectively .", "@xmath35 : run @xmath36 algorithm @xmath14 times to encrypt each plaintext haplotype @xmath37 to become an encrypted value @xmath38 , and further run @xmath39 algorithm to build a searchable trapdoor @xmath40 for each letter @xmath41 in a haplotype @xmath37 , where @xmath42 $ ] .", "we have @xmath43 , @xmath44 .", "@xmath45 : run @xmath46 to generate a search index @xmath47 for each letter @xmath48 in the haplotype @xmath49 , and further set @xmath50 , where @xmath51 $ ] .", "@xmath52 : given two - equal length `` sequences '' @xmath53 and @xmath54 , the calculation of shared ( positions ) length is reduced to the counting of the number of `` 0 '' output by the algorithm @xmath55 .", "some dynamic distance programming algorithms ( e.g. hamming / edit distance ) can be used here . by intaking an index sequence @xmath16 and a trapdoor sequence @xmath56 ( with equal length ) , for @xmath57 to @xmath58 , the algorithm checks the output of @xmath59 . if the algorithm outputs 0 , indicating there is a mismatch for the corresponding @xmath60-th position ( of both sequences ) , we add 1 to the distance . we calculate the shared length ( between @xmath16 and @xmath38 ) by subtracting @xmath58 with the distance , and store the result as an @xmath61-th tuple in @xmath18 .", "after @xmath14 rounds of the test , output @xmath18 .    * ppgrt security .", "* a secure ppgrt system needs to guarantee that : ( 1 ) @xmath5 must not leak the underlying encrypted sensitive haplotype data to spu , ( 2 ) spu can not learn the underlying haplotype embedded in the search index @xmath56 , and ( 3 ) the underlying haplotype can not be compromised from the test result @xmath18 by spu .    under the assumption that the underlying symmetric key encryption is chosen plaintext secure , and the underlying pkse is secure against chosen keyword attacks with trapdoor privacy  @xcite , the above generic construction is secure .", "since the generic construction ppgrt consists of two parts , namely encrypted data and encrypted search index structure , its security mainly depends on two aspects : one is the security of the underlying symmetric key encryption , and the other one is the security of the pkse .", "the chosen plaintext security of the symmetric encryption guarantees the secrecy of the underlying haplotype .", "the security holding against chosen keyword attacks is used to ensure the query haplotype search index ( issued by ti ) to be hidden from the view of search server .", "the trapdoor privacy of the pkse is to protect the haplotype embedded in the searchable trapdoor from being known by the `` curious '' server .", "the security of the whole ppgrt construction can be proved by following the universally composable model ( introduced in  @xcite ) with the above security assumption .", "below we review the paillier encryption .", "please refer to @xcite for more definition and technical details .", "we let @xmath62 be a safe number , @xmath63 be the set of elements of order @xmath64 , @xmath65 be their disjoint union for @xmath66 , where @xmath67 , @xmath68 , @xmath69 are large primes , carmichael s function @xmath70 , and set @xmath71 to be @xmath72 .", "suppose @xmath73 is a base of @xmath65 , and set @xmath74 .", "in paillier encryption system , we set @xmath75 as public parameters , and @xmath72 as secret key .", "the encryption algorithms works as @xmath76 where @xmath77 is the ciphertext , @xmath78 is the message , @xmath79 is a random value , and @xmath80 , @xmath81 .", "the decryption algorithm runs as @xmath82 where the ciphertext @xmath83 .", "the paillier encryption supports the following homomorphic properties : @xmath84      following the generic construction , we now build a concrete system for ppgrt ( via the usage of lcs ) on top of a single keyword equality variant of the searchable encryption scheme  @xcite . in the construction ,", "we revise the lcs algorithm ( note our lcs algorithm is revised from the one given in http://introcs.cs.princeton.edu/java/96optimization/lcs.java . ) so that it only outputs the shared length between a pair of encrypted haplotypes but not the lcs sequence .", "* * setup(@xmath85 ) : * the ci chooses two target collision resistant hash functions @xmath86 ,", "@xmath87 , @xmath88 , a cyclic group @xmath89 with order @xmath90 ( @xmath91 is a base of the group , the computation is based on the modulus @xmath90 ) , sets @xmath92 , @xmath93 , where @xmath94 are parameters in paillier encryption ( please refer to the previous section and  @xcite for more details ) , and euler s totient function @xmath95 , and @xmath96 . the ci sets system public key as @xmath97 , sets its secret key as @xmath98 , and sends spu a public key tuple @xmath99 . * * genedb(@xmath9 , @xmath10 , @xmath4 ) : * before delivering a @xmath4 to an spu , the ci encrypts it as follows .", "below we use @xmath100 to denote a plain haplotype .", "1 .   for @xmath101 to @xmath102 ,", "the ci runs as 1 .", "given @xmath10 and a haplotype @xmath100 with length @xmath103 , the ci works as follows .", "to randomize each letter , for each @xmath104 ( @xmath105 $ ] ) , the ci randomly chooses new random @xmath106 , and sets @xmath107 and @xmath108 .", "the haplotype s encrypted index sequence @xmath56 now is represented by a set of @xmath109 in which @xmath110 . for convenience , we use a notation @xmath111 later , where @xmath112 , @xmath113 stores random factors @xmath114 and @xmath115 is with a set of random @xmath116 .", "note that ci will choose a pair of distinct random factors ( @xmath117 , @xmath118 ) for each letter so as to avoid the case where the repeated letters share with the same random pair .", "for example , for a two - bit string @xmath119 , ci chooses random factors @xmath120 and @xmath121 for the first and second @xmath0 , respectively .", "the ci also encrypts @xmath100 by using a symmetric key encryption system @xmath122 , @xmath123 , @xmath124 .", "it sets @xmath125 , and computes @xmath126 .", "we suppose the length of @xmath33 is identical to that of @xmath100 , such that the symmetric encryption is a perfect one - time pad to @xmath100 .", "the ci finally outputs an @xmath127 . *", "* genquery(@xmath10 , @xmath49 ) : * before sending its haplotype sequence @xmath49 to the spu for relatedness test , the ti encrypts the sequence as follows .", "suppose @xmath128 , and @xmath129 .", "the ti sets @xmath130 , and interprets the sequence as @xmath131 , where @xmath132 , @xmath133 is a paillier encryption for @xmath134 , and @xmath135 .", "each @xmath136 is hidden ( with uniformly random value @xmath137 ) by the paillier probabilistic encryption . *", "* test(@xmath138 , @xmath5 , @xmath16 ) : * given an encrypted @xmath5 and an encrypted haplotype sequence @xmath16 , the spu first extracts a tuple @xmath139 of an encrypted haplotype @xmath140 from the @xmath5 , where @xmath141 $ ] and @xmath142 . from @xmath143 to @xmath14 , the spu runs the algorithm @xmath144 ( see the privacy - preserving longest common subsequence algorithm  [ a1 ] ) , and finally outputs the length of the shared positions between @xmath140 and @xmath16 . note that we use @xmath145 $ ] and @xmath146 $ ] to denote the @xmath61-th element in array @xmath147 and @xmath148 , respectively .", "note that the lcs algorithm here is revised from the one shared in princeton java library ( http://introcs.cs.princeton.edu/java/96optimization/lcs.java . ) .", "+ int @xmath149 $ ] ; + for @xmath150 to @xmath103 do @xmath151 @xmath152=0 $ ] ; @xmath153 + for @xmath154 to @xmath155 do @xmath151 @xmath156=0 $ ] ; @xmath153 + for @xmath101 to @xmath103 do @xmath151 for @xmath57 to @xmath155 do @xmath151 + if ( ) @xmath151 @xmath157 = 1 + w[i-1 , j-1]$ ] ; @xmath153 + else if @xmath158 \\ge w[i , j-1 ] ) $ ] @xmath151 @xmath157= w[i-1 , j]$ ] ; @xmath153 + else @xmath151 @xmath157= w[i , j-1]$ ] ; @xmath153 + @xmath153 + @xmath153 + return @xmath159 $ ] ;    since the correctness mainly follows that of  @xcite , we can have the check below .", "one may verify the equality of @xmath160^{\\beta \\cdot b_z[i]}\\ mod\\ n^2)==s_z[i]\\ ( mod\\ n)\\ ) $ ] to see if the letter @xmath161 in @xmath146 $ ] is equal to the letter @xmath136 in the encryption @xmath162 $ ] as in  @xcite . since the decryption algorithm of paillier encryption system can be represented as @xmath163^\\xi)^{\\lambda}\\ mod\\ n^2)}{l(g^{\\lambda } \\", "mod\\ n^2 ) } \\", "( mod\\ n)$ ] , we have @xmath164^\\xi)^{\\lambda}\\ mod\\ n^2)\\ ( mod\\ n)$ ] . set @xmath165 , we have @xmath166^{\\rho_i   \\sigma   h_0(\\eta_i p)})^{\\lambda}\\ mod\\ n^2)\\ ( mod\\ n)\\\\ & \\rho_i   h(x_j ) h_0(\\eta_i p ) \\sigma", "l(g^{\\lambda } \\", "mod\\ n^2 )   =   l(c[j]^{\\rho_i \\sigma   h_0(\\eta_i p ) \\lambda}\\ mod\\ n^2)\\ ( mod\\ n)\\\\ & \\rho_i   h(x_j ) h_0(\\eta_i p )", "\\gamma   =   l(c[j]^{b[i ] \\beta}\\ mod\\ n^2)\\ ( mod\\ n)\\end{split}\\ ] ] if the letter @xmath167 ( on the left hand side of the equation ) is equal to the letter @xmath136 embedded in the encryption @xmath162 $ ] , we then definitely have @xmath168^{b[i ] \\beta}\\ mod\\ n^2)\\ ( mod\\ n)$ ] , so that @xmath169   =   l(c[j]^{b[i ] \\beta}\\ mod\\ n^2)$ ] @xmath170 .", "we note that the equation can be checked by spu without knowing @xmath9 .", "the concrete construction above is secure assuming the underlying symmetric encryption is secure , the paillier encryption is secure and @xcite is secure with trapdoor privacy in the indistinguishability of ciphertext from random game .", "we note that the details of the proof follow that of  @xcite .      in the previous section ,", "a basic concrete construction for our generic ppgrt is built based on the searchable encryption scheme  @xcite .", "it is not fully secure yet because of suffering from deterministic identifier and offline keyword guessing attacks .", "below we present some solutions to tackle the attacks and meanwhile , we show that the construction can be extended to friendly support edit and hamming distance algorithms .      from the construction of the haplotype encrypted index sequence", ", we can see that @xmath171 and @xmath172 are constructed as @xmath173 and @xmath174 , respectively .", "recall that the tuple ( @xmath171 , @xmath172 ) is given to the spu . here , a malicious spu can easily obtain @xmath175 . taking two distinct @xmath176 and @xmath177 ,", "the malicious spu can compute @xmath178 to recover @xmath179 .", "with knowledge of @xmath179 , it may correctly guess the haplotype sequence , since the message space ( @xmath180 ) is relatively small in our context . to prevent the attack ,", "the ci may choose to add random factor into the hash function .", "it can choose a random value , say @xmath181 , so that @xmath182 , where @xmath183 is a new target collision resistance hash function so that @xmath184 .", "now , even being able to achieve @xmath185 , the malicious spu may not easily guess what is the input of the hash function .", "this naive solution , however , incurs another deterministic issue .", "while the input of the hash function is identical - meaning that a snps letter is repeated , the hash output leads to the same value , for example , @xmath186 .", "in such a case , the malicious spu may make use of some statistical analysis ( e.g. , 70% of the repetition occurs for the letter @xmath0 or @xmath16 ) to reveal the whole haplotype sequence .", "a better countermeasure is to bring more random factors into the construction of @xmath172 .", "specifically , the ci may set @xmath187 , @xmath188 , where @xmath189 , @xmath190 and @xmath191 .", "note the @xmath56 now includes ( @xmath113 , @xmath115 , @xmath192 ) , where @xmath192 is the set of all @xmath193 .", "accordingly , the equality check needs to be revised as @xmath160^{\\beta \\cdot b_z[i]+ k_z[i]}\\ mod\\ n^2)==s_z[i]\\ ( mod\\ n)\\ ) $ ] .", "one may check the correctness by setting @xmath194 .", "note we will talk about the random factor @xmath172 later .", "since the message space of haplotype is relative small , @xmath1 , it is extremely important for ci to produce randomly indistinguishable index sequence in the privacy point of view .", "recall that this index sequence can be exactly seen as a searchable trapdoor ( in our generic ppgrt construction ) .", "therefore , the privacy of the index sequence is now reduced to the trapdoor privacy .", "there have been some research works that introduce effective methods to tackle the trapdoor privacy issue , e.g.  @xcite . the crucial idea behind trapdoor privacy is to disable spu s ability of telling / identifying the relationship between ( any ) two given trapdoors ( with respective keywords inside ) .", "it is much like the anonymity feature in the context of identity - based encryption .", "the premise of doing so is to ensure a fresh randomness to each trapdoor .", "we here recommend readers to take a public key - based searchable encryption system with trapdoor privacy as an input building block to our generic ppgrt `` compiler '' , while attempting to construct a concrete scheme , so as to guarantee the privacy of haplotype encrypted index sequence .", "int @xmath195 ;    if @xmath196 @xmath197 @xmath198 @xmath151 _ please input equal length segments .", "_ @xmath153 //recall that @xmath199 which is the test issuer s encrypted sequence , while @xmath142 which is the genomic index sequence , and @xmath147 is the random factor sequence .", "else for @xmath101 to @xmath103 do @xmath151    if ( ) @xmath151 @xmath200 ; @xmath153    @xmath153    return @xmath201 ;      since the paillier encryption algorithm is publicly known , the spu may launch offline keyword dictionary attack to guess the information of the haplotypes stored in its database . specifically , the spu can randomly choose a haplotype sequence @xmath202 , and further runs the algorithm @xmath203 to generate an encrypted sequence @xmath204 . by running the algorithm @xmath55 with input @xmath204 and a client s haplotype - hidden index @xmath56 stored in @xmath5 , the spu may learn how similarity the two sequences share with .", "accordingly , the client s haplotype information definitely suffers from a leakage risk . to thwart this possible attack ,", "our system allows the ci to share a secret information @xmath205 , being seen as an extra input for the hash function , with the ti via a well - studied and efficient diffie - hellman key exchange protocol ( in des way ) with ebc mode , so that the ti will construct each encrypted element @xmath133 ( of the corresponding letter @xmath136 ) as @xmath206 .", "accordingly , the secret information @xmath205 has to be put in the index element @xmath172 by the ci as well . the spu will fail to launch effective offline keyword dictionary attack , since it can not compute a valid hash value without knowledge of the shared secret @xmath205 .", "the key exchange protocol above can be seen as a permission granted from the ci , so that the ti can proceed to the test phase .", "imagine that in practice , a public tester usually needs to request a test permission from the edb owner , i.e. the ci , even the edb is outsourced to the spu .", "if the ci grants the permission , it will share the secret information with the ti ; otherwise , nothing will be shared .    to keep consistency with the revision introduced in the previous subsection , a random factor @xmath205 is replaced with a @xmath207 , where @xmath208 and @xmath209 $ ] .", "we note that there is no need for the ci and the ti to share with the whole vector @xmath210 in the exchange protocol .", "for example , the ci may leverage the equation @xmath211 to calculate the value @xmath172 , where @xmath212 and @xmath167 is some `` common '' information that the ti knows as well ( e.g. @xmath213 , where @xmath214 is the current position of the letter @xmath136 on the sequence ) . in this case", ", the ci will only need to share @xmath215 with the ti in the key exchange protocol .", "int @xmath216 ; int @xmath217 ; int @xmath218 ;    if ( @xmath219 ) @xmath151 @xmath220 ; @xmath153    else @xmath151 @xmath221 ; @xmath153    int [ ] [ ] @xmath222 = new int [ @xmath223 [ @xmath224 ;    for @xmath150 to @xmath225 do @xmath151 @xmath226[0]=i$ ] ; @xmath153    for @xmath154 to @xmath227 do @xmath151 @xmath228[j]=j$ ] ; @xmath153    for @xmath150 to @xmath229 do    for @xmath154 to @xmath230 do @xmath151    if ( ) @xmath151 @xmath231[j+1]=dp[i][j]$ ] ; @xmath153    else @xmath151    int @xmath232[j]+1 $ ] ;    int @xmath233[j+1]+1 $ ] ;    int @xmath234[j]+1 $ ] ;    int @xmath235 ;    @xmath236 ;    @xmath231[j+1 ] = min$ ] ;    @xmath153    @xmath153    return @xmath237[len2]$ ] ;      we state that our concrete construction is naturally compatible with edit distance and hamming distance algorithms .", "the intuition is to see the equation highlighted in the box in algorithm  [ a1 ] as a condition for distance counting  that is , if the equation does not hold ( indicating a mismatch ) , we proceed to a distance adding operation  and , meanwhile , the final output is set to only show the shared length of two given strings .", "the privacy - preserving hamming distance algorithm is shown in  algorithm  [ a3 ] , in which we assume that each segment of the test issuer s encrypted sequence ( being taken into the algorithm ) is with the same length of the genomic index .", "the privacy - preserving variant for edit distance algorithm is presented in  algorithm  [ a2 ] .", "we note that the output of our privacy - preserving edit distance algorithm is somewhat different from that of the original edit distance algorithm  outputting the number of the `` difference '' of two strings .    [", "cols=\"^,^\",options=\"header \" , ]     we further assume that there is a pair of haplotypes , one encrypted by ci , and the other encrypted by ti : both encryptions will be sent to spu for two groups of test - one group of test is for original ( non - privacy - preserving ) lcs , edit and hamming distance algorithms , and the other is for privacy - preserving ones .", "note that both test groups will intake the identical haplotype sample .", "our pair of haplotypes are in the `` a , g , c , t '' format with the length 36@xmath238500 letter bits ( note in this paper we refer one letter to as a letter bit , e.g. `` ag '' are two letter bits ) . for matching 1000 genomes data format , we cut a total 18,000 letter bits haplotype into 500 segments of which contains 36 letter bits .", "note that the running times of each algorithm are averaged over 100 executions .", "the running time of non - privacy - preserving distance algorithms is depicted in figure  [ fg2 ] .", "it can be seen that hamming distance algorithm significantly outperforms edit distance algorithm , while the lcs suffers from the worst performance .", "the running time test here appropriately show the efficiency of the three algorithms .", "the running time of privacy - preserving hamming distance algorithm ( in figure  [ fg3 ] ) only takes approximately 900s to finish the test by intaking two encrypted haplotypes with 36@xmath238500 snps letter bits , while the privacy - preserving lcs and edit distance ( nearly overlapped ) requires over @xmath239s .", "accordingly , dealing with phased haplotype , it is better to leverage hamming distance algorithm as a secure building block for our ppgrt .", "we here state that the matching result of using privacy - preserving algorithms is identical to that of using original algorithms by inputing the same sample , 36@xmath238500 snps .", "note that please refer to our supplement test materials .", "we therefore conclude that the privacy - preserving algorithms maintain the exact test accuracy compared to the ones without any privacy protection .    however , the privacy - preserving algorithms , specially for edit distance and lcs , suffer from huge efficiency loss as a price to maintain privacy .", "this research work creates an interesting open problem on how to improve the test efficiency without loss of privacy ."], ["this research work introduced an interesting observation about how to generically construct a privacy preserving relatedness test protocol based on public key searchable encryption , and next proposed a concrete construction as well as its performance evaluation .", "the evaluation showed that our construction built on top of hamming distance algorithm is very efficient , while the designs for edit distance and lcs suffer from efficiency loss .", "the simulation also leverages real - world genetic database ( 1000 genomes ) to show the test accuracy of the system .", "the efficiency improvement of the construction will be seen as a future work .", "this paper also leave the academic and industrial communities followings interesting open problems - to improve the efficiency , can we make use of a symmetric searchable encryption to achieve the same grt test function without loss of privacy ; is it possible to directly use a normal but not reversed searchable encryption technique in our context ; how can we protect the haplotype ( encrypted and outsourced to spu ) even in the case where spu colludes with a group of tis ; how does ci `` update '' edb ( which is outsourced to spu ) but also `` control '' which parts of the edb to be `` performed '' test tasks by the spu ; what if there are different encryption formats in the back - end of the spu , how does it perform efficient and effective test .", "* acknowledgments . *", "the authors wish to thank alexandros mittos for presenting the paper at the genopri workshop .", "this work is supported by a google faculty award grant and eu project h2020-msca - itn `` privacy & us '' ( grant no ."]]}
{"article_id": "1505.07417", "article_text": ["the lhcb experiment records millions of proton collision events every second .", "most of them are not needed for further analysis and are discarded by a sophisticated multi - layer trigger system @xcite . what is left amounts to @xmath0 events in run 1 .", "before physics analysis takes place , the number of events is further reduced by a factor of around 10 .", "this `` stripping '' process takes place after the full reconstruction of the events , and produces a set of a dozen `` streams '' of the analysis dataset . @xcite .", "those streams contain candidate events for different processes  identified by `` stripping lines . ''", "events that passed the stripping process are indexed by event index .    along the stripping lines", "some other information is indexed  global activity counters ( such as total number of tracks and hits in individual subdetectors ) , logical file names ( lfns ) on the grid , and run conditions database tags .", "event index consists of four primary parts : backend , which hosts the indexes and processes the queries ; frontend , which interacts with the user ; the grid collector for downloading events from the grid ; and an indexer for compiling the indexes .", "their relationship is expressed on the figure [ fig : arch ] .", "the principle component that stores events and handles queries is a 7-node cluster .", "each node hosts several shards .", "a shard is an apache lucene index .", "indexes are build from .root files using mapreduce with events being evenly distributed between the nodes .", "events are represented in backend in a problem - agnostic generic format .", "thus event index can be used on new datasets with minimal modification .", "event index is optimized for read - only indexes on a static hardware configuration .", "cluster expansion is still possible and can be accomplished in two ways .", "first , if both new data and new nodes are available , the data can be indexed on these nodes without changes to the existing structure .", "this approach may be suboptimal , as the best performance is only achieved when the data is evenly distributed among the nodes .", "second , if only nodes are added , we must either redistribute the existing shards between nodes or reindex the dataset to include them into the cluster .", "index splitting is possible but constitutes a highly experimental @xcite procedure with computational costs similar to that of reindexing .", "requests are handled by a java application as follows .", "any node can become a master node by virtue of initiating a request .", "* search request : a master node receives a query , sends it to all the nodes , each in turn sends it to its shards , shards run the query , and cache the resulting bitset .", "* partial search results retrieval : a master node receives a query , asks all the nodes for the results counts , determines the nodes to send the request to .", "nodes receiving the following request do the same with shards .", "the master node then gathers the responses and forwards them to the user .", "* field value aggregation : a master node receives a query , sends it to all the nodes , each in turn sends it to its shards , each shard aggregates the field values from the matching events .", "the master node aggregates the results and returns them to the user . *", "histogram calculation : a master node receives a query , sends it to all the nodes , each in turn sends it to its shards , each shard counts unique values of the requested fields , and returns them to the master node , which computes the resulting histogram .", "queries are transformed into lucene filters using a simple top - down parser for context - free grammar .", "it consists of two parts : the tokenizer and the parser itself .", "the tokenizer transforms a query string into a list of tokens ( @xmath1and , or , has ) and values", ". the parser uses the list to build the solution tree , using prefix notation to handle parentheses and substituting has and and for missing unary and binary operators .", "it then converts the tree into a lucene filter .", "indexing @xmath2 events took three days and 0.5 tb of hard drive space per node .", "the backend response times for various requests can be seen in figure [ fig : performance ] .", "this response was within 20 seconds for the majority of requests .", "the outliers are currently being investigated .", "all user interaction is done through the web interface , protected by cern single sign - on @xcite .", "queries can either be typed manually or constructed with the help of an interactive wizard .", "example searches :    * for a specific stripping line : + * by file location : + `` lfn = lfn:/lhcb / lhcb / collision11/charmtobeswum.dst/ 00022760/0002/00022760_00029252_1.charmtobeswum.dst and stripping=20r1 '' * stripping line and npvs value : +    event index can compile a list of logical file names ( lfn ) containing the search results . if there are less than 1000 results , event index can fetch them from grid as a .root file and display them in the web browser using event display @xcite .", "users can also plot histograms for the global activity counters .", "the grid collector handles the .root file download requests .", "it resides on a dedicated server at cern .", "it uses lhcb dirac @xcite for retrieving event locations on the grid .", "then it launches parallel gaudi@xcite jobs for events retrieval and format conversion for event display .", "the source code is available on https://gitlab.cern.ch/ysda/grid_collector .", "event index is deployed into production on https://eindex.cern.ch/. data from strippnigs 20 , 20r1 , 21 , 21r1 is available .", "we are currently studying the needs of different groups in lhcb to make event index a better tool .", "plans include python api , mc and turbo stream @xcite indexing , and free form query processing .", "event index allows selection of events and viewing of histograms of basic properties in a matter of seconds .", "this is much faster than the current use of grid , which can take hours .", "event index s core architecture will allow it to scale with data and be used for different datasets .", "99 https://lucene.apache.org/ , apache software foundation head t , 2014 , the lhcb trigger system , _ jinst 9 c09015 _ doi:10.1088/1748 - 0221/9/09/c09015 charpentier p ( on behalf ofthe lhcb collaboration ) , the lhcb computing model and real data _ journal of physics : conference series , vol .  331 , num .  7 , chep-2010 _ lucene 5.1.0 misc api https://lucene.apache.org/core/5_1_0/misc/org/apache/lucene/index/indexsplitter.html ormancey e , 2007 , cern single sign on solution , _", "langenbruch  c , couturier  b , frank  m , a webgl event display for lhcb : status update , _", "5th lhcb computing workshop , may 18 , 2015 _ stagni  f , et al , lhcbdirac : distributed computing in lhcb , _ journal of physics : conference series , vol . 396 , num .", "3 , chep-2012 _ barrand  g et al . , gaudi - a software architecture and framework for building hep data processing applications , _", "computer physics communications 140 ( 2001 ) 4555 _ benson s , 2015 , the lhcb turbo stream _ chep-2015 _"], "abstract_text": ["<S> during lhc run 1 , the lhcb experiment recorded around @xmath0 collision events . </S>", "<S> this paper describes event index  an event search system . </S>", "<S> its primary function is to quickly select subsets of events from a combination of conditions , such as the estimated decay channel or number of hits in a subdetector . </S>", "<S> event index is essentially apache lucene @xcite optimized for read - only indexes distributed over independent shards on independent nodes . </S>"], "labels": null, "section_names": ["introduction", "architecture", "status", "future works", "summary", "references"], "sections": [["the lhcb experiment records millions of proton collision events every second .", "most of them are not needed for further analysis and are discarded by a sophisticated multi - layer trigger system @xcite . what is left amounts to @xmath0 events in run 1 .", "before physics analysis takes place , the number of events is further reduced by a factor of around 10 .", "this `` stripping '' process takes place after the full reconstruction of the events , and produces a set of a dozen `` streams '' of the analysis dataset . @xcite .", "those streams contain candidate events for different processes  identified by `` stripping lines . ''", "events that passed the stripping process are indexed by event index .    along the stripping lines", "some other information is indexed  global activity counters ( such as total number of tracks and hits in individual subdetectors ) , logical file names ( lfns ) on the grid , and run conditions database tags ."], ["event index consists of four primary parts : backend , which hosts the indexes and processes the queries ; frontend , which interacts with the user ; the grid collector for downloading events from the grid ; and an indexer for compiling the indexes .", "their relationship is expressed on the figure [ fig : arch ] .", "the principle component that stores events and handles queries is a 7-node cluster .", "each node hosts several shards .", "a shard is an apache lucene index .", "indexes are build from .root files using mapreduce with events being evenly distributed between the nodes .", "events are represented in backend in a problem - agnostic generic format .", "thus event index can be used on new datasets with minimal modification .", "event index is optimized for read - only indexes on a static hardware configuration .", "cluster expansion is still possible and can be accomplished in two ways .", "first , if both new data and new nodes are available , the data can be indexed on these nodes without changes to the existing structure .", "this approach may be suboptimal , as the best performance is only achieved when the data is evenly distributed among the nodes .", "second , if only nodes are added , we must either redistribute the existing shards between nodes or reindex the dataset to include them into the cluster .", "index splitting is possible but constitutes a highly experimental @xcite procedure with computational costs similar to that of reindexing .", "requests are handled by a java application as follows .", "any node can become a master node by virtue of initiating a request .", "* search request : a master node receives a query , sends it to all the nodes , each in turn sends it to its shards , shards run the query , and cache the resulting bitset .", "* partial search results retrieval : a master node receives a query , asks all the nodes for the results counts , determines the nodes to send the request to .", "nodes receiving the following request do the same with shards .", "the master node then gathers the responses and forwards them to the user .", "* field value aggregation : a master node receives a query , sends it to all the nodes , each in turn sends it to its shards , each shard aggregates the field values from the matching events .", "the master node aggregates the results and returns them to the user . *", "histogram calculation : a master node receives a query , sends it to all the nodes , each in turn sends it to its shards , each shard counts unique values of the requested fields , and returns them to the master node , which computes the resulting histogram .", "queries are transformed into lucene filters using a simple top - down parser for context - free grammar .", "it consists of two parts : the tokenizer and the parser itself .", "the tokenizer transforms a query string into a list of tokens ( @xmath1and , or , has ) and values", ". the parser uses the list to build the solution tree , using prefix notation to handle parentheses and substituting has and and for missing unary and binary operators .", "it then converts the tree into a lucene filter .", "indexing @xmath2 events took three days and 0.5 tb of hard drive space per node .", "the backend response times for various requests can be seen in figure [ fig : performance ] .", "this response was within 20 seconds for the majority of requests .", "the outliers are currently being investigated .", "all user interaction is done through the web interface , protected by cern single sign - on @xcite .", "queries can either be typed manually or constructed with the help of an interactive wizard .", "example searches :    * for a specific stripping line : + * by file location : + `` lfn = lfn:/lhcb / lhcb / collision11/charmtobeswum.dst/ 00022760/0002/00022760_00029252_1.charmtobeswum.dst and stripping=20r1 '' * stripping line and npvs value : +    event index can compile a list of logical file names ( lfn ) containing the search results . if there are less than 1000 results , event index can fetch them from grid as a .root file and display them in the web browser using event display @xcite .", "users can also plot histograms for the global activity counters .", "the grid collector handles the .root file download requests .", "it resides on a dedicated server at cern .", "it uses lhcb dirac @xcite for retrieving event locations on the grid .", "then it launches parallel gaudi@xcite jobs for events retrieval and format conversion for event display .", "the source code is available on https://gitlab.cern.ch/ysda/grid_collector ."], ["event index is deployed into production on https://eindex.cern.ch/. data from strippnigs 20 , 20r1 , 21 , 21r1 is available ."], ["we are currently studying the needs of different groups in lhcb to make event index a better tool .", "plans include python api , mc and turbo stream @xcite indexing , and free form query processing ."], ["event index allows selection of events and viewing of histograms of basic properties in a matter of seconds .", "this is much faster than the current use of grid , which can take hours .", "event index s core architecture will allow it to scale with data and be used for different datasets ."], ["99 https://lucene.apache.org/ , apache software foundation head t , 2014 , the lhcb trigger system , _ jinst 9 c09015 _ doi:10.1088/1748 - 0221/9/09/c09015 charpentier p ( on behalf ofthe lhcb collaboration ) , the lhcb computing model and real data _ journal of physics : conference series , vol .  331 , num .  7 , chep-2010 _ lucene 5.1.0 misc api https://lucene.apache.org/core/5_1_0/misc/org/apache/lucene/index/indexsplitter.html ormancey e , 2007 , cern single sign on solution , _", "langenbruch  c , couturier  b , frank  m , a webgl event display for lhcb : status update , _", "5th lhcb computing workshop , may 18 , 2015 _ stagni  f , et al , lhcbdirac : distributed computing in lhcb , _ journal of physics : conference series , vol . 396 , num .", "3 , chep-2012 _ barrand  g et al . , gaudi - a software architecture and framework for building hep data processing applications , _", "computer physics communications 140 ( 2001 ) 4555 _ benson s , 2015 , the lhcb turbo stream _ chep-2015 _"]]}
{"article_id": "1609.07589", "article_text": ["interference between wireless links has been taken into account as a critical problem in wireless communication systems .", "recently , interference alignment  ( ia ) was proposed for fundamentally solving the interference problem when there are two communication pairs  @xcite .", "it was shown in  @xcite that the ia scheme can achieve the optimal degrees of freedom  ( dof ) , which is equal to @xmath4 , in the @xmath1-user interference channel with time - varying channel coefficients . since then , interference management schemes based on ia have been further developed and analyzed in various wireless network environments : multiple - input multiple - output ( mimo ) interference networks  @xcite , x networks  @xcite , and cellular networks  @xcite .", "on the one hand , following up on these successes for single - hop networks , more recent and emerging work has studied multihop networks with multiple source - destination ( s  d ) pairs . for the 2-user 2-hop network with 2 relays ( referred to as the @xmath5 interference channel ) , it was shown in  @xcite that interference neutralization combining with symbol extension achieves the optimal dof .", "a more challenging network model is to consider @xmath1-user two - hop relay - aided interference channels , consisting of @xmath1 source - destination ( s  d ) pairs and @xmath2 helping relay nodes located in the path between s ", "d pairs , so - called the @xmath3 channel .", "several achievability schemes have been known for the network , but more detailed understanding is still in progress . by applying the result from  @xcite to the @xmath0 channel , one can show that @xmath4 dof is achieved by using orthogonalize - and - forward relaying , which completely neutralizes interference at all destinations if @xmath2 is greater than or equal to @xmath6 .", "another achievable scheme , called aligned network diagonalization , was introduced in  @xcite and was shown to achieve the optimal dof in the @xmath0 channel while tightening the required number of relays .", "the scheme in  @xcite is based on the real interference alignment framework  @xcite . in  @xcite", ", however , the system model under consideration assumes that there is no interfering signal between relays and the relays are full - duplex .", "moreover , in  @xcite , the @xmath5 interference channel with full - duplex relays interfering with each other was characterized and its dof achievability was shown using aligned interference neutralization .", "interference channel  @xcite . ]", "on the other hand , there are lots of results on the usefulness of fading in the literature , where one can obtain the multiuser diversity gain in broadcast channels : opportunistic scheduling  @xcite , opportunistic beamforming  @xcite , and random beamforming  @xcite .", "such opportunism can also be fully utilized in multi - cell uplink or downlink networks by using an opportunistic interference alignment strategy  @xcite .", "various scenarios exploiting the multiuser diversity gain have been studied in cooperative networks by applying an opportunistic two - hop relaying protocol  @xcite and an opportunistic routing  @xcite , and in cognitive radio networks with opportunistic scheduling  @xcite .", "in addition , recent results  @xcite have shown how to utilize the opportunistic gain when there are a large number of channel realizations .", "more specifically , to amplify signals and cancel interference , the idea of opportunistically pairing complementary channel instances has been studied in interference networks  @xcite and multi - hop relay networks  @xcite . in cognitive radio environments", "@xcite , opportunistic spectrum sharing was introduced by allowing secondary users to share the radio spectrum originally allocated to primary users via transmit adaptation in space , time , or frequency .      in this paper , we study the _", "@xmath0 channel with interfering relays _ , which can be taken into account as one of practical multi - source interfering relay networks and be regarded as a fundamentally different channel model from the conventional @xmath0 channel in  @xcite .", "then , we introduce an _ opportunistic network decoupling ( ond ) _ protocol that achieves full dof with comparatively easy implementation under the channel model .", "this work focuses on the @xmath0 channel with one additional assumption that @xmath2 _ half - duplex _ ( hd ) relays interfere with each other , which is a more feasible scenario .", "the scheme adopts the notion of the multiuser diversity gain for performing interference management over two hops . more precisely , in our scheme , a scheduling strategy is presented in time - division duplexing ( tdd ) two - hop environments with time - invariant channel coefficients , where a subset of relays is opportunistically selected in terms of producing the minimum total interference level . to improve the spectral efficiency , the _ alternate relaying _ protocol in  @xcite", "is employed with a modification , which eventually enables our system to operate in _ virtual full - duplex _ mode . as our main result ,", "it turns out that in a high signal - to - noise ratio ( snr ) regime , the ond protocol asymptotically achieves the min - cut upper bound of @xmath1 dof even in the presence of inter - relay interference and half - duplex assumption , provided the number of relays , @xmath2 , scales faster than @xmath7 , which is the minimum number of relays required to guarantee our achievability result .", "numerical evaluation also indicates that the ond protocol has higher sum - rates than those of other relaying methods under realistic network conditions ( e.g. , finite @xmath2 and snr ) since the inter - relay interference is significantly reduced owing to the opportunistic gain . for comparison , the ond scheme without alternate relaying and the max - min snr scheme are also shown as baseline schemes . note that our protocol basically operates with local channel state information ( csi ) at the transmitter and thus is suitable for distributed / decentralized networks .", "our main contributions are fourfold as follows :    * in the @xmath0 channel with interfering relays , we introduce a new achievability scheme , termed ond with virtual full - duplex operation .", "* under the channel model , we completely analyze the optimal dof , the required relay scaling condition , and the decaying rate of the interference level , where the ond scheme is shown to approach the min - cut upper bound on the dof . * our achievability result ( i.e.", ", the derived dof and relay scaling law ) is validated via numerical evaluation .", "* we perform extensive computer simulations with other baseline schemes .", "the rest of this paper is organized as follows . in section  [ sec:2 ] , we describe the system and channel models . in section  [ sec:3 ] , the proposed ond scheme is specified and its lower bound on the dof is analyzed .", "section  [ sec:4 ] shows an upper bound on the dof .", "numerical results of the proposed ond scheme are provided in section  [ sec:5 ] .", "finally , we summarize the paper with some concluding remarks in section  [ sec:6 ] .      throughout this paper , @xmath8 , @xmath9 $ ] , and @xmath10 indicate the field of complex numbers , the statistical expectation , and the ceiling operation , respectively . unless otherwise stated , all logarithms are assumed to be to the base 2 .", "moreover , table  [ tab : notations ] summarizes the notations used throughout this paper .", "some notations will be more precisely defined in the following sections , where we introduce our channel model and achievability results .", ".summary of notations [ cols=\"^,^ \" , ]", "as one of two - hop cooperative scenarios , we consider the @xmath11 channel model with interfering relays , which fits into the case where each s  d pair is geographically far apart and/or experiences strong shadowing ( thus requiring the response to a huge challenge for achieving the target spectral efficiency ) . in the channel model , it is thus assumed that each source transmits its own message to the corresponding destination only through one of @xmath2 relays , and thus there is no direct path between an s ", "d pair . note that unlike the conventional @xmath12 channel , relay nodes are assumed to interfere with each other in our model .", "there are @xmath1 s  d pairs , where each receiver is the destination of exactly one source node and is interested only in traffic demands of the source . as in the typical cooperative relaying setup ,", "@xmath2 relay nodes are located in the path between s ", "d pairs so as to help to reduce path - loss attenuations .", "suppose that each node is equipped with a single transmit antenna .", "each relay node is assumed to operate in half - duplex mode and to fully decode , re - encode , and retransmit the source message i.e. , decode - and - forward protocol is taken into account .", "we assume that each node ( either a source or a relay ) has an average transmit power constraint @xmath13 .", "unlike the work in  @xcite , @xmath2 relays are assumed to interfere with each other .", "channel can also be applied here . ] to improve the spectral efficiency , the alternate relaying protocol in  @xcite is employed with a modification . with alternate relaying ,", "each selected relay node toggles between the transmit and listen modes for alternate time slots of message transmission of the sources . if @xmath2 is sufficiently large , then it is possible to exploit the channel randomness for each hop and thus to obtain the opportunistic gain in multiuser environments . in this work", ", we do not assume the use of any sophisticated multiuser detection schemes at each receiver ( either a relay or a destination node ) , thereby resulting in an easier implementation .", "now , let us turn to channel modeling .", "let @xmath14 , @xmath15 , and @xmath16 denote the @xmath17th source , the corresponding @xmath17th destination , and the @xmath18th relay node , respectively , where @xmath19 and @xmath20 .", "the terms @xmath21 denote the channel coefficients from @xmath14 to @xmath16 and from @xmath16 to @xmath15 , corresponding to the first and second hops , respectively .", "the term @xmath22 indicates the channel coefficient between two relays @xmath16 and @xmath23 .", "all the channels are assumed to be rayleigh , having zero - mean and unit variance , and to be independent across different @xmath18 , @xmath17 , @xmath24 , and hop index @xmath25 .", "we assume the block - fading model , i.e. , the channels are constant during one block ( e.g. , frame ) , consisting of one scheduling time slot and @xmath26 data transmission time slots , and changes to a new independent value for every block .     channel with interfering relays . , scaledwidth=57.0% ]", "in this section , we describe the ond protocol , operating in virtual full - duplex mode , in the @xmath0 channel with interfering relays .", "then , its performance is analyzed in terms of achievable dof along with a certain relay scaling condition .", "the decaying rate of the interference level is also analyzed .", "in addition , the ond protocol with no alternate relaying and its achievability result are shown for comparison .      in this subsection", ", we introduce an ond protocol as the achievable scheme to guarantee the optimal dof of the @xmath12 channel with inter - relay interference , where @xmath27 relay nodes among @xmath2 candidates are opportunistically selected for data forwarding in the sense of producing a sufficiently small amount of interference level .", "the proposed scheme is basically performed by utilizing the channel reciprocity of tdd systems .", "suppose that @xmath28 and @xmath29 denote the indices of two relays communicating with the @xmath17th s ", "d pair for @xmath19 . in this case , without loss of generality , assuming that the number of data transmission time slots , @xmath26 , is an odd number , the specific steps of each node during one block are described as follows :    * time slot 1 : sources @xmath30 transmit their first encoded symbols @xmath31 , where @xmath32 represents the @xmath33th transmit symbol of the @xmath17th source node .", "a set of @xmath1 selected relay nodes , @xmath34 , operating in receive mode at each odd time slot , listens to @xmath31 ( note that a relay selection strategy will be specified later ) . other @xmath35 relay nodes and destinations", "@xmath36 remain idle .", "* time slot 2 : the @xmath1 sources transmit their encoded symbols @xmath37 .", "the @xmath1 selected relays in the set @xmath38 forward their first re - encoded symbols @xmath39 to the corresponding @xmath1 destinations .", "if the relays in @xmath38 successfully decode the corresponding symbols , then @xmath40 is the same as @xmath41 .", "another set of @xmath1 selected relay nodes , @xmath42 , operating in receive mode at each even time slot , listens to and decodes @xmath37 while being interfered with by @xmath43 .", "the @xmath1 destinations receive and decode @xmath39 from @xmath44 @xmath45 .", "the remaining @xmath46 relays keep idle . * time slot 3 : the @xmath1 sources transmit their encoded symbols @xmath47 .", "the @xmath1 relays @xmath48 forward their re - encoded symbols @xmath49 to the corresponding @xmath1 destinations .", "another @xmath1 relays in @xmath38 receive and decode @xmath50 @xmath51 while being interfered with by @xmath52 .", "the @xmath1 destinations receive and decode @xmath49 from @xmath53 @xmath54 .", "the remaining @xmath46 relays keep idle . *", "the processes in time slots 2 and 3 are repeated to the @xmath55th time slot .", "* time slot @xmath26 : the @xmath1 relays in @xmath56 forward their re - encoded symbols @xmath57 @xmath58 to the corresponding @xmath1 destinations", ". the @xmath1 sources and the other @xmath35 relays remain idle .    at each odd time slot @xmath33 ( i.e. , @xmath59 ) , let us consider the received signal at each selected relay for the first hop and the received signal at each destination for the second hop , respectively .    for the first hop ( phase 1 ) , the received signal @xmath60 at @xmath61 is given by @xmath62 where @xmath32 and @xmath63 are the @xmath33th transmit symbol of @xmath14 and the @xmath64th transmit symbol of @xmath65 , respectively . as addressed earlier ,", "if relay @xmath66 successfully decodes the received symbol , then it follows that @xmath67 .", "the received signal @xmath68 at @xmath61 is corrupted by the independent and identically distributed ( i.i.d . ) and circularly symmetric complex additive white gaussian noise ( awgn ) @xmath69 having zero - mean and variance @xmath70 . note that the second term in the right - hand side ( rhs ) of ( [ eq : y1 ] ) indicates the inter - relay interference , which occurs when the @xmath1 relays in the set @xmath38 , operating in receive mode , listen to the sources , the relays are interfered with by the other set @xmath56 , operating in transmit mode .", "note that when @xmath71 , relays have no symbols to transmit , and the second term in the rhs of ( [ eq : y1 ] ) becomes zero .", "similarly when @xmath72 , sources do not transmit symbols , and the first term in the rhs of ( [ eq : y1 ] ) becomes zero .", "for the second hop ( phase 2 ) , assuming that the @xmath1 selected relay nodes transmit their data packets simultaneously , the received signal @xmath73 at @xmath15 is given by @xmath74 where @xmath75 is the i.i.d .", "awgn having zero - mean and variance @xmath70 .", "we also note that when @xmath71 , there are no signals from relays .    likewise , at each even time slot ( i.e. , @xmath76 ) , the received signals at @xmath77 and @xmath15 ( i.e. , the first and second hops ) are given by @xmath78 and @xmath79 respectively .", "the illustration of the aforementioned ond protocol is geographically shown in fig .  [", "fig : dig ] ( two terms @xmath80 and @xmath81 are specified later in the following relay selection steps ) .", "now , let us describe how to choose two types of relay sets , @xmath38 and @xmath56 , among @xmath2 relay nodes , where @xmath2 is sufficiently large ( the minimum @xmath2 required to guarantee the dof optimality will be analyzed in section  [ subsec : ana ] ) .", "let us first focus on selecting the set @xmath82 , operating in receive and transmit modes in odd and even time slots , respectively . for every scheduling period , it is possible for relay @xmath16 to obtain all the channel coefficients @xmath83 and @xmath84 by using a pilot signaling sent from all of the source and destination nodes due to the channel reciprocity before data transmission , where @xmath20 and @xmath19 ( note that this is our local csi assumption ) .", "when @xmath16 is assumed to serve the @xmath17th s ", "d pair @xmath85 , it then examines both i ) how much interference is received from the other sources and ii ) how much interference is generated by itself to the other destinations , by computing the following scheduling metric @xmath86 : @xmath87 where @xmath88 and @xmath89 .", "we remark that the first term @xmath90 in ( [ eq : il ] ) denotes the sum of interference power received at @xmath16 for the first hop ( i.e. , phase 1 ) . on the other hand ,", "the second term @xmath91 indicates the sum of interference power generating at @xmath16 , which can be interpreted as the _ leakage of interference _ to the @xmath92 receivers expect for the corresponding destination , for the second hop ( i.e. , phase 2 ) under the same assumption .", "suppose that a short duration cts ( clear to send ) message is transmitted by the destination who finds its desired relay node ( or the master destination ) .", "then according to the computed metrics @xmath86 in ( [ eq : il ] ) , a timer - based method can be used for the relay selection similarly as in  @xcite .", "times that of the single s - d pair case  @xcite . ]", "note that the method based on the timer is considerably suitable in distributed systems in the sense that information exchange among all the relay nodes can be minimized . at the beginning of every scheduling period ,", "the relay @xmath16 computes the set of @xmath1 scheduling metrics , @xmath93 , and then starts its own timer with @xmath1 initial values , which can be set to be proportional to the @xmath1 metrics .", "thus , there exist @xmath94 metrics over the whole relay nodes , and we need to compare them so as to determine who will be selected .", "the timer of the relay @xmath95 with the least one @xmath96 among @xmath94 metrics will expire first , where @xmath97 and @xmath98 .", "the relay then transmits a short duration rts message , signaling its presence , to the other @xmath99 relays , where each rts message is composed of @xmath100 bits to indicate which s ", "d pair the relay wants to serve .", "thereafter , the relay @xmath95 is first selected to forward the @xmath101th s ", "d pair s packet .", "all the other relays are in listen mode while waiting for their timer to be set to zero ( i.e. , to expire ) . at the stage of deciding who will send the second rts message ,", "it is assumed that the other relays are not allowed to communicate with the @xmath101th s ", "d pair , and thus the associated metrics @xmath102 are discarded with respect to timer operation .", "if another relay has an opportunity to send the second rts message of @xmath103 bits in order to declare its presence , then it is selected to communicate with the corresponding s ", "when such @xmath1 rts messages , consisting of at most @xmath104 bits , are sent out in consecutive order , i.e. , the set of @xmath1 relays , @xmath82 , is chosen , the timer - based algorithm for the first relay set selection terminates , yielding no rts collision with high probability .", "we remark that when @xmath105 ( i.e. , the single s  d pair case ) , @xmath1 relay nodes are _ arbitrarily _ chosen as the first relay set @xmath38 since there is no interference in this step .", "now let us turn to choosing the set of @xmath1 relay nodes ( among @xmath35 candidates ) , @xmath106 @xmath107 , operating in receive and transmit modes in even and odd time slots , respectively . using @xmath1 rts messages broadcasted from the @xmath1 relay nodes in the set @xmath38 , it is possible for relay node @xmath108 to compute the sum of inter - relay interference power generated from the relays in @xmath38 , denoted by @xmath109 .", "when @xmath16 is again assumed to serve the @xmath17th s ", "d pair @xmath85 , it examines both i ) how much interference is received from the undesired sources and the selected relays in the set @xmath38 for the first hop and ii ) how much interference is generated by itself to the other destinations by computing the following metric @xmath110 , termed _ total interference level  ( til ) _ : @xmath111 where @xmath88 and @xmath89 .", "we note that steps 1 and 2 can not be exchangeable due to the fact that the inter - relay interference term @xmath112 is measured after determining the first relay set @xmath38 .", "if the relay set selection order is switched , then the metric til in ( [ eq : til ] ) will not be available .    according to the computed til @xmath110", ", we also apply the timer - based method used in step 1 for the second relay set selection . the relay @xmath113 computes the set of @xmath1 tils , @xmath114 , and then starts its timer with @xmath1 initial values , proportional to the @xmath1 tils .", "thus , we need to compare @xmath115 til metrics over the relay nodes in the set @xmath116 in order to determine who will be selected as the second relay set .", "the rest of the relay set selection protocol ( i.e. , rts message exchange among relay nodes ) almost follows the same line as that of step 1 .", "the timer - based algorithm for the second relay set selection terminates when @xmath1 rts messages are sent out in consecutive order .", "then , @xmath1 relay nodes having a sufficiently small amount of til @xmath110 are selected as the second relay set @xmath56 .", "owing to the channel reciprocity of tdd systems , the sum of inter - relay interference power received at any relay @xmath117 , @xmath118 , also turns out to be sufficiently small when @xmath2 is large .", "that is , it is also guaranteed that @xmath1 selected relays in the set @xmath38 have a sufficiently small amount of til .    the overhead of each scheduling time slot ( i.e. , the total number of bits required for exchanging rts messages among the relay nodes ) can be made arbitrarily small , compared to one transmission block . from the fact that @xmath1 rts messages , consisting of at most @xmath104 bits , are sent out in each relay set selection step ,", "only @xmath119 bit transmission could suffice .", "the @xmath27 selected relays request data transmission to their desired source nodes .", "each source ( @xmath14 ) then starts to transmit data to the corresponding destination ( @xmath15 ) via one of its two relay nodes alternately ( @xmath120 or @xmath66 ) , which was specified earlier .", "if the tils of the selected relays are arbitrarily small , then i ) the associated undesired source  relay and relay  destination channel links and ii ) the inter - relay channel links are all in deep fade . in section  [ subsec : ana ] , we will show that it is possible to choose such relays with the help of the multiuser diversity gain .    at the receiver side ,", "each relay or destination detects the signal sent from its desired transmitter , while simply treating interference as gaussian noise .", "thus , no multiuser detection is performed at each receiver , thereby resulting in an easier implementation .      in this subsection , using the scaling argument bridging between the number of relays , @xmath2 , and the received snr ( refer to  @xcite for the details ) , we shall show 1 ) the lower bound on the dof of the @xmath121 channel with interfering relays as @xmath2 increases and 2 ) the minimum @xmath2 required to guarantee the achievability result .", "the total number of dof , denoted by @xmath122 , is defined as  @xcite @xmath123 where @xmath124 denotes the transmission rate of source @xmath14 . using the ond framework in the @xmath125 channel with interfering relays where @xmath26 transmission slots per block are used ,", "the achievable @xmath122 is lower - bounded by @xmath126 where @xmath127 denotes the received signal - to - interference - and - noise ratio ( sinr ) at the relay @xmath128 and @xmath129 denotes the received sinr at the destination @xmath15 when the relay @xmath128 transmits the desired signal ( @xmath130 and @xmath19 ) .", "more specifically , the above sinrs can be formally expressed as , the third term in the denominator of @xmath131 ( i.e. , the inter - relay interference term ) becomes zero . ]", "@xmath132 where the second term in the denominator of @xmath133 indicates the interference power at relay @xmath134 received from the sources while the third term indicates the inter - relay interference , and the second term in the denominator of @xmath135 indicates the interference power at the destination @xmath136 received from the active relays . here ,", "@xmath137 , i.e. , @xmath138 if @xmath139 , and vice versa .", "we focus on the first relay set @xmath140 s perspective to examine the received sinr values according to each time slot .", "let us first denote @xmath141 for @xmath142 .", "for the first hop , at time slot @xmath143 ( i.e. , each odd time slot ) , @xmath144 , the received @xmath131 at @xmath145 is lower - bounded by @xmath146 where @xmath147 indicates the scheduling metric in ( [ eq : il ] ) when @xmath61 is assumed to serve the @xmath18th s ", "d pair ( @xmath148 , @xmath136 ) . for the second hop , at time slot @xmath149", "( i.e. , each even time slot ) , @xmath150 , the received @xmath151 at @xmath152 is lower - bounded by @xmath153 where the second inequality holds due to the channel reciprocity .", "the term @xmath154 in the denominator of ( [ eq : sinr_1_pi1 ] ) and ( [ eq : sinr_2_pi1 ] ) needs to scale as @xmath155 , i.e. , @xmath156 , so that both @xmath157 and @xmath158 scale as @xmath159 with increasing snr , which eventually enables to achieve the dof of @xmath160 per s ", "d pair from ( [ eq : dof_ond ] ) .", "means that there exist constant @xmath161 and @xmath162 such that @xmath163 for all @xmath164 .", "ii ) @xmath165 if @xmath166 .", "iii ) @xmath167 means that @xmath168  @xcite . ]", "even if such a bounding technique in ( [ eq : sinr_1_pi1 ] ) and ( [ eq : sinr_2_pi1 ] ) leads to a loose lower bound on the sinr , it is sufficient to prove our achievability result in terms of dof and relay scaling law .", "now , let us turn to the second relay set @xmath169 .", "similarly as in ( [ eq : sinr_1_pi1 ] ) , for the first hop , at time slot @xmath149 , @xmath170 , the received @xmath171 at @xmath172 is lower - bounded by @xmath173 where @xmath174 indicates the til in when @xmath77 is assumed to serve the @xmath18th s ", "d pair ( @xmath148 , @xmath136 ) . for the second hop , at time slot @xmath175 , @xmath144 ,", "the received @xmath176 at @xmath152 can also be lower - bounded by @xmath177 the next step is thus to characterize the three metrics @xmath86 , @xmath178 , and @xmath110 ( @xmath179 and @xmath180 ) and their cumulative density functions ( cdfs ) in the @xmath0 channel with interfering relays , which is used to analyze the lower bound on the dof and the required relay scaling law in the model under consideration . since it is obvious to show that the cdf of @xmath178 is identical to that of @xmath110 , we focus only on the characterization of @xmath110 .", "the scheduling metric @xmath86 follows the chi - square distribution with @xmath181 degrees of freedom since it represents the sum of i.i.d .", "@xmath182 chi - square random variables with 2 degrees of freedom .", "similarly , the til @xmath110 follows the chi - square distribution with @xmath183 degrees of freedom .", "the cdfs of the two metrics @xmath86 and @xmath110 are given by @xmath184 respectively , where @xmath185 is the gamma function and @xmath186 is the lower incomplete gamma function  ( * ? ? ?", "( 8.310.1 ) ) .", "we start from the following lemma .", "[ lem:1 ] for any @xmath187 , the cdfs of the random variables @xmath86 and @xmath110 in and are lower - bounded by @xmath188 and @xmath189 , respectively , where @xmath190 and @xmath191 is the gamma function .", "the detailed proof of this argument is omitted here since it essentially follows the similar line to the proof of  ( * ? ? ?", "* lemma 1 ) with a slight modification .    in the following theorem ,", "we establish our first main result by deriving the lower bound on the total dof in the @xmath192 channel with interfering relays .", "[ thrm:1 ] suppose that the ond scheme with alternate relaying is used for the @xmath121 channel with interfering relays .", "then , for @xmath26 data transmission time slots , @xmath193 is achievable if @xmath194 .", "from ( [ eq : dof_ond])([eq : sinr_2_pi2 ] ) , the ond scheme achieves @xmath195 provided that the two values @xmath196 and @xmath197 are less than or equal to some constant @xmath198 , independent of snr , for all s  d pairs .", "then , a lower bound on the achievable @xmath122 is given by @xmath199 which indicates that @xmath200 dof is achievable for a fraction @xmath201 of the time for actual transmission , where @xmath202    we now examine the relay scaling condition such that @xmath201 converges to one with high probability .", "for the simplicity of the proof , suppose that the first and the second relay sets @xmath140 and @xmath169 are selected out of two mutually exclusive relaying candidate sets @xmath203 and @xmath204 , respectively , i.e. , @xmath205 , @xmath206 , @xmath207 , @xmath208 , and @xmath209 .", "then , we are interested in how @xmath210 and @xmath211 scale with snr in order to guarantee that @xmath201 tends to one , where @xmath212 denotes the cardinality of @xmath213 for @xmath130 . from ( [ eq : pond : def0 ] ) , we further have @xmath214    let @xmath215 with @xmath216 and @xmath217 be the candidate set associated with the second relay set and the @xmath218th s - d pair and its cardinality , respectively . for a constant @xmath198 , we can bound the second term in as follows : @xmath219 where the inequality @xmath220 holds from the de morgan s law ; @xmath221 follows from the union bound ; @xmath222 follows since @xmath223 ; @xmath224 follows since @xmath225 are the i.i.d .", "random variables @xmath226 for a given @xmath18 , owning to the fact that the channels are i.i.d .", "variables ; and @xmath227 follows from lemma  [ lem:1 ] with @xmath228 since @xmath229 as @xmath230 and from the fact that @xmath231 .", "we now pay our attention to the first term in , which can be bounded by @xmath232 where the equality follows from the fact that @xmath233 and @xmath234 for @xmath235 are the functions of different random variables and thus are independent of each other . by letting @xmath236 , by the definition of @xmath233", ", we have @xmath237 where the inequality follows from the fact that for any random variables @xmath238 and @xmath239 , @xmath240 @xmath241 @xcite . in the same manner ,", "let @xmath242 with @xmath243 and @xmath244 be the candidate set associated with the first relay set and the @xmath218th s ", "d pair and its cardinality , respectively .", "then , we can bound the first two terms in the rhs of   as follows : @xmath245 where the last inequality follows from lemma  [ lem:1 ] with @xmath246 .", "finally , from , it follows that @xmath247 tends to zero as @xmath248 grows large by noting that @xmath249 due to the reciprocal property of tdd systems . from , , and , it is obvious that if @xmath250 and @xmath248 scale faster than @xmath251 and @xmath7 , respectively , then @xmath252 therefore , @xmath201 asymptotically approaches one , which means that the dof of @xmath253 is achievable with high probability if @xmath254 .", "this completes the proof of the theorem .", "note that the lower bound on the dof asymptotically approaches @xmath1 for large @xmath26 , which implies that our system operates in virtual full - duplex mode .", "the parameter @xmath2 required to obtain full dof ( i.e. , @xmath1 dof ) needs to increase exponentially with the number of s  d pairs , @xmath1 , in order to make the sum of @xmath255 interference terms in the til metric ( [ eq : til ] ) non - increase with increasing snr at each relay . scales slower than @xmath7 , it does not affect the performance in terms of dof and relay scaling laws . ] here , from the perspective of each relay in @xmath56 , the snr exponent @xmath255 indicates the total number of interference links and stems from the following three factors : the sum of interference power received from other sources , the sum of interference power generated to other destinations , and the sum of inter - relay interference power generated from the relays in @xmath38 . from theorem  [ thrm:1 ] ,", "let us provide the following interesting discussions regarding the dof achievability .", "@xmath1 dof can be achieved by using the proposed ond scheme in the @xmath0 channel with interfering links among relay nodes , if the number of relay nodes , @xmath2 , scales faster than @xmath7 and the number of transmission slots in one block , @xmath26 , is sufficiently large . in this case , all the interference signals are almost nulled out at each selected relay by exploiting the multiuser diversity gain . in other words , by applying the ond scheme to the interference - limited @xmath0 channel such that the channel links are inherently coupled with each other , the links among each s  d path via one relay can be completely decoupled , thus enabling us to achieve the same dof as in the interference - free channel case .", "it is not difficult to show that the centralized relay selection method that maximizes the received sinr ( at either the relay or the destination ) using global csi at the transmitter , which is a combinatorial problem with exponential complexity , gives the same relay scaling result @xmath194 along with full dof .", "however , even with our ond scheme using a decentralized relay selection based only on local csi , the same achievability result is obtained , thus resulting in a much easier implementation .      in this subsection , we analyze the til decaying rate under the ond scheme with alternate relaying , which is meaningful since the desired relay scaling law is closely related to the til decaying rate with respect to @xmath2 for given snr .", "let @xmath256 denote the @xmath1th smallest til among the ones that @xmath2 selected relay nodes compute .", "since the @xmath1 relays yielding the til values up to the @xmath1th smallest one are selected , the @xmath1th smallest til is the largest among the tils that the selected relays compute .", "similarly as in  @xcite , by markov s inequality , a lower bound on the average decaying rate of @xmath256 with respect to @xmath2 , @xmath257 $ ] , is given by @xmath258\\ge \\frac{1}{\\epsilon}\\pr(l_{k\\text{th - min}}\\le \\epsilon),\\end{aligned}\\ ] ] where the inequality always holds for @xmath259 .", "we denote @xmath260 as the probability that there are only @xmath1 relays satisfying @xmath261 , which is expressed as @xmath262 where @xmath263 is the cdf of the til .", "since @xmath264 is lower - bounded by @xmath265 , a lower bound on the average til decaying rate is given by @xmath258\\ge \\frac{1}{\\epsilon}\\mathcal{p}_k(\\epsilon ) .", "\\label{eq : til_min}\\end{aligned}\\ ] ]    the next step is to find the parameter @xmath266 that maximizes @xmath260 in terms of @xmath267 in order to provide the tightest lower bound .", "[ lem:2 ] when a constant @xmath266 satisfies the condition @xmath268 , @xmath269 in ( [ eq : p_k ] ) is maximized for a given @xmath2 .    to find the parameter @xmath266 that maximizes @xmath260", ", we take the first derivative with respect to @xmath267 , resulting in @xmath270 which is zero when @xmath271 the parameter @xmath266 is the unique value that maximizes @xmath260 since @xmath272 which completes the proof of the lemma .", "now , we establish our second main theorem , which shows a lower bound on the til decaying rate with respect to @xmath2 .", "[ thrm:2 ] suppose that the ond scheme with alternate relaying is used for the @xmath0 channel with interfering relays .", "then , the decaying rate of til is lower - bounded by @xmath258 \\ge \\theta\\left(n^{\\frac{1}{3k-2}}\\right).\\end{aligned}\\ ] ]    as shown in ( [ eq : til_min ] ) , the til decaying rate is lower - bounded by the maximum of @xmath273 over @xmath267 . by lemma", "[ lem:2 ] , @xmath274 is maximized when @xmath275 .", "thus , we have @xmath258 & \\ge \\frac{1}{\\mathcal{f}_l^{-1}(k / n)}{{n}\\choose{k}}\\left(\\frac{k}{n}\\right)^k \\left(1-\\frac{k}{n}\\right)^{n - k } \\nonumber\\\\ & \\ge \\frac{1}{\\mathcal{f}_l^{-1}(k / n)}\\left(\\frac{n - k+1}{n}\\right)^k \\!\\!\\!\\left(1-\\frac{k}{n}\\right)^{n - k } \\nonumber\\\\ & \\ge \\frac{1}{\\mathcal{f}_l^{-1}(k /", "n ) } \\left(\\frac{1}{k}\\right)^k e^{-k } \\nonumber\\\\ & \\ge \\theta\\left(n^{\\frac{1}{3k-2}}\\right ) , \\nonumber\\end{aligned}\\ ] ] where the second and third inequalities hold since @xmath276 and @xmath277 , respectively . by lemma  [ lem:1 ]", ", it follows that @xmath278 , where @xmath279 is given by ( [ eq : c1 ] ) .", "hence , the last inequality also holds , which completes the proof of the theorem .    from theorem", "[ thrm:2 ] , the following valuable insight is provided : the smaller snr exponent of the relay scaling law , the faster til decaying rate with respect to @xmath2 .", "this asymptotic result will be verified in a finite @xmath2 regime via numerical evaluation in section  [ sec:5 ] .      for comparison , the ond scheme without alternate relaying", "is also explained in this subsection .", "it is worth noting that there exists a trade - off between the lower bound on the dof and the minimum number of relays required to guarantee our achievability result by additionally introducing the ond protocol without alternate relaying . in the scheme ,", "the first relay set @xmath38 only participates in data forwarding .", "that is , the second relay set @xmath56 does not need to be selected for the ond protocol without alternate relaying .", "specifically , the steps of each node during one block are then described as follows :    * time slot 1 : sources @xmath30 transmit their first encoded symbols @xmath31 , where @xmath32 represents the @xmath33th transmitted symbol of the @xmath17th source node .", "a set of @xmath1 selected relay nodes , @xmath34 , operating in receive mode at each odd time slot , listens to @xmath31 .", "other @xmath35 relay nodes and destinations @xmath36 remain idle . * time slot 2 : the @xmath1 relays in the set @xmath38 forward their first re - encoded symbols @xmath280 @xmath281 to the corresponding @xmath1 destinations .", "the @xmath1 destinations receive from @xmath44 @xmath45 and decode @xmath39 .", "the remaining @xmath35 relays keep idle . *", "the processes in time slots 1 and 2 are repeated to the @xmath55th time slot .", "* time slot @xmath26 : the @xmath1 relays in @xmath38 forward their re - encoded symbols @xmath282 @xmath283 to the corresponding @xmath1 destinations", ". the @xmath1 sources and the other @xmath35 relays remain idle .    when @xmath16 is assumed to serve the @xmath17th s ", "d pair @xmath85 ( @xmath20 and @xmath19 ) , it computes the scheduling metric @xmath86 in ( [ eq : il ] ) . according to the computed @xmath86 ,", "a timer based method is used for relay selection as in section  [ subsub:1strelay ] . because there is no inter - relay interference for the ond scheme without alternate relaying", ", it is expected that the minimum required @xmath2 to achieve the optimal dof is significantly reduced .", "our third main theorem is established as follows .", "[ thrm:3 ] suppose that the ond scheme without alternate relaying is used for the @xmath121 channel .", "then , for @xmath26 data transmission time slots , @xmath284 is achievable if @xmath285 .", "the detailed proof of this argument is omitted here since it basically follows the same line as the proof of theorem  [ thrm:1 ] .    in section  [ sec:5 ] , it will be also seen that in a finite @xmath2 regime , there exists the case even where the ond without alternate relaying outperforms that of the ond with alternate relaying in terms of achievable sum - rates via computer simulations .", "in this section , to show the optimality of the proposed ond scheme in the @xmath0 channel with interfering relays , which consists of @xmath1 s  d pairs and @xmath2 relay nodes , we derive an upper bound on the dof using the cut - set bound  @xcite as a counterpart of the lower bound on the total dof in section  [ subsec : ana ] .", "suppose that @xmath286 relay nodes are active , i.e. , receive packets and retransmit their re - encoded ones , simultaneously , where @xmath287 .", "this is a generalized version of our transmission since it is not characterized how many relays need to be activated simultaneously to obtain the optimal dof .", "we consider the two cuts @xmath288 and @xmath289 dividing our network into two parts in a different manner .", "let @xmath290 and @xmath291 denote the sets of sources and destinations , respectively , for the cut @xmath292 in the network ( @xmath293 ) . for the @xmath0 channel model with interfering relays", ", we now use the fact that there is no direct path between an s ", "then , it follows that under @xmath288 , @xmath1 transmit nodes in @xmath294 are on the left of the network , while @xmath286 active relay nodes and @xmath1 ( final ) destination nodes in @xmath295 are on the right and act as receivers . in this case , we can create the @xmath296 multiple - input multiple - output ( mimo ) channel between the two sets of nodes separated by the cut . similarly , the @xmath297 mimo channel are obtained under the cut @xmath289 .", "it is obvious to show that dof for the two mimo channels is upper - bounded by @xmath1 .", "hence , it turns out that even with the half - duplex assumption , our lower bound on the dof based on the ond with alternate relaying asymptotically approaches this upper bound on the dof for large @xmath26 .", "note that this upper bound is generally derived regardless of whether the number of relays , @xmath2 , tends to infinity or not , whereas the scaling condition @xmath298 is included in the achievability proof .", "when @xmath299 and @xmath300 in the @xmath121 channel .", "it is assumed that @xmath300 and @xmath301 are used for the ond schemes with and without alternate relaying , respectively .", ", scaledwidth=57.0% ]     when @xmath302 in the @xmath121 channel .", ", scaledwidth=57.0% ]", "in this section , we perform computer simulations to validate the achivability result of the proposed ond scheme in section  [ sec:3 ] for finite parameters @xmath2 and snr in the @xmath0 channel model with interfering relays . in our simulation , the channel coefficients in ( [ eq : y1 ] ) and ( [ eq : y2 ] ) are generated @xmath303 times for each system parameter .", "figure  [ fig:1 ] shows the achievable sum - rates of the @xmath304 channel for the ond schemes with and without alternate relaying according to @xmath305 in db scale when @xmath299 .", "note that @xmath2 is set to a different scalable value according to @xmath305 , i.e. , @xmath306 for the ond with alternate relaying and @xmath307 for the ond without alternate relaying , respectively , to see whether the slope of each curve follows the dof in theorems  [ thrm:1 ] and  [ thrm:2 ] . in the figure", ", the dotted green lines are also plotted to indicate the first order approximation of the achievable rates with a proper bias , where the slopes are given by @xmath1 and @xmath4 for the ond schemes with and without alternate relaying , respectively .    in fig .", "[ fig:2 ] , the log - log plot of the average til of the ond with alternate relaying versus @xmath2 is shown for the @xmath308 channel when @xmath302 . is taken into account to precisely see some trends of curves varying with @xmath2 . ]", "it can be seen that the til tends to decrease linearly with @xmath2 .", "it is further seen how many relays are required with the ond scheme with alternate relaying to guarantee that the til is less than a small constant for a given parameter @xmath1 . in this figure , the dashed lines are also plotted from theoretical results in theorem  [ thrm:2 ] with a proper bias to check the slope of the til .", "we can see that the til decaying rates are consistent with the relay scaling law condition in theorem  [ thrm:1 ] .", "more specifically , the til is reduced as @xmath2 increases with the slope of 0.25 for @xmath299 and 0.143 for @xmath309 , respectively .     when @xmath299 and @xmath310 in the @xmath121 channel .", "both ond schemes with and without alternate relaying are compared .", ", scaledwidth=57.0% ]     when @xmath299 and @xmath310 in the @xmath121 channel . the ond scheme with alternate relaying and the max - min snr scheme", "are compared .", ", scaledwidth=57.0% ]    figure  [ fig:3 ] illustrates the achievable sum - rates of the @xmath121 channel for the ond schemes with and without alternate relaying versus @xmath305 ( in db scale ) when @xmath299 and @xmath311 .", "we can see that in a finite @xmath2 regime , there exists the case where the ond without alternate relaying outperforms that of the ond with alternate relaying .", "this is because for finite @xmath2 , the achievable sum - rates for the alternate relaying case tend to approach a floor with increasing snr faster than no alternate relaying case due to more residual interference in each dimension .", "we can also see that the crossing points slightly move to the right as @xmath2 increases ; this is due to the fact that our ond scheme with alternate relaying always benefits from having more relays for selection , thus resulting in more multiuser diversity gain .", "this highly motivates us to operate our system in a switched fashion when the relay selection scheme is chosen between the ond schemes with and without alternate relaying depending on the operating regime of our system .    to further ascertain the efficacy of our scheme ,", "a performance comparison is performed with a baseline scheduling .", "specifically , in the _ max - min snr _ scheme , each s  d pair selects one relay @xmath16 ( @xmath20 ) such that the minimum out of the desired channel gains of two communication links ( either from @xmath14 to @xmath16 or from @xmath16 to @xmath15 ) becomes the maximum among the associated minimum channel gains over all the unselected relays .", "this max - min snr scheme is well - suited for relay - aided systems if interfering links are absent .", "the achievable sum - rates are illustrated in fig .", "[ fig:4 ] according to @xmath305 ( in db scale ) when @xmath299 and @xmath310 .", "we can see that our ond scheme with alternate relaying outperforms this baseline scheme beyond a certain low snr point .", "we also see that the rate gaps increase when @xmath2 increases in the high snr regime . on the other hand , for fixed @xmath2 ,", "the sum - rates of the max - min scheme are slightly changed with respect to @xmath305 due to more residual interference in each dimension .", "an efficient distributed ond protocol operating in virtual full - duplex mode was proposed for the @xmath0 channel with interfering relays , referred to as one of multi - source interfering relay networks . a novel relay scheduling strategy with alternate half - duplex relaying", "was presented in two - hop environments , where a subset of relays is opportunistically selected in terms of producing the minimum total interference level , thereby resulting in network decoupling .", "it was shown that the ond protocol asymptotically achieves full dof even in the presence of inter - relay interference and half - duplex assumption , provided that the number of relays , @xmath2 , scales faster than @xmath7 .", "numerical evaluation was also shown to verify that our scheme outperforms the other relay selection methods under realistic network conditions ( e.g. , finite @xmath2 and snr ) with respect to sum - rates .", "i. shomorony and a. s. avestimehr ,  degrees of freedom of two - hop wireless networks :  everyone gets the entire cake `` , '' in _ proc .", "50th annual allerton conf .", "control , and computing _ , monticello , il , oct .", "t. gou , c. wang , and s. a. jafar ,  aligned interference neutralization and the degrees of freedom of the @xmath5 interference channel with interfering relays , \" in _ proc .", "49th annual allerton conf .", "commun . , control , and computing _ , monticello , il , sep .", "t. gou , c. wang , and s. a. jafar ,  degrees of freedom of a class of non - layered two unicast wireless networks , \" in _ proc .", "45th asilomar conf .", "signals , systems and computers _ , pacific grove , ca , nov . 2011 .", "h.  j. yang , w .- y .", "shin , b.  c. jung , and a.  paulraj , `` opportunistic interference alignment for mimo interfering multiple access channels , '' _ ieee trans .", "wireless commun .", "_ , vol .  12 , no .  5 , pp . 21802192 , may 2013 .", "h.  j. yang , w .- y .", "shin , b.  c. jung , c.  suh , and a.  paulraj , `` opportunistic downlink interference alignment , '' in _ proc .", "inf . theory ( isit ) _ , honolulu , hi , jun./jul .", "2014 , pp . 15881592 .        c. shen and m. p. fitz ,", " opportunistic spatial orthogonalization and its application to fading cognitive radio networks , \" , vol . 5 , no .", "1 , pp . 182189 , feb . 2011 . t. w. ban , w. choi , b. c. jung , and d. k. sung ,  multi - user diversity in a spectrum sharing system , \" , vol .", "1 , pp . 102106 , jan .", "2009 .", "r. zhang and y. c. liang ,  exploiting multi - antennas for opportunistic spectrum sharing in cognitive radio networks , \" , vol .", "1 , pp . 88102 , feb . 2008 .", "y. fan , c. wang , j. thompson , and h. v. poor ,  recovering multiplexing loss through successive relaying using repetition coding , \" , vol . 6 , no . 12 , pp .", "44844493 , dec . 2007 .", "shin , h. j. yang , and b. c. jung ,  opportunistic network decoupling in multi - source interfering relay networks , \" in _ proc . ieee conf . commun . ( icc ) _ , sydney , australia , jun .", "2014 , pp . 26712676 ."], "abstract_text": ["<S> we introduce a new achievability scheme , termed _ opportunistic network decoupling ( ond ) _ , operating in virtual full - duplex mode . in the scheme , </S>", "<S> a novel relay scheduling strategy is utilized in the _ </S>", "<S> @xmath0 channel with interfering relays _ , consisting of @xmath1 source  destination pairs and @xmath2 half - duplex relays in - between them . </S>", "<S> a subset of relays using alternate relaying is opportunistically selected in terms of producing the minimum total interference level , thereby resulting in network decoupling . as our main result , it is shown that under a certain relay scaling condition , the ond protocol achieves @xmath1 degrees of freedom even in the presence of interfering links among relays . </S>", "<S> numerical evaluation is also shown to validate the performance of the proposed ond . </S>", "<S> our protocol basically operates in a fully distributed fashion along with local channel state information , thereby resulting in a relatively easy implementation .    </S>", "<S> degrees of freedom ( dof ) , half - duplex , interference , @xmath3 channel , opportunistic network decoupling ( ond ) , relay , virtual full - duplex ( fd ) . </S>"], "labels": null, "section_names": ["introduction", "system and channel models", "achievability results", "upper bound for dof", "numerical evaluation", "concluding remarks"], "sections": [["interference between wireless links has been taken into account as a critical problem in wireless communication systems .", "recently , interference alignment  ( ia ) was proposed for fundamentally solving the interference problem when there are two communication pairs  @xcite .", "it was shown in  @xcite that the ia scheme can achieve the optimal degrees of freedom  ( dof ) , which is equal to @xmath4 , in the @xmath1-user interference channel with time - varying channel coefficients . since then , interference management schemes based on ia have been further developed and analyzed in various wireless network environments : multiple - input multiple - output ( mimo ) interference networks  @xcite , x networks  @xcite , and cellular networks  @xcite .", "on the one hand , following up on these successes for single - hop networks , more recent and emerging work has studied multihop networks with multiple source - destination ( s  d ) pairs . for the 2-user 2-hop network with 2 relays ( referred to as the @xmath5 interference channel ) , it was shown in  @xcite that interference neutralization combining with symbol extension achieves the optimal dof .", "a more challenging network model is to consider @xmath1-user two - hop relay - aided interference channels , consisting of @xmath1 source - destination ( s  d ) pairs and @xmath2 helping relay nodes located in the path between s ", "d pairs , so - called the @xmath3 channel .", "several achievability schemes have been known for the network , but more detailed understanding is still in progress . by applying the result from  @xcite to the @xmath0 channel , one can show that @xmath4 dof is achieved by using orthogonalize - and - forward relaying , which completely neutralizes interference at all destinations if @xmath2 is greater than or equal to @xmath6 .", "another achievable scheme , called aligned network diagonalization , was introduced in  @xcite and was shown to achieve the optimal dof in the @xmath0 channel while tightening the required number of relays .", "the scheme in  @xcite is based on the real interference alignment framework  @xcite . in  @xcite", ", however , the system model under consideration assumes that there is no interfering signal between relays and the relays are full - duplex .", "moreover , in  @xcite , the @xmath5 interference channel with full - duplex relays interfering with each other was characterized and its dof achievability was shown using aligned interference neutralization .", "interference channel  @xcite . ]", "on the other hand , there are lots of results on the usefulness of fading in the literature , where one can obtain the multiuser diversity gain in broadcast channels : opportunistic scheduling  @xcite , opportunistic beamforming  @xcite , and random beamforming  @xcite .", "such opportunism can also be fully utilized in multi - cell uplink or downlink networks by using an opportunistic interference alignment strategy  @xcite .", "various scenarios exploiting the multiuser diversity gain have been studied in cooperative networks by applying an opportunistic two - hop relaying protocol  @xcite and an opportunistic routing  @xcite , and in cognitive radio networks with opportunistic scheduling  @xcite .", "in addition , recent results  @xcite have shown how to utilize the opportunistic gain when there are a large number of channel realizations .", "more specifically , to amplify signals and cancel interference , the idea of opportunistically pairing complementary channel instances has been studied in interference networks  @xcite and multi - hop relay networks  @xcite . in cognitive radio environments", "@xcite , opportunistic spectrum sharing was introduced by allowing secondary users to share the radio spectrum originally allocated to primary users via transmit adaptation in space , time , or frequency .      in this paper , we study the _", "@xmath0 channel with interfering relays _ , which can be taken into account as one of practical multi - source interfering relay networks and be regarded as a fundamentally different channel model from the conventional @xmath0 channel in  @xcite .", "then , we introduce an _ opportunistic network decoupling ( ond ) _ protocol that achieves full dof with comparatively easy implementation under the channel model .", "this work focuses on the @xmath0 channel with one additional assumption that @xmath2 _ half - duplex _ ( hd ) relays interfere with each other , which is a more feasible scenario .", "the scheme adopts the notion of the multiuser diversity gain for performing interference management over two hops . more precisely , in our scheme , a scheduling strategy is presented in time - division duplexing ( tdd ) two - hop environments with time - invariant channel coefficients , where a subset of relays is opportunistically selected in terms of producing the minimum total interference level . to improve the spectral efficiency , the _ alternate relaying _ protocol in  @xcite", "is employed with a modification , which eventually enables our system to operate in _ virtual full - duplex _ mode . as our main result ,", "it turns out that in a high signal - to - noise ratio ( snr ) regime , the ond protocol asymptotically achieves the min - cut upper bound of @xmath1 dof even in the presence of inter - relay interference and half - duplex assumption , provided the number of relays , @xmath2 , scales faster than @xmath7 , which is the minimum number of relays required to guarantee our achievability result .", "numerical evaluation also indicates that the ond protocol has higher sum - rates than those of other relaying methods under realistic network conditions ( e.g. , finite @xmath2 and snr ) since the inter - relay interference is significantly reduced owing to the opportunistic gain . for comparison , the ond scheme without alternate relaying and the max - min snr scheme are also shown as baseline schemes . note that our protocol basically operates with local channel state information ( csi ) at the transmitter and thus is suitable for distributed / decentralized networks .", "our main contributions are fourfold as follows :    * in the @xmath0 channel with interfering relays , we introduce a new achievability scheme , termed ond with virtual full - duplex operation .", "* under the channel model , we completely analyze the optimal dof , the required relay scaling condition , and the decaying rate of the interference level , where the ond scheme is shown to approach the min - cut upper bound on the dof . * our achievability result ( i.e.", ", the derived dof and relay scaling law ) is validated via numerical evaluation .", "* we perform extensive computer simulations with other baseline schemes .", "the rest of this paper is organized as follows . in section  [ sec:2 ] , we describe the system and channel models . in section  [ sec:3 ] , the proposed ond scheme is specified and its lower bound on the dof is analyzed .", "section  [ sec:4 ] shows an upper bound on the dof .", "numerical results of the proposed ond scheme are provided in section  [ sec:5 ] .", "finally , we summarize the paper with some concluding remarks in section  [ sec:6 ] .      throughout this paper , @xmath8 , @xmath9 $ ] , and @xmath10 indicate the field of complex numbers , the statistical expectation , and the ceiling operation , respectively . unless otherwise stated , all logarithms are assumed to be to the base 2 .", "moreover , table  [ tab : notations ] summarizes the notations used throughout this paper .", "some notations will be more precisely defined in the following sections , where we introduce our channel model and achievability results .", ".summary of notations [ cols=\"^,^ \" , ]"], ["as one of two - hop cooperative scenarios , we consider the @xmath11 channel model with interfering relays , which fits into the case where each s  d pair is geographically far apart and/or experiences strong shadowing ( thus requiring the response to a huge challenge for achieving the target spectral efficiency ) . in the channel model , it is thus assumed that each source transmits its own message to the corresponding destination only through one of @xmath2 relays , and thus there is no direct path between an s ", "d pair . note that unlike the conventional @xmath12 channel , relay nodes are assumed to interfere with each other in our model .", "there are @xmath1 s  d pairs , where each receiver is the destination of exactly one source node and is interested only in traffic demands of the source . as in the typical cooperative relaying setup ,", "@xmath2 relay nodes are located in the path between s ", "d pairs so as to help to reduce path - loss attenuations .", "suppose that each node is equipped with a single transmit antenna .", "each relay node is assumed to operate in half - duplex mode and to fully decode , re - encode , and retransmit the source message i.e. , decode - and - forward protocol is taken into account .", "we assume that each node ( either a source or a relay ) has an average transmit power constraint @xmath13 .", "unlike the work in  @xcite , @xmath2 relays are assumed to interfere with each other .", "channel can also be applied here . ] to improve the spectral efficiency , the alternate relaying protocol in  @xcite is employed with a modification . with alternate relaying ,", "each selected relay node toggles between the transmit and listen modes for alternate time slots of message transmission of the sources . if @xmath2 is sufficiently large , then it is possible to exploit the channel randomness for each hop and thus to obtain the opportunistic gain in multiuser environments . in this work", ", we do not assume the use of any sophisticated multiuser detection schemes at each receiver ( either a relay or a destination node ) , thereby resulting in an easier implementation .", "now , let us turn to channel modeling .", "let @xmath14 , @xmath15 , and @xmath16 denote the @xmath17th source , the corresponding @xmath17th destination , and the @xmath18th relay node , respectively , where @xmath19 and @xmath20 .", "the terms @xmath21 denote the channel coefficients from @xmath14 to @xmath16 and from @xmath16 to @xmath15 , corresponding to the first and second hops , respectively .", "the term @xmath22 indicates the channel coefficient between two relays @xmath16 and @xmath23 .", "all the channels are assumed to be rayleigh , having zero - mean and unit variance , and to be independent across different @xmath18 , @xmath17 , @xmath24 , and hop index @xmath25 .", "we assume the block - fading model , i.e. , the channels are constant during one block ( e.g. , frame ) , consisting of one scheduling time slot and @xmath26 data transmission time slots , and changes to a new independent value for every block .     channel with interfering relays . , scaledwidth=57.0% ]"], ["in this section , we describe the ond protocol , operating in virtual full - duplex mode , in the @xmath0 channel with interfering relays .", "then , its performance is analyzed in terms of achievable dof along with a certain relay scaling condition .", "the decaying rate of the interference level is also analyzed .", "in addition , the ond protocol with no alternate relaying and its achievability result are shown for comparison .      in this subsection", ", we introduce an ond protocol as the achievable scheme to guarantee the optimal dof of the @xmath12 channel with inter - relay interference , where @xmath27 relay nodes among @xmath2 candidates are opportunistically selected for data forwarding in the sense of producing a sufficiently small amount of interference level .", "the proposed scheme is basically performed by utilizing the channel reciprocity of tdd systems .", "suppose that @xmath28 and @xmath29 denote the indices of two relays communicating with the @xmath17th s ", "d pair for @xmath19 . in this case , without loss of generality , assuming that the number of data transmission time slots , @xmath26 , is an odd number , the specific steps of each node during one block are described as follows :    * time slot 1 : sources @xmath30 transmit their first encoded symbols @xmath31 , where @xmath32 represents the @xmath33th transmit symbol of the @xmath17th source node .", "a set of @xmath1 selected relay nodes , @xmath34 , operating in receive mode at each odd time slot , listens to @xmath31 ( note that a relay selection strategy will be specified later ) . other @xmath35 relay nodes and destinations", "@xmath36 remain idle .", "* time slot 2 : the @xmath1 sources transmit their encoded symbols @xmath37 .", "the @xmath1 selected relays in the set @xmath38 forward their first re - encoded symbols @xmath39 to the corresponding @xmath1 destinations .", "if the relays in @xmath38 successfully decode the corresponding symbols , then @xmath40 is the same as @xmath41 .", "another set of @xmath1 selected relay nodes , @xmath42 , operating in receive mode at each even time slot , listens to and decodes @xmath37 while being interfered with by @xmath43 .", "the @xmath1 destinations receive and decode @xmath39 from @xmath44 @xmath45 .", "the remaining @xmath46 relays keep idle . * time slot 3 : the @xmath1 sources transmit their encoded symbols @xmath47 .", "the @xmath1 relays @xmath48 forward their re - encoded symbols @xmath49 to the corresponding @xmath1 destinations .", "another @xmath1 relays in @xmath38 receive and decode @xmath50 @xmath51 while being interfered with by @xmath52 .", "the @xmath1 destinations receive and decode @xmath49 from @xmath53 @xmath54 .", "the remaining @xmath46 relays keep idle . *", "the processes in time slots 2 and 3 are repeated to the @xmath55th time slot .", "* time slot @xmath26 : the @xmath1 relays in @xmath56 forward their re - encoded symbols @xmath57 @xmath58 to the corresponding @xmath1 destinations", ". the @xmath1 sources and the other @xmath35 relays remain idle .    at each odd time slot @xmath33 ( i.e. , @xmath59 ) , let us consider the received signal at each selected relay for the first hop and the received signal at each destination for the second hop , respectively .    for the first hop ( phase 1 ) , the received signal @xmath60 at @xmath61 is given by @xmath62 where @xmath32 and @xmath63 are the @xmath33th transmit symbol of @xmath14 and the @xmath64th transmit symbol of @xmath65 , respectively . as addressed earlier ,", "if relay @xmath66 successfully decodes the received symbol , then it follows that @xmath67 .", "the received signal @xmath68 at @xmath61 is corrupted by the independent and identically distributed ( i.i.d . ) and circularly symmetric complex additive white gaussian noise ( awgn ) @xmath69 having zero - mean and variance @xmath70 . note that the second term in the right - hand side ( rhs ) of ( [ eq : y1 ] ) indicates the inter - relay interference , which occurs when the @xmath1 relays in the set @xmath38 , operating in receive mode , listen to the sources , the relays are interfered with by the other set @xmath56 , operating in transmit mode .", "note that when @xmath71 , relays have no symbols to transmit , and the second term in the rhs of ( [ eq : y1 ] ) becomes zero .", "similarly when @xmath72 , sources do not transmit symbols , and the first term in the rhs of ( [ eq : y1 ] ) becomes zero .", "for the second hop ( phase 2 ) , assuming that the @xmath1 selected relay nodes transmit their data packets simultaneously , the received signal @xmath73 at @xmath15 is given by @xmath74 where @xmath75 is the i.i.d .", "awgn having zero - mean and variance @xmath70 .", "we also note that when @xmath71 , there are no signals from relays .    likewise , at each even time slot ( i.e. , @xmath76 ) , the received signals at @xmath77 and @xmath15 ( i.e. , the first and second hops ) are given by @xmath78 and @xmath79 respectively .", "the illustration of the aforementioned ond protocol is geographically shown in fig .  [", "fig : dig ] ( two terms @xmath80 and @xmath81 are specified later in the following relay selection steps ) .", "now , let us describe how to choose two types of relay sets , @xmath38 and @xmath56 , among @xmath2 relay nodes , where @xmath2 is sufficiently large ( the minimum @xmath2 required to guarantee the dof optimality will be analyzed in section  [ subsec : ana ] ) .", "let us first focus on selecting the set @xmath82 , operating in receive and transmit modes in odd and even time slots , respectively . for every scheduling period , it is possible for relay @xmath16 to obtain all the channel coefficients @xmath83 and @xmath84 by using a pilot signaling sent from all of the source and destination nodes due to the channel reciprocity before data transmission , where @xmath20 and @xmath19 ( note that this is our local csi assumption ) .", "when @xmath16 is assumed to serve the @xmath17th s ", "d pair @xmath85 , it then examines both i ) how much interference is received from the other sources and ii ) how much interference is generated by itself to the other destinations , by computing the following scheduling metric @xmath86 : @xmath87 where @xmath88 and @xmath89 .", "we remark that the first term @xmath90 in ( [ eq : il ] ) denotes the sum of interference power received at @xmath16 for the first hop ( i.e. , phase 1 ) . on the other hand ,", "the second term @xmath91 indicates the sum of interference power generating at @xmath16 , which can be interpreted as the _ leakage of interference _ to the @xmath92 receivers expect for the corresponding destination , for the second hop ( i.e. , phase 2 ) under the same assumption .", "suppose that a short duration cts ( clear to send ) message is transmitted by the destination who finds its desired relay node ( or the master destination ) .", "then according to the computed metrics @xmath86 in ( [ eq : il ] ) , a timer - based method can be used for the relay selection similarly as in  @xcite .", "times that of the single s - d pair case  @xcite . ]", "note that the method based on the timer is considerably suitable in distributed systems in the sense that information exchange among all the relay nodes can be minimized . at the beginning of every scheduling period ,", "the relay @xmath16 computes the set of @xmath1 scheduling metrics , @xmath93 , and then starts its own timer with @xmath1 initial values , which can be set to be proportional to the @xmath1 metrics .", "thus , there exist @xmath94 metrics over the whole relay nodes , and we need to compare them so as to determine who will be selected .", "the timer of the relay @xmath95 with the least one @xmath96 among @xmath94 metrics will expire first , where @xmath97 and @xmath98 .", "the relay then transmits a short duration rts message , signaling its presence , to the other @xmath99 relays , where each rts message is composed of @xmath100 bits to indicate which s ", "d pair the relay wants to serve .", "thereafter , the relay @xmath95 is first selected to forward the @xmath101th s ", "d pair s packet .", "all the other relays are in listen mode while waiting for their timer to be set to zero ( i.e. , to expire ) . at the stage of deciding who will send the second rts message ,", "it is assumed that the other relays are not allowed to communicate with the @xmath101th s ", "d pair , and thus the associated metrics @xmath102 are discarded with respect to timer operation .", "if another relay has an opportunity to send the second rts message of @xmath103 bits in order to declare its presence , then it is selected to communicate with the corresponding s ", "when such @xmath1 rts messages , consisting of at most @xmath104 bits , are sent out in consecutive order , i.e. , the set of @xmath1 relays , @xmath82 , is chosen , the timer - based algorithm for the first relay set selection terminates , yielding no rts collision with high probability .", "we remark that when @xmath105 ( i.e. , the single s  d pair case ) , @xmath1 relay nodes are _ arbitrarily _ chosen as the first relay set @xmath38 since there is no interference in this step .", "now let us turn to choosing the set of @xmath1 relay nodes ( among @xmath35 candidates ) , @xmath106 @xmath107 , operating in receive and transmit modes in even and odd time slots , respectively . using @xmath1 rts messages broadcasted from the @xmath1 relay nodes in the set @xmath38 , it is possible for relay node @xmath108 to compute the sum of inter - relay interference power generated from the relays in @xmath38 , denoted by @xmath109 .", "when @xmath16 is again assumed to serve the @xmath17th s ", "d pair @xmath85 , it examines both i ) how much interference is received from the undesired sources and the selected relays in the set @xmath38 for the first hop and ii ) how much interference is generated by itself to the other destinations by computing the following metric @xmath110 , termed _ total interference level  ( til ) _ : @xmath111 where @xmath88 and @xmath89 .", "we note that steps 1 and 2 can not be exchangeable due to the fact that the inter - relay interference term @xmath112 is measured after determining the first relay set @xmath38 .", "if the relay set selection order is switched , then the metric til in ( [ eq : til ] ) will not be available .    according to the computed til @xmath110", ", we also apply the timer - based method used in step 1 for the second relay set selection . the relay @xmath113 computes the set of @xmath1 tils , @xmath114 , and then starts its timer with @xmath1 initial values , proportional to the @xmath1 tils .", "thus , we need to compare @xmath115 til metrics over the relay nodes in the set @xmath116 in order to determine who will be selected as the second relay set .", "the rest of the relay set selection protocol ( i.e. , rts message exchange among relay nodes ) almost follows the same line as that of step 1 .", "the timer - based algorithm for the second relay set selection terminates when @xmath1 rts messages are sent out in consecutive order .", "then , @xmath1 relay nodes having a sufficiently small amount of til @xmath110 are selected as the second relay set @xmath56 .", "owing to the channel reciprocity of tdd systems , the sum of inter - relay interference power received at any relay @xmath117 , @xmath118 , also turns out to be sufficiently small when @xmath2 is large .", "that is , it is also guaranteed that @xmath1 selected relays in the set @xmath38 have a sufficiently small amount of til .    the overhead of each scheduling time slot ( i.e. , the total number of bits required for exchanging rts messages among the relay nodes ) can be made arbitrarily small , compared to one transmission block . from the fact that @xmath1 rts messages , consisting of at most @xmath104 bits , are sent out in each relay set selection step ,", "only @xmath119 bit transmission could suffice .", "the @xmath27 selected relays request data transmission to their desired source nodes .", "each source ( @xmath14 ) then starts to transmit data to the corresponding destination ( @xmath15 ) via one of its two relay nodes alternately ( @xmath120 or @xmath66 ) , which was specified earlier .", "if the tils of the selected relays are arbitrarily small , then i ) the associated undesired source  relay and relay  destination channel links and ii ) the inter - relay channel links are all in deep fade . in section  [ subsec : ana ] , we will show that it is possible to choose such relays with the help of the multiuser diversity gain .    at the receiver side ,", "each relay or destination detects the signal sent from its desired transmitter , while simply treating interference as gaussian noise .", "thus , no multiuser detection is performed at each receiver , thereby resulting in an easier implementation .      in this subsection , using the scaling argument bridging between the number of relays , @xmath2 , and the received snr ( refer to  @xcite for the details ) , we shall show 1 ) the lower bound on the dof of the @xmath121 channel with interfering relays as @xmath2 increases and 2 ) the minimum @xmath2 required to guarantee the achievability result .", "the total number of dof , denoted by @xmath122 , is defined as  @xcite @xmath123 where @xmath124 denotes the transmission rate of source @xmath14 . using the ond framework in the @xmath125 channel with interfering relays where @xmath26 transmission slots per block are used ,", "the achievable @xmath122 is lower - bounded by @xmath126 where @xmath127 denotes the received signal - to - interference - and - noise ratio ( sinr ) at the relay @xmath128 and @xmath129 denotes the received sinr at the destination @xmath15 when the relay @xmath128 transmits the desired signal ( @xmath130 and @xmath19 ) .", "more specifically , the above sinrs can be formally expressed as , the third term in the denominator of @xmath131 ( i.e. , the inter - relay interference term ) becomes zero . ]", "@xmath132 where the second term in the denominator of @xmath133 indicates the interference power at relay @xmath134 received from the sources while the third term indicates the inter - relay interference , and the second term in the denominator of @xmath135 indicates the interference power at the destination @xmath136 received from the active relays . here ,", "@xmath137 , i.e. , @xmath138 if @xmath139 , and vice versa .", "we focus on the first relay set @xmath140 s perspective to examine the received sinr values according to each time slot .", "let us first denote @xmath141 for @xmath142 .", "for the first hop , at time slot @xmath143 ( i.e. , each odd time slot ) , @xmath144 , the received @xmath131 at @xmath145 is lower - bounded by @xmath146 where @xmath147 indicates the scheduling metric in ( [ eq : il ] ) when @xmath61 is assumed to serve the @xmath18th s ", "d pair ( @xmath148 , @xmath136 ) . for the second hop , at time slot @xmath149", "( i.e. , each even time slot ) , @xmath150 , the received @xmath151 at @xmath152 is lower - bounded by @xmath153 where the second inequality holds due to the channel reciprocity .", "the term @xmath154 in the denominator of ( [ eq : sinr_1_pi1 ] ) and ( [ eq : sinr_2_pi1 ] ) needs to scale as @xmath155 , i.e. , @xmath156 , so that both @xmath157 and @xmath158 scale as @xmath159 with increasing snr , which eventually enables to achieve the dof of @xmath160 per s ", "d pair from ( [ eq : dof_ond ] ) .", "means that there exist constant @xmath161 and @xmath162 such that @xmath163 for all @xmath164 .", "ii ) @xmath165 if @xmath166 .", "iii ) @xmath167 means that @xmath168  @xcite . ]", "even if such a bounding technique in ( [ eq : sinr_1_pi1 ] ) and ( [ eq : sinr_2_pi1 ] ) leads to a loose lower bound on the sinr , it is sufficient to prove our achievability result in terms of dof and relay scaling law .", "now , let us turn to the second relay set @xmath169 .", "similarly as in ( [ eq : sinr_1_pi1 ] ) , for the first hop , at time slot @xmath149 , @xmath170 , the received @xmath171 at @xmath172 is lower - bounded by @xmath173 where @xmath174 indicates the til in when @xmath77 is assumed to serve the @xmath18th s ", "d pair ( @xmath148 , @xmath136 ) . for the second hop , at time slot @xmath175 , @xmath144 ,", "the received @xmath176 at @xmath152 can also be lower - bounded by @xmath177 the next step is thus to characterize the three metrics @xmath86 , @xmath178 , and @xmath110 ( @xmath179 and @xmath180 ) and their cumulative density functions ( cdfs ) in the @xmath0 channel with interfering relays , which is used to analyze the lower bound on the dof and the required relay scaling law in the model under consideration . since it is obvious to show that the cdf of @xmath178 is identical to that of @xmath110 , we focus only on the characterization of @xmath110 .", "the scheduling metric @xmath86 follows the chi - square distribution with @xmath181 degrees of freedom since it represents the sum of i.i.d .", "@xmath182 chi - square random variables with 2 degrees of freedom .", "similarly , the til @xmath110 follows the chi - square distribution with @xmath183 degrees of freedom .", "the cdfs of the two metrics @xmath86 and @xmath110 are given by @xmath184 respectively , where @xmath185 is the gamma function and @xmath186 is the lower incomplete gamma function  ( * ? ? ?", "( 8.310.1 ) ) .", "we start from the following lemma .", "[ lem:1 ] for any @xmath187 , the cdfs of the random variables @xmath86 and @xmath110 in and are lower - bounded by @xmath188 and @xmath189 , respectively , where @xmath190 and @xmath191 is the gamma function .", "the detailed proof of this argument is omitted here since it essentially follows the similar line to the proof of  ( * ? ? ?", "* lemma 1 ) with a slight modification .    in the following theorem ,", "we establish our first main result by deriving the lower bound on the total dof in the @xmath192 channel with interfering relays .", "[ thrm:1 ] suppose that the ond scheme with alternate relaying is used for the @xmath121 channel with interfering relays .", "then , for @xmath26 data transmission time slots , @xmath193 is achievable if @xmath194 .", "from ( [ eq : dof_ond])([eq : sinr_2_pi2 ] ) , the ond scheme achieves @xmath195 provided that the two values @xmath196 and @xmath197 are less than or equal to some constant @xmath198 , independent of snr , for all s  d pairs .", "then , a lower bound on the achievable @xmath122 is given by @xmath199 which indicates that @xmath200 dof is achievable for a fraction @xmath201 of the time for actual transmission , where @xmath202    we now examine the relay scaling condition such that @xmath201 converges to one with high probability .", "for the simplicity of the proof , suppose that the first and the second relay sets @xmath140 and @xmath169 are selected out of two mutually exclusive relaying candidate sets @xmath203 and @xmath204 , respectively , i.e. , @xmath205 , @xmath206 , @xmath207 , @xmath208 , and @xmath209 .", "then , we are interested in how @xmath210 and @xmath211 scale with snr in order to guarantee that @xmath201 tends to one , where @xmath212 denotes the cardinality of @xmath213 for @xmath130 . from ( [ eq : pond : def0 ] ) , we further have @xmath214    let @xmath215 with @xmath216 and @xmath217 be the candidate set associated with the second relay set and the @xmath218th s - d pair and its cardinality , respectively . for a constant @xmath198 , we can bound the second term in as follows : @xmath219 where the inequality @xmath220 holds from the de morgan s law ; @xmath221 follows from the union bound ; @xmath222 follows since @xmath223 ; @xmath224 follows since @xmath225 are the i.i.d .", "random variables @xmath226 for a given @xmath18 , owning to the fact that the channels are i.i.d .", "variables ; and @xmath227 follows from lemma  [ lem:1 ] with @xmath228 since @xmath229 as @xmath230 and from the fact that @xmath231 .", "we now pay our attention to the first term in , which can be bounded by @xmath232 where the equality follows from the fact that @xmath233 and @xmath234 for @xmath235 are the functions of different random variables and thus are independent of each other . by letting @xmath236 , by the definition of @xmath233", ", we have @xmath237 where the inequality follows from the fact that for any random variables @xmath238 and @xmath239 , @xmath240 @xmath241 @xcite . in the same manner ,", "let @xmath242 with @xmath243 and @xmath244 be the candidate set associated with the first relay set and the @xmath218th s ", "d pair and its cardinality , respectively .", "then , we can bound the first two terms in the rhs of   as follows : @xmath245 where the last inequality follows from lemma  [ lem:1 ] with @xmath246 .", "finally , from , it follows that @xmath247 tends to zero as @xmath248 grows large by noting that @xmath249 due to the reciprocal property of tdd systems . from , , and , it is obvious that if @xmath250 and @xmath248 scale faster than @xmath251 and @xmath7 , respectively , then @xmath252 therefore , @xmath201 asymptotically approaches one , which means that the dof of @xmath253 is achievable with high probability if @xmath254 .", "this completes the proof of the theorem .", "note that the lower bound on the dof asymptotically approaches @xmath1 for large @xmath26 , which implies that our system operates in virtual full - duplex mode .", "the parameter @xmath2 required to obtain full dof ( i.e. , @xmath1 dof ) needs to increase exponentially with the number of s  d pairs , @xmath1 , in order to make the sum of @xmath255 interference terms in the til metric ( [ eq : til ] ) non - increase with increasing snr at each relay . scales slower than @xmath7 , it does not affect the performance in terms of dof and relay scaling laws . ] here , from the perspective of each relay in @xmath56 , the snr exponent @xmath255 indicates the total number of interference links and stems from the following three factors : the sum of interference power received from other sources , the sum of interference power generated to other destinations , and the sum of inter - relay interference power generated from the relays in @xmath38 . from theorem  [ thrm:1 ] ,", "let us provide the following interesting discussions regarding the dof achievability .", "@xmath1 dof can be achieved by using the proposed ond scheme in the @xmath0 channel with interfering links among relay nodes , if the number of relay nodes , @xmath2 , scales faster than @xmath7 and the number of transmission slots in one block , @xmath26 , is sufficiently large . in this case , all the interference signals are almost nulled out at each selected relay by exploiting the multiuser diversity gain . in other words , by applying the ond scheme to the interference - limited @xmath0 channel such that the channel links are inherently coupled with each other , the links among each s  d path via one relay can be completely decoupled , thus enabling us to achieve the same dof as in the interference - free channel case .", "it is not difficult to show that the centralized relay selection method that maximizes the received sinr ( at either the relay or the destination ) using global csi at the transmitter , which is a combinatorial problem with exponential complexity , gives the same relay scaling result @xmath194 along with full dof .", "however , even with our ond scheme using a decentralized relay selection based only on local csi , the same achievability result is obtained , thus resulting in a much easier implementation .      in this subsection , we analyze the til decaying rate under the ond scheme with alternate relaying , which is meaningful since the desired relay scaling law is closely related to the til decaying rate with respect to @xmath2 for given snr .", "let @xmath256 denote the @xmath1th smallest til among the ones that @xmath2 selected relay nodes compute .", "since the @xmath1 relays yielding the til values up to the @xmath1th smallest one are selected , the @xmath1th smallest til is the largest among the tils that the selected relays compute .", "similarly as in  @xcite , by markov s inequality , a lower bound on the average decaying rate of @xmath256 with respect to @xmath2 , @xmath257 $ ] , is given by @xmath258\\ge \\frac{1}{\\epsilon}\\pr(l_{k\\text{th - min}}\\le \\epsilon),\\end{aligned}\\ ] ] where the inequality always holds for @xmath259 .", "we denote @xmath260 as the probability that there are only @xmath1 relays satisfying @xmath261 , which is expressed as @xmath262 where @xmath263 is the cdf of the til .", "since @xmath264 is lower - bounded by @xmath265 , a lower bound on the average til decaying rate is given by @xmath258\\ge \\frac{1}{\\epsilon}\\mathcal{p}_k(\\epsilon ) .", "\\label{eq : til_min}\\end{aligned}\\ ] ]    the next step is to find the parameter @xmath266 that maximizes @xmath260 in terms of @xmath267 in order to provide the tightest lower bound .", "[ lem:2 ] when a constant @xmath266 satisfies the condition @xmath268 , @xmath269 in ( [ eq : p_k ] ) is maximized for a given @xmath2 .    to find the parameter @xmath266 that maximizes @xmath260", ", we take the first derivative with respect to @xmath267 , resulting in @xmath270 which is zero when @xmath271 the parameter @xmath266 is the unique value that maximizes @xmath260 since @xmath272 which completes the proof of the lemma .", "now , we establish our second main theorem , which shows a lower bound on the til decaying rate with respect to @xmath2 .", "[ thrm:2 ] suppose that the ond scheme with alternate relaying is used for the @xmath0 channel with interfering relays .", "then , the decaying rate of til is lower - bounded by @xmath258 \\ge \\theta\\left(n^{\\frac{1}{3k-2}}\\right).\\end{aligned}\\ ] ]    as shown in ( [ eq : til_min ] ) , the til decaying rate is lower - bounded by the maximum of @xmath273 over @xmath267 . by lemma", "[ lem:2 ] , @xmath274 is maximized when @xmath275 .", "thus , we have @xmath258 & \\ge \\frac{1}{\\mathcal{f}_l^{-1}(k / n)}{{n}\\choose{k}}\\left(\\frac{k}{n}\\right)^k \\left(1-\\frac{k}{n}\\right)^{n - k } \\nonumber\\\\ & \\ge \\frac{1}{\\mathcal{f}_l^{-1}(k / n)}\\left(\\frac{n - k+1}{n}\\right)^k \\!\\!\\!\\left(1-\\frac{k}{n}\\right)^{n - k } \\nonumber\\\\ & \\ge \\frac{1}{\\mathcal{f}_l^{-1}(k /", "n ) } \\left(\\frac{1}{k}\\right)^k e^{-k } \\nonumber\\\\ & \\ge \\theta\\left(n^{\\frac{1}{3k-2}}\\right ) , \\nonumber\\end{aligned}\\ ] ] where the second and third inequalities hold since @xmath276 and @xmath277 , respectively . by lemma  [ lem:1 ]", ", it follows that @xmath278 , where @xmath279 is given by ( [ eq : c1 ] ) .", "hence , the last inequality also holds , which completes the proof of the theorem .    from theorem", "[ thrm:2 ] , the following valuable insight is provided : the smaller snr exponent of the relay scaling law , the faster til decaying rate with respect to @xmath2 .", "this asymptotic result will be verified in a finite @xmath2 regime via numerical evaluation in section  [ sec:5 ] .      for comparison , the ond scheme without alternate relaying", "is also explained in this subsection .", "it is worth noting that there exists a trade - off between the lower bound on the dof and the minimum number of relays required to guarantee our achievability result by additionally introducing the ond protocol without alternate relaying . in the scheme ,", "the first relay set @xmath38 only participates in data forwarding .", "that is , the second relay set @xmath56 does not need to be selected for the ond protocol without alternate relaying .", "specifically , the steps of each node during one block are then described as follows :    * time slot 1 : sources @xmath30 transmit their first encoded symbols @xmath31 , where @xmath32 represents the @xmath33th transmitted symbol of the @xmath17th source node .", "a set of @xmath1 selected relay nodes , @xmath34 , operating in receive mode at each odd time slot , listens to @xmath31 .", "other @xmath35 relay nodes and destinations @xmath36 remain idle . * time slot 2 : the @xmath1 relays in the set @xmath38 forward their first re - encoded symbols @xmath280 @xmath281 to the corresponding @xmath1 destinations .", "the @xmath1 destinations receive from @xmath44 @xmath45 and decode @xmath39 .", "the remaining @xmath35 relays keep idle . *", "the processes in time slots 1 and 2 are repeated to the @xmath55th time slot .", "* time slot @xmath26 : the @xmath1 relays in @xmath38 forward their re - encoded symbols @xmath282 @xmath283 to the corresponding @xmath1 destinations", ". the @xmath1 sources and the other @xmath35 relays remain idle .    when @xmath16 is assumed to serve the @xmath17th s ", "d pair @xmath85 ( @xmath20 and @xmath19 ) , it computes the scheduling metric @xmath86 in ( [ eq : il ] ) . according to the computed @xmath86 ,", "a timer based method is used for relay selection as in section  [ subsub:1strelay ] . because there is no inter - relay interference for the ond scheme without alternate relaying", ", it is expected that the minimum required @xmath2 to achieve the optimal dof is significantly reduced .", "our third main theorem is established as follows .", "[ thrm:3 ] suppose that the ond scheme without alternate relaying is used for the @xmath121 channel .", "then , for @xmath26 data transmission time slots , @xmath284 is achievable if @xmath285 .", "the detailed proof of this argument is omitted here since it basically follows the same line as the proof of theorem  [ thrm:1 ] .    in section  [ sec:5 ] , it will be also seen that in a finite @xmath2 regime , there exists the case even where the ond without alternate relaying outperforms that of the ond with alternate relaying in terms of achievable sum - rates via computer simulations ."], ["in this section , to show the optimality of the proposed ond scheme in the @xmath0 channel with interfering relays , which consists of @xmath1 s  d pairs and @xmath2 relay nodes , we derive an upper bound on the dof using the cut - set bound  @xcite as a counterpart of the lower bound on the total dof in section  [ subsec : ana ] .", "suppose that @xmath286 relay nodes are active , i.e. , receive packets and retransmit their re - encoded ones , simultaneously , where @xmath287 .", "this is a generalized version of our transmission since it is not characterized how many relays need to be activated simultaneously to obtain the optimal dof .", "we consider the two cuts @xmath288 and @xmath289 dividing our network into two parts in a different manner .", "let @xmath290 and @xmath291 denote the sets of sources and destinations , respectively , for the cut @xmath292 in the network ( @xmath293 ) . for the @xmath0 channel model with interfering relays", ", we now use the fact that there is no direct path between an s ", "then , it follows that under @xmath288 , @xmath1 transmit nodes in @xmath294 are on the left of the network , while @xmath286 active relay nodes and @xmath1 ( final ) destination nodes in @xmath295 are on the right and act as receivers . in this case , we can create the @xmath296 multiple - input multiple - output ( mimo ) channel between the two sets of nodes separated by the cut . similarly , the @xmath297 mimo channel are obtained under the cut @xmath289 .", "it is obvious to show that dof for the two mimo channels is upper - bounded by @xmath1 .", "hence , it turns out that even with the half - duplex assumption , our lower bound on the dof based on the ond with alternate relaying asymptotically approaches this upper bound on the dof for large @xmath26 .", "note that this upper bound is generally derived regardless of whether the number of relays , @xmath2 , tends to infinity or not , whereas the scaling condition @xmath298 is included in the achievability proof .", "when @xmath299 and @xmath300 in the @xmath121 channel .", "it is assumed that @xmath300 and @xmath301 are used for the ond schemes with and without alternate relaying , respectively .", ", scaledwidth=57.0% ]     when @xmath302 in the @xmath121 channel .", ", scaledwidth=57.0% ]"], ["in this section , we perform computer simulations to validate the achivability result of the proposed ond scheme in section  [ sec:3 ] for finite parameters @xmath2 and snr in the @xmath0 channel model with interfering relays . in our simulation , the channel coefficients in ( [ eq : y1 ] ) and ( [ eq : y2 ] ) are generated @xmath303 times for each system parameter .", "figure  [ fig:1 ] shows the achievable sum - rates of the @xmath304 channel for the ond schemes with and without alternate relaying according to @xmath305 in db scale when @xmath299 .", "note that @xmath2 is set to a different scalable value according to @xmath305 , i.e. , @xmath306 for the ond with alternate relaying and @xmath307 for the ond without alternate relaying , respectively , to see whether the slope of each curve follows the dof in theorems  [ thrm:1 ] and  [ thrm:2 ] . in the figure", ", the dotted green lines are also plotted to indicate the first order approximation of the achievable rates with a proper bias , where the slopes are given by @xmath1 and @xmath4 for the ond schemes with and without alternate relaying , respectively .    in fig .", "[ fig:2 ] , the log - log plot of the average til of the ond with alternate relaying versus @xmath2 is shown for the @xmath308 channel when @xmath302 . is taken into account to precisely see some trends of curves varying with @xmath2 . ]", "it can be seen that the til tends to decrease linearly with @xmath2 .", "it is further seen how many relays are required with the ond scheme with alternate relaying to guarantee that the til is less than a small constant for a given parameter @xmath1 . in this figure , the dashed lines are also plotted from theoretical results in theorem  [ thrm:2 ] with a proper bias to check the slope of the til .", "we can see that the til decaying rates are consistent with the relay scaling law condition in theorem  [ thrm:1 ] .", "more specifically , the til is reduced as @xmath2 increases with the slope of 0.25 for @xmath299 and 0.143 for @xmath309 , respectively .     when @xmath299 and @xmath310 in the @xmath121 channel .", "both ond schemes with and without alternate relaying are compared .", ", scaledwidth=57.0% ]     when @xmath299 and @xmath310 in the @xmath121 channel . the ond scheme with alternate relaying and the max - min snr scheme", "are compared .", ", scaledwidth=57.0% ]    figure  [ fig:3 ] illustrates the achievable sum - rates of the @xmath121 channel for the ond schemes with and without alternate relaying versus @xmath305 ( in db scale ) when @xmath299 and @xmath311 .", "we can see that in a finite @xmath2 regime , there exists the case where the ond without alternate relaying outperforms that of the ond with alternate relaying .", "this is because for finite @xmath2 , the achievable sum - rates for the alternate relaying case tend to approach a floor with increasing snr faster than no alternate relaying case due to more residual interference in each dimension .", "we can also see that the crossing points slightly move to the right as @xmath2 increases ; this is due to the fact that our ond scheme with alternate relaying always benefits from having more relays for selection , thus resulting in more multiuser diversity gain .", "this highly motivates us to operate our system in a switched fashion when the relay selection scheme is chosen between the ond schemes with and without alternate relaying depending on the operating regime of our system .    to further ascertain the efficacy of our scheme ,", "a performance comparison is performed with a baseline scheduling .", "specifically , in the _ max - min snr _ scheme , each s  d pair selects one relay @xmath16 ( @xmath20 ) such that the minimum out of the desired channel gains of two communication links ( either from @xmath14 to @xmath16 or from @xmath16 to @xmath15 ) becomes the maximum among the associated minimum channel gains over all the unselected relays .", "this max - min snr scheme is well - suited for relay - aided systems if interfering links are absent .", "the achievable sum - rates are illustrated in fig .", "[ fig:4 ] according to @xmath305 ( in db scale ) when @xmath299 and @xmath310 .", "we can see that our ond scheme with alternate relaying outperforms this baseline scheme beyond a certain low snr point .", "we also see that the rate gaps increase when @xmath2 increases in the high snr regime . on the other hand , for fixed @xmath2 ,", "the sum - rates of the max - min scheme are slightly changed with respect to @xmath305 due to more residual interference in each dimension ."], ["an efficient distributed ond protocol operating in virtual full - duplex mode was proposed for the @xmath0 channel with interfering relays , referred to as one of multi - source interfering relay networks . a novel relay scheduling strategy with alternate half - duplex relaying", "was presented in two - hop environments , where a subset of relays is opportunistically selected in terms of producing the minimum total interference level , thereby resulting in network decoupling .", "it was shown that the ond protocol asymptotically achieves full dof even in the presence of inter - relay interference and half - duplex assumption , provided that the number of relays , @xmath2 , scales faster than @xmath7 .", "numerical evaluation was also shown to verify that our scheme outperforms the other relay selection methods under realistic network conditions ( e.g. , finite @xmath2 and snr ) with respect to sum - rates .", "i. shomorony and a. s. avestimehr ,  degrees of freedom of two - hop wireless networks :  everyone gets the entire cake `` , '' in _ proc .", "50th annual allerton conf .", "control , and computing _ , monticello , il , oct .", "t. gou , c. wang , and s. a. jafar ,  aligned interference neutralization and the degrees of freedom of the @xmath5 interference channel with interfering relays , \" in _ proc .", "49th annual allerton conf .", "commun . , control , and computing _ , monticello , il , sep .", "t. gou , c. wang , and s. a. jafar ,  degrees of freedom of a class of non - layered two unicast wireless networks , \" in _ proc .", "45th asilomar conf .", "signals , systems and computers _ , pacific grove , ca , nov . 2011 .", "h.  j. yang , w .- y .", "shin , b.  c. jung , and a.  paulraj , `` opportunistic interference alignment for mimo interfering multiple access channels , '' _ ieee trans .", "wireless commun .", "_ , vol .  12 , no .  5 , pp . 21802192 , may 2013 .", "h.  j. yang , w .- y .", "shin , b.  c. jung , c.  suh , and a.  paulraj , `` opportunistic downlink interference alignment , '' in _ proc .", "inf . theory ( isit ) _ , honolulu , hi , jun./jul .", "2014 , pp . 15881592 .        c. shen and m. p. fitz ,", " opportunistic spatial orthogonalization and its application to fading cognitive radio networks , \" , vol . 5 , no .", "1 , pp . 182189 , feb . 2011 . t. w. ban , w. choi , b. c. jung , and d. k. sung ,  multi - user diversity in a spectrum sharing system , \" , vol .", "1 , pp . 102106 , jan .", "2009 .", "r. zhang and y. c. liang ,  exploiting multi - antennas for opportunistic spectrum sharing in cognitive radio networks , \" , vol .", "1 , pp . 88102 , feb . 2008 .", "y. fan , c. wang , j. thompson , and h. v. poor ,  recovering multiplexing loss through successive relaying using repetition coding , \" , vol . 6 , no . 12 , pp .", "44844493 , dec . 2007 .", "shin , h. j. yang , and b. c. jung ,  opportunistic network decoupling in multi - source interfering relay networks , \" in _ proc . ieee conf . commun . ( icc ) _ , sydney , australia , jun .", "2014 , pp . 26712676 ."]]}
{"article_id": "1212.2054", "article_text": ["massive use of mobile storage media raises data security problem seriously .", "data security of mobile storage media can be realized in different ways , e.g. , file unit encryption , file system level encryption , and full disk encryption [ 1 , 3 , 4 , 5 , 6 ] . reliability of security system should not be based on system mechanism or complexity of system analysis , and it should guarantee safety even if system mechanism or encryption algorithm is opened to third party .", "we analyze security problems of existing disk encryption methods in the next section and describe our disk encryption method based on sdms in the following section . in the last section ,", "we analyze security performance of our system .", "several different methods for disk encryption , such as loopaes , efs , truecrypt , ncryptfs , were suggested [ 4 , 5 , 9 , 10 , 11 ] . in these methods ,", "encryption of disk block is expressed as follows .", "@xmath0    here , be is block encryption function ( aes , 3des , etc ) , op is operation function ( cbc , lrw , xts , etc ) , dek is disk encryption key , c is ciphertext , p is plaintext and i is block index . as above expression shows ,", "these disk encryption systems encrypt plaintext using symmetric - key algorithm through certain operation and apply this operation again to encrypted result . [ 7 ] and [ 8 ] explain bitlocker disk encryption method and its security strength , which is offered in windows vista . here ,", "plaintext is xored with sector key , passed two diffusers in succession , and finally encrypted using aes of cbc mode .", "these disk encryption methods have some weakness in terms of time passage and space expansion", ". we call it temporal limitation and spatial limitation , respectively , in this paper", ".    * @xmath1 + if third party succeed to detect encryption key of a certain sector , data which is stored later in this sector can be decoded .", "* @xmath2 + if third party succeed to detect encryption key of a certain block , whole data of disk is in danger of being decoded .", "gbde based encryption method which was implemented in freebsd overcame these temporal and spatial limitations to a certain extent [ 6 ] . in this method", ", data is stored in sector by being encrypted with different key each time data is writen , because it generates random data newly and encrypt plaintext using it .", "therefore , it is impossible to decrypt data stored newly even though third party succeeds to detect key by attacking a sector . and each key encrypting plaintext sectors are different each other when it writes data on disk , because gbde encrypts plaintext using randomly generated key .", "thus , it is impossible to decrypt data of other block even if third party detects encryption key by attacking a data sector .", "although gbde based disk encryption method overcomes temporal and spatial limitations of previous disk encryption methods considerably , it still has some security problems to be solved .    at first", ", key - key for a given sector is fixed because it is decided depending on the sector address .", "that is , when it was writen new data on sector , sector key is encrypted by same key - key .", "then , if attacker detect 128bit sector key by attacking aes / cbc/128 encrypted plaintext data and detect key - key subsequently by attacking aes / cbc/256 encrypted sector key , it is possible to decrypt newly stored data on this sector .", "we think this is temporal limitation of gbde based disk encryption .", "next , it is easy to get keychain used to encrypt plaintext data , if the correlation between random data generated consecutively by prng is revealed , because it directly uses random data generated by prng as key for plaintext .", "however , strictly speaking , prng generates data deterministically based on the initial value .", "if attacker succeeds to get sector key by attacking key sector and subsequently succeeds to know inner state of prng , he can detect following sector keys easily .", "this allows possibility of decrypting consecutive ciphers by attacking one sector .", "of course , it is possible to make difficult to predict future data from past data using cryptographically secure prng , but it causes another security problem that safety of system depends on the safety of prng too much .", "we think this is spatial limitation of gbde based disk encryption . in the next section", ", we present a new disk encryption method based on the sdms .", "in section 2 , we discussed temporal and spatial limitations of previous works for disk encryption and concluded that gbde still has security problem to be solved , while it is a good disk encryption method . in this section , we propose sdms ( secure disk mixed system ) aimed to solve temporal and spatial limitation of existing disk encryption methods and to control security performance flexibly according to the security requirement of system .", "sdms is a method to encrypt each sector by generating sector key using randomly generated seed and disk encryption key dek . in our method", ", encryption key of each sector is different each other and it is changed whenever encryption is done .", "sdms manages data area of media by dividing into sdms blocks .", "each block consists of sdms_block_da area storing encrypted data and sdms_block_sa area storing random numbers which are used to generate encryptioin key for the encryption of sdms_block_da area .", "sdms_block_sa area consists of sdms_unit_seeds which are seed data for each sector .", "1 shows data structure of sdms .", "size of each area is determined depeding on the size of random data sdms_unit_seed needed to generate sector key .", "@xmath3    if we store sdms_block_sa in one sector , the number of sector of sdms _", "block_da is equal to 512 / size(sdms_unit_seed ) .", "for example , if we set size(sdms_unit_seed ) = 8 byte ( 128 bit ) , then the number of sectors of sdms_block_da is equal to @xmath4 .", "that is , 64 plaintext sectors constitute a sdms block ( sdms_block ) and one seed sector ( sdms_block_sa ) is in this block .", "there is no constraint that sdms_block_sa must be one sector in sdms block .", "system designer can adjust this setting freely according to the security requirement and this setting will change processing of blocks of sdms .", "encryption mode of sdms is expressed as follows .", "@xmath5    here , ea is encryption function , rtek is encryption key , c is cipher , and p is plaintext .", "rtek is determined on the fly in time of real - time encryption ( or decryption ) as follows .", "@xmath6    here , dek is disk encryption key , seed is random data in sdms_unit_seed area , i is sector index and dk_func is key derivation function .", "dk_func is a function to derive real - time encryption key from disk encryption key , seed and sector index .", "it is a one - way function where the length of output is constantly the same as the length of rtek .", "encryption and decryption algorithms of sdms are as follows .", "* @xmath7 * * writing request for i - th sector * * random generation of seed * * calculation of rtek * * encryption of plaintext with rtek * * writing cipher on sdms_block_da * * writing seed on sdms_block_sa * @xmath8 * * reading request for i - th sector * * getting seed from sdms_block_sa * * reading cipher from sdms_block_sa * * calculation of rtek * * decryption of cipher with rtek          dek is generated when disk is initialized and is used to encrypt whole data of disk . if dek is revealed , attacker can decrypt whole data of disk .", "dek can be stored in the same disk with plaintext or in physically separated memory device such as usb memory or file server of high security level .", "no matter it is stored in data disk or physically isolated memory device , dek must be encrypted based on the user authentication information .", "user can be authenticated through pkcs#5 based password authentication or pkcs#11 based smart card authentication or ssl based network authentication , but what is important is to receive key safely for the decryption of dek [ 12 , 13 ] . in this paper , we only consider disk data encryption which uses dek , on the premise that dek is managed safely , though dek is very important for the reliabile management of data .", "dk_func is a function to get sector encryption key rtek in real time for the encryption of sector .", "@xmath6    as we can see here , it outputs sector key for sector encryption from dek , seed corresponding to the sector , and the sector index .", "input data space must be larger enough than output data space in the design of dk_func .", "for instance , if dek is 2048 bit , sector index is 32bit and seed size is 128bit , then input data space is @xmath9 .", "therefore , in case of using aes / xts for sector encryption , 512 bit key is needed , and hence output space is @xmath10 which is smaller enough than input space .", "dk_func must be implemented so as to satisfy one - wayness and collision resistance as possible .", "so it is desirable to construct dk_func using cryptographically safe hash function or hash chain .", "firstly , sdms solved temporal limitation problem of data encryption .", "sdms generates encryption key by generating random seed newly each time when writing request occurs .", "this makes it impossible to decrypt sector data written later , although attacker succeeds to break rtek of the sector .", "secondly , sdms solved spatial limitation problem of data encryption to a certain extent . in sdms , even though attacker succeeds to attack ea encrypted certain sector , the only thing he knows is rtek of that sector .", "he ca nt read the contents of other sectors unless he knows dek through the attack on ea or dk_func . in this way , sdms overcomes remarkably security weakness that whole data of disk can be revealed by succeeding to attack a particular sector .", "thirdly , sdms can control security performance flexibly according to the sequrity requirements .", "sdms generates encryption key using disk encryption key dek and random seed .", "dek can be set big enough according to the security requirement .", "for instance , if we set dek as bigger than 1024 bit , then search space will be increased more than @xmath11 when attacker attacks dkfunc to know dek .", "the size of seed ( size(sdms_unit_seed ) ) which is used to generate sector key for sector encryption can also be set big enough according to the security requirement of system and there is no restriction that sdms_unit_seed must be arranged to each sector .", "configuring system to generate sector key by arranging one sdms_unit_seed to several sectors , we can coordinate balance between security performance and operation cost reasonably .", "attacker must attack encrypted sector data unless he knows user authentication information or decrypted dek by evil code .", "if sector keys that encrypted sector data have no statistical characteristics and there is no information or algorithm helpful to estimate sector key , brute force search will be appropriate method . in case using aes / xts/256 for sector encryption", ", the amount of computation will be equal to @xmath12 . here", ", aes / xts encryption is expressed as follows [ 16 ] .      here", ", @xmath14 is multiplication operator in modulo @xmath15 , k1 is key of symmetric key encryption algorithm ( e ) , k2 is secondary key , i is block index in encryption unit ( sector ) , n is address of encryption unit ( sector ) and  is base of gf ( galois field ) .    attacker also can decrypt whole data of disk if he knows dek .", "therefore , attacker may try to attack dek and then to decrypt sector data using it . to attack dek , he must attack dk_func which derives sector key", ". he can find necessary items in @xmath16 space because he knows seed and sector index which are the inputs of dk_func .", "for instance , if size(dek)=2048 , attacker must search @xmath17 space . in case", "aes / xts/256 is selected as ea , 256bit key for xts operation and 256bit key for final aes block encryption are needed , and thus totally needed key is 512bit .", "therefore , attacker can find candidates outputting same rtek if he calculate dk_func for about @xmath10 dek candidates with computation @xmath18 .", "there exists about @xmath19 candidates in this case .", "that is because we use sector index and seed when we calculate rtek for other sector . to get correct dek from @xmath19 dek candidates in @xmath17 space", "is very difficult , though it would be possible to get sector key by attacking particular sector .      security problem in case using the result of prng as key directly for sector encryption was considered in section 2 .", "sdms uses the output of prng as input of dk_func for getting sector key and stores output of prng on disk without changing . to attack disk data", "encrypted using sdms needs not attack prng .", "random data generated by prng    * solves temporal limitation of disk encryption by changing sector key each time it writes data , * solves spatial limitation of disk encryption by setting sector key of each sector differently , * makes it more difficult to attack dk_func .", "r. k. watkins , j. c. isaacs , and s. y. foo : evolvable random number generators : a schemata - based approach . in 2001 genetic and evolutionary computation conference late breaking papers , pages 469473 , 2001 ."], "abstract_text": ["<S> we propose a disk encryption method , called secure disk mixed system ( sdms ) in this paper , for data protection of disk storages such as usb flash memory , usb hard disk and cd / dvd . </S>", "<S> it is aimed to solve temporal and spatial limitation problems of existing disk encryption methods and to control security performance flexibly according to the security requirement of system . </S>", "<S> sdms stores data by encrypting with different encryption key per sector and updates sector encryption keys each time data is written . </S>", "<S> security performance of sdms is analyzed at the end of the paper . </S>"], "labels": null, "section_names": ["introduction", "previous works", "sdms-based disk encryption", "analysis result"], "sections": [["massive use of mobile storage media raises data security problem seriously .", "data security of mobile storage media can be realized in different ways , e.g. , file unit encryption , file system level encryption , and full disk encryption [ 1 , 3 , 4 , 5 , 6 ] . reliability of security system should not be based on system mechanism or complexity of system analysis , and it should guarantee safety even if system mechanism or encryption algorithm is opened to third party .", "we analyze security problems of existing disk encryption methods in the next section and describe our disk encryption method based on sdms in the following section . in the last section ,", "we analyze security performance of our system ."], ["several different methods for disk encryption , such as loopaes , efs , truecrypt , ncryptfs , were suggested [ 4 , 5 , 9 , 10 , 11 ] . in these methods ,", "encryption of disk block is expressed as follows .", "@xmath0    here , be is block encryption function ( aes , 3des , etc ) , op is operation function ( cbc , lrw , xts , etc ) , dek is disk encryption key , c is ciphertext , p is plaintext and i is block index . as above expression shows ,", "these disk encryption systems encrypt plaintext using symmetric - key algorithm through certain operation and apply this operation again to encrypted result . [ 7 ] and [ 8 ] explain bitlocker disk encryption method and its security strength , which is offered in windows vista . here ,", "plaintext is xored with sector key , passed two diffusers in succession , and finally encrypted using aes of cbc mode .", "these disk encryption methods have some weakness in terms of time passage and space expansion", ". we call it temporal limitation and spatial limitation , respectively , in this paper", ".    * @xmath1 + if third party succeed to detect encryption key of a certain sector , data which is stored later in this sector can be decoded .", "* @xmath2 + if third party succeed to detect encryption key of a certain block , whole data of disk is in danger of being decoded .", "gbde based encryption method which was implemented in freebsd overcame these temporal and spatial limitations to a certain extent [ 6 ] . in this method", ", data is stored in sector by being encrypted with different key each time data is writen , because it generates random data newly and encrypt plaintext using it .", "therefore , it is impossible to decrypt data stored newly even though third party succeeds to detect key by attacking a sector . and each key encrypting plaintext sectors are different each other when it writes data on disk , because gbde encrypts plaintext using randomly generated key .", "thus , it is impossible to decrypt data of other block even if third party detects encryption key by attacking a data sector .", "although gbde based disk encryption method overcomes temporal and spatial limitations of previous disk encryption methods considerably , it still has some security problems to be solved .    at first", ", key - key for a given sector is fixed because it is decided depending on the sector address .", "that is , when it was writen new data on sector , sector key is encrypted by same key - key .", "then , if attacker detect 128bit sector key by attacking aes / cbc/128 encrypted plaintext data and detect key - key subsequently by attacking aes / cbc/256 encrypted sector key , it is possible to decrypt newly stored data on this sector .", "we think this is temporal limitation of gbde based disk encryption .", "next , it is easy to get keychain used to encrypt plaintext data , if the correlation between random data generated consecutively by prng is revealed , because it directly uses random data generated by prng as key for plaintext .", "however , strictly speaking , prng generates data deterministically based on the initial value .", "if attacker succeeds to get sector key by attacking key sector and subsequently succeeds to know inner state of prng , he can detect following sector keys easily .", "this allows possibility of decrypting consecutive ciphers by attacking one sector .", "of course , it is possible to make difficult to predict future data from past data using cryptographically secure prng , but it causes another security problem that safety of system depends on the safety of prng too much .", "we think this is spatial limitation of gbde based disk encryption . in the next section", ", we present a new disk encryption method based on the sdms ."], ["in section 2 , we discussed temporal and spatial limitations of previous works for disk encryption and concluded that gbde still has security problem to be solved , while it is a good disk encryption method . in this section , we propose sdms ( secure disk mixed system ) aimed to solve temporal and spatial limitation of existing disk encryption methods and to control security performance flexibly according to the security requirement of system .", "sdms is a method to encrypt each sector by generating sector key using randomly generated seed and disk encryption key dek . in our method", ", encryption key of each sector is different each other and it is changed whenever encryption is done .", "sdms manages data area of media by dividing into sdms blocks .", "each block consists of sdms_block_da area storing encrypted data and sdms_block_sa area storing random numbers which are used to generate encryptioin key for the encryption of sdms_block_da area .", "sdms_block_sa area consists of sdms_unit_seeds which are seed data for each sector .", "1 shows data structure of sdms .", "size of each area is determined depeding on the size of random data sdms_unit_seed needed to generate sector key .", "@xmath3    if we store sdms_block_sa in one sector , the number of sector of sdms _", "block_da is equal to 512 / size(sdms_unit_seed ) .", "for example , if we set size(sdms_unit_seed ) = 8 byte ( 128 bit ) , then the number of sectors of sdms_block_da is equal to @xmath4 .", "that is , 64 plaintext sectors constitute a sdms block ( sdms_block ) and one seed sector ( sdms_block_sa ) is in this block .", "there is no constraint that sdms_block_sa must be one sector in sdms block .", "system designer can adjust this setting freely according to the security requirement and this setting will change processing of blocks of sdms .", "encryption mode of sdms is expressed as follows .", "@xmath5    here , ea is encryption function , rtek is encryption key , c is cipher , and p is plaintext .", "rtek is determined on the fly in time of real - time encryption ( or decryption ) as follows .", "@xmath6    here , dek is disk encryption key , seed is random data in sdms_unit_seed area , i is sector index and dk_func is key derivation function .", "dk_func is a function to derive real - time encryption key from disk encryption key , seed and sector index .", "it is a one - way function where the length of output is constantly the same as the length of rtek .", "encryption and decryption algorithms of sdms are as follows .", "* @xmath7 * * writing request for i - th sector * * random generation of seed * * calculation of rtek * * encryption of plaintext with rtek * * writing cipher on sdms_block_da * * writing seed on sdms_block_sa * @xmath8 * * reading request for i - th sector * * getting seed from sdms_block_sa * * reading cipher from sdms_block_sa * * calculation of rtek * * decryption of cipher with rtek          dek is generated when disk is initialized and is used to encrypt whole data of disk . if dek is revealed , attacker can decrypt whole data of disk .", "dek can be stored in the same disk with plaintext or in physically separated memory device such as usb memory or file server of high security level .", "no matter it is stored in data disk or physically isolated memory device , dek must be encrypted based on the user authentication information .", "user can be authenticated through pkcs#5 based password authentication or pkcs#11 based smart card authentication or ssl based network authentication , but what is important is to receive key safely for the decryption of dek [ 12 , 13 ] . in this paper , we only consider disk data encryption which uses dek , on the premise that dek is managed safely , though dek is very important for the reliabile management of data .", "dk_func is a function to get sector encryption key rtek in real time for the encryption of sector .", "@xmath6    as we can see here , it outputs sector key for sector encryption from dek , seed corresponding to the sector , and the sector index .", "input data space must be larger enough than output data space in the design of dk_func .", "for instance , if dek is 2048 bit , sector index is 32bit and seed size is 128bit , then input data space is @xmath9 .", "therefore , in case of using aes / xts for sector encryption , 512 bit key is needed , and hence output space is @xmath10 which is smaller enough than input space .", "dk_func must be implemented so as to satisfy one - wayness and collision resistance as possible .", "so it is desirable to construct dk_func using cryptographically safe hash function or hash chain .", "firstly , sdms solved temporal limitation problem of data encryption .", "sdms generates encryption key by generating random seed newly each time when writing request occurs .", "this makes it impossible to decrypt sector data written later , although attacker succeeds to break rtek of the sector .", "secondly , sdms solved spatial limitation problem of data encryption to a certain extent . in sdms , even though attacker succeeds to attack ea encrypted certain sector , the only thing he knows is rtek of that sector .", "he ca nt read the contents of other sectors unless he knows dek through the attack on ea or dk_func . in this way , sdms overcomes remarkably security weakness that whole data of disk can be revealed by succeeding to attack a particular sector .", "thirdly , sdms can control security performance flexibly according to the sequrity requirements .", "sdms generates encryption key using disk encryption key dek and random seed .", "dek can be set big enough according to the security requirement .", "for instance , if we set dek as bigger than 1024 bit , then search space will be increased more than @xmath11 when attacker attacks dkfunc to know dek .", "the size of seed ( size(sdms_unit_seed ) ) which is used to generate sector key for sector encryption can also be set big enough according to the security requirement of system and there is no restriction that sdms_unit_seed must be arranged to each sector .", "configuring system to generate sector key by arranging one sdms_unit_seed to several sectors , we can coordinate balance between security performance and operation cost reasonably ."], ["attacker must attack encrypted sector data unless he knows user authentication information or decrypted dek by evil code .", "if sector keys that encrypted sector data have no statistical characteristics and there is no information or algorithm helpful to estimate sector key , brute force search will be appropriate method . in case using aes / xts/256 for sector encryption", ", the amount of computation will be equal to @xmath12 . here", ", aes / xts encryption is expressed as follows [ 16 ] .      here", ", @xmath14 is multiplication operator in modulo @xmath15 , k1 is key of symmetric key encryption algorithm ( e ) , k2 is secondary key , i is block index in encryption unit ( sector ) , n is address of encryption unit ( sector ) and  is base of gf ( galois field ) .    attacker also can decrypt whole data of disk if he knows dek .", "therefore , attacker may try to attack dek and then to decrypt sector data using it . to attack dek , he must attack dk_func which derives sector key", ". he can find necessary items in @xmath16 space because he knows seed and sector index which are the inputs of dk_func .", "for instance , if size(dek)=2048 , attacker must search @xmath17 space . in case", "aes / xts/256 is selected as ea , 256bit key for xts operation and 256bit key for final aes block encryption are needed , and thus totally needed key is 512bit .", "therefore , attacker can find candidates outputting same rtek if he calculate dk_func for about @xmath10 dek candidates with computation @xmath18 .", "there exists about @xmath19 candidates in this case .", "that is because we use sector index and seed when we calculate rtek for other sector . to get correct dek from @xmath19 dek candidates in @xmath17 space", "is very difficult , though it would be possible to get sector key by attacking particular sector .      security problem in case using the result of prng as key directly for sector encryption was considered in section 2 .", "sdms uses the output of prng as input of dk_func for getting sector key and stores output of prng on disk without changing . to attack disk data", "encrypted using sdms needs not attack prng .", "random data generated by prng    * solves temporal limitation of disk encryption by changing sector key each time it writes data , * solves spatial limitation of disk encryption by setting sector key of each sector differently , * makes it more difficult to attack dk_func .", "r. k. watkins , j. c. isaacs , and s. y. foo : evolvable random number generators : a schemata - based approach . in 2001 genetic and evolutionary computation conference late breaking papers , pages 469473 , 2001 ."]]}
{"article_id": "1010.1595", "article_text": ["the metropolis  hastings ( mh ) algorithm provides an iterative and converging scheme to sample from a complex target density @xmath0 .", "each iteration of the algorithm generates a new value of the markov chain that relies on the result of the previous iteration .", "the underlying markov principle is well - understood and leads to a generic convergence principle as described , e.g. , in @xcite . however , due to its markovian nature , this algorithm is not straightforward to parallelize , which creates difficulties in slower languages like r @xcite . nevertheless , the increasing number of parallel cores that are available at a very low cost drives more and more interest in `` parallel - friendly '' algorithms , that is , in algorithms that can benefit from the available parallel processing units on standard computers ( see .", "e.g. , @xcite , @xcite , @xcite ) .", "different techniques have already been used to enhance some degree of parallelism in generic metropolis  hastings ( mh ) algorithms , beside the basic scheme of running @xmath1 mcmc algorithms independently in parallel and merging the results .", "for instance , a natural entry is to rely on renewal properties of the markov chain @xcite , waiting for all @xmath1 chains to exhibit a renewal event and then using the blocks as iid , but the constraint of markovianity can not be removed .", "@xcite also points out the difficult issue of accounting for the burn - in time : while , for a single mcmc run , the burn - in time is essentially negligible , it does create a significant bias when running parallel chains ( unless perfect sampling can be implemented ) .", "@xcite mix antithetic coupling and stratification with perfect sampling .", "using a different approach , @xcite rely on @xmath1 parallel chains to build an adaptive mcmc algorithm , considering in essence that the product of the target densities over the chains is their target , a perspective that obviously impacts the convergence properties of the multiple chain . @xcite", "take advantage of parallelization to build a non - reversible algorithm that can avoid the scaling effect of specific neighborhood structures , hence focussing on a very special type of problem .", "a particular family of mh algorithm is the independent metropolis  hastings ( imh ) algorithm , where the proposal distribution ( and hence the proposed value ) does not depend on the current state of the markov chain . due to this characteristic , this specific algorithm is easier to parallelize and can therefore be considered as a good building block toward efficient parallel markov chain monte carlo algorithms , as will be explained in section [ sec : imh ] .", "we will focus on cases where the computation of the likelihood function constitutes the major part of the execution time in the mh algorithm .", "a most realistic example of this setting is provided in @xcite , where the model is based on a very complex fortran program translating the results of several cosmological experiments , hence highly demanding in computing time . in this model ,", "@xcite use adaptive importance sampling and massive parallelization , rather than mcmc .", "the fundamental idea in the current paper is that one can take advantage of the parallel abilities of arbitrary items of computing machinery , from cloud computing to graphical cards ( gpu ) , in the case of the generic imh algorithm , producing an output that corresponds to a much improved monte carlo approximation machine at the same computational cost .", "the techniques presented here are related with those explained in @xcite and more closely to those in @xcite ( @xcite , section 3.1 ) , since these authors condition upon the order statistic of the values proposed by the imh algorithm , although in those earlier papers the links with parallel computation were not established and hence the implementation of the rao - blackwellization scheme became problematic for long chains .", "the plan of the paper is as follows : the standard imh algorithm is recalled in section [ sec : imh ] , followed by a description of our improving scheme , called here `` block independent metropolis ", "hastings '' ( block imh ) .", "this improvement depends on a choice of permutations on @xmath2 that is described in details in section [ sec : permut ] .", "we demonstrate the connections between block imh and rao  blackwellization in section [ sec : rb ] .", "results for a toy example are presented throughout the paper and a realistic probit regression example is described in section [ sec : applications ] as an illustration of the method .", "we recall here the basic imh algorithm , assuming the availability of a proposal distribution that we can sample , and which probability density @xmath3 is known up to a normalization constant . the independent metropolis  hastings algorithm , described in algorithm [ algo : imh ] , generates a markov chain with invariant density @xmath0 , corresponding to the target distribution .", "set @xmath4 to an arbitrary value generate @xmath5 [ basichmacpt ] compute the ratio : @xmath6 set @xmath7 with probability @xmath8 ; otherwise set @xmath9    in the larger picture of monte carlo and mcmc algorithms , the imh algorithm holds a rather special status .", "it has certainly been studied more often than other mcmc schemes @xcite , but it is undoubtedly a less practical solution than the more generic random walk metropolis  hastings algorithm .", "for instance , it is rather rarely used by itself because it requires the derivation of a tolerably good approximation to the true target , approximation that most often is unavailable . on the other hand ,", "first - order approximations and metropolis - within - gibbs schemes are not foreign to calling for imh local moves based on gaussian representations of the targets .", "the reason theoretical studies of the imh algorithm abound is that it has strong links with the non - markovian simulation methods such as importance sampling .", "contrary to random - walk metropolis ", "hastings schemes , imh algorithms may enjoy very good convergence properties and may also reach acceptance probabilities that are close to one .", "furthermore , the potentially large gain in variance reduction provided by the parallelization scheme developped in this paper may counteract the lesser efficiency of the original imh compared with the random walk metropolis  hastings algorithm .", "an important feature of the imh algorithm , when addressing parallelism , is that it can not work but in an iterative manner , since the outcome of step @xmath10 , namely the value @xmath11 , is required to compute the acceptance ratio at step @xmath12 .", "this sequential construction is compulsory for the validation of the algorithm given the markov property at its core @xcite .", "nonetheless , given that , in the imh algorithm , the proposed values @xmath13 are generated independently from the current state of the markov chain , @xmath11 , it is altogether possible to envision the generation of @xmath14 proposed values @xmath15 first , along with the computation of the associated ratios @xmath16 .", "once this computation requirement is completed , only the acceptance steps need to be considered iteratively .", "this two - step perspective makes for a huge saving in computing time when the simulation of the @xmath15 s and the derivation of the @xmath17 s can be achieved in parallel since both the remaining computation of the ratios @xmath8 given the @xmath17 s and their subsequent comparison with uniform draws typically are orders of magnitude faster .    in this respect", "the imh algorithm strongly differs from the random walk metropolis  hastings ( rwmh ) algorithm , for which acceptance ratios can not be processed beforehand because the proposed simulated values depend on the current value of the markov chain .", "the universal availability of parallel processing schemes may thus lead to a new surge of popularity for the imh algorithm .", "indeed , when taking advantage of @xmath1 parallel processing units , an imh can be run for @xmath1 times as many iterations as rwmh , at almost the same computing cost since rwmh can not be directly parallelized .    in order to better describe this increased computing power , we first note that , once @xmath14 successive values of a markov chain have been produced , the sequence is usually processed as a regular monte carlo sample to obtain an approximation of an expectation under the target distribution , @xmath18 $ ] say , for some arbitrary functions @xmath19 .", "we propose in this paper a technique that improves the precision of the estimation of this expectation by taking advantage of parallel processing units without jeopardizing the markov property .    before presenting our improvement scheme", ", we introduce the notation @xmath20 ( read `` or '' ) for the operator that represents a single step of the imh algorithm . using this notation ,", "given the current value @xmath11 and a sequence of @xmath1 independent proposed values @xmath21 , the imh algorithm goes from step @xmath10 to step @xmath22 according to the diagram in figure [ fig : imh ] .", "imh steps between iteration @xmath10 and iteration @xmath22.,scaledwidth=100.0% ]      we propose to take full advantage of the simulated proposed values and of the computation of their corresponding @xmath23 ratios . to this effect", ", we introduce the _ block imh algorithm _ , made of successive simulations of blocks of size @xmath24 . in this alternative scheme , the number of blocks @xmath25 is such that the number of desired iterations @xmath14 is equal to @xmath26 , in order to keep the comparison with a standard imh output fair .", "usually @xmath1 needs not be calibrated since it represents the number of physical parallel processing units that can be exploited by the code .", "however , in principle , this number @xmath1 can be set arbitrarily high and based on virtual parallel processing units , the drawback being then an increase in the computing cost .", "( note that the block imh algorithm can also be implemented with no parallel abilities , still it provides a gain in variance that may counteract the increase in time . ) in the following examples , we take @xmath1 varying from @xmath27 to @xmath28 .", "we first explain how a block is simulated , and then how to move from one block to the next .", "a @xmath29 block consists in the generation of @xmath1 parallel generations of @xmath1 values of markov chains , all starting at time @xmath10 from the current state @xmath11 and all based on the _ same _ proposed simulated values @xmath30 .", "the different between the @xmath1 floes is the orders in which those @xmath31 s are included .", "for instance , these orders may be the @xmath1 circular permutations of @xmath30 , or they may be instead random permutations , as discussed in detail ( and compared ) in section [ sec : permut ] . the block imh algorithm is illustrated in figure [ fig : imhblock ] for the circular set of permutations .", "block simulation from step @xmath12 to step @xmath22 . here", ", circular permutations of the proposed values are used for illustration purposes.,scaledwidth=100.0% ]    it should be clear that each of the @xmath1 parallel chains found in this block is a valid mcmc sequence of length @xmath1 when taken separately . as such", ", it can be processed as a regular mcmc output . in particular , if @xmath11 is simulated from the stationary distribution , any of the subsequent @xmath32 is also simulated from the stationary distribution .", "however , the point of the @xmath1 parallel flows is double :    * it aims at integrating out part of the randomness resulting from the ancillary order in which the @xmath33 s are chosen , getting close to the conditioning on the order statistics of the @xmath33 s advocated by @xcite ; * it also aims at partly integrating out the randomness resulting from the generation of uniform variables in the selection process , since the block implementation results in drawing @xmath34 uniform realizations instead of @xmath1 uniform realizations for a standard imh setting .", "both of those points essentially amount to implementing a new rao  blackwellization technique ( a more precise connection is drawn in section [ sec : rb ] ) . in an independent setting ,", "each of the @xmath33 s occurs a number @xmath35 of times across the @xmath1 steps of the @xmath1 parallel chains , i.e.  for a number @xmath34 of realizations .", "therefore , when considering the standard estimator @xmath36 of @xmath18 $ ] , based on a _ single _ mcmc chain , @xmath37this estimator necessarily has a larger variance than the double average @xmath38where @xmath39 and @xmath40 is the number of times @xmath11 is repeated .", "( the proof for the reduction of the variance from @xmath36 to @xmath41 easily follows from a double integration argument . )", "we again insist on the compelling feature that computing @xmath41 using @xmath1 parallel processing units does not cost more time than computing @xmath36 using a single processing unit .", "the block imh algorithm runs @xmath1 parallel chains during @xmath1 steps , then picks one of the final values ( represented by the black squares ) and iterates .", "( an alternative transition mechanism involves sampling randomly one of the @xmath34 terms within the block.),scaledwidth=100.0% ]    in order to preserve its markov validation , the algorithm must properly continue at time @xmath22 . an obvious choice is to pick one of the @xmath1 sequences at random and to take the corresponding @xmath42 as the value of @xmath43 , starting point of the next parallel block .", "this mechanism is represented in figure [ fig : imhnextblock ] . while valid from a markovian perspective , since the sequences are marginally produced by a regular imh algorithm , this means that the chain deduced from the block imh algorithm is converging at _ exactly _ the same speed as the original imh algorithm .", "an alternative choice for the starting points of the blocks takes advantage of the weights @xmath44 on the @xmath33 s that are computed via the block structure .", "indeed , those weights essentially act as importance weights and they allow for a selection of any of the @xmath34 @xmath45 s as the starting point of the incoming block , which corresponds to choosing one of the proposed @xmath33 s with probability proportional to @xmath44 .", "while this proposal does reduce the length of the resulting chain , it does not impact the estimation aspects ( which still involve all of the @xmath34 values ) and it could improve convergence , given that the weighted @xmath33 s behave like a discretized version of a sample from the target density @xmath0 .", "we will not cover this alternative any further .", "the original version of the block imh algorithm is described in algorithm [ algo : blockimh ] , the algorithm is made of a loop on the @xmath25 blocks and an inner loop on the @xmath1 parallel chains of each block .", "the @xmath1 steps of this inner loop are actually meant to be implemented in parallel .", "the output of algorithm [ algo : blockimh ] is double :    * a standard markov chain of length @xmath14 , which is made of @xmath25 chains of length @xmath1 , each of which is chosen among @xmath1 chains at line [ algo : statepickindex ] of algorithm [ algo : blockimh ] , * a @xmath46 array @xmath47 , on which the estimator @xmath41 is based .", "set @xmath4 to an arbitrary value , compute @xmath48 set @xmath49 , @xmath50 set a block size @xmath1 , and a number of blocks @xmath25 , such that @xmath51 generate all proposed values @xmath52 [ algo : stateintensive ] compute all ratios @xmath53 [ algo : chooseperm ] choose @xmath1 permutations @xmath54 run @xmath1 steps of an imh given :    * @xmath55 * @xmath1 proposed values @xmath56 shuffled with the permutation @xmath57 * the @xmath1 corresponding ratios @xmath58 s    save as @xmath59 the resulting chain [ algo : statepickindex ] draw an index @xmath60 uniformly in @xmath61 , set @xmath62 , set @xmath63 as the corresponding ratio @xmath23 .      since the point - wise evaluation of the target density @xmath64 is usually the most computer - intensive part of the algorithm , sampling additional uniform variables has a negligible impact here , as do further costs related to the storage of vectors larger than in the original imh .", "this is particularly compelling since the multiple chains do not need to be stored further than during a single block execution time .", "that is why , although we sample @xmath1 times more uniforms in the block imh algorithm , we still consider it to be running at roughly of the same cost as the imh algorithm .", "the number of target density evaluations indeed is the same for both and most often represent the overwhelming part of the computing time in the metropolis  hastings algorithm . besides", ", pseudo - random generation of uniforms can also benefit from parallel processing , see e.g. @xcite .    in the following monte carlo experiment ,", "various versions of the block imh algorithm are compared one to another , as well as to standard imh and importance sampling .", "we stress that a straightforward reason for not conducting a comparison with a plain parallel algorithm based on @xmath1 independent parallel chains is that it does not make much sense cost - wise . indeed , running @xmath1 parallel mcmc chains of the same length @xmath14 does cost @xmath1 times more in terms of target density evaluations .", "obviously , if one insists on running @xmath1 independent chains , for instance as to initialize an mcmc algorithm from several well - dispersed starting points , each of those chains can benefit from our stabilizing method , which will improve the resulting estimation .", "the method is presented here for square blocks of dimension @xmath65 , but blocks could be rectangular as well : the algorithm is equally valid when using @xmath66 permutations , leading to @xmath67 blocks .", "we focus here on square blocks because when the machine at hand provides @xmath1 parallel processing units , then it is most efficient to simulate the proposed values and the uniforms , and to compute the target densities and the acceptance ratios at the @xmath1 proposed values in parallel .", "once again , the block imh algorithm with @xmath29 square blocks has about the same cost as the original imh algorithm , because computing target densities and acceptance ratios does more than compensate for the cost of randomly picking an index at the end of each block .", "this amounts to say that line [ basichmacpt ] of algorithm [ algo : imh ] and line [ algo : stateintensive ] of algorithm [ algo : blockimh ] are ( by far ) the most computationally demanding ones in the respective algorithms .", "we now introduce a toy example that we will follow throughout the paper .", "the target @xmath0 is the density of the standard @xmath68 normal distribution and the proposal @xmath3 is the density of the @xmath69 cauchy distribution .", "hence , the density ratio is @xmath70 ( 1 + x^2 ) we only consider the integral @xmath71 , the mean of @xmath0 equal to zero in this case .", "the acceptance rate of the imh algorithm for this example is around @xmath72 .", "( note that imh with higher acceptance rates are considered to be more efficient , in opposition to other metropolis  hastings algorithms , see @xcite . )    in all results related to the toy example presented thereafter , @xmath73 independent runs are used to compute the variance of the estimates .", "the value of @xmath1 represents the number of parallel processing units that are available , ranging from @xmath27 for a desktop computer to @xmath28 for a cluster or a graphics processing unit ( gpu ) ( this value could even be larger for computers equipped with multiple gpus ) .", "the results of the simulation experiments are presented in figures [ fig : barplotpermutations][fig : barplotvariousscale ] as barplots , which indicate the percentage of variance decrease associated with the estimators under comparison , the reference estimator being the standard imh output @xmath36 . in agreement with the block sampling perspective", ", the same proposed values and uniform draws were used for all the estimators that are plotted on the same graph ( that is , for a given value of @xmath1 ) , so that the comparison is not perturbed by an additional noise associated with the simulation .", "while the choice of permutations in line [ algo : chooseperm ] of algorithm [ algo : blockimh ] is irrelevant for the validation of the parallelization , it has important consequences on the variance improvement and we now discuss several natural choices .", "the idea of testing various orders of the proposed values in a imh algorithm appeared in @xcite where the permutations were chosen to be circular .", "we first list natural types of permutations along with some justifications , and then we empirically compare their impact on estimation performances for the toy example .", "let @xmath74 be the set of permutations of @xmath75 .", "its size is @xmath76 , therefore too large to allow for averaging over all permutations , although this solution would be ideal .", "we consider the simpler option of finding @xmath1 efficient permutations in @xmath74 , denoted by @xmath77 , the goal being a choice favoring the largest possible decrease in the variance of the estimator @xmath41 defined in section [ sec : imh ] .", "the most basic choice is to pick the same permutation on each of the @xmath1 chains : @xmath78 this selection may sound counterproductive , but we still obtain a significant decrease in the variance of @xmath41 using this set of permutations , when compared with @xmath36 .", "the reason for the improvement is that @xmath1 times more uniforms are used in @xmath41 than in @xmath36 , leading to a natural rao - blackwellization phenomenon that is studied in details in section [ sec : rb ] .", "nonetheless this simplistic set of permutations is certainly not the best choice since it does not integrate out the ancillary randomness resulting from the arbitrary ordering of the proposed values .", "another simple choice is to use circular permutations . for @xmath79", ", we define @xmath80 an appealing property of the circular permutations is that each simulated value @xmath33 is proposed and evaluated at a different step for each chain .", "however , a drawback is that the order is not deeply changed : for instance @xmath81 will always be proposed one step before @xmath33 except for one of the @xmath1 chains , for which @xmath33 is proposed first .", "a third choice is to use random orders , that is random shufflings of the sequence @xmath75 .", "we can either draw those random permutations with or without replacement in the set @xmath74 , but considering the cardinality of the set @xmath74 this does not make a large difference . indeed , it is unlikely to draw twice the same permutation , except for very small values of @xmath1 .", "a slightly different choice of permutations consists in drawing @xmath82 permutations at random ( @xmath1 is taken to be even here to simplify the notations ) .", "then , denoting the first @xmath82 permutations by @xmath83 , we define for @xmath84 : @xmath85 the motivation for this inversion of the orders is that , in the second half of the permutations , the opposition with the `` reversed '' first half is maximal .", "this choice , suggestion of art owen ( personal communication ) , aims at minimizing the possible common history among the @xmath1 parallel chains . indeed", "two chains with the same proposed values in reverse order can not have a common path of length more than 1 .", "finally we can try to draw permutations that are far from one another in the set @xmath74 .", "for instance we can define the lexicographic order on @xmath74 , draw indices from a low discrepancy sequence on the set @xmath86 and select the permutations corresponding to these indices . in a simpler manner", ", we do use here a stratified sampling scheme : we first draw a random permutation conditionally on its first element being @xmath87 , then another permutation beginning with @xmath88 , and so forth until the last permutation which begins with @xmath1 .", "we compare the five described types of permutations on the toy example .", "figure [ fig : barplotpermutations ] shows the results for various values of @xmath1 , displaying the variance reduction of @xmath41 associated with each of the permutation orders , compared to the variance of the original imh estimator @xmath36 . for each of the @xmath73 independent replications ,", "the block imh algorithm was launched on one single @xmath29 block , e.g. with @xmath89 using the notation of section [ sec : imh ] , since @xmath25 plays no role whatsoever in this comparison .    as mentioned above , using the same order in the @xmath33 s for each of the @xmath1 parallel chains already produces a significant decrease of about @xmath90 in the variance of the estimators .", "this simulation experiment shows that the three random permutations ( random , half - random half - reversed and stratified ) are quite equivalent in terms of variance improvement and that they are significantly better than the circular permutation proposal , which only slightly improves over the `` same order '' scheme .", "therefore , in the next monte carlo experiments , we will only use the random order solution , simplest of the random schemes .", "an amount of improvement like @xmath91 when @xmath92 is quite impressive when considering that it is essentially obtained cost - free for a computer with parallel abilities @xcite .", "another generic improvement that can be brought over classical mh algorithms is rao  blackwellization @xcite . in this section , two rao ", "blackwellization methods are presented , one that is computationally free and one that , on the contrary , is computationally expensive .", "we then implement both solutions within the block imh algorithm and explain why the `` same order '' scheme already improves upon the imh algorithm .      within the standard imh algorithm of section [ sec : imhb ]", ", a cost - free improvement can be obtained by a straightforward rao  blackwellization argument . given that at step @xmath93 , @xmath31 is accepted with probability @xmath94 and rejected with probability @xmath95 , the weight of @xmath31 can be updated by @xmath94 and the weight of the simulated value @xmath96 corresponding to @xmath97 can be similarly updated by the probability @xmath95 .", "considering next the block imh algorithm , at the beginning of each block we can define @xmath1 weights , denoted by @xmath98 , initialized at @xmath99 and then , for the first of the @xmath1 parallel chains , denoting by @xmath60 the index such that @xmath100 , we update these weights at each time @xmath93 as @xmath101 this is obviously repeated for each of the other parallel chains , ending up with @xmath102 .", "this leads to a new estimator @xmath103 this estimator still depends on all uniform generations created within the block , since those weights @xmath104 depend upon the acceptances and rejections of the @xmath33 s made during the block update .", "however , along the steps of the block , the @xmath104 s are basically updated by the expectations of the acceptance indicators conditionally upon the results of the previous iterations , whereas the @xmath44 of section [ sec : imh ] are directly updated according to the acceptance indicators .", "hence , the @xmath104 s have a smaller variance than the @xmath44 s by virtue of the rao ", "blackwell theorem , leading to @xmath105 necessarily having a smaller variance than @xmath41 .", "we now discuss a more involved rao - blackwellization technique first proposed by @xcite .      exploiting the rao ", "blackwellization technique of @xcite within each parallel chain provides via a conditioning argument an even more stable approximation of arbitrary posterior quantities . as developed in @xcite , for a single markov chain @xmath106 , a rao  blackwell weighting scheme on the proposed values @xmath15 , with weights @xmath107 ,", "is given by a recursive scheme @xmath108 where @xmath109 @xmath110and @xmath111 associated with the metropolis ", "hastings ratios @xmath112 the cumulated computation of the @xmath113 s , of the @xmath114 s and of the @xmath115 s requires an @xmath116 computing time .", "given that @xmath1 is usually not very large , this additional cost is not redhibitory as in the original proposal of @xcite who were considering the application of this rao  blackwellization technique over the whole chain , with a cost of @xmath117 ( see also @xcite ) .", "therefore , starting from the estimator @xmath41 , the weight @xmath44 counting the number of occurrences of @xmath33 in the @xmath29 block can be replaced with the expected number @xmath118 of times @xmath33 occurs in this block ( given the @xmath1 proposed values ) , which is the sum of the expected numbers of times @xmath33 occurs in each of the @xmath1 parallel chain : @xmath119 since the @xmath1 parallel chains incorporate the proposed values with different orders , the @xmath120 s may differ for each chain and must therefore be computed @xmath1 times . note that the cost is still in @xmath116 if this computation can be implemented in parallel .", "then , by a rao - blackwell argument , @xmath41 and @xmath121 are dominated by @xmath122 defined as follows : @xmath123therefore , this rao  blackwellization scheme involves _ no _ uniform generation for the computation of @xmath122 : the randomness associated with these uniforms is completely integrated out .", "the four estimators defined up to now can be summarized as follows :    * @xmath36 is the basic imh estimator of @xmath18 $ ] , * @xmath41 improves @xmath36 by averaging over permutations of the proposed values , and by using @xmath1 times more uniforms than @xmath36 , * @xmath105 improves upon @xmath41 by a basic rao - blackwell argument , * @xmath122 improves upon the above by a further rao - blackwell argument , integrating out the ancillary uniform variables , but at a cost of @xmath116 .", "note that these four estimators all involve the same number @xmath1 of target density evaluations , which again represent the overwhelming part of the computing time .", "figure [ fig : barplotrb ] gives a comparison between the variances of the three improved estimators defined above and the variance of the basic imh estimator .", "the permutations are random in this case .", "as was already apparent on figure [ fig : barplotpermutations ] , the block estimator @xmath41 is significantly better than @xmath36 for any value of @xmath1 .", "moreover , both rao - blackwellization modifications seem to improve only very slightly the estimation when compared with @xmath41 , even though the improvement increases with @xmath1 .", "recall that the `` same order '' scheme already provided a significant decrease in the variance of the estimation . in the light of our results ,", "our interpretation is that using @xmath1 parallel chains with the same proposed values acts like a `` poor man '' rao  blackwellization technique since @xmath1 times more uniforms are used .", "specifically , each of the @xmath1 proposed values is proposed @xmath1 times instead of once , thus reducing the impact of each single uniform draw on the overall estimation .    when we use rao", " blackwellization on top of the block imh , in the estimators @xmath105 and @xmath122 , we try indeed to integrate out a randomness that already is partly gone .", "this explains why , although rao ", "blackwellization techniques provide a significant improvement over standard imh , the improvement is much lower and thus rather unappealing when used in the block imh setting .", "this outcome was at first frustrating since rao ", "blackwellization is indeed affordable at a cost of only @xmath116 .", "however , this shows _ in fine _ that the improvement brought by the block imh algorithm roughly provides the same improvement as the rao  blackwell solution , at a much lower cost .", "the proposal density @xmath3 may also be used to construct directly an importance sampling ( is ) estimator @xmath124 where the values @xmath15 are drawn from @xmath3 .", "it therefore makes sense to compare the imh algorithm with an is approximation because is is similarly easy to parallelize , and straightforward to program .", "furthermore , since the is estimator does not involve ancillary uniform variables , it is comparable to the rao ", "blackwellized version of imh , and hence to the block imh .", "obviously , is can not necessarily be used in the settings when imh algorithms are used , because the latter are also considered for approximating simulations from the target density @xmath0 . in particular , when considering metropolis - within - gibbs algorithms , is can not be used in a straightforward manner , even for approximating integrals .    before giving numerical results for a comparison run on the toy example ,", "we now explain why in this comparison we took the number of blocks to be larger than @xmath87 .", "the previous comparisons were computed with @xmath125 , i.e.  on a single @xmath126 block and for a large number of independent runs .", "the choice of @xmath25 was then irrelevant since we were comparing methods that were exploiting in different ways the @xmath1 proposed values generated in each block .", "when considering the block imh algorithm as a whole , as explained in section [ sec : imh ] , the end of each block sees a new starting value chosen from the current block .", "this ensures that the algorithm produces a valid markov chain .", "however , our construction also implies that the successive blocks produced by the algorithm are correlated , which should lead to lesser performances than for the is estimator .    in the comparison between imh and", "is , we therefore need to take into account this correlation between successive blocks . to this effect", ", we produce the variance reductions for several values of @xmath25 .", "those reductions are presented in figure [ fig : barplotis ] for @xmath127 and different values of @xmath128 .", "once again , the permutations in the block imh algorithm are chosen to be random .", "figure [ fig : barplotis ] shows the a priori surprising result that , when selecting @xmath89 in the experiment , the variance results are in favor of the block imh solutions over the is estimator , but , for any realistic application , @xmath25 is ( much ) larger than @xmath87 . for all @xmath129 ,", "the is estimator has a smaller variance than the three alternative block imh estimators , if only by a small margin .", "( note that the variance improvement over the original mcmc estimator is slightly increasing with @xmath25 despite the correlation between blocks , given that the correlation between the @xmath34 terms involved in the block imh estimators is lower than the correlation in the original mcmc chain . )", "this experiment thus shows that the block imh solution gets very close to the is estimator , while preserving the markovian features of the original imh algorithm .", "in order to evaluate the performances of the parallel processing presented in this paper on a realistic example , we examine its implementation for the probit model already analyzed in @xcite for the comparison of model choice techniques because the  plug - in \" normal distribution based on mle estimates of the first two moments works perfectly as an independent proposal .", "a probit model can be represented as a natural latent variable model in that , if we consider a sample @xmath130 of @xmath131 independent latent variables associated with a standard regression model , i.e.  such that @xmath132 , where the @xmath133 s are @xmath1-dimensional covariates and @xmath134 is the vector of regression coefficients , then @xmath135 such that @xmath136 is a probit sample .", "indeed , given @xmath134 , the @xmath31 s are independent bernoulli rv s with @xmath137 where @xmath138 is the standard normal cdf .", "the choice of a prior distribution for the probit model is open to debate , but the above connection with the latent regression model induced @xcite to suggest a @xmath139-prior model , @xmath140 , with @xmath131 as the @xmath139 factor and @xmath141 as the regressor matrix .", "while a gibbs sampler taking advantage of the latent variable structure is implemented in @xcite and earlier references @xcite , a straightforward metropolis ", "hastings algorithm may be used as well , based on an independent proposal @xmath142 , where @xmath143 is the mle estimator , @xmath144 its asymptotic variance , and @xmath145 a scaling factor .", "as in @xcite and @xcite , we use the r pima indian benchmark dataset @xcite , which contains medical information about @xmath146 pima indian women with seven covariates and one explained binary diabetes variable .    for the purpose of illustrating the implementation of the block imh algorithm", ", we only consider here three covariates , namely plasma glucose concentration ( with coefficient @xmath147 ) , diastolic blood pressure ( with coefficient @xmath148 ) and diabetes pedigree function ( with coefficient @xmath149 ) .", "we are interested in the posterior mean of those three regression parameters . in our experiment", ", we ran @xmath73 independent replications of our algorithm to produce a reliable evaluation of the variance of the estimators under comparison . in figure", "[ fig : barplotprobit ] we present the variance comparison of the four estimators described in section [ sec : rb ] , for @xmath150 and @xmath151 and for each of the three regression parameters . in the independent proposal ,", "the scale factor is chosen to be @xmath152 since pilot runs showed that it led to an acceptance rate around @xmath153 , with thus enough rejections to exhibit improvement by rao  blackwellization .", "the results shown in figure [ fig : barplotprobit ] confirm the huge decrease in variance previously observed in the toy example .", "the gains represented in those figures indicate that the block estimator @xmath41 is nearly as good ( in terms of variance improvement ) as the rao  blackwellized block estimators @xmath105 and @xmath122 , especially when @xmath1 moves from @xmath27 to @xmath154 .", "this confirms the previous interpretation given in section [ sec : rb ] that the block imh algorithm provides a cost - free rao  blackwellization as well as a partial averaging over the order of the proposed values .", "the fact that the toy example showed decreases in the variance that were around @xmath91 whereas the probit regression shows decreases around @xmath155 is worth discussing .", "the amount of decrease in the variance differs from one example to the other , but it is more importantly depending on the acceptance rate of the metropolis  hastings algorithm .", "in fact , rao  blackwellization and permutations of the proposed values are useless steps if the acceptance rate is exactly @xmath87 . on the opposite", ", it may result in a significant improvement when the acceptance rate is low ( since the part of the variance due to the uniform draws would then be much more important ) .    to illustrate the connection between the observed improvement and the acceptance rate , we propose in figure [ fig : barplotvariousscale ] a variance comparison for @xmath127 and two scaling factors @xmath145 of the proposal covariance matrix in the probit regression model . in the previous experiment ,", "we have used @xmath156 , which leads to an acceptance ratio around @xmath153 .", "here , if we take @xmath157 , the acceptance ratio rises to @xmath158 , and hence almost all the proposed values are accepted . in this case permuting the proposed values and using rao ", "blackwellization techniques does not bring much of a variance decrease ( figure [ fig : barplotvariousscale ] , top ) . on the other hand ,", "if we take @xmath159 , the acceptance ratio drops down to @xmath160 and the observed decrease in variance is huge .", "in this second case using all the proposed values gives much better results than relying on the standard imh estimator , which is only based on @xmath160 of the proposed values that were accepted ( figure [ fig : barplotvariousscale ] , bottom ) .", "the difference observed with this range of scaling factors is thus in agreement with the larger decrease in variance observed for the probit regression .", "this is a positive feature of the block imh method , since in a complex model , the target distribution is most often poorly approximated by the proposal and thus the acceptance rate of the imh algorithm is quite likely to be low .", "the monte carlo experiments produced in this paper have shown that the proposed method improves significantly the precision of the estimation , when compared with the standard imh algorithm . beyond these examples ,", "we see multiple situations where the block imh algorithm can be used to improve the estimation in challenging problems .", "first , as already stated , the imh algorithm can be used in metropolis - within - gibbs algorithms @xcite .", "obviously if a single imh step is performed for each component of the state , then the block imh technique can not be applied without incurring additional costs .", "however , it is also correct to update each component multiple times instead of once .", "furthermore , an uniform gibbs scan is rarely the optimal way to update the components and more sophisticated schemes have been studied , resulting in random scan gibbs samplers and adaptive gibbs samplers @xcite , where the probability of updating a given component depends on the component and is learned along the iterations of the algorithm .", "hence if a component is updated @xmath131 times more often than another , @xmath131 imh can be performed in a row , which allows the use of the block imh technique with @xmath161 .", "imh steps are also used within sequential monte carlo ( smc ) samplers ( @xcite , @xcite ) , to diversify the particles after resampling steps . in this context", ", an independent proposal can be designed by fitting a ( usually gaussian ) distribution on the particles .", "if the move step is repeated multiple times in a row , for instance to ensure a satisfying particle diversification , then the block imh algorithm can be used .", "related to smc , another context where the variance reduction provided by block imh might be valuable is the class of particle markov chain monte carlo methods @xcite . for these methods , a particle filter is computed at each iteration of the mh algorithm to estimate the target density , and hence it is paramount to make the most out of the expensive computations involved by those estimates .", "this is thus a natural framework for parallelization .    as a final message ,", "the block imh method is close to being @xmath162 parallel ( except for the random draw of an index at the end of each block ) .", "since parallel computing is getting increasingly easy to use , the free improvement brought by @xmath105 is available for all implementations of the imh algorithm .", "furthermore , even without considering parallel computing , since the method uses the most of each target density evaluation , it brings significant improvement when computing the target density is very costly . in such settings ,", "the cost of drawing @xmath34 instead of @xmath1 uniform variates is negligible and the block imh algorithm thus runs in about the same time as the standard imh algorithm .", "we note that the time required to complete a block in the algorithm is essentially the maximum of the @xmath1 times required to calculate the density ratios @xmath163 .", "therefore , if these times widely vary , there could be a diminishing saving in computation time as @xmath1 increases for both the standard imh and the block imh algorithms .", "nonetheless , even in such extreme cases , using @xmath105 in the block imh algorithm would bring a significant variance improvement at essentially no additional cost .", "the work of the second author ( cpr ) was partly supported by the agence nationale de la recherche ( anr , 212 , rue de bercy 75012 paris ) through the 2009 project anr-08-blan-0218 bigmc and the 2009 project anr-09-blan-01 emile .", "pierre jacob is supported by a phd fellowship from the axa research fund . since this research", "was initiated during the valencia 9 bayesian statistics conference , the paper is dedicated to jos miguel bernardo for the organization of this series of unique meetings since 1979 .", "discussions of the second author with participants during a seminar in stanford university in august 2010 were quite helpful , in particular the suggestion made by art owen to include the  half - inversed half - random \" permutations .", "the authors are grateful to julien cornebise for helpful discussions , in particular those leading to the stratified strategy , and to franois perron for his advice on the permutations .", "comments and suggestions from the editorial team of jcgs were most helpful in improving the presentation of our results ."], "abstract_text": ["<S> in this paper , we consider the implications of the fact that parallel raw - power can be exploited by a generic metropolis  </S>", "<S> hastings algorithm if the proposed values are independent from the current value of the markov chain . </S>", "<S> in particular , we present improvements to the independent metropolis  </S>", "<S> hastings algorithm that significantly decrease the variance of any estimator derived from the mcmc output , at a null computing cost since those improvements are based on a fixed number of target density evaluations that can be produced in parallel . </S>", "<S> the techniques developed in this paper do not jeopardize the markovian convergence properties of the algorithm , since they are based on the rao  </S>", "<S> blackwell principles of @xcite , already exploited in @xcite , @xcite and @xcite . </S>", "<S> we illustrate those improvements both on a toy normal example and on a classical probit regression model , but stress the fact that they are applicable in any case where the independent metropolis  hastings is applicable . </S>", "<S> * keywords : * mcmc algorithm , independent metropolis  </S>", "<S> hastings , parallel computation , rao - blackwellization , permutation . </S>"], "labels": null, "section_names": ["introduction", "improving the imh algorithm", "permutations[sec:permut]", "raoblackwellization[sec:rb]", "a probit regression illustration[sec:applications]", "conclusion", "acknowledgements"], "sections": [["the metropolis  hastings ( mh ) algorithm provides an iterative and converging scheme to sample from a complex target density @xmath0 .", "each iteration of the algorithm generates a new value of the markov chain that relies on the result of the previous iteration .", "the underlying markov principle is well - understood and leads to a generic convergence principle as described , e.g. , in @xcite . however , due to its markovian nature , this algorithm is not straightforward to parallelize , which creates difficulties in slower languages like r @xcite . nevertheless , the increasing number of parallel cores that are available at a very low cost drives more and more interest in `` parallel - friendly '' algorithms , that is , in algorithms that can benefit from the available parallel processing units on standard computers ( see .", "e.g. , @xcite , @xcite , @xcite ) .", "different techniques have already been used to enhance some degree of parallelism in generic metropolis  hastings ( mh ) algorithms , beside the basic scheme of running @xmath1 mcmc algorithms independently in parallel and merging the results .", "for instance , a natural entry is to rely on renewal properties of the markov chain @xcite , waiting for all @xmath1 chains to exhibit a renewal event and then using the blocks as iid , but the constraint of markovianity can not be removed .", "@xcite also points out the difficult issue of accounting for the burn - in time : while , for a single mcmc run , the burn - in time is essentially negligible , it does create a significant bias when running parallel chains ( unless perfect sampling can be implemented ) .", "@xcite mix antithetic coupling and stratification with perfect sampling .", "using a different approach , @xcite rely on @xmath1 parallel chains to build an adaptive mcmc algorithm , considering in essence that the product of the target densities over the chains is their target , a perspective that obviously impacts the convergence properties of the multiple chain . @xcite", "take advantage of parallelization to build a non - reversible algorithm that can avoid the scaling effect of specific neighborhood structures , hence focussing on a very special type of problem .", "a particular family of mh algorithm is the independent metropolis  hastings ( imh ) algorithm , where the proposal distribution ( and hence the proposed value ) does not depend on the current state of the markov chain . due to this characteristic , this specific algorithm is easier to parallelize and can therefore be considered as a good building block toward efficient parallel markov chain monte carlo algorithms , as will be explained in section [ sec : imh ] .", "we will focus on cases where the computation of the likelihood function constitutes the major part of the execution time in the mh algorithm .", "a most realistic example of this setting is provided in @xcite , where the model is based on a very complex fortran program translating the results of several cosmological experiments , hence highly demanding in computing time . in this model ,", "@xcite use adaptive importance sampling and massive parallelization , rather than mcmc .", "the fundamental idea in the current paper is that one can take advantage of the parallel abilities of arbitrary items of computing machinery , from cloud computing to graphical cards ( gpu ) , in the case of the generic imh algorithm , producing an output that corresponds to a much improved monte carlo approximation machine at the same computational cost .", "the techniques presented here are related with those explained in @xcite and more closely to those in @xcite ( @xcite , section 3.1 ) , since these authors condition upon the order statistic of the values proposed by the imh algorithm , although in those earlier papers the links with parallel computation were not established and hence the implementation of the rao - blackwellization scheme became problematic for long chains .", "the plan of the paper is as follows : the standard imh algorithm is recalled in section [ sec : imh ] , followed by a description of our improving scheme , called here `` block independent metropolis ", "hastings '' ( block imh ) .", "this improvement depends on a choice of permutations on @xmath2 that is described in details in section [ sec : permut ] .", "we demonstrate the connections between block imh and rao  blackwellization in section [ sec : rb ] .", "results for a toy example are presented throughout the paper and a realistic probit regression example is described in section [ sec : applications ] as an illustration of the method ."], ["we recall here the basic imh algorithm , assuming the availability of a proposal distribution that we can sample , and which probability density @xmath3 is known up to a normalization constant . the independent metropolis  hastings algorithm , described in algorithm [ algo : imh ] , generates a markov chain with invariant density @xmath0 , corresponding to the target distribution .", "set @xmath4 to an arbitrary value generate @xmath5 [ basichmacpt ] compute the ratio : @xmath6 set @xmath7 with probability @xmath8 ; otherwise set @xmath9    in the larger picture of monte carlo and mcmc algorithms , the imh algorithm holds a rather special status .", "it has certainly been studied more often than other mcmc schemes @xcite , but it is undoubtedly a less practical solution than the more generic random walk metropolis  hastings algorithm .", "for instance , it is rather rarely used by itself because it requires the derivation of a tolerably good approximation to the true target , approximation that most often is unavailable . on the other hand ,", "first - order approximations and metropolis - within - gibbs schemes are not foreign to calling for imh local moves based on gaussian representations of the targets .", "the reason theoretical studies of the imh algorithm abound is that it has strong links with the non - markovian simulation methods such as importance sampling .", "contrary to random - walk metropolis ", "hastings schemes , imh algorithms may enjoy very good convergence properties and may also reach acceptance probabilities that are close to one .", "furthermore , the potentially large gain in variance reduction provided by the parallelization scheme developped in this paper may counteract the lesser efficiency of the original imh compared with the random walk metropolis  hastings algorithm .", "an important feature of the imh algorithm , when addressing parallelism , is that it can not work but in an iterative manner , since the outcome of step @xmath10 , namely the value @xmath11 , is required to compute the acceptance ratio at step @xmath12 .", "this sequential construction is compulsory for the validation of the algorithm given the markov property at its core @xcite .", "nonetheless , given that , in the imh algorithm , the proposed values @xmath13 are generated independently from the current state of the markov chain , @xmath11 , it is altogether possible to envision the generation of @xmath14 proposed values @xmath15 first , along with the computation of the associated ratios @xmath16 .", "once this computation requirement is completed , only the acceptance steps need to be considered iteratively .", "this two - step perspective makes for a huge saving in computing time when the simulation of the @xmath15 s and the derivation of the @xmath17 s can be achieved in parallel since both the remaining computation of the ratios @xmath8 given the @xmath17 s and their subsequent comparison with uniform draws typically are orders of magnitude faster .    in this respect", "the imh algorithm strongly differs from the random walk metropolis  hastings ( rwmh ) algorithm , for which acceptance ratios can not be processed beforehand because the proposed simulated values depend on the current value of the markov chain .", "the universal availability of parallel processing schemes may thus lead to a new surge of popularity for the imh algorithm .", "indeed , when taking advantage of @xmath1 parallel processing units , an imh can be run for @xmath1 times as many iterations as rwmh , at almost the same computing cost since rwmh can not be directly parallelized .    in order to better describe this increased computing power , we first note that , once @xmath14 successive values of a markov chain have been produced , the sequence is usually processed as a regular monte carlo sample to obtain an approximation of an expectation under the target distribution , @xmath18 $ ] say , for some arbitrary functions @xmath19 .", "we propose in this paper a technique that improves the precision of the estimation of this expectation by taking advantage of parallel processing units without jeopardizing the markov property .    before presenting our improvement scheme", ", we introduce the notation @xmath20 ( read `` or '' ) for the operator that represents a single step of the imh algorithm . using this notation ,", "given the current value @xmath11 and a sequence of @xmath1 independent proposed values @xmath21 , the imh algorithm goes from step @xmath10 to step @xmath22 according to the diagram in figure [ fig : imh ] .", "imh steps between iteration @xmath10 and iteration @xmath22.,scaledwidth=100.0% ]      we propose to take full advantage of the simulated proposed values and of the computation of their corresponding @xmath23 ratios . to this effect", ", we introduce the _ block imh algorithm _ , made of successive simulations of blocks of size @xmath24 . in this alternative scheme , the number of blocks @xmath25 is such that the number of desired iterations @xmath14 is equal to @xmath26 , in order to keep the comparison with a standard imh output fair .", "usually @xmath1 needs not be calibrated since it represents the number of physical parallel processing units that can be exploited by the code .", "however , in principle , this number @xmath1 can be set arbitrarily high and based on virtual parallel processing units , the drawback being then an increase in the computing cost .", "( note that the block imh algorithm can also be implemented with no parallel abilities , still it provides a gain in variance that may counteract the increase in time . ) in the following examples , we take @xmath1 varying from @xmath27 to @xmath28 .", "we first explain how a block is simulated , and then how to move from one block to the next .", "a @xmath29 block consists in the generation of @xmath1 parallel generations of @xmath1 values of markov chains , all starting at time @xmath10 from the current state @xmath11 and all based on the _ same _ proposed simulated values @xmath30 .", "the different between the @xmath1 floes is the orders in which those @xmath31 s are included .", "for instance , these orders may be the @xmath1 circular permutations of @xmath30 , or they may be instead random permutations , as discussed in detail ( and compared ) in section [ sec : permut ] . the block imh algorithm is illustrated in figure [ fig : imhblock ] for the circular set of permutations .", "block simulation from step @xmath12 to step @xmath22 . here", ", circular permutations of the proposed values are used for illustration purposes.,scaledwidth=100.0% ]    it should be clear that each of the @xmath1 parallel chains found in this block is a valid mcmc sequence of length @xmath1 when taken separately . as such", ", it can be processed as a regular mcmc output . in particular , if @xmath11 is simulated from the stationary distribution , any of the subsequent @xmath32 is also simulated from the stationary distribution .", "however , the point of the @xmath1 parallel flows is double :    * it aims at integrating out part of the randomness resulting from the ancillary order in which the @xmath33 s are chosen , getting close to the conditioning on the order statistics of the @xmath33 s advocated by @xcite ; * it also aims at partly integrating out the randomness resulting from the generation of uniform variables in the selection process , since the block implementation results in drawing @xmath34 uniform realizations instead of @xmath1 uniform realizations for a standard imh setting .", "both of those points essentially amount to implementing a new rao  blackwellization technique ( a more precise connection is drawn in section [ sec : rb ] ) . in an independent setting ,", "each of the @xmath33 s occurs a number @xmath35 of times across the @xmath1 steps of the @xmath1 parallel chains , i.e.  for a number @xmath34 of realizations .", "therefore , when considering the standard estimator @xmath36 of @xmath18 $ ] , based on a _ single _ mcmc chain , @xmath37this estimator necessarily has a larger variance than the double average @xmath38where @xmath39 and @xmath40 is the number of times @xmath11 is repeated .", "( the proof for the reduction of the variance from @xmath36 to @xmath41 easily follows from a double integration argument . )", "we again insist on the compelling feature that computing @xmath41 using @xmath1 parallel processing units does not cost more time than computing @xmath36 using a single processing unit .", "the block imh algorithm runs @xmath1 parallel chains during @xmath1 steps , then picks one of the final values ( represented by the black squares ) and iterates .", "( an alternative transition mechanism involves sampling randomly one of the @xmath34 terms within the block.),scaledwidth=100.0% ]    in order to preserve its markov validation , the algorithm must properly continue at time @xmath22 . an obvious choice is to pick one of the @xmath1 sequences at random and to take the corresponding @xmath42 as the value of @xmath43 , starting point of the next parallel block .", "this mechanism is represented in figure [ fig : imhnextblock ] . while valid from a markovian perspective , since the sequences are marginally produced by a regular imh algorithm , this means that the chain deduced from the block imh algorithm is converging at _ exactly _ the same speed as the original imh algorithm .", "an alternative choice for the starting points of the blocks takes advantage of the weights @xmath44 on the @xmath33 s that are computed via the block structure .", "indeed , those weights essentially act as importance weights and they allow for a selection of any of the @xmath34 @xmath45 s as the starting point of the incoming block , which corresponds to choosing one of the proposed @xmath33 s with probability proportional to @xmath44 .", "while this proposal does reduce the length of the resulting chain , it does not impact the estimation aspects ( which still involve all of the @xmath34 values ) and it could improve convergence , given that the weighted @xmath33 s behave like a discretized version of a sample from the target density @xmath0 .", "we will not cover this alternative any further .", "the original version of the block imh algorithm is described in algorithm [ algo : blockimh ] , the algorithm is made of a loop on the @xmath25 blocks and an inner loop on the @xmath1 parallel chains of each block .", "the @xmath1 steps of this inner loop are actually meant to be implemented in parallel .", "the output of algorithm [ algo : blockimh ] is double :    * a standard markov chain of length @xmath14 , which is made of @xmath25 chains of length @xmath1 , each of which is chosen among @xmath1 chains at line [ algo : statepickindex ] of algorithm [ algo : blockimh ] , * a @xmath46 array @xmath47 , on which the estimator @xmath41 is based .", "set @xmath4 to an arbitrary value , compute @xmath48 set @xmath49 , @xmath50 set a block size @xmath1 , and a number of blocks @xmath25 , such that @xmath51 generate all proposed values @xmath52 [ algo : stateintensive ] compute all ratios @xmath53 [ algo : chooseperm ] choose @xmath1 permutations @xmath54 run @xmath1 steps of an imh given :    * @xmath55 * @xmath1 proposed values @xmath56 shuffled with the permutation @xmath57 * the @xmath1 corresponding ratios @xmath58 s    save as @xmath59 the resulting chain [ algo : statepickindex ] draw an index @xmath60 uniformly in @xmath61 , set @xmath62 , set @xmath63 as the corresponding ratio @xmath23 .      since the point - wise evaluation of the target density @xmath64 is usually the most computer - intensive part of the algorithm , sampling additional uniform variables has a negligible impact here , as do further costs related to the storage of vectors larger than in the original imh .", "this is particularly compelling since the multiple chains do not need to be stored further than during a single block execution time .", "that is why , although we sample @xmath1 times more uniforms in the block imh algorithm , we still consider it to be running at roughly of the same cost as the imh algorithm .", "the number of target density evaluations indeed is the same for both and most often represent the overwhelming part of the computing time in the metropolis  hastings algorithm . besides", ", pseudo - random generation of uniforms can also benefit from parallel processing , see e.g. @xcite .    in the following monte carlo experiment ,", "various versions of the block imh algorithm are compared one to another , as well as to standard imh and importance sampling .", "we stress that a straightforward reason for not conducting a comparison with a plain parallel algorithm based on @xmath1 independent parallel chains is that it does not make much sense cost - wise . indeed , running @xmath1 parallel mcmc chains of the same length @xmath14 does cost @xmath1 times more in terms of target density evaluations .", "obviously , if one insists on running @xmath1 independent chains , for instance as to initialize an mcmc algorithm from several well - dispersed starting points , each of those chains can benefit from our stabilizing method , which will improve the resulting estimation .", "the method is presented here for square blocks of dimension @xmath65 , but blocks could be rectangular as well : the algorithm is equally valid when using @xmath66 permutations , leading to @xmath67 blocks .", "we focus here on square blocks because when the machine at hand provides @xmath1 parallel processing units , then it is most efficient to simulate the proposed values and the uniforms , and to compute the target densities and the acceptance ratios at the @xmath1 proposed values in parallel .", "once again , the block imh algorithm with @xmath29 square blocks has about the same cost as the original imh algorithm , because computing target densities and acceptance ratios does more than compensate for the cost of randomly picking an index at the end of each block .", "this amounts to say that line [ basichmacpt ] of algorithm [ algo : imh ] and line [ algo : stateintensive ] of algorithm [ algo : blockimh ] are ( by far ) the most computationally demanding ones in the respective algorithms .", "we now introduce a toy example that we will follow throughout the paper .", "the target @xmath0 is the density of the standard @xmath68 normal distribution and the proposal @xmath3 is the density of the @xmath69 cauchy distribution .", "hence , the density ratio is @xmath70 ( 1 + x^2 ) we only consider the integral @xmath71 , the mean of @xmath0 equal to zero in this case .", "the acceptance rate of the imh algorithm for this example is around @xmath72 .", "( note that imh with higher acceptance rates are considered to be more efficient , in opposition to other metropolis  hastings algorithms , see @xcite . )    in all results related to the toy example presented thereafter , @xmath73 independent runs are used to compute the variance of the estimates .", "the value of @xmath1 represents the number of parallel processing units that are available , ranging from @xmath27 for a desktop computer to @xmath28 for a cluster or a graphics processing unit ( gpu ) ( this value could even be larger for computers equipped with multiple gpus ) .", "the results of the simulation experiments are presented in figures [ fig : barplotpermutations][fig : barplotvariousscale ] as barplots , which indicate the percentage of variance decrease associated with the estimators under comparison , the reference estimator being the standard imh output @xmath36 . in agreement with the block sampling perspective", ", the same proposed values and uniform draws were used for all the estimators that are plotted on the same graph ( that is , for a given value of @xmath1 ) , so that the comparison is not perturbed by an additional noise associated with the simulation ."], ["while the choice of permutations in line [ algo : chooseperm ] of algorithm [ algo : blockimh ] is irrelevant for the validation of the parallelization , it has important consequences on the variance improvement and we now discuss several natural choices .", "the idea of testing various orders of the proposed values in a imh algorithm appeared in @xcite where the permutations were chosen to be circular .", "we first list natural types of permutations along with some justifications , and then we empirically compare their impact on estimation performances for the toy example .", "let @xmath74 be the set of permutations of @xmath75 .", "its size is @xmath76 , therefore too large to allow for averaging over all permutations , although this solution would be ideal .", "we consider the simpler option of finding @xmath1 efficient permutations in @xmath74 , denoted by @xmath77 , the goal being a choice favoring the largest possible decrease in the variance of the estimator @xmath41 defined in section [ sec : imh ] .", "the most basic choice is to pick the same permutation on each of the @xmath1 chains : @xmath78 this selection may sound counterproductive , but we still obtain a significant decrease in the variance of @xmath41 using this set of permutations , when compared with @xmath36 .", "the reason for the improvement is that @xmath1 times more uniforms are used in @xmath41 than in @xmath36 , leading to a natural rao - blackwellization phenomenon that is studied in details in section [ sec : rb ] .", "nonetheless this simplistic set of permutations is certainly not the best choice since it does not integrate out the ancillary randomness resulting from the arbitrary ordering of the proposed values .", "another simple choice is to use circular permutations . for @xmath79", ", we define @xmath80 an appealing property of the circular permutations is that each simulated value @xmath33 is proposed and evaluated at a different step for each chain .", "however , a drawback is that the order is not deeply changed : for instance @xmath81 will always be proposed one step before @xmath33 except for one of the @xmath1 chains , for which @xmath33 is proposed first .", "a third choice is to use random orders , that is random shufflings of the sequence @xmath75 .", "we can either draw those random permutations with or without replacement in the set @xmath74 , but considering the cardinality of the set @xmath74 this does not make a large difference . indeed , it is unlikely to draw twice the same permutation , except for very small values of @xmath1 .", "a slightly different choice of permutations consists in drawing @xmath82 permutations at random ( @xmath1 is taken to be even here to simplify the notations ) .", "then , denoting the first @xmath82 permutations by @xmath83 , we define for @xmath84 : @xmath85 the motivation for this inversion of the orders is that , in the second half of the permutations , the opposition with the `` reversed '' first half is maximal .", "this choice , suggestion of art owen ( personal communication ) , aims at minimizing the possible common history among the @xmath1 parallel chains . indeed", "two chains with the same proposed values in reverse order can not have a common path of length more than 1 .", "finally we can try to draw permutations that are far from one another in the set @xmath74 .", "for instance we can define the lexicographic order on @xmath74 , draw indices from a low discrepancy sequence on the set @xmath86 and select the permutations corresponding to these indices . in a simpler manner", ", we do use here a stratified sampling scheme : we first draw a random permutation conditionally on its first element being @xmath87 , then another permutation beginning with @xmath88 , and so forth until the last permutation which begins with @xmath1 .", "we compare the five described types of permutations on the toy example .", "figure [ fig : barplotpermutations ] shows the results for various values of @xmath1 , displaying the variance reduction of @xmath41 associated with each of the permutation orders , compared to the variance of the original imh estimator @xmath36 . for each of the @xmath73 independent replications ,", "the block imh algorithm was launched on one single @xmath29 block , e.g. with @xmath89 using the notation of section [ sec : imh ] , since @xmath25 plays no role whatsoever in this comparison .    as mentioned above , using the same order in the @xmath33 s for each of the @xmath1 parallel chains already produces a significant decrease of about @xmath90 in the variance of the estimators .", "this simulation experiment shows that the three random permutations ( random , half - random half - reversed and stratified ) are quite equivalent in terms of variance improvement and that they are significantly better than the circular permutation proposal , which only slightly improves over the `` same order '' scheme .", "therefore , in the next monte carlo experiments , we will only use the random order solution , simplest of the random schemes .", "an amount of improvement like @xmath91 when @xmath92 is quite impressive when considering that it is essentially obtained cost - free for a computer with parallel abilities @xcite ."], ["another generic improvement that can be brought over classical mh algorithms is rao  blackwellization @xcite . in this section , two rao ", "blackwellization methods are presented , one that is computationally free and one that , on the contrary , is computationally expensive .", "we then implement both solutions within the block imh algorithm and explain why the `` same order '' scheme already improves upon the imh algorithm .      within the standard imh algorithm of section [ sec : imhb ]", ", a cost - free improvement can be obtained by a straightforward rao  blackwellization argument . given that at step @xmath93 , @xmath31 is accepted with probability @xmath94 and rejected with probability @xmath95 , the weight of @xmath31 can be updated by @xmath94 and the weight of the simulated value @xmath96 corresponding to @xmath97 can be similarly updated by the probability @xmath95 .", "considering next the block imh algorithm , at the beginning of each block we can define @xmath1 weights , denoted by @xmath98 , initialized at @xmath99 and then , for the first of the @xmath1 parallel chains , denoting by @xmath60 the index such that @xmath100 , we update these weights at each time @xmath93 as @xmath101 this is obviously repeated for each of the other parallel chains , ending up with @xmath102 .", "this leads to a new estimator @xmath103 this estimator still depends on all uniform generations created within the block , since those weights @xmath104 depend upon the acceptances and rejections of the @xmath33 s made during the block update .", "however , along the steps of the block , the @xmath104 s are basically updated by the expectations of the acceptance indicators conditionally upon the results of the previous iterations , whereas the @xmath44 of section [ sec : imh ] are directly updated according to the acceptance indicators .", "hence , the @xmath104 s have a smaller variance than the @xmath44 s by virtue of the rao ", "blackwell theorem , leading to @xmath105 necessarily having a smaller variance than @xmath41 .", "we now discuss a more involved rao - blackwellization technique first proposed by @xcite .      exploiting the rao ", "blackwellization technique of @xcite within each parallel chain provides via a conditioning argument an even more stable approximation of arbitrary posterior quantities . as developed in @xcite , for a single markov chain @xmath106 , a rao  blackwell weighting scheme on the proposed values @xmath15 , with weights @xmath107 ,", "is given by a recursive scheme @xmath108 where @xmath109 @xmath110and @xmath111 associated with the metropolis ", "hastings ratios @xmath112 the cumulated computation of the @xmath113 s , of the @xmath114 s and of the @xmath115 s requires an @xmath116 computing time .", "given that @xmath1 is usually not very large , this additional cost is not redhibitory as in the original proposal of @xcite who were considering the application of this rao  blackwellization technique over the whole chain , with a cost of @xmath117 ( see also @xcite ) .", "therefore , starting from the estimator @xmath41 , the weight @xmath44 counting the number of occurrences of @xmath33 in the @xmath29 block can be replaced with the expected number @xmath118 of times @xmath33 occurs in this block ( given the @xmath1 proposed values ) , which is the sum of the expected numbers of times @xmath33 occurs in each of the @xmath1 parallel chain : @xmath119 since the @xmath1 parallel chains incorporate the proposed values with different orders , the @xmath120 s may differ for each chain and must therefore be computed @xmath1 times . note that the cost is still in @xmath116 if this computation can be implemented in parallel .", "then , by a rao - blackwell argument , @xmath41 and @xmath121 are dominated by @xmath122 defined as follows : @xmath123therefore , this rao  blackwellization scheme involves _ no _ uniform generation for the computation of @xmath122 : the randomness associated with these uniforms is completely integrated out .", "the four estimators defined up to now can be summarized as follows :    * @xmath36 is the basic imh estimator of @xmath18 $ ] , * @xmath41 improves @xmath36 by averaging over permutations of the proposed values , and by using @xmath1 times more uniforms than @xmath36 , * @xmath105 improves upon @xmath41 by a basic rao - blackwell argument , * @xmath122 improves upon the above by a further rao - blackwell argument , integrating out the ancillary uniform variables , but at a cost of @xmath116 .", "note that these four estimators all involve the same number @xmath1 of target density evaluations , which again represent the overwhelming part of the computing time .", "figure [ fig : barplotrb ] gives a comparison between the variances of the three improved estimators defined above and the variance of the basic imh estimator .", "the permutations are random in this case .", "as was already apparent on figure [ fig : barplotpermutations ] , the block estimator @xmath41 is significantly better than @xmath36 for any value of @xmath1 .", "moreover , both rao - blackwellization modifications seem to improve only very slightly the estimation when compared with @xmath41 , even though the improvement increases with @xmath1 .", "recall that the `` same order '' scheme already provided a significant decrease in the variance of the estimation . in the light of our results ,", "our interpretation is that using @xmath1 parallel chains with the same proposed values acts like a `` poor man '' rao  blackwellization technique since @xmath1 times more uniforms are used .", "specifically , each of the @xmath1 proposed values is proposed @xmath1 times instead of once , thus reducing the impact of each single uniform draw on the overall estimation .    when we use rao", " blackwellization on top of the block imh , in the estimators @xmath105 and @xmath122 , we try indeed to integrate out a randomness that already is partly gone .", "this explains why , although rao ", "blackwellization techniques provide a significant improvement over standard imh , the improvement is much lower and thus rather unappealing when used in the block imh setting .", "this outcome was at first frustrating since rao ", "blackwellization is indeed affordable at a cost of only @xmath116 .", "however , this shows _ in fine _ that the improvement brought by the block imh algorithm roughly provides the same improvement as the rao  blackwell solution , at a much lower cost .", "the proposal density @xmath3 may also be used to construct directly an importance sampling ( is ) estimator @xmath124 where the values @xmath15 are drawn from @xmath3 .", "it therefore makes sense to compare the imh algorithm with an is approximation because is is similarly easy to parallelize , and straightforward to program .", "furthermore , since the is estimator does not involve ancillary uniform variables , it is comparable to the rao ", "blackwellized version of imh , and hence to the block imh .", "obviously , is can not necessarily be used in the settings when imh algorithms are used , because the latter are also considered for approximating simulations from the target density @xmath0 . in particular , when considering metropolis - within - gibbs algorithms , is can not be used in a straightforward manner , even for approximating integrals .    before giving numerical results for a comparison run on the toy example ,", "we now explain why in this comparison we took the number of blocks to be larger than @xmath87 .", "the previous comparisons were computed with @xmath125 , i.e.  on a single @xmath126 block and for a large number of independent runs .", "the choice of @xmath25 was then irrelevant since we were comparing methods that were exploiting in different ways the @xmath1 proposed values generated in each block .", "when considering the block imh algorithm as a whole , as explained in section [ sec : imh ] , the end of each block sees a new starting value chosen from the current block .", "this ensures that the algorithm produces a valid markov chain .", "however , our construction also implies that the successive blocks produced by the algorithm are correlated , which should lead to lesser performances than for the is estimator .    in the comparison between imh and", "is , we therefore need to take into account this correlation between successive blocks . to this effect", ", we produce the variance reductions for several values of @xmath25 .", "those reductions are presented in figure [ fig : barplotis ] for @xmath127 and different values of @xmath128 .", "once again , the permutations in the block imh algorithm are chosen to be random .", "figure [ fig : barplotis ] shows the a priori surprising result that , when selecting @xmath89 in the experiment , the variance results are in favor of the block imh solutions over the is estimator , but , for any realistic application , @xmath25 is ( much ) larger than @xmath87 . for all @xmath129 ,", "the is estimator has a smaller variance than the three alternative block imh estimators , if only by a small margin .", "( note that the variance improvement over the original mcmc estimator is slightly increasing with @xmath25 despite the correlation between blocks , given that the correlation between the @xmath34 terms involved in the block imh estimators is lower than the correlation in the original mcmc chain . )", "this experiment thus shows that the block imh solution gets very close to the is estimator , while preserving the markovian features of the original imh algorithm ."], ["in order to evaluate the performances of the parallel processing presented in this paper on a realistic example , we examine its implementation for the probit model already analyzed in @xcite for the comparison of model choice techniques because the  plug - in \" normal distribution based on mle estimates of the first two moments works perfectly as an independent proposal .", "a probit model can be represented as a natural latent variable model in that , if we consider a sample @xmath130 of @xmath131 independent latent variables associated with a standard regression model , i.e.  such that @xmath132 , where the @xmath133 s are @xmath1-dimensional covariates and @xmath134 is the vector of regression coefficients , then @xmath135 such that @xmath136 is a probit sample .", "indeed , given @xmath134 , the @xmath31 s are independent bernoulli rv s with @xmath137 where @xmath138 is the standard normal cdf .", "the choice of a prior distribution for the probit model is open to debate , but the above connection with the latent regression model induced @xcite to suggest a @xmath139-prior model , @xmath140 , with @xmath131 as the @xmath139 factor and @xmath141 as the regressor matrix .", "while a gibbs sampler taking advantage of the latent variable structure is implemented in @xcite and earlier references @xcite , a straightforward metropolis ", "hastings algorithm may be used as well , based on an independent proposal @xmath142 , where @xmath143 is the mle estimator , @xmath144 its asymptotic variance , and @xmath145 a scaling factor .", "as in @xcite and @xcite , we use the r pima indian benchmark dataset @xcite , which contains medical information about @xmath146 pima indian women with seven covariates and one explained binary diabetes variable .    for the purpose of illustrating the implementation of the block imh algorithm", ", we only consider here three covariates , namely plasma glucose concentration ( with coefficient @xmath147 ) , diastolic blood pressure ( with coefficient @xmath148 ) and diabetes pedigree function ( with coefficient @xmath149 ) .", "we are interested in the posterior mean of those three regression parameters . in our experiment", ", we ran @xmath73 independent replications of our algorithm to produce a reliable evaluation of the variance of the estimators under comparison . in figure", "[ fig : barplotprobit ] we present the variance comparison of the four estimators described in section [ sec : rb ] , for @xmath150 and @xmath151 and for each of the three regression parameters . in the independent proposal ,", "the scale factor is chosen to be @xmath152 since pilot runs showed that it led to an acceptance rate around @xmath153 , with thus enough rejections to exhibit improvement by rao  blackwellization .", "the results shown in figure [ fig : barplotprobit ] confirm the huge decrease in variance previously observed in the toy example .", "the gains represented in those figures indicate that the block estimator @xmath41 is nearly as good ( in terms of variance improvement ) as the rao  blackwellized block estimators @xmath105 and @xmath122 , especially when @xmath1 moves from @xmath27 to @xmath154 .", "this confirms the previous interpretation given in section [ sec : rb ] that the block imh algorithm provides a cost - free rao  blackwellization as well as a partial averaging over the order of the proposed values .", "the fact that the toy example showed decreases in the variance that were around @xmath91 whereas the probit regression shows decreases around @xmath155 is worth discussing .", "the amount of decrease in the variance differs from one example to the other , but it is more importantly depending on the acceptance rate of the metropolis  hastings algorithm .", "in fact , rao  blackwellization and permutations of the proposed values are useless steps if the acceptance rate is exactly @xmath87 . on the opposite", ", it may result in a significant improvement when the acceptance rate is low ( since the part of the variance due to the uniform draws would then be much more important ) .    to illustrate the connection between the observed improvement and the acceptance rate , we propose in figure [ fig : barplotvariousscale ] a variance comparison for @xmath127 and two scaling factors @xmath145 of the proposal covariance matrix in the probit regression model . in the previous experiment ,", "we have used @xmath156 , which leads to an acceptance ratio around @xmath153 .", "here , if we take @xmath157 , the acceptance ratio rises to @xmath158 , and hence almost all the proposed values are accepted . in this case permuting the proposed values and using rao ", "blackwellization techniques does not bring much of a variance decrease ( figure [ fig : barplotvariousscale ] , top ) . on the other hand ,", "if we take @xmath159 , the acceptance ratio drops down to @xmath160 and the observed decrease in variance is huge .", "in this second case using all the proposed values gives much better results than relying on the standard imh estimator , which is only based on @xmath160 of the proposed values that were accepted ( figure [ fig : barplotvariousscale ] , bottom ) .", "the difference observed with this range of scaling factors is thus in agreement with the larger decrease in variance observed for the probit regression .", "this is a positive feature of the block imh method , since in a complex model , the target distribution is most often poorly approximated by the proposal and thus the acceptance rate of the imh algorithm is quite likely to be low ."], ["the monte carlo experiments produced in this paper have shown that the proposed method improves significantly the precision of the estimation , when compared with the standard imh algorithm . beyond these examples ,", "we see multiple situations where the block imh algorithm can be used to improve the estimation in challenging problems .", "first , as already stated , the imh algorithm can be used in metropolis - within - gibbs algorithms @xcite .", "obviously if a single imh step is performed for each component of the state , then the block imh technique can not be applied without incurring additional costs .", "however , it is also correct to update each component multiple times instead of once .", "furthermore , an uniform gibbs scan is rarely the optimal way to update the components and more sophisticated schemes have been studied , resulting in random scan gibbs samplers and adaptive gibbs samplers @xcite , where the probability of updating a given component depends on the component and is learned along the iterations of the algorithm .", "hence if a component is updated @xmath131 times more often than another , @xmath131 imh can be performed in a row , which allows the use of the block imh technique with @xmath161 .", "imh steps are also used within sequential monte carlo ( smc ) samplers ( @xcite , @xcite ) , to diversify the particles after resampling steps . in this context", ", an independent proposal can be designed by fitting a ( usually gaussian ) distribution on the particles .", "if the move step is repeated multiple times in a row , for instance to ensure a satisfying particle diversification , then the block imh algorithm can be used .", "related to smc , another context where the variance reduction provided by block imh might be valuable is the class of particle markov chain monte carlo methods @xcite . for these methods , a particle filter is computed at each iteration of the mh algorithm to estimate the target density , and hence it is paramount to make the most out of the expensive computations involved by those estimates .", "this is thus a natural framework for parallelization .    as a final message ,", "the block imh method is close to being @xmath162 parallel ( except for the random draw of an index at the end of each block ) .", "since parallel computing is getting increasingly easy to use , the free improvement brought by @xmath105 is available for all implementations of the imh algorithm .", "furthermore , even without considering parallel computing , since the method uses the most of each target density evaluation , it brings significant improvement when computing the target density is very costly . in such settings ,", "the cost of drawing @xmath34 instead of @xmath1 uniform variates is negligible and the block imh algorithm thus runs in about the same time as the standard imh algorithm .", "we note that the time required to complete a block in the algorithm is essentially the maximum of the @xmath1 times required to calculate the density ratios @xmath163 .", "therefore , if these times widely vary , there could be a diminishing saving in computation time as @xmath1 increases for both the standard imh and the block imh algorithms .", "nonetheless , even in such extreme cases , using @xmath105 in the block imh algorithm would bring a significant variance improvement at essentially no additional cost ."], ["the work of the second author ( cpr ) was partly supported by the agence nationale de la recherche ( anr , 212 , rue de bercy 75012 paris ) through the 2009 project anr-08-blan-0218 bigmc and the 2009 project anr-09-blan-01 emile .", "pierre jacob is supported by a phd fellowship from the axa research fund . since this research", "was initiated during the valencia 9 bayesian statistics conference , the paper is dedicated to jos miguel bernardo for the organization of this series of unique meetings since 1979 .", "discussions of the second author with participants during a seminar in stanford university in august 2010 were quite helpful , in particular the suggestion made by art owen to include the  half - inversed half - random \" permutations .", "the authors are grateful to julien cornebise for helpful discussions , in particular those leading to the stratified strategy , and to franois perron for his advice on the permutations .", "comments and suggestions from the editorial team of jcgs were most helpful in improving the presentation of our results ."]]}
{"article_id": "1605.02474", "article_text": ["wireless networks are ubiquitous and are on their way to become even more prevalent , e.g. , with the advent of internet - of - things . wireless communication is , however , particularly challenging to model algorithmically . in two crucial interrelated aspects ,", "wireless networks on the ground differ from models typically assumed in algorithmic studies .", "one is the _ communication modeling _ : when is a transmission successfully decoded , as a function of the environment and the _ interference _ from other transmissions .", "the other is _ variability with time _ : wireless networks are particularly susceptible to changes .", "both of these are hard to capture accurately with well - defined , clear - cut rules .", "we aim in this paper to address core information dissemination problems  _ local broadcast _ and ( global ) _ broadcast _  in dynamic distributed networks , under very weak assumptions on the communication . to this end", ", we propose a communication model with significant flexibility that allows for adversarial control , generalizing essentially all known analytic wireless models .", "it allows for more general interference relationships than treated before", ". the network can experience adversarial dynamic behavior , both edge changes ( change of signal strengths ) and node insertions / deletions .", "wireless communication has traditionally been modeled theoretically by graphs , either geometric or general .", "interference is then also transmitted on graph edges ( precluding a node from receiving a message from a neighbor if another neighbor is transmitting ) , but is sometimes represented by a supergraph . _", "fading channel _ or _ physical _ models common in communication engineering , on the other hand , consider interference as cumulative , decreasing with distance but adding up .", "they have been popular in recent algorithmic studies , adding more realism to the formulation .", "the standard assumption of _ geometric signal decay _ ( that signal decreases inverse polynomially with distance ) in the sinr model , the prototypical fading channel , is though equally at odds with experimental evidence . ultimately , it may prove futile to hope for a clean deterministic model , or even a purely stochastic one , without a significant dose of unpredictability and non - determinism .", "wireless communication is commonly closely linked to mobility , as the transceivers are more often than not on the move .", "dynamic changes to reception conditions have many causes other than node mobility , since almost any changes in the environment affect transmissions due to reflections of signals over multiple paths , antenna characteristics , scattering , and diffraction .", "these changes are by nature hard to predict , even when assuming a `` mobility model '' . the most robust approach would then be to assume a non - trivial adversarial component .", "while the study of algorithms in dynamic networks has a long history , little has been done in cases where interference plays a role .", "* setting and model*.   nodes are distributed and autonomous . there is no built - in structure and the nodes have no information besides bounds on model parameters and an upper bound on the number of nodes .", "communication is locally synchronous , but there is no global clock .", "we will assume a very general model for when nodes successfully communicate .", "nodes are located in space , with separation between points given by the relative decrease in strength of signal ( or interference ) sent between the points .", "this induces a metric space , when these decays are raised to the appropriate power  actually , it is a _ quasi - metric _ , since symmetry need not hold .", "for distributed computation to be possible , the quasi - metric must have _ bounded independence _", "( to be defined precisely ) .", "_ edge changes _ can occur , with some restrictions , which are changes in the signal strength between the pair of points .", "the rule for when communication is successful is only partially pre - specified : transmission succeeds on a `` clear channel '' .", "that is , if a sender is within a _ communication radius _ from the receiver , if no other node transmits within a ( larger ) radius , and if the combined interference from all other transmitting nodes is ( quite ) small , then the transmission succeeds .", "otherwise , success is up to the adversary , or it can be further specified by the particular model assumptions desired .", "this captures essentially all known algorithmic wireless models ( including quasi - unit disc graphs , unit - ball graphs , bounded - independence graphs , @xmath0-hop extensions , and sinr ) .", "the only exception is the radio network model with general graphs , which can not be extended to involve comprehensive interference without a major hit in time complexity .", "the generality of our model is a key feature .", "given the vagaries of actual wireless environments , it is preferable for robustness reasons to make minimal assumptions about the communication model .", "a conservative approach is then to seek algorithms that work in most established models rather than depending on model - specific factors that simplify the life of the algorithm designer .", "[ [ dynamic - network - model ] ] dynamic network model + + + + + + + + + + + + + + + + + + + + +    the network is dynamic , both in terms of nodes and links .", "nodes can come and go , potentially adversarially .", "this includes the situation where nodes are wakened by other nodes .", "we only expect nodes that have been awake for long enough ( logarithmic number of rounds ) to participate in the computation , i.e. , the _", "stable _ nodes .    in the _ local broadcast _", "problem , all stable nodes should inform all of their ( stable ) neighbors ( i.e. , within the communication radius ) . in the _ broadcast _", "problem , the nodes in the network are to be informed of a message that initially resides at a source node .", "the network need not be connected at any instant in time , but we need to make assumptions about what transient paths are usable by an algorithm operating under interference .", "this leads to a definition of a _ dynamic diameter _ of the network , that becomes the competitive objective for the time complexity .", "* our approach and results*.   the key algorithmic technique is a natural randomized _ contention balancing _ procedure , where a node continuously adjusts its transmission probability based on the interference that it senses .", "it allows nodes to stabilize quickly from any initial conditions , or after waking up .", "this routine is a variation on an old story , a simple backoff procedure to manage local contention :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ if a node @xmath1 senses contention in a given round beyond a fixed threshold , then @xmath1 halves its transmission probability in the next round and otherwise doubles it . _", "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    our main technical contribution is to show logarithmic - round convergence of this method to a steady state of nearly balanced contention , from an arbitrary starting configuration and in the presence of network changes .", "it proves also to be surprisingly tolerant of different communication models .", "the higher level algorithms are then built on top of this primitive .", "a crucial component is the use of _ carrier sense _ to detect the cumulative amount of signals in the air .", "since it is supplied by the cheapest available hardware today as rss ( received signal strength ) readings , we posit that carrier - sense capability should be the default assumption in wireless algorithmics ( while exploring the necessity of different assumptions is interesting theoretically ) . as carrier - sense indicators can provide fine - grained information , we are interested in restraining its use and identifying which aspects are necessary to achieve the results obtained . to this end , we identify several _ primitives _ that carrier - sense can supply , restrict the algorithm to use only a subset of the primitives , and examine which of these are truly necessary .", "the local broadcast algorithm simply runs the contention balancing procedure , with nodes bowing out when they are sure to have completed their transmission .", "the broadcast algorithms are based on sparsifying the instance , so that only nodes of constant density actually participate in the global broadcast action .", "the former is achieved in @xmath2 time , where @xmath3 is the maximum number of neighbors that a node can have , while the latter takes @xmath4 rounds , where @xmath5 is a dynamic diameter .", "these dissemination algorithms are efficient enough to improve on some of the results known for static versions of the problems .", "the local broadcast algorithm is strongly optimal , or within constant factors on every instance . in the standard", "setting ( static , spontaneous case ) , the algorithm is _ uniform _ , in that it need not know the network size .", "the broadcast algorithm is also optimal and uniform in the same setting , while in the non - spontaneous setting it is faster by a logarithmic factor than the previous algorithm of @xcite that however does not require carrier sense .", "* closely related work*.   there are two largely disjoint bodies of work of wireless algorithmic results , with work on fading models like sinr slowly catching up with the better studied graph - based models .", "one approach for capturing more realism in sinr model is to move beyond euclidean metrics @xcite , even to general ones @xcite .", "one can view relative signal decrease as implicitly defining a quasi - distance metric @xcite .", "link scheduling problems can be formulated on edge - weighted interference graphs @xcite that properly generalize both graph - based and sinr models , linked by a graph - theoretic parameter . distributed dissemination problems , however , necessarily require metric restrictions , such as doubling or _ fading _", "metrics @xcite , and limits on communication abilities in order to capture both types of models .", "the local broadcast and global broadcast problems have been extensively studied in both graph - based radio network models  @xcite and the sinr model  @xcite . for local broadcast ,", "the best results known in the radio network model are both @xmath6 with and without knowing ( an upper bound on ) @xmath3  @xcite . in the sinr model , with knowledge of @xmath3 , the local broadcast can be accomplished in the same time bound as in the radio network model @xcite .", "if @xmath3 is not known , the best result is @xmath7  @xcite , which is improved to @xmath2 with free acknowledgments @xcite .", "this can be further improved to @xmath8 in the spontaneous case , when @xmath3 is known  @xcite .", "the time complexity of non - spontaneous broadcasting in the radio network model is @xmath9  @xcite without collision detection . with collision detection , this lower bound", "was recently broken in  @xcite , where a solution of @xmath10 was given .", "broadcasting has also been treated in the sinr model under a variety of assumptions .", "some are stronger than ours ( location information @xcite , power control @xcite ) , while others relax the assumption about the connectivity property , incurring necessarily much higher complexity @xcite . results in our setting , but without carrier sensing , include time complexity of @xmath11 @xcite ( see also @xcite ) , where @xmath12 denotes the maximum ratio between distances of stations connected in the communication graph ; and @xmath13 @xcite . in the spontaneous setting , where the nodes can build an overlay structure along which", "the message is then propagated , scheideler et al .", "@xcite used carrier sense to give a dominator algorithms , which can be applied to solve broadcast in @xmath14 rounds .", "yu et al .", "@xcite solved the problem in the same time bound using power control , while the algorithm in  @xcite that requires neither power control nor carrier sense runs in time @xmath15 .", "these problems have also been treated in dynamic networks . in", "the _ unstructured _ model @xcite , where nodes may wake up asynchronously ( modeling the node insertion ) , the local broadcast problem is well studied , even in the sinr setting  @xcite , but this model does not consider node deletion . in the _ dual graph", "_ model  @xcite ( originally due to @xcite ) , both the local broadcast  @xcite and global broadcast  @xcite problems are studied . but", "this model involves only edge behavior and not node changes ( churn ) .", "hence , the impact of dynamicity on wireless information dissemination is still largely unexplored .", "considering wireless interference , there are two classes of wireless network models : graph - based and physical models .", "basically , the graph - based models define a local and binary type of interference , while the physical models consider fading effect of signal on wireless channels .", "there are many widely used graph - based models , such as the unit disc graph ( udg ) model  @xcite ( unit ball graph ( ubg ) model  @xcite ) , the quasi unit disc graph ( qudg ) model  @xcite , the protocol model  @xcite , the bounded - independence graph ( big ) model  @xcite and the @xmath0-hop variants of the above models  @xcite .", "physical models , also known as sinr models  @xcite , capture the missing practical issues in wireless interference , such as fading and cumulativeness of wireless interference . most of current works in the sinr model focus on networks embedded in euclidean space , while many of the results hold also for doubling or `` bounded growth '' metrics @xcite .", "those doubling metrics constrain growth at every ( or arbitrarily small ) granularity , while ours only bounds regions proportional to the transmission range , in particular capturing bounded independence graphs ( big ) .", "all above models are included by our proposed communication model as shown in sec .", "[ sec : primitiveproofs ] . for", "more on wireless models , please refer to  @xcite .", "more detailed related work is introduced in sec .", "[ drw ] .    * our contributions*.", "we have obtained generalized and improved algorithms for two of the most fundamental dissemination problems , in some cases improving the best results known in static settings . beyond these specific results , we identify the following technical contributions :    _ unified model of wireless networks_. the model proposed appears to be the first that allows for the development of pan - model distributed dissemination algorithms .", "this hopefully prompts further studies crossing the artificial boundary between graph- and fading - based models .", "_ dynamic networks under interference_. this appears to be the first work to address dynamic networks in the presence of comprehensive interference .", "_ uniform algorithms_. our algorithms in the static spontaneous setting appear to be the first in fading models that work independent of instance parameters ( number of nodes , max .  degree ) .    _", "primitives for carrier - sense .", "_ we introduce several primitives or _ capabilities _ that can be implemented using environmental sensing , and propose to study the power of such primitives .    _ stabilization mechanisms .", "_ we identify contention adaptation as a fundamental ability in wireless networks , that appears to be of crucial value to implement other distributed tasks .", "* roadmap*.   the formal model and basic definitions , including the definitions of communication model and carrier sensing primitives , are given in sec .", "[ sec : model ] .", "[ sec : balancingcontention ] contains the core technical part of the paper that includes the basic contention balance routine and its analysis .", "the main results concerning local and global broadcast problems are presented in sections  [ sec : localbroadcast ] and  [ sec : broadcast ] , respectively . due to space constraints", ", most proofs are relegated to appendices .", "we consider a dynamic network of point - size wireless devices ( nodes ) .", "nodes can transmit messages in time slots / rounds that are sufficiently long to allow a transmission of a single message .", "no global clock or synchronization of rounds is required , but the clocks of different nodes run at a similar rate , i.e. , the length of a round differs between nodes at most by a factor of 2 .", "nodes may arrive and leave the network at any time .", "unless specified otherwise , the nodes are assumed to work _ non - spontaneously _ : they can initially be in sleep state and join the execution of an algorithm only after receiving a message .", "we say a node is _ alive _ at some point in time if it is present in the network .", "we assume the total number of nodes in the network is polynomially bounded by a number @xmath16 in each round .", "we use @xmath17 to denote the set of alive nodes at any fixed point in time and also use @xmath16 to denote the current number of nodes , i.e. @xmath18 .", "we assume all nodes use the same transmission power @xmath19 for communication in all rounds .    _", "metrics_.   the _ signal strength _  or _ interference _ , depending on context  of transmitting node @xmath20 on a node @xmath1 is @xmath21 , where @xmath22 is the _ path loss _ from @xmath20 to @xmath1 .", "the _ metricity _ of a space @xmath23 is the smallest number @xmath24 such that for every triplet @xmath25 ,", "@xmath26 @xcite .", "we define @xmath27 if @xmath20 and @xmath1 are different nodes and @xmath28 when @xmath29 .", "note that @xmath30 is a _ quasi - metric _ , as all metric axioms except symmetry hold . in the rest of the paper", ", we assume that in each round the metricity of the network is bounded by a fixed constant @xmath24 and will work with values @xmath31 instead of @xmath32 .", "we assume the quasi - metric @xmath30 has _ bounded independence _ , defined below , roughly stating that there can not be many nodes each causing high interference to a _ fixed _", "node , while having low mutual interferences .", "first , some notations . the _ ball _ with radius @xmath33 centered at @xmath20", "is defined as @xmath34 .", "the _ in - ball _ with radius @xmath33 centered at @xmath20 is defined as @xmath35 ; clearly , @xmath36 .", "a set @xmath37 is a @xmath33-_packing _ for set @xmath38 if balls of radius @xmath33 centered at nodes in @xmath39 are contained in @xmath38 and are disjoint .", "@xmath39 is a @xmath40-_cover _ for @xmath38 if the union of balls of radius @xmath40 centered at nodes in @xmath39 contains @xmath38 .", "note that any maximal @xmath33-packing is a @xmath41-cover , and thus one can bound sizes of covers by packings .", "we say that @xmath30 has @xmath42-_bounded independence _ , for given @xmath43 and @xmath44 , if for every @xmath45 and every in - ball @xmath46 of radius @xmath47 , the size of a maximum cardinality @xmath48-packing of @xmath46 is at most @xmath49 , where @xmath50 is a constant , possibly depending on @xmath51 .", "for instance , the euclidean plane is @xmath52-bounded independent , for every @xmath33 . _", "neighborhoods , communication graph and dissemination problems_.   let @xmath53 denote the maximum transmission distance possible when no other node transmits . as the latter event is arguably very rare ,", "we define the _ communication radius _ @xmath54 as a slightly smaller distance , where @xmath55 is a _ precision _ parameter", ". we will drop the parameter @xmath55 whenever it is fixed and clear from the context .", "fix a round @xmath56 .", "the _ neighborhood _ of a node is @xmath57 , describing who @xmath20 can communicate with directly .", "the basic operation of interest is when @xmath20 broadcasts a message to its neighbors @xmath58 .", "the _ communication graph _ is a directed graph @xmath59 , where @xmath60 if and only if @xmath61 .", "thus , the sequence @xmath62 defines a _ dynamic graph_. the _ vicinity _ of @xmath20 refers to a larger region , @xmath63 , for a parameter @xmath64 .", "the data dissemination problems that we consider are defined below .", "we say that a node @xmath20 _ mass - delivers _ in round @xmath56 if it transmits and all its neighbors ( @xmath58 ) receive the message .    in the _ local broadcast _ problem , given a node @xmath20 , it is required to minimize the time from the beginning of the algorithm until node @xmath20 mass - delivers at least once , assuming it stays alive during that time .    in the _", "( global ) broadcast _ problem , given a distinguished _ source _ node that initially holds a message , the goal is to minimize the time needed to deliver the message to every node in the network through multihop transmissions .", "_ one hop communication_.   when is a transmission successfully received ?", "suppose a node @xmath20 transmits in round @xmath56 , and let @xmath39 be the set of concurrently transmitting nodes .", "let @xmath65 and @xmath66 be parameters that depend on the precision @xmath55 .", "( , success on a clear channel )", "if no other node in @xmath67 transmits _ and _ the total interference at node @xmath20 is at most @xmath68 ( i.e. @xmath69 and @xmath70 ) , then the transmission of node @xmath20 is successfully received by _ all _ its neighbors ( @xmath58 ) .", "otherwise , the reception is under adversarial control .    _", "randomized algorithms_.   we mainly consider randomized algorithms of the following form : in each round @xmath56 , node @xmath1 makes a transmission with probability @xmath71 , independent of other nodes transmissions in that round .", "an important notion for the analysis of such algorithms is _ local contention _ , the sum of the transmission probabilities in a close region .", "the contention in the _ close neighborhood _ of a node @xmath1 in round @xmath56 is @xmath72 , where the radius @xmath73 allows all pair of nodes in @xmath74 to potentially communicate .", "also , let @xmath75 denote the contention in the larger _ vicinity _ of @xmath1 in round @xmath56 ( @xmath76 will be fixed later ) .", "we will also use the notation @xmath77 to denote the interference at @xmath1 from nodes outside its vicinity ( in @xmath78 ) in round @xmath56 .", "the expected value of @xmath77 is @xmath79 .    _", "sensing primitives_.   we assume the nodes have abilities to sense activity on the channel .", "namely , we assume the nodes are able to detect high and low contention in their vicinity , detect ( under some conditions ) whether their transmission in a given round succeeded and detect a single very near transmission . in the following ,", "we formalize these notions in three primitives : , and .", "we show in sec .", "[ sec : primitiveproofs ] how _ all _ these primitives can be implemented with _", "basic _ physical carrier sensing and possibly also with other means .", "( ) . contention can be probabilistically deduced from measured level of radio activity .", "we want to relax this ability and will use the following variant , where the outcome of is one of the two values : busy or idle channel . formally ,", "for each node @xmath1 and round @xmath56 :    if contention among close neighbors is high ( @xmath80 ) then they _ all _ detect busy channel in round @xmath56 with probability at least @xmath81 , for given @xmath82 , where @xmath83 is a constant ,    if the contention in the vicinity of @xmath1 is low ( @xmath84 ) _ and _ the interference on @xmath1 from outside its vicinity is above a threshold ( @xmath85 ) then @xmath1 detects idle channel in round @xmath56 with probability at least @xmath86 , for given @xmath87 , where @xmath88 and @xmath89 are constants .", "if a node @xmath20 has the = ( @xmath55 ) primitive ( depending on the precision parameter @xmath55 ) then : if @xmath20 transmits in round @xmath56 , the interference at @xmath20 is bounded by @xmath90 and the transmission is received by all nodes in @xmath91 , then the outcome of is 1 , where @xmath90 is a parameter .", "if the transmission is not received by a node @xmath92 then the outcome is @xmath93 . otherwise , the outcome is @xmath93 or @xmath94 , adversarially .", "( ) . with =", "( @xmath55 ) primitive , a node is able to detect if a transmitter is very close , assuming that it receives the transmitted message .", "the outcome of is @xmath94 for node @xmath1 in round @xmath56 if @xmath1 _ receives _ a transmission from a node @xmath20 , @xmath95 .", "otherwise , the outcome of is @xmath93 .", "this can also be made approximate .    _", "dynamicity_.   we consider a dynamic network where the topology may change adversarially in each round due to node churn ( node arrivals / departures ) and edge changes .", "we assume that arriving nodes start running the algorithms from an initial configuration , so we do not limit the rate of churn . with edge changes , existing nodes that were not neighbors before , may become neighbors ( e.g. due to mobility ) .", "the new neighbors may cause too much interference in a too short time , so the edge changes should be limited .", "we assume the amount of edge changes is bounded for each node @xmath1 , as follows . consider a time interval @xmath96 of length @xmath97 .", "we require that the number of new neighbors of @xmath1 during @xmath96 ( not counting churn ) is bounded by @xmath98 , where @xmath99 denotes the number of rounds in @xmath96 and @xmath100 is a constant , to be fixed later .", "we further assume the fraction of rounds in @xmath96 when there are more than @xmath101 new neighbors of @xmath1 is bounded by @xmath102 for every @xmath103 , where @xmath104 .", "note that there is no restriction on distance changes inside the neighborhood of @xmath1 ( e.g. it is fine for node @xmath1 if its neighbors move , as far as they remain neighbors ) .", "note that the edge changes may affect the underlying metric , but we require that the upper bounds on metricity and independence are maintained .    _ requirements and assumptions_.   for the convenience of the reader , we gather together all of our assumptions and requirements in a single place .", "communication is assumed to succeed in a clear channel ( ) . we assume constant metricity @xmath105 and that @xmath30 has @xmath42-bounded independence with @xmath106 .", "for the local broadcast problem , we assume that @xmath107 , and for the broadcast problem that @xmath108 . as is standard in fading models , the communication radius @xmath109 is necessarily an @xmath110-fraction of the maximum transmission distance in a clear channel .", "besides the knowledge required by the primitives that are needed for a particular algorithm , the nodes are assumed to know the precision parameter @xmath55 .", "a polynomial estimate on the number of nodes , @xmath16 , is needed in dynamic and non - spontaneous algorithms , but not in the static spontaneous problems .", "knowledge of approximations of model parameters are needed to implement primitives , including @xmath111 , @xmath105 , @xmath53 , @xmath112 , and @xmath68 .", "knowledge of the maximum degree @xmath3 is not needed .", "synchronous operation is only assumed in the broadcast algorithm .", "aspects not defined or constrained are assumed to be under ( adaptive ) adversarial control , including when transmissions that fail are successful , or when nodes appear or disappear from the network .", "the extent of increases in edge strengths over a period is restricted , as detailed above , while decreases are not and neither are node changes .", "the dissemination problems are only expected to function with the set of nodes that are sufficiently stable , as detailed in the respective section .", "in order to keep the contention in the network balanced , we propose a basic procedure called try&adjust , which will be the main building block in our algorithms . the idea is to let each node adapt its transmission probability to the contention detected using the assumed primitive .", "the parameter @xmath113 describes the passiveness of the newly arriving nodes .    _", "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ try&adjust(@xmath114 ) : each node @xmath1 maintains transmission probability @xmath115 in each round @xmath56 , initialized as @xmath116 when @xmath1 enters the network . in round @xmath117 , @xmath1 does : + 1 . transmit with probability @xmath71 , and + 2 .", "set @xmath118 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the aim for controlling contention is , of course , to ensure that transmissions made have a fair chance of being successful , which means they sufficiently overpower the interference experienced at intended receiver from all other transmissions made in that round", ". we account for this interference in two ways : the _ contention _ captures the expected interference from the nodes neighbors , while the _ interference _ integrates also the interference from nodes further away .", "we will measure the contention in the _ vicinity _ of each node @xmath1 , i.e. in @xmath119 , where @xmath76 is a large enough constant .", "we specify a threshold @xmath120 ( recall @xmath121 from definition ) for measuring contention : if @xmath122 then round @xmath56 is a _ high contention round _ for node @xmath1 and is a _ low contention round _ , otherwise .", "we further specify a threshold @xmath123 for interference : if @xmath124 then round @xmath56 is a _ high interference round _ for @xmath1 and is _ low interference round _ , otherwise .", "these thresholds are chosen so as to ensure that in a low - contention / interference round , node @xmath1 will be likely to succeed if it transmits . however , requiring all or most rounds to be low contention for all nodes will lead to high delays", "instead , it turns out that most rounds will have _ bounded contention _ and low interference , which allows for good progress ; we say node @xmath1 experiences _ bounded contention _ in round @xmath56 if @xmath125 , where @xmath126 is a large enough constant , to be specified later .", "we say that a round @xmath56 is _ good _ for node @xmath1 if @xmath56 is both bounded contention and low interference round ( in which case , some node in @xmath1 s vicinity has a good chance of successfully transmitting ) .", "we analyze the properties of try&adjust using the notion of _ a phase _ , the shortest time in which at least @xmath127 rounds occur for all nodes , where @xmath128 is sufficiently large ( given in prop .", "[ pr : pgoodroundl ] ) .", "we use @xmath129 to denote a general phase and also the set of rounds in that phase .", "@xmath130 denotes the number of rounds in a phase @xmath129 ( i.e. @xmath131 ) .", "the fundamental property of try&adjust ( prop .", "[ pr : pgoodroundl ] ) is that , for each node and each phase , most of the rounds in the phase are good for that node .", "this property is then used to show that :    if most of the good rounds in a phase have low contention , then node @xmath1 detects idle channel in most of those rounds .    otherwise , during a constant fraction of the rounds , a node in the vicinity of @xmath1 mass - delivers .    by choosing the parameters carefully ,", "i.e.  requiring _ low enough _ contention , we can make sure that during a phase with mostly low contention , the node detects idle channel in _ most of the rounds _ ( more than half ) in a phase , thus leading to an increase of transmission probability by the end of the phase , which , after sufficiently many phases ensures message delivery , w.h.p . on the other hand , during a phase with mostly high contention , there will be many nodes in the vicinity of @xmath1 that successfully transmit , leading to lower contention .", "these ideas are applied in thms .", "[ thm : localbroadcast ] and  [ thm : broadcast ] . the core idea behind this analysis", "is based on  @xcite .", "[ pr : pgoodroundl ] let @xmath132 .", "if constants @xmath133 and @xmath134 are large enough then for each node @xmath1 and phase @xmath129 , with probability @xmath135 , a @xmath136-fraction of the rounds in @xmath129 are good", ".    the proof is rather technical and is deferred to sec .  [ sec : contentiondetails ] but the intuition is as follows .", "the contention in each neighborhood must be bounded most of the time , because when it becomes large , it has a high chance of being decreased due to busy channel .", "moreover , we show that in expectation , the contention in _ all _ local neighborhoods is bounded , which is then combined with a geometric argument to show that the expected interference at each node is low most of the time .", "we derive from the fundamental property two useful propositions .", "the first says that if contention is high , then nodes in the vicinity deliver the message .", "[ pr : pdecrease ] assume that constants @xmath137 are large enough . for each node @xmath1 and phase @xmath129 , if at least @xmath138-fraction of the rounds of @xmath129 are of high contention , then @xmath139 nodes in @xmath140 mass - deliver , with probability @xmath135 .    to this end", ", we first show that if a round is good for node @xmath20 and a node in its vicinity transmits , then it mass - delivers with constant probability , utilizing both metric assumptions and the properties of good rounds .", "we then argue that since most rounds are good ( by prop .", "[ pr : pgoodroundl ] ) and most rounds have by assumption sufficient contention , many rounds will be both good and with sufficient contention , and in each of those , a node in the vicinity of @xmath1 is likely to transmit and succeed .", "when contention is low in a lot of rounds of a phase , the node will detect idle channel by the primitive in many rounds .", "this will actually happen during many good rounds , which have the low local contention and low external interference to allow for this detection .", "[ pr : idle ] assume that @xmath137 are large enough .", "for each node @xmath1 and phase @xmath129 , if at least @xmath141-fraction of the rounds of @xmath129 are low contention rounds , then with probability @xmath135 , in at least @xmath142-fraction of the rounds of @xmath129 , @xmath1 will detect idle channel and have low contention and low interference .", "we propose an algorithm for asynchronous local broadcast in dynamic networks .", "the algorithm is an extension of the try&adjust procedure , where the nodes try to balance the contention in the network and stop transmitting as soon as they deliver their message .", "we assume the nodes are powered with and primitives .", "note that the passiveness parameter is set to @xmath143 , which means that the transmission probability of nodes does not get below @xmath144 .", "localbcast : each node @xmath1 executes try&adjust(1 ) with the following additional step : if @xmath1 transmits and detects , it stops ( i.e. @xmath145 for @xmath146 ) .    we will estimate the performance of the algorithm using the notion of _ dynamic degree _ , defined as follows . given a parameter @xmath147", ", we denote @xmath148 for node @xmath1 and rounds @xmath149 with @xmath150 , where @xmath151 denotes the in - ball @xmath119 in round @xmath33 .    below", "we prove that if there are not too many node insertions in the neighborhood of a node @xmath1 , then @xmath1 mass - delivers ( delivers to all its neighbors ) in time comparable to its dynamic degree with @xmath76 a constant .", "the main tools for proving the bound are props .", "[ pr : pdecrease ] and  [ pr : idle ] .", "first we argue that if there is a phase of mostly low contention , then node @xmath1 will deliver its message , w.h.p .", "then we show that if the insertions are not too intensive then the contention around @xmath1 will decrease and a phase with mostly low contention will happen .", "[ thm : localbroadcast ] there is a constant @xmath147 , such that a node @xmath1 performing localbcast asynchronously in a time interval @xmath152 $ ] with @xmath153 mass - delivers , w.h.p .", ", provided that @xmath154 .", "note that the assumption @xmath154 is needed only for making the claim w.h.p .", ": it can be relaxed to higher degree polynomials by only increasing constant factors .", "let us fix constants @xmath155 so that props .", "[ pr : pdecrease ] and  [ pr : idle ] with @xmath156 .", "we partition @xmath96 into phases ( for node @xmath1 ) and classify them into two types : ( type a ) phases @xmath129 where at least @xmath138-fraction of rounds are high contention rounds ( i.e. @xmath157 ) , and ( type b ) phases @xmath129 where at least @xmath141-fraction of rounds are low contention rounds ( i.e. @xmath158 ) .", "node @xmath1 mass - delivers in a type b phase , w.h.p .", "let @xmath159 be the low contention and interference rounds during phase @xmath129 where @xmath1 detects idle channel . by prop .", "[ pr : idle ] @xmath160 > 1-o(n^{-3})$ ] .", "assume for now that the latter happens .", "for each @xmath161 we have @xmath162 .", "let us call this operation doubling .", "the value of @xmath71 at the beginning of the phase is at least @xmath144 , so @xmath163 doubling operations are sufficient to raise it to @xmath164 .", "the probability can be further halved during the phase at most @xmath165 times .", "thus , we may assume we have at most @xmath166 halving and at least @xmath167 doubling operations applied to an initial value @xmath164 .", "if @xmath168 , then the total number of halving operations is less than @xmath169 .", "it follows that @xmath1 has @xmath170 in at least @xmath171 low contention / interference rounds . by lemma  [ le : broadgood ] , in each such round , @xmath1 mass - delivers with probability at least @xmath172 ; hence , if @xmath128 is large enough , @xmath1 mass - delivers in @xmath129 , w.h.p .", "it remains to argue that there will be a type b phase during time interval @xmath96 .", "consider a type a phase @xmath129 .", "[ pr : pdecrease ] implies that with probability @xmath135 , @xmath173 nodes in @xmath119 deliver their message and stop during phase @xmath129 .", "thus , with probability @xmath174 , there are at most @xmath175 type a phases with @xmath176 a constant , as there are at most @xmath177 nodes in @xmath140 during the time interval @xmath96 ( also recall that @xmath178 ) .", "we conclude that if @xmath96 consists of at least @xmath179 phases , it will contain a type b phase and @xmath1 will deliver its message w.h.p .", "* implications for static networks*.   in static networks , the parameter @xmath177 is at most @xmath180 if @xmath76 is constant , where @xmath181 is the maximum size of a neighborhood in the network .", "thus , we obtain the following optimal result ( up to constant factors ) for static networks , as @xmath3 and @xmath163 are lower bounds even when running in the spontaneous mode  @xcite .", "when running localbcast in a static asynchronous network , each node @xmath1 completes local broadcast in @xmath182 rounds , w.h.p .    * remark . * in the special case when the nodes can start executing the algorithm simultaneously , i.e. in the _ spontaneous mode _ , the nodes need not know an upper bound on the size of the network . indeed ,", "each node may start running try&adjust with initial probability set to an arbitrary value and with no lower limit .", "the first phase will be spent for stabilization and can be ignored , while the argument for the rest of the phases is nearly identical to the one in thm .", "[ thm : localbroadcast ] .", "for the broadcast problem , we assume nodes communicate in synchronized rounds of equal length .", "each round consists of two slots .", "the idea is to use the first slot of each round for disseminating the message with try&adjust and the second slot for notifying nodes which have no uninformed neighbors .", "the latter is accomplished by using higher precision primitives , namely ( @xmath183 ) and ( @xmath183 ) , when executing try&adjust .", "this helps to detect a transmission that is successfully received by all nodes in @xmath184 of a node @xmath1 . upon detecting such a transmission , node", "@xmath1 resends the message in the second slot , in order to inform nodes @xmath20 with @xmath185 that their neighborhood ( @xmath186 ) has been covered .", "a node @xmath20 can detect that @xmath185 using .", "the algorithm is presented below .", "we assume the passiveness parameter @xmath114 of try&adjust is large enough , to be defined later .", "note that the algorithm works for the non - spontaneous mode , as nodes act only after receiving the message .", "bcast(@xmath114 ) : initially , only a source node @xmath187 has the message .", "a node @xmath1 , upon receiving a message , starts executing try&adjust(@xmath114 ) in the first slot of rounds .", "in addition , in each round @xmath56 , + 1 . if @xmath1 detects in the first slot , it retransmits in the second slot and restarts try&adjust(@xmath114 ) , + 2", ". if @xmath1 receives a message in the first slot and detects in the second slot , it restarts try&adjust(@xmath114 ) .    in order to evaluate the progress of the algorithm ,", "we use a notion of a _ dynamic distance _ , as defined below .", "let @xmath188 be a parameter .", "a sequence @xmath189 is called a _ stable @xmath187-@xmath1 path _ if there is a sequence @xmath190 of time intervals with @xmath191 $ ] , such that @xmath192 , @xmath193 and nodes @xmath194 and @xmath195 are both alive and @xmath196 during @xmath197 .", "the _ time - length _ of a stable @xmath187-@xmath1 path is @xmath198 .", "the _ stable @xmath187-@xmath1 distance _", "@xmath199 is defined as the minimum time - length of a stable @xmath187-@xmath1 path .", "note that a stable path need not be connected at any fixed point in time .", "moreover , most of the nodes might be missing at any given point in time .", "the core idea behind the analysis of the following theorem is similar to the case of local broadcast : we show that as soon as a neighbor @xmath20 of a node @xmath1 has the message and @xmath20 and @xmath1 keep being neighbors for @xmath200 rounds , @xmath1 will receive a transmission of @xmath20 during those rounds .", "[ thm : broadcast ] assume the edge change rate @xmath100 is sufficiently small .", "there are constants @xmath201 , such that when running bcast(@xmath114 ) in the synchronous mode , each node @xmath1 receives the message in @xmath202 rounds w.h.p .", "let us fix constants @xmath155 so that props .", "[ pr : pdecrease ] and  [ pr : idle ] hold with interference threshold @xmath203 , where @xmath204 and @xmath205 are the parameters corresponding to precision @xmath183 .", "we set the passiveness parameter of try&adjust to @xmath206 and the stable distance parameter @xmath207 .", "consider a node @xmath20 that receives the message in round @xmath56 .", "let @xmath1 be such that @xmath208 in the time interval @xmath209 $ ] ( and both stay alive during @xmath96 ) .", "the theorem follows by an induction and union bound from the claim below .", "node @xmath1 gets the message of @xmath20 during @xmath96 , with probability @xmath210 .", "we prove the claim by contradiction , and assume that @xmath1 can not get the message during @xmath96 .", "let us split @xmath96 into phases ( for @xmath20 ) and , similar to the proof of thm .", "[ thm : localbroadcast ] , classify the phases @xmath129 into types : ( type a ) at least @xmath138-th of rounds in @xmath129 are high contention rounds for @xmath20  @xmath211 , and ( type b ) at least @xmath141-th of rounds in @xmath129 are low contention rounds for @xmath20  @xmath212 .", "we show that with probability @xmath135 , all phases in @xmath96 are of type b. consider a type a phase @xmath129 . we know from prop .", "[ pr : pdecrease ] that , with probability @xmath135 , there is a set @xmath39 of @xmath139 nodes in @xmath140 that deliver their messages and restart try&adjust during @xmath129 .", "we split @xmath39 into three subsets , @xmath213  those inserted by churn during @xmath129 , @xmath214  those inserted by edge change during @xmath129 and @xmath215  the rest .", "consider @xmath214 first .", "we know that in each local neighborhood , there are at most @xmath216 nodes arriving due to edge changes .", "note that @xmath217 can be covered with @xmath218 local neighborhoods ; hence , the total number of nodes arriving in @xmath217 due to edge changes is @xmath219 . since @xmath220 , setting @xmath221 a small enough constant gives @xmath222 .", "now consider @xmath215 . by the primitive , for each node @xmath223 , all nodes in @xmath224 also restart try&adjust , setting their transmission probability to @xmath225 . by the setting of @xmath226 ,", "the probability that a node restarting try&adjust in phase @xmath129 transmits again in @xmath129 is at most @xmath227 .", "thus , with probability at least @xmath135 , the nodes in @xmath214 constitute an @xmath228-packing of @xmath140 and are at most @xmath229 . as for @xmath213 ,", "the probability that a newly arriving node transmits during phase @xmath129 is at most @xmath227 , so @xmath230 w.h.p .", "these bounds lead to a contradiction  @xmath231 , if constant @xmath128 is large enough .", "thus , each phase @xmath129 of @xmath96 is of type b with probability @xmath135 .", "it remains to recall that @xmath96 contains constant number of phases .", "now assume that @xmath129 is of type b. in a similar way as in theorem  [ thm : localbroadcast ] , we can apply prop .", "[ pr : idle ] and show that if @xmath232 at the beginning of @xmath129 , then by the end of @xmath129 , the effect on @xmath71 is equivalent to applying at least @xmath233 of @xmath234 operations on @xmath235 .", "since @xmath236 at the beginning of @xmath96 is at least @xmath225 , it will take at most 11 phases to increase the probability to the value @xmath164 .", "then , a single phase will suffice for @xmath20 to deliver its message ( including to node @xmath1 ) , with probability at least @xmath210 .", "* implications for static networks*.   when the network is static , theorem  [ thm : broadcast ] can be reformulated in terms of hop - distance @xmath237 in the communication graph , which is defined as the length of the shortest directed @xmath187-@xmath1 path in @xmath238 : we have that @xmath239 for any node @xmath1 .", "note also that in this setting nodes that succeeded transmitting or detected need not continue the algorithm , so they stop transmitting . in this case , setting the passiveness parameter to @xmath143 suffices .", "we call this variant of the algorithm bcast@xmath240 .    when running bcast@xmath240 in synchronous non - spontaneous mode in a static network with source @xmath187 , each node @xmath1 receives the message in @xmath241 rounds w.h.p .", "when the communication graph is strongly connected , the broadcast from any source node is completed in @xmath242 rounds , where @xmath243 is the diameter of the communication graph .    in the spontaneous mode ,", "the bound above can be further improved to @xmath244 ; see appendix [ sec : spontaneous ] .", "this is based on finding a constant - density dominating set in @xmath200 time @xcite and simultaneously propagating along the dominators in @xmath244 time .", "we can therefore extend the approach based on @xcite to uniform algorithms in bounded - independence metrics .", "these results are close to best possible .", "we show below that in order to obtain bounds of that magnitude , it is necessary to have the primitive . to this end , we extend the lower bound construction of  ( * ? ? ?", "* thm .  7 ) for `` compact sinr '' to our setting .", "this construction leverages the property of our model that there can be arbitrarily many nodes that are mutually close to each other .", "namely , the bounded - independence metric is strictly more relaxed than the standard euclidean metrics .", "indeed , there is a @xmath245-round broadcast algorithm for the sinr model that does not need or other carrier sensing primitives @xcite .", "what the lower bound then illustrates is that to obtain such results , one must depend on opportune traits of the sinr model that we have tried to avoid and are not necessary for problems like local broadcast .", "thus we can observe concrete tradeoffs depending on model assumptions .", "[ thm : brlowerbound ] for every ( possibly randomized ) broadcast algorithm @xmath246 that uses neither node coordinates nor primitive , there is a @xmath247-bounded - independence metric space where @xmath246 needs @xmath248 rounds to do broadcast in a @xmath249-broadcastable network , even if the nodes have and primitives and operate spontaneously .", "10    n.  alon , a.  bar - noy , n.  linial , and d.  peleg . .", "in _ stoc _ , pages 274285 , 1989 .", "n.  alon , a.  bar - noy , n.  linial , and d.  peleg .", "a lower bound for radio broadcast . , 43(2):290298 , 1991 .", "r.  bar - yehuda , o.  goldreich , and a.  itai . on the time - complexity of broadcast in radio networks : an exponential gap between determinism and randomization . in _", "podc87 _ , 1987 .", "l.  barenboim and d.  peleg .", "nearly optimal local broadcasting in the sinr model with feedback . in _ sirocco _ , pages 164178 , 2015 .", "m.  h. bodlaender and m.  m. halldrsson . beyond geometry : towards fully realistic wireless models . in _ podc _ , pages 347356 , 2014 .", "m.  h. bodlaender , m.  m. halldrsson , and p.  mitra . .", "in _ podc _ , pages 355364 .", "acm , 2013 .", "i.  chlamtac and s.  kutten . on broadcasting in radio networks ", "problem analysis and protocol design .", ", 33(12):12401246 , 1985 .", "b.  s. chlebus , d.  r. kowalski , and s.  vaya . distributed communication in bare - bones wireless networks . , 2015 .", "b.  n. clark , c.  j. colbourn , and d.  s. johnson .", "unit disk graphs .", ", 86(1):165177 , 1990 .", "a.  e. clementi , a.  monti , and r.  silvestri .", "round robin is optimal for fault - tolerant broadcasting on wireless networks . , 64(1):8996 , 2004 .", "a.  czumaj and w.  rytter . broadcasting algorithms in radio networks with unknown topology . in _", "focs03 _ , 2003 .", "s.  daum , s.  gilbert , f.  kuhn , and c.  newport . .", "in _ disc _ , pages 358372 , 2013 .", "b.  derbel and e.  talbi . .", "in _ icdcn10 _ , pages 155166 , 2010 .", "s.  dolev , s.  gilbert , m.  khabbazian , and c.  newport . .", "in _ disc _ , pages 252267 , 2011 .", "a.  fanghnel , t.  kesselheim , and b.  vcking . .", ", 412(24):26572667 , 2011 .", "m.  ghaffari , s.  gilbert , c.  newport , and h.  tan . optimal broadcast in shared spectrum radio networks . in _ opodis12 _ , 2012 .", "m.  ghaffari , b.  haeupler , and m.  khabbazian . randomized broadcast in radio networks with collision detection . in _ podc13 _ , 2013 .", "m.  ghaffari , b.  haeupler , n.  lynch , and c.  newport .", "bounds on contention management in radio networks . in _ disc _ , 2012 .", "m.  ghaffari , e.  kantor , n.  lynch , and c.  newport .", "multi - message broadcast with abstract mac layers and unreliable links . in _", "podc14 _ , 2014 .", "m.  ghaffari , n.  lynch , and c.  newport . the cost of radio network broadcast for different models of unreliable links . in", "podc13 _ , 2013 .", "o.  goussevskaia , t.  moscibroda , and r.  wattenhofer . . in _ dialm - pomc 08", "_ , pages 3544 , 2008 .    p.  gupta and p.  kumar .", ", 46(2):388404 , 2000 .", "m.  m. halldrsson . .", ", 9(1):7:17:20 , dec .", "m.  m. halldrsson , s.  holzer , and n.  a. lynch . a local broadcast layer for the sinr network model . in _", "podc _ , pages 129138 , 2015 .", "m.  m. halldrsson , s.  holzer , p.  mitra , and r.  wattenhofer . .", "in _ soda13 _ , pages 15951606 , 2013 .", "m.  m. halldrsson and p.  mitra . .", "in _ icalp _ , pages 625636 , 2011 .", "m.  m. halldrsson and p.  mitra . .", "in _ podc _ , 2012 .", "m.  m. halldrsson and p.  mitra . .", "fomc _ , 2012 .", "m.  m. halldrsson , y.  wang , and d.  yu .", "leveraging multiple channels in ad hoc networks . in _ podc15 _ , 2015 .", "m.  hoefer , t.  kesselheim , and b.  vcking .", "approximation algorithms for secondary spectrum auctions . in _ proceedings of the twenty - third annual acm symposium on parallelism in algorithms and architectures _ , spaa 11 , pages 177186 , new york , ny , usa , 2011 .", "r.  impagliazzo and v.  kabanets", ". constructive proofs of concentration bounds . in _", "approx - random _ , pages 617631 , 2010 .", "t.  jurdzinski , d.  kowalski , m.  rozanski , and g.  stachowiak .", "distributed randomized broadcasting in wireless networks under the sinr model . in _ disc _ ,", "pages 373387 , 2013 .", "t.  jurdzinski , d.  kowalski , m.  rozanski , and g.  stachowiak . on the impact of geometry on ad hoc communication in wireless networks . in _ podc 14 _ , 2014 .", "t.  jurdzinski , d.  r. kowalski , and g.  stachowiak . .", "in _ fct13 _ , pages 195209 , 2013 .", "t.  jurdzinski , d.  r. kowalski , and g.  stachowiak .", "distributed deterministic broadcasting in wireless networks of weak devices . in _", "icalp13 _ , 2013 .", "t.  kesselheim and b.  vcking . .", "in _ disc _ , pages 163178 , 2010 .    d.  kowalski and a.  pelc .", "broadcasting in undirected ad hoc radio networks . in _", "podc03 _ , 2003 .", "f.  kuhn , n.  lynch , and c.  newport .", "brief announcement : hardness of broadcasting in wireless networks with unreliable communication . in _ disc _ , 2009 .", "f.  kuhn , n.  lynch , c.  newport , r.  oshman , and a.  richa . .", "in _ podc _ , pages 336345 .", "acm , 2010 .", "f.  kuhn , t.  moscibroda , and r.  wattenhofer . .", "in _ mobicom _ , pages 260274 .", "acm , 2004 .", "f.  kuhn and r.  oshman .", "dynamic networks : models and algorithms . , 42(1):8296 , 2011 .", "f.  kuhn , r.  wattenhofer , and a.  zollinger .", "ad hoc networks beyond unit disk graphs .", ", 14:715729 , 2008 .", "e.  kushilevitz and y.  mansour . .", ", 27(3):702712 , 1998 .    c.  lenzen and r.  wattenhofer .", "distributed algorithms for sensor networks . , 370(1958 ) , 2012 .", "t.  moscibroda and r.  wattenhofer . .", ", 21(4):271284 , 2008 .    c.  newport", "radio network lower bounds made easy . in _", "disc14 _ , 2014 .", "c.  scheideler , a.  richa , and p.  santi . . in _", "mobihoc 08 _ ,", "pages 91100 , 2008 .", "s.  schmid and r.  wattenhofer .", "algorithmic models for sensor networks . in _ wpdrts06 _ , 2006 .", "j.  schneider and r.  wattenhofer . .", "in _ podc _ , pages 210219 , 2009 .    j.  schneider and r.  wattenhofer . in _ disc _ , pages 133147 .", "springer - verlag , 2010 .", "d.  yu , q .- s .", "hua , y.  wang , and f.  lau . .", "dcoss 12 _ , pages 132139 , 2012 .", "d.  yu , q .- s .", "hua , y.  wang , h.  tan , and f.  lau . .", "in _ sirocco12 _ , pages 111122 , 2012 .", "d.  yu , q .- s .", "hua , y.  wang , j.  yu , and f.  lau . . in _ infocom13 _ , pages 24272435 , 2013 .    d.  yu , y.  wang , q .- s .", "hua , and f.  c.  m. lau . .", "in _ distributed computing in sensor systems and workshops ( dcoss ) , 2011 international conference on _ , pages 18 , 2011 .", "d.  yu , y.  wang , y.  yan , j.  yu , and f.  lau . speedup of information exchange using multiple channels in wireless ad hoc networks . in _", "infocom15 _ , 2015 .", "_ wireless models : _ considering wireless interference , there are two classes of wireless network models : graph - based and physical models .", "basically , the graph - based models define a local and binary type of interference , while the physical models consider fading effect of signal on wireless channels .", "the most classical graph - based model is the radio network model  @xcite . in this model", ", the network is modeled using a communication graph , where each pair of nodes that can communicate with each other is connected by an edge .", "it defines the interference just from direct neighbors , and a transmission can succeed if and only if there is only one neighbor of the receiver transmitting .", "there are many widely used variants of the classical radio network models , including : 1 ) the @xmath0-hop model where the interference comes from @xmath0-hop neighbors  @xcite ; 2 ) unit disc graph ( udg ) model  @xcite which defines the neighborhood using a unit disc ; 3 ) quasi unit disc graph ( qudg ) model  @xcite which just defines all pairs of nodes with distance at most @xmath76 for some given @xmath250 $ ] are adjacent , and leave the ` grey ' area in @xmath251 $ ] being determined by an adversary ; 4 ) protocol model  @xcite , where each node has a transmission range and an interference range , and a successful transmission occurs if a node falls into the transmission range of a transmitter and outside the interference ranges of all other transmitter ; 5 ) bounded - independence graph ( big ) model  @xcite , which defines abstractly and requires that the size of the maximal independent set in the @xmath33-hop neighborhood of each node is bounded by a polynomial function with @xmath33 . though the graph - based interference models miss certain crucial aspects of actual wireless networks , the simple definition of these models can help derive novel insights into distributed solutions to wireless problems", ".    physical models , also known as sinr models  @xcite , capture the fading and cumulative features of receptions in actual wireless environments .", "the default assumption is that interference fades with a polynomial of the distance , and transmission succeeds only if the received signal strength is sufficiently larger than the total interference plus noise .", "recently , the sinr model has attracted great attentions in the distributed community  @xcite .", "most of these works focus on networks embedded in euclidean space , while many of the results hold also for doubling or `` bounded growth '' metrics @xcite .", "those doubling metrics constrain growth at every ( or arbitrarily small ) granularity , while ours only bounds regions proportional to the transmission range , in particular capturing bounded independence graphs ( big ) . for more on wireless models", ", please refer to  @xcite .", "_ local broadcast _ : in the radio network model , probably the first local broadcast result was a randomized algorithm of alon et al .", "@xcite in a synchronous model , running in @xmath6 rounds .", "derbel and talbi  @xcite later generalized their algorithm to work without knowledge of @xmath3 and their proposed algorithm can accomplish local broadcast in @xmath7 rounds .", "the decay strategy also yields an @xmath6 time algorithm for local broadcast without knowledge of @xmath3  @xcite .", "goussevskaia et al .", "@xcite gave the first results for local broadcast in the sinr model , running in time @xmath6 and @xmath252 with and without knowledge of @xmath3 , respectively .", "the latter was improved in @xcite and further improved , independently , to @xmath7 time @xcite . with free acknowledgements ,", "this was improved to @xmath253 @xcite .", "when additionally @xmath3 is known , this was further improved recently to @xmath8 in the spontaneous setting @xcite .", "the speedup of multiple channels on local broadcast was considered in  @xcite    _ broadcast _ : the complexity of broadcasting is well understood in graph - based models . in the radio network model , bar - yehuda et al .", "@xcite presented the decay protocol which can accomplish non - spontaneous broadcast in @xmath254 rounds , where @xmath46 is the diameter .", "this result was improved to @xmath9 independently by czumaj and rytter  @xcite , and kowalski and pelc  @xcite .", "these algorithms can be viewed as clever optimizations of the decay protocol and match the lower bound  @xcite . with collision detection ,", "this lower bound was recently broken in  @xcite , where a solution of @xmath10 was given .", "broadcast in multi - channel radio networks was considered in  @xcite . for the udg model ,", "an @xmath255 time algorithm was given in  @xcite in the spontaneous setting .", "broadcasting has been treated in the sinr model under a variety of assumptions .", "some are stronger than ours ( location information @xcite , power control @xcite ) .", "some relax the assumption about the connectivity property , but incur necessarily super - linear complexity @xcite .", "results in our setting , but without carrier sensing , include time complexity of @xmath256 @xcite ( see also @xcite ) , where @xmath12 denotes the maximum ratio between distances of stations connected in the communication graph ; and @xmath13 @xcite . in the spontaneous setting , where the nodes can build an overlay structure along which the message is then propagated , scheideler et al .", "@xcite solved the problem in @xmath257 rounds using physical carrier sensing .", "yu et al .", "@xcite solved the problem in @xmath258 rounds using power control , allowing stations to decide the strength of a transmitted signal in each step . and", "the algorithm in  @xcite then runs in time @xmath254 .", "_ distributed models of temporal variability _ : dynamic networks have been studied extensively in recent years ( see @xcite for a survey ) , but generally not in the presence of interference . the _ dual graph _", "model  @xcite ( originally due to @xcite ) was designed to capture inherent _ unreliability _ in wireless networks , much of which can be due to dynamicity .", "the main focus of that work is on extending the radio network model in general graphs .", "importantly , the dual graph model does not distinguish between interference and communication edges ; only that the unreliable edges can transmit both interference and the usual communication , but their availability is under adversarial control .", "thus , there is no way to capture interference from further away nodes .", "most problems become extremely difficult against a powerful adversary , and to get good result , one must assume a much weaker one  @xcite .", "this model only involves edge behavior and not node changes ( churn ) .", "both the local broadcast  @xcite and global broadcast  @xcite problems are studied in the dual graph model .    a dynamic model that considers node insertion is the _ unstructured _ model  @xcite , which admits arbitrary wake - up mode and asynchronous communication .", "this model was first proposed in the unit - disc setting , and then extended to bounded independence graphs ( big )  @xcite and sinr  @xcite .", "it has been widely used in the solution of a variety of distributed wireless problems  @xcite , including local broadcast  @xcite , but there are no known global broadcast results .", "the model neither consider node deletion nor edge changes .", "hence , the impact of dynamicity on global communication is largely unexplored .", "[ [ modeling - communication ] ] modeling communication + + + + + + + + + + + + + + + + + + + + + +    our communication model captures most known algorithmic wireless models , as it is demonstrated below on the example of sinr , disk - graph based and protocol models .", "note that we assume below the distance @xmath259 to be symmetric , but the results hold also for `` almost symmetric '' functions , i.e. when there is a constant @xmath260 such that @xmath261 for all @xmath262 .", "_ sinr model .", "_ consider a network in a metric space . in the sinr model of communication , if @xmath39 is the set of simultaneously transmitting nodes in the network , a node @xmath1 receives the transmission of node @xmath20 if and only if @xmath263 where constants @xmath113 and @xmath264 denote the minimum sinr threshold and the ambient noise , respectively .", "note that @xmath265 in this setting .", "we can implement with parameters @xmath266 and @xmath267 , as shown in the proposition below .", "[ pr : scc ] if the interference at a node @xmath1 is less than @xmath68 in round @xmath56 , then @xmath1 will deliver its message if it transmits .", "note that if the interference at node @xmath1 is not more than @xmath68 then there is no node @xmath268 transmitting in round @xmath56 , as otherwise the interference at @xmath1 would be at least @xmath269 ; hence , only nodes in @xmath270 can transmit .", "consider an arbitrary node @xmath271 .", "for each node @xmath272 , @xmath273 .", "then , the interference at @xmath274 is at most : @xmath275 which implies that node @xmath274 receives @xmath1 s transmission : @xmath276 .", "_ the udg and ubg models . _", "these models are described by geometric graphs : a node @xmath20 receives a message from another node @xmath1 if and only if @xmath1 is the only transmitting neighbor of @xmath20 .    in the unit disk graph ( udg ) and unit ball graph ( ubg ) models", "the nodes are located in a metric space and two nodes are connected by an edge if and only if their distance is at most @xmath277 .", "the functionality of can be modeled as follows : the transmission of a node @xmath1 is received by all its neighbors if there is no other node at distance less than @xmath278 from @xmath1 transmitting simultaneously , i.e. we can set the parameters to @xmath279 and @xmath280 .    _ the quasi - udg model . _", "the quasi - udg model is an extension of the udg model : a ) if @xmath281 then @xmath20 and @xmath1 are connected by an edge , b ) if @xmath282 then they are disconnected , c ) otherwise , @xmath20 and @xmath1 may be connected or not . in this case", "may be implemented by setting @xmath279 and @xmath283 , with the adversary constrained to follow the specific static situation captured by the qudg .    _ the protocol model .", "_ in the protocol model , the nodes are in a metric space and there are two radii : @xmath277  the communication radius , and @xmath284  the interference radius .", "a node @xmath1 receives the transmission of a node @xmath20 if and only if : 1 ) @xmath1 is in the communication range of node @xmath20 , i.e. @xmath281 , and 2 ) there is no transmitting node @xmath274 such that @xmath1 is in the interference range of @xmath274 : for each transmitting node @xmath285 , @xmath286 .", "may be implemented here by setting @xmath279 and @xmath283 .", "_ the big model . _ in the bounded independence graph ( big ) model , for a parameter @xmath287 , we are given a graph on the nodes with the property that for every node @xmath1 and every @xmath288 , the maximum independent set in the @xmath0-neighborhood of @xmath1 is @xmath289 .", "the shortest - path distance metric on the graph is now naturally a @xmath290-bounded independence metric . to fit in our model ,", "the growth parameter @xmath287 must be less than @xmath105 .", "_ @xmath0-hop variants .", "_ these graph models can be naturally generalized to a model on interference , where nodes of distance at most @xmath0 cause interference , for some @xmath291 .", "we capture this by extending @xmath112 as needed .", "[ [ implementing - primitives - with - physical - carrier - sensing ] ] implementing primitives with physical carrier sensing + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    one way of implementing the primitives mentioned in this paper is to use physical carrier sensing , i.e. we assume the nodes have technology to detect if the interference ( plus noise ) is higher than a given threshold .", "we show below how to implement , and primitives using carrier sensing .", "* primitive .", "* the primitive can be implemented using a carrier sensing threshold @xmath292 and setting the parameter @xmath293 ; busy channel is detected if and only if the interference is at least @xmath96 .", "we will need the following technical fact .", "[ ieq ] for every @xmath294 $ ] , @xmath295 , it holds that @xmath296    if @xmath297 in round @xmath56 then all nodes in @xmath298 detect busy channel with probability at least @xmath299 . in particular", ", we can take @xmath300 if @xmath301 .    by the setting of @xmath96 and the definition of @xmath74 ,", "if two nodes in @xmath74 transmit in round @xmath56 , then all nodes in @xmath74 will detect busy channel .", "hence , the probability of all nodes in @xmath74 detecting busy channel is at least the probability of more than one node transmitting in round @xmath56 .", "the probability of no node transmitting is @xmath302 by lemma  [ ieq ] .", "the probability of exactly one node transmitting is : @xmath303 where we used the assumption that @xmath304 and lemma  [ ieq ] .", "thus , the probability of detecting busy channel is at least @xmath305 .    for every @xmath147 ,", "if @xmath306 and @xmath307 then @xmath1 detects idle channel with probability at least @xmath308 .    by the setting of the threshold @xmath96 , if there is no node transmitting in @xmath119 then node @xmath1 will detect idle channel .", "thus , the probability that @xmath1 detects idle channel is at least @xmath309 using lemma  [ ieq ] .", "* primitive . * in order to detect successful transmission , we can use interference threshold @xmath310 and set @xmath311 , where @xmath312 and @xmath313 are the parameters of .", "if a node @xmath1 senses that the interference is no higher than @xmath96 , it knows that : 1 .", "there is no node in @xmath314 transmitting , as otherwise the interference would be at least @xmath315 , 2 .", "the interference is at most @xmath68 ; thus , it knows that its transmission has been received by all neighbors in @xmath316 by .", "* primitive . *", "if a node @xmath1 receives a message from a node @xmath20 then @xmath1 can separate the signal from the interference and measure the received signal strength . as the nodes use uniform power assignment", ", node @xmath1 knows that @xmath95 if the received signal is stronger than @xmath317 .", "[ [ implementing - primitives - by - other - means ] ] implementing primitives by other means + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the primitives can frequently be implemented in other ways , often with the logarithmic blowup that explains the differences with the best carrier - sense - free results .    *", "* in an asynchronous system , it may be impossible to implement by other means than carrier sense . in a synchronized system , however , we can be achieved with logarithmic or polylogarithmic factor overhead .", "consider a given round .", "for each probability @xmath318 , repeat @xmath319 times : the senders in the original round transmit with probability @xmath320 .", "using concentration bound with @xmath50 sufficiently large , one can infer the contention within an small approximation , with high probability .", "such a strategy has been applied , e.g. , in @xcite .    *", "primitive . *", "a simple strategy is to work with only probabilistic guarantees of a transmission being received by all neighbors .", "then , simply repeat the protocol until this has been achieved @xmath319 times , which gives an guarantee , w.h.p .", "this approach underlies , e.g. , the local broadcast algorithms without carrier sense @xcite .", "* primitive . *", "this primitive , which is essential for dominator - based strategies for broadcast , can be implemented using power control : by lowering the power on all units appropriately , one can ensure that nodes further away ( by a small constant factor ) will not be able to hear the message due to the ambient noise term , see e.g.  @xcite .", "alternatively , one can assume that distances can be determined in other ways , such as by gps @xcite .", "recall that we need to prove the following .", "we assume that at the beginning of the _ first _ phase under consideration , the contention in the whole network is bounded by a constant .", "this holds for all algorithms in this paper , as the initial probability of nodes is always at most @xmath321 .    *", "* proposition [ pr : pgoodroundl ] . * * let @xmath132 . if constants @xmath133 and @xmath134 are large enough then for each node @xmath1 and phase @xmath129 , with probability @xmath135 , a @xmath136-fraction of the rounds in @xmath129 are good .", "recall that in a good round , there should be both bounded contention and low interference .", "the proof is split into two parts , each handling one of these properties .", "[ pr : pgoodroundl ] follows by simply combining those two parts .", "we will need the following concentration bounds .", "@xcite[le : chernoff ] consider a collection of binary random variables @xmath322 , and let @xmath323 .", "if there are probabilities @xmath324 with @xmath325\\leq \\prod_{i\\in s}p_i$ ] for every set @xmath326 , then it holds for @xmath327 and @xmath328 that @xmath329\\leq\\left(\\frac{e^{\\delta}}{(1+\\delta)^{1+\\delta}}\\right)^\\mu\\leq e^{-\\frac{\\delta^2\\mu}{2(1+\\delta/3)}}.\\ ] ] if , on the other side , there are probabilities @xmath324 with @xmath325\\geq \\prod_{i\\in s}p_i$ ] for every set @xmath326 , then it holds for @xmath327 and @xmath330 that @xmath331\\leq\\left(\\frac{e^{-\\delta}}{(1-\\delta)^{1-\\delta}}\\right)^\\mu\\leq e^{-\\delta^2\\mu/2}.\\ ] ]      first , we show that for each fixed node @xmath20 , the contention in the _ local _ neighborhood @xmath332 is bounded in most of the rounds of a phase . to this end , we show that in each round , the contention is either already low or will be halved with significant probability , then apply a concentration bound to show the claim .", "recall the constant @xmath333 from the definition of .", "[ le : phi ] let @xmath129 be a time interval and assume that @xmath334 at the beginning of @xmath129 .", "then for every @xmath335 , @xmath336 happens at most @xmath337 times during @xmath129 , with probability @xmath338 , where @xmath339 and @xmath340 is the edge change parameter .    by the assumptions on edge changes", ", we have that during phase @xmath129 , the fraction of rounds where more than @xmath341 nodes become a neighbor of @xmath20 because of edge changes is @xmath102 .", "let @xmath159 denote the remaining set of rounds .", "we have @xmath342 . in each of those rounds ,", "the contribution of edge changes in @xmath343 is clearly at most @xmath344 .", "next we bound the number of rounds in @xmath159 , where @xmath345 .", "consider such a round @xmath161 .", "let @xmath346 . by the definition of , all nodes in @xmath332 detect busy channel in round @xmath56 and halve their transmission probabilities with probability at least @xmath347 .", "thus , @xmath348\\ge 1-\\mu'$ ] , where the additive @xmath164 accounts for the sum of probabilities of the nodes that just join the ball @xmath332 due to node churn ( recall that each of them has an initial probability at most @xmath349 ) and @xmath344 is an upper bound on the contention due to edge changes ( by the definition of @xmath159 ) . for each round @xmath161 ,", "let us define a binary random variable @xmath350 as follows : @xmath351 if @xmath352 or @xmath353 and @xmath354 ( recall that @xmath355 ) and @xmath356 otherwise . by the discussion above , we have : @xmath357&=pr[p_{t}(u)\\leq\\phi]+pr[p_{t}(u)>\\phi\\wedge p_{t+1}(u)\\leq\\frac{5}{6}p_t(u)]\\\\ & = pr[p_{t}(u)\\leq\\phi]+pr[p_{t}(u)>\\phi]\\cdot pr[p_{t+1}(u)\\leq\\frac{5}{6}p_t(u)|p_{t}(u)>\\phi]\\\\ & \\geq pr[p_{t}(u)\\leq\\phi]+pr[p_{t}(u)>\\phi]\\cdot ( 1-\\mu')\\\\ & \\geq 1-\\mu ' . \\end{aligned}\\ ] ] this implies that for each round @xmath358 and each subset @xmath359 of earlier rounds ( @xmath360 for @xmath361 ) , @xmath362\\geq 1-\\mu'$ ] and @xmath363\\leq \\mu'$ ] .", "thus , for each subset @xmath364 we get @xmath365\\le \\mu'^{|s| } , $ ] and if we denote @xmath366 , then @xmath367\\le \\mu ' |h'|=\\mu |h|$ ] , where we denote @xmath368 .", "thus , we can apply chernoff bound ( lemma  [ le : chernoff ] ) with @xmath369 to bound @xmath370 with high probability : @xmath371\\le 2^{-\\mu|h|}.\\ ] ] using this bound , we obtain a bound on the number of rounds @xmath56 with @xmath372 .", "let @xmath373 denote this number .", "consider a maximal interval @xmath374\\subseteq h'$ ] such that @xmath345 for all @xmath375 .", "for each round @xmath375 , @xmath376 if @xmath351 and @xmath377 otherwise . by maximality of @xmath378 , if @xmath379 _ is not _ the first round of phase @xmath129 , then we have @xmath380 .", "then a simple calculation shows that @xmath356 for at least @xmath381 part of the rounds of @xmath378 , i.e. @xmath382 . on the other hand , if there is a unique maximal interval @xmath378 starting at the first round of phase @xmath129 and such that @xmath383 , then it must hold that @xmath384 , where @xmath385 . combining these observations", ", we get that @xmath386 .    by combining this bound with  ( [ eq : xupperl ] )", ", we have : @xmath387 \\le pr[x\\ge \\delta\\mu|h| ] < 2^{-\\mu|h|}.\\ ] ] thus , with probability @xmath388 , in at most @xmath389 fraction of rounds in @xmath129 there can be @xmath345 .", "recall that for node @xmath1 we need to show that @xmath390 for most rounds in a phase .", "we prove this by taking a _ constant size _ @xmath73-cover of @xmath119 and applying lemma  [ le : phi ] to all nodes in the cover _", "[ pr : lowc ] for every @xmath132 and @xmath147 , there are constants @xmath391 and @xmath392 , such that for every node @xmath1 and phase @xmath129 , with probability @xmath135 , at least @xmath136-fraction of rounds in @xmath129 are bounded contention rounds , i.e. @xmath393", ".    it will be sufficient to choose @xmath128 and @xmath394 such that @xmath395 and @xmath396 . by the definition of @xmath287 and", "bounded independence , there is a @xmath73-cover @xmath39 of @xmath119 of size @xmath397 .", "set @xmath398 .", "let us fix a node @xmath399 .", "if @xmath129 is the first phase or @xmath400 then we set @xmath401 .", "otherwise , consider the phase @xmath159 of @xmath127 rounds preceding @xmath129 . by lemma  [ le : phi ] , if we set @xmath402 then , with probability @xmath403 , there is a round in @xmath159 where @xmath404 .", "let @xmath405 be such a round and set @xmath406 $ ] , where @xmath407 is the last round of @xmath129 .", "clearly , @xmath408 .", "now we can apply lemma  [ le : phi ] for node @xmath20 with @xmath101 as above and conclude that with probability at least @xmath403 , there are at most @xmath409 rounds in @xmath378 ( note that @xmath410 ) where @xmath345 , so there are at most @xmath411 rounds in @xmath129 where @xmath345 .", "now we can apply the same argument for all nodes in @xmath412 simultaneously and conclude that with probability at least @xmath135 , there are at most @xmath413 rounds in @xmath129 where @xmath345 for _ every _", "@xmath399 , if we take @xmath395 to be large enough . in the remaining @xmath414 rounds we have @xmath415 .", "we extract the following result from the proof of lemma  [ le : phi ] , to use it later .", "the proof is similar to the proof of prop .", "[ pr : lowc ] .", "[ co : pexpectation ] for every @xmath335 and each node @xmath1 , the expected number of rounds in each phase @xmath129 where @xmath416 , is @xmath411 .", "let @xmath378 be the shortest time interval containing @xmath129 such that @xmath417 at the beginning of @xmath378 .", "let @xmath418 denote the event that @xmath408 .", "as in the proof of prop .", "[ pr : lowc ] , we have @xmath419 > 1-n^{-3}$ ] .", "let @xmath370 denote the number of rounds @xmath375 with @xmath80 .", "we know from the proof of lemma  [ le : phi ] that @xmath420 = o(\\phi^{-k})\\cdot|\\hat h| = o(\\phi^{-k})\\cdot |h|$ ] and @xmath367=e[x|\\mathcal{e}]\\cdot pr[\\mathcal{e } ] + e[x|\\bar{\\mathcal{e}}]\\cdot pr[\\bar{\\mathcal{e}}]= o(\\phi^{-k})\\cdot |h|$ ] .", "this completes the proof as @xmath421 .", "next we show that if @xmath76 is appropriately chosen then for each node @xmath1 , @xmath422 happens most of the time during each phase w.h.p .", "in order to show this , first we split the set of nodes in @xmath423 into local neighborhoods .", "we show that if in a given round the contention in each local neighborhood is bounded by an appropriate threshold , then the expected interference is small . for each local neighborhood", ", the expected number of rounds when the contention is higher than its threshold is bounded using corollary  [ co : pexpectation ] . combining these results into one we get a bound on the expected number of rounds when @xmath424 is small .", "it then remains to apply a concentration bound to conclude the proof .", "[ pr : pinterferenceboundl ] for every @xmath132 and @xmath425 , there are constants @xmath426 and @xmath427 , such that for each node @xmath1 and phase @xmath129 , with probability @xmath135 , at least @xmath136-fraction of rounds in @xmath129 are low interference rounds , i.e. @xmath428 .", "let @xmath429 denote the in - ball of radius @xmath430 centered at node @xmath1 for @xmath431 .", "let @xmath432 be an @xmath73-cover of @xmath429 of size @xmath433 for a constant @xmath434 , which exists by bounded independence .", "let @xmath435 for @xmath436 , where @xmath437 is a large enough constant to be defined later .", "[ le : pexpectal ] if @xmath438 holds for all @xmath439 and @xmath440 and @xmath441 then @xmath442 .", "we have , by the definition of expected interference ( explanations below ) , @xmath443 where the first inequality follows from the definition of sets @xmath444 and the assumption of the claim , the second one follows by @xmath435 and @xmath445 , the third one is a rearrangement of the sum , the fourth one follows by the fact that @xmath446 for every @xmath447 and @xmath448 ( here , @xmath449 ) , and the last one follows because @xmath450 .", "thus , if @xmath441 then the claim follows .", "let us put the nodes in an arbitrary order and let @xmath451 denote the number of rounds @xmath452 s.t .", "@xmath453 , where @xmath454 is the @xmath455-th node in @xmath456 . by corollary  [ co : pexpectation ] , @xmath457= o(\\phi_i^{-k})\\cdot|h| $ ] holds for each @xmath458 .", "let @xmath459 denote the number of rounds in @xmath129 when @xmath460 . by claim  [ le : pexpectal ] , @xmath461 $ ] can be bounded as follows , @xmath462\\leq\\sum_{i}\\sum_{j}e[z_{i , j } ] \\leq \\sum_i|s_i\\setminus s_{i-1}|\\cdot o(\\phi_i^{-k})\\cdot |h| \\leq \\frac{\\sigma}{3}|h|,\\ ] ] where the last inequality holds if @xmath463 for a large enough constant @xmath464 ; we have @xmath465 and @xmath466 with @xmath467 ( by definition ) , so we have @xmath468 .", "each @xmath451 can be considered as the sum of binary random variables similar to that in the proof of lemma  [ le : phi ] .", "thus , @xmath459 can also be seen as the sum of binary random variables .", "moreover , the binary random variables satisfy the conditions of the chernoff bound in lemma  [ le : chernoff ] .", "thus , by taking @xmath469 and @xmath470 , we have : @xmath471\\leq 2^{-\\sigma|h|/3}\\le n^{-3}. $ ]", "[ le : broadgood ] let @xmath472 , @xmath473 and @xmath474 , where @xmath475 are the parameters of .", "let @xmath56 be a round such that for a node @xmath20 , @xmath476 and @xmath477 .", "+ if a node in @xmath332 transmits in round @xmath56 then it mass - delivers , with probability at least @xmath478 .", "let @xmath1 be the transmitting node , which could possibly be @xmath20 itself .", "the probability that no other node in @xmath479 transmits is @xmath480 where we used lemma  [ ieq ] with the assumption that @xmath481 and the bounded contention assumption .    on the other hand", ", we have that @xmath482\\ge 9/10 $ ] .", "indeed , @xmath483\\ge 9/10 $ ] follows by the bounded interference assumption and markov inequality , so it suffices to show that @xmath484 , which we have because for each node @xmath485 , @xmath486 , implying that @xmath487 thus , we have that with probability @xmath478 , @xmath1 transmits , no other node in @xmath217 transmits and the interference from @xmath488 at @xmath1 is at most @xmath68 , so @xmath1 delivers its message , by .", "* proposition [ pr : pdecrease ] * _ assume that constants @xmath137 are large enough . for each node @xmath1 and phase @xmath129 ,", "if at least @xmath138-fraction of the rounds of @xmath129 are of high contention , then @xmath139 nodes in @xmath140 mass - deliver , with probability @xmath135 .", "_    let @xmath489 .", "let us choose constants @xmath490 such that prop .", "[ pr : pgoodroundl ] holds for @xmath491 and @xmath492 and such that @xmath493 is large enough , as stated below , where @xmath475 are the parameters of .", "we will use the fact that in prop .", "[ pr : pgoodroundl ] , @xmath394 has the following dependency on @xmath494 : @xmath495 , where @xmath0 is the edge change parameter ( see the proof of prop .", "[ pr : lowc ] ) .", "we set @xmath496 .", "namely , if we choose @xmath137 large enough ( satisfying the relation above ) , then we have that with probability @xmath135 , there are at least @xmath497 rounds @xmath358 such that @xmath498 and @xmath499 .", "let @xmath56 be such a round .    for each node @xmath500", ", it holds that @xmath501 and @xmath502 .", "let us fix a node @xmath500 .", "first note that @xmath503 , implying that @xmath504 .", "it remains to bound the expected interference at @xmath20 by nodes in @xmath488 .", "this interference can be split into two parts : 1 .", "the interference by nodes in @xmath505 and 2 .", "the interference by nodes in @xmath506 . the first part can be bounded by @xmath507 using a computation as in the proof of lemma  [ le : broadgood ] .", "the second part can be bounded as follows : the contention in the area @xmath506 is at most @xmath394 and the distance of those nodes to node @xmath20 is at least @xmath76 , so the expected interference by nodes in @xmath506 is at most @xmath508 , and is less than @xmath509 if @xmath76 is large enough , as @xmath510 .", "here we used the assumption that @xmath511 .", "recall that there are at least @xmath512 high contention rounds for node @xmath1 , i.e. rounds where @xmath122 .", "let @xmath56 be such a round . by the bounded independence property", ", there is a @xmath513-cover @xmath39 of @xmath119 of constant size . by the pigeonhole principle", ", there is a node @xmath399 such that @xmath514 .    by summarizing the above said ,", "we get that there are at least @xmath497 rounds @xmath56 s.t . for all nodes", "@xmath500 , @xmath515 and @xmath516", ". on the other hand , there are at least @xmath512 rounds where there is a node in @xmath119 that has @xmath517 .", "thus , there are at least @xmath518 rounds @xmath56 with the following property : there is a node @xmath500 such that @xmath519 and @xmath516 .", "fix such round @xmath56 and node @xmath20 . by lemma", "[ le : broadgood ] , there is a node @xmath520 that transmits ( the probability of this is at least @xmath521 ) and delivers its message with probability @xmath522 .", "thus , the expected number of nodes in @xmath119 that deliver their message during @xmath129 is @xmath523 .", "the proof now follows by applying a chernoff bound with the assumption that the constant @xmath128 is large enough : @xmath524 .", "* proposition  [ pr : idle ] * _ assume that @xmath137 are large enough . for each node @xmath1 and phase @xmath129 ,", "if at least @xmath141-fraction of the rounds of @xmath129 are low contention rounds , then with probability @xmath135 , in at least @xmath142-fraction of the rounds of @xmath129 , @xmath1 will detect idle channel and have low contention and low interference .", "_    let @xmath525 .", "assume that @xmath137 large enough , such that prop .", "[ pr : pgoodroundl ] holds for @xmath526 and @xmath527 , where @xmath528 is the parameter of .", "let @xmath418 denote the event that there are at least @xmath529 good rounds for @xmath1 in phase @xmath129 . by prop .", "[ pr : pgoodroundl ] , we have @xmath419 = 1-o(n^{-3})\\ge 19/20 $ ] , if @xmath128 is large enough . given @xmath418 , there are at least @xmath530 rounds @xmath56 that are both good and low contention for @xmath1 .", "let @xmath159 denote the set of such rounds and let @xmath350 be a binary random variable with value 1 if and only if @xmath1 detects idle channel in round @xmath161 .", "let @xmath531 .", "for each round @xmath161 , since @xmath56 is a good round and @xmath532 , it holds with probability at least @xmath141 that the interference at node @xmath1 from nodes in @xmath119 is at most @xmath528 . given this and the fact that in low contention rounds we have @xmath533", ", @xmath1 will detect idle channel with probability at least @xmath534 in round @xmath56 .", "thus , in each round @xmath161 , irrespective of prior rounds , we have @xmath535\\ge 81/100 $ ] .", "this implies : @xmath420\\ge ( 9 ^ 2/10 ^ 2)|h'|\\ge      \\frac{9 ^ 2}{10 ^ 2}\\cdot \\frac{9 ^ 2}{10 ^ 2}|h|$ ] and @xmath536 \\ge ( 19/20)\\cdot\\frac{9 ^ 4}{10 ^ 4}|h| > 0.62|h|.\\ ] ] just as in the proof of lemma  [ le : phi ] , we can now apply chernoff bound to obtain that @xmath537 < n^{-3}$ ] if @xmath128 is large enough .", "this completes the proof by recalling that all rounds of @xmath370 are low contention and low interference rounds for node @xmath1 .", "* theorem [ thm : broadcast ] * _ assume the edge change rate @xmath100 is sufficiently small .", "there are constants @xmath201 , such that when running bcast(@xmath114 ) in the synchronous mode , each node @xmath1 receives the message in @xmath202 rounds w.h.p .", "_    let us fix constants @xmath155 so that props .", "[ pr : pdecrease ] and  [ pr : idle ] hold with interference threshold @xmath203 , where @xmath204 and @xmath205 are the parameters corresponding to precision @xmath183 .", "we set the passiveness parameter of try&adjust to @xmath206 and the stable distance parameter @xmath207 .", "consider a node @xmath20 that receives the message in round @xmath56 .", "let @xmath1 be such that @xmath208 in the time interval @xmath209 $ ] ( and both stay alive during @xmath96 ) .", "the theorem follows by an induction and union bound from the claim below .", "node @xmath1 gets the message of @xmath20 during @xmath96 , with probability @xmath210 .", "we prove the claim by contradiction , and assume that @xmath1 can not get the message during @xmath96 .", "let us split @xmath96 into phases ( for @xmath20 ) and , similar to the proof of thm .", "[ thm : localbroadcast ] , classify the phases @xmath129 into types : ( type a ) at least @xmath138-th of rounds in @xmath129 are high contention rounds for @xmath20  @xmath211 , and ( type b ) at least @xmath141-th of rounds in @xmath129 are low contention rounds for @xmath20  @xmath212 .", "we show that with probability @xmath135 , all phases in @xmath96 are of type b. consider a type a phase @xmath129 . we know from prop .  [ pr : pdecrease ] that , with probability @xmath135", ", there is a set @xmath39 of @xmath139 nodes in @xmath140 that deliver their messages and restart try&adjust during @xmath129 .", "we split @xmath39 into three subsets , @xmath213  those inserted by churn during @xmath129 , @xmath214  those inserted by edge change during @xmath129 and @xmath215  the rest .", "consider @xmath214 first .", "we know that in each local neighborhood , there are at most @xmath216 nodes arriving due to edge changes .", "note that @xmath217 can be covered with @xmath218 local neighborhoods ; hence , the total number of nodes arriving in @xmath217 due to edge changes is @xmath219 . since @xmath220 , setting @xmath221 a small enough constant gives @xmath222 .", "now consider @xmath215 . by the primitive , for each node @xmath223 , all nodes in @xmath224 also restart try&adjust , setting their transmission probability to @xmath225 . by the setting of @xmath226 ,", "the probability that a node restarting try&adjust in phase @xmath129 transmits again in @xmath129 is at most @xmath227 .", "thus , with probability at least @xmath135 , the nodes in @xmath214 constitute an @xmath228-packing of @xmath140 and are at most @xmath229 . as for @xmath213 ,", "the probability that a newly arriving node transmits during phase @xmath129 is at most @xmath227 , so @xmath230 w.h.p .", "these bounds lead to a contradiction  @xmath231 , if constant @xmath128 is large enough .", "thus , each phase @xmath129 of @xmath96 is of type b with probability @xmath135 . it remains to recall that @xmath96 contains constant number of phases .", "now assume that @xmath129 is of type b. in a similar way as in theorem  [ thm : localbroadcast ] , we can apply prop .", "[ pr : idle ] and show that if @xmath232 at the beginning of @xmath129 , then by the end of @xmath129 , the effect on @xmath71 is equivalent to applying at least @xmath233 of @xmath234 operations on @xmath235 . since @xmath236 at the beginning of @xmath96 is at least @xmath225 , it will take at most 11 phases to increase the probability to the value @xmath164 .", "then , a single phase will suffice for @xmath20 to deliver its message ( including to node @xmath1 ) , with probability at least @xmath210 .", "the basic observation is that if all the nodes start running bcast@xmath240 simultaneously , a _ constant density dominating set _ can be computed in @xmath200 rounds ( cf .", "@xcite ) , where an @xmath33-dominating set of density @xmath538 is a set @xmath39 of nodes such that for each node @xmath20 , @xmath539 .", "the dominator algorithm is as follows : all nodes run bcast@xmath240 simultaneously , and 1 .", "if a node @xmath1 stops by then it is a _", "dominator _ ,", "if a node @xmath20 stops by detecting of node @xmath1 then it is dominated by @xmath1 .", "having formed a constant density dominating set , it remains to disseminate the message using only dominators . in order for the set of dominators to derive the connectivity properties of the original graph", ", we need that @xmath30 forms a @xmath540-bounded independence _", "metric _ space .    in the static spontaneous setting", ", there is a uniform algorithm that performs broadcast in @xmath541 rounds , w.h.p .", "[ thm : static - spont - bcast ]    the algorithm for spontaneous broadcast consists of two stages : 1 . compute a constant density @xmath542-dominating set @xmath543 , 2 . transmit the message using only the nodes in @xmath543 .    [ [ dominating - set . ] ] dominating set .", "+ + + + + + + + + + + + + + +    the dominating set is constructed by running bcast@xmath240 in the spontaneous mode , i.e. all nodes start running the algorithm simultaneously .", "let @xmath543 be the set of nodes that stop the algorithm by .", "since the dominated nodes use @xmath544 as a stopping condition , @xmath543 is a @xmath542-dominating set .", "moreover , @xmath543 is a @xmath545-packing , which implies that each in - ball @xmath46 of radius @xmath546 contains at most @xmath547 dominators .", "in particular each node @xmath20 is dominated by at most constant number of dominators .", "the proof the algorithm terminates after at most @xmath200 rounds is almost identical to the proof of theorem  [ thm : broadcast ] .", "[ [ broadcast . ] ] broadcast .", "+ + + + + + + + + +    recall that we assume that the communication graph is connected and has diameter @xmath243 .", "also , since @xmath548 is a metric , the graph is undirected .", "the broadcast part is as follows : in the first round , the source node transmits the message to its neighbors ; each dominator @xmath20 , upon receiving the message , transmits it in each round with probability @xmath232 until detecting ( @xmath183 ) , where @xmath235 is a small enough constant .", "note that as soon as all dominators successfully transmit at least once , all nodes will get the message .", "moreover , if the constant @xmath235 is small enough , then the two algorithms can be run simultaneously : the key point is that the number of dominators in each @xmath549-neighborhood is bounded by a constant ( not depending on @xmath235 ) .", "this ensures that the nodes _ need not know @xmath16 _ , in order to coordinate the two algorithms .", "it remains to show that the message will get to all dominators in @xmath244 rounds using the broadcast algorithm .", "consider a graph @xmath129 defined over the dominating set @xmath543 , where for every pair of nodes @xmath550 , @xmath551 form an edge if @xmath552 .", "the diameter of @xmath129 is at most @xmath243 .", "note that for every pair of nodes @xmath553 with @xmath554 , the corresponding dominators @xmath555 and @xmath556 are adjacent in @xmath129 . indeed , by the definition of the dominating set ,", "we have @xmath557 let @xmath558 be two arbitrary nodes in @xmath543 and let @xmath19 be the path of length at most @xmath243 in @xmath238 connecting nodes @xmath555 and @xmath556 . by the observation above ,", "if we replace each node @xmath559 with its dominator @xmath560 , we obtain a path @xmath561 of length @xmath46 in @xmath129 , connecting @xmath555 and @xmath556 .", "the completes the proof of the claim .    for every transmitting node @xmath562 ,", "the probability that @xmath274 delivers its message to all its neighbors in @xmath129 is @xmath563 in each round @xmath56 , if constant @xmath235 is sufficiently small .", "let @xmath39 denote the set of neighbors of node @xmath274 in @xmath129 , i.e. the nodes of @xmath543 that are at distance at most @xmath564 from @xmath274 .", "it suffices to show that all nodes in @xmath39 receive the message in a fixed round @xmath56 where @xmath274 transmits with @xmath563 probability .", "this event holds if the interference at @xmath274 is no more than @xmath68  event @xmath565 , and no other node transmits in @xmath566 ", "event @xmath567 , in round @xmath56 .", "by the properties of the dominating set , the contention in a ball of radius @xmath568 is at most @xmath569 .", "similarly to the proof of prop .", "[ pr : pinterferenceboundl ] , it can be shown that the expected interference by nodes in @xmath570 is @xmath571 .", "thus , by markov inequality , @xmath572 if @xmath235 is small enough", ". on the other hand , since the contention in @xmath566 is @xmath571 , @xmath573 follows by lemma  [ ieq ] .", "thus , @xmath574 .", "given that each informed node in @xmath543 has probability at least @xmath575 of successfully broadcasting the message , the rest of the proof essentially follows along the lines of the proof of  ( * ? ? ?", "* lemma 6 ) .", "we present a sketch of the proof for completeness of the argument .", "let @xmath576 be the set of nodes in @xmath543 that have been informed by round @xmath56 , where @xmath577 contains only the source node .", "let us fix a node @xmath578 and let @xmath579 be the distance ( in graph @xmath129 ) from @xmath1 to the nearest node in @xmath576 .", "note that @xmath580 .", "the difference @xmath581 is the progress made in round @xmath56 and is a bernoulli random variable with @xmath582\\ge \\eta$ ] for all @xmath56 when @xmath583 and @xmath584 otherwise .", "let @xmath585 be a random variable that has the same distribution as @xmath586 when @xmath583 and is an i.i.d .", "bernoulli random variable with mean @xmath587 otherwise .", "let @xmath588 .", "note that node @xmath1 has been informed by round @xmath56 iff @xmath589 .", "thus , we need to bound the probability @xmath590 .", "let @xmath591 .", "it is easy to show that the sequence @xmath592 is a submartingale .", "moreover , for a round @xmath593 and constant @xmath594 , @xmath595 implies that @xmath596 .", "since the sequence @xmath592 is a submartingale , we can apply azuma - hoeffding bound to show that @xmath597 .", "now the theorem follows by union bound over all nodes , by choosing the constant @xmath260 large enough .", "* theorem  [ thm : brlowerbound ] * _ for every ( possibly randomized ) broadcast algorithm @xmath246 that uses neither node coordinates nor primitive , there is a @xmath247-bounded - independence metric space where @xmath246 needs @xmath248 rounds to do broadcast in a @xmath249-broadcastable network , even if the nodes have and primitives and operate spontaneously . _", "assume that @xmath598 .", "we present the construction using a distance function @xmath548 : it is then straightforward to construct the corresponding path - loss matrix .", "denote @xmath599 .", "first , assume the nodes operate non - spontaneously , i.e. a non - source node may need to receive a message in order to start participating in a protocol .", "recall that @xmath600 .", "consider @xmath16 points @xmath601 , such that for every @xmath602 , it holds that @xmath603 , @xmath604 , @xmath605 and @xmath606 , where @xmath607 ( see the diagram in fig .", "[ fig : nonspon ] ) .", "clearly , this set of points forms a @xmath608-bounded independence metric space .", "we place @xmath16 wireless nodes at distinct points @xmath601 uniformly at random .", "let @xmath195 be the node at @xmath609 .", "we assume , further , that communication only happens according to ; in particular , if the interference at a node is more than @xmath68 then _ none _ of its neighbors receives its transmission .", "note that broadcast in this network can be completed in 2 steps , starting at any point .", "note also that @xmath610 can not be directly reached from nodes @xmath195 with @xmath611 . moreover ,", "if at least @xmath612 nodes @xmath195 with @xmath611 transmit simultaneously , _ no node _ receives a message , including the potential communication between @xmath613 and @xmath610 .", "this follows by observing that @xmath614 : the signal power at the neighboring nodes must be more than the interference threshold , otherwise a node could receive two signals of the same power .", "assume @xmath195 with some @xmath611 is the source .", "at the first round of the algorithm , nodes @xmath615 receive the message .", "let @xmath616 denote the event that @xmath613 transmits and no more than @xmath617 nodes @xmath195 with @xmath611 transmit in round @xmath56 . by the observations above , @xmath610 will not receive the message until @xmath616 happens .", "moreover , during the subsequent steps of the algorithm , until @xmath616 happens for the first time , all nodes at @xmath615 will have the same history and will be symmetric with respect to and primitives .    it remains to bound the expected time until @xmath616 happens for the first time .", "we can assume w.l.o.g .", "that in each round there are no more than @xmath187 nodes transmitting . as discussed above ,", "if @xmath610 is not informed in such a round , then the nodes that transmitted in the given round can learn at best that none of them is at @xmath613 and stop transmitting thereafter .", "thus , we can assume that after each unsuccessful round there are at most @xmath618 nodes that stop transmitting and the probability @xmath619 of success at round @xmath620 ( if no success occurred before round @xmath33 ) is at most @xmath621 where the numerator counts the number subsets of an @xmath622-element set that contain a fixed element ( the node @xmath613 ) and no more than @xmath187 elements in total , and the denominator counts all subsets containing no more than @xmath187 elements .", "a further straightforward calculation shows that the expected number of rounds until the first success is @xmath248 ( see e.g.  ( * ? ? ?"], "abstract_text": ["<S> we give efficient algorithms for the fundamental problems of broadcast and local broadcast in dynamic wireless networks . </S>", "<S> we propose a general model of communication which captures and includes both fading models ( like sinr ) and graph - based models ( such as quasi unit disc graphs , bounded - independence graphs , and protocol model ) . </S>", "<S> the only requirement is that the nodes can be embedded in a bounded growth quasi - metric , which is the weakest condition known to ensure distributed operability . </S>", "<S> both the nodes and the links of the network are dynamic : nodes can come and go , while the signal strength on links can go up or down .    </S>", "<S> the results improve some of the known bounds even in the static setting , including an optimal algorithm for local broadcasting in the sinr model , which is additionally uniform ( independent of network size ) . </S>", "<S> an essential component is a procedure for balancing contention , which has potentially wide applicability . </S>", "<S> the results illustrate the importance of carrier sensing , a stock feature of wireless nodes today , which we encapsulate in primitives to better explore its uses and usefulness . </S>"], "labels": null, "section_names": ["introduction", "models and definitions", "controlling contention", "local broadcast", "broadcast", "other related work", "implementing communication and primitives", "proof of proposition [pr:pgoodroundl]: contention control", "proof of proposition[pr:pdecrease]: transmissions in good rounds", "proof of proposition [pr:idle]: low contention rounds", "proof of thm.[thm:broadcast]: non-spontaneous broadcast", "spontaneous broadcast", "proof of theorem[thm:brlowerbound]: necessity of"], "sections": [["wireless networks are ubiquitous and are on their way to become even more prevalent , e.g. , with the advent of internet - of - things . wireless communication is , however , particularly challenging to model algorithmically . in two crucial interrelated aspects ,", "wireless networks on the ground differ from models typically assumed in algorithmic studies .", "one is the _ communication modeling _ : when is a transmission successfully decoded , as a function of the environment and the _ interference _ from other transmissions .", "the other is _ variability with time _ : wireless networks are particularly susceptible to changes .", "both of these are hard to capture accurately with well - defined , clear - cut rules .", "we aim in this paper to address core information dissemination problems  _ local broadcast _ and ( global ) _ broadcast _  in dynamic distributed networks , under very weak assumptions on the communication . to this end", ", we propose a communication model with significant flexibility that allows for adversarial control , generalizing essentially all known analytic wireless models .", "it allows for more general interference relationships than treated before", ". the network can experience adversarial dynamic behavior , both edge changes ( change of signal strengths ) and node insertions / deletions .", "wireless communication has traditionally been modeled theoretically by graphs , either geometric or general .", "interference is then also transmitted on graph edges ( precluding a node from receiving a message from a neighbor if another neighbor is transmitting ) , but is sometimes represented by a supergraph . _", "fading channel _ or _ physical _ models common in communication engineering , on the other hand , consider interference as cumulative , decreasing with distance but adding up .", "they have been popular in recent algorithmic studies , adding more realism to the formulation .", "the standard assumption of _ geometric signal decay _ ( that signal decreases inverse polynomially with distance ) in the sinr model , the prototypical fading channel , is though equally at odds with experimental evidence . ultimately , it may prove futile to hope for a clean deterministic model , or even a purely stochastic one , without a significant dose of unpredictability and non - determinism .", "wireless communication is commonly closely linked to mobility , as the transceivers are more often than not on the move .", "dynamic changes to reception conditions have many causes other than node mobility , since almost any changes in the environment affect transmissions due to reflections of signals over multiple paths , antenna characteristics , scattering , and diffraction .", "these changes are by nature hard to predict , even when assuming a `` mobility model '' . the most robust approach would then be to assume a non - trivial adversarial component .", "while the study of algorithms in dynamic networks has a long history , little has been done in cases where interference plays a role .", "* setting and model*.   nodes are distributed and autonomous . there is no built - in structure and the nodes have no information besides bounds on model parameters and an upper bound on the number of nodes .", "communication is locally synchronous , but there is no global clock .", "we will assume a very general model for when nodes successfully communicate .", "nodes are located in space , with separation between points given by the relative decrease in strength of signal ( or interference ) sent between the points .", "this induces a metric space , when these decays are raised to the appropriate power  actually , it is a _ quasi - metric _ , since symmetry need not hold .", "for distributed computation to be possible , the quasi - metric must have _ bounded independence _", "( to be defined precisely ) .", "_ edge changes _ can occur , with some restrictions , which are changes in the signal strength between the pair of points .", "the rule for when communication is successful is only partially pre - specified : transmission succeeds on a `` clear channel '' .", "that is , if a sender is within a _ communication radius _ from the receiver , if no other node transmits within a ( larger ) radius , and if the combined interference from all other transmitting nodes is ( quite ) small , then the transmission succeeds .", "otherwise , success is up to the adversary , or it can be further specified by the particular model assumptions desired .", "this captures essentially all known algorithmic wireless models ( including quasi - unit disc graphs , unit - ball graphs , bounded - independence graphs , @xmath0-hop extensions , and sinr ) .", "the only exception is the radio network model with general graphs , which can not be extended to involve comprehensive interference without a major hit in time complexity .", "the generality of our model is a key feature .", "given the vagaries of actual wireless environments , it is preferable for robustness reasons to make minimal assumptions about the communication model .", "a conservative approach is then to seek algorithms that work in most established models rather than depending on model - specific factors that simplify the life of the algorithm designer .", "[ [ dynamic - network - model ] ] dynamic network model + + + + + + + + + + + + + + + + + + + + +    the network is dynamic , both in terms of nodes and links .", "nodes can come and go , potentially adversarially .", "this includes the situation where nodes are wakened by other nodes .", "we only expect nodes that have been awake for long enough ( logarithmic number of rounds ) to participate in the computation , i.e. , the _", "stable _ nodes .    in the _ local broadcast _", "problem , all stable nodes should inform all of their ( stable ) neighbors ( i.e. , within the communication radius ) . in the _ broadcast _", "problem , the nodes in the network are to be informed of a message that initially resides at a source node .", "the network need not be connected at any instant in time , but we need to make assumptions about what transient paths are usable by an algorithm operating under interference .", "this leads to a definition of a _ dynamic diameter _ of the network , that becomes the competitive objective for the time complexity .", "* our approach and results*.   the key algorithmic technique is a natural randomized _ contention balancing _ procedure , where a node continuously adjusts its transmission probability based on the interference that it senses .", "it allows nodes to stabilize quickly from any initial conditions , or after waking up .", "this routine is a variation on an old story , a simple backoff procedure to manage local contention :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ if a node @xmath1 senses contention in a given round beyond a fixed threshold , then @xmath1 halves its transmission probability in the next round and otherwise doubles it . _", "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    our main technical contribution is to show logarithmic - round convergence of this method to a steady state of nearly balanced contention , from an arbitrary starting configuration and in the presence of network changes .", "it proves also to be surprisingly tolerant of different communication models .", "the higher level algorithms are then built on top of this primitive .", "a crucial component is the use of _ carrier sense _ to detect the cumulative amount of signals in the air .", "since it is supplied by the cheapest available hardware today as rss ( received signal strength ) readings , we posit that carrier - sense capability should be the default assumption in wireless algorithmics ( while exploring the necessity of different assumptions is interesting theoretically ) . as carrier - sense indicators can provide fine - grained information , we are interested in restraining its use and identifying which aspects are necessary to achieve the results obtained . to this end , we identify several _ primitives _ that carrier - sense can supply , restrict the algorithm to use only a subset of the primitives , and examine which of these are truly necessary .", "the local broadcast algorithm simply runs the contention balancing procedure , with nodes bowing out when they are sure to have completed their transmission .", "the broadcast algorithms are based on sparsifying the instance , so that only nodes of constant density actually participate in the global broadcast action .", "the former is achieved in @xmath2 time , where @xmath3 is the maximum number of neighbors that a node can have , while the latter takes @xmath4 rounds , where @xmath5 is a dynamic diameter .", "these dissemination algorithms are efficient enough to improve on some of the results known for static versions of the problems .", "the local broadcast algorithm is strongly optimal , or within constant factors on every instance . in the standard", "setting ( static , spontaneous case ) , the algorithm is _ uniform _ , in that it need not know the network size .", "the broadcast algorithm is also optimal and uniform in the same setting , while in the non - spontaneous setting it is faster by a logarithmic factor than the previous algorithm of @xcite that however does not require carrier sense .", "* closely related work*.   there are two largely disjoint bodies of work of wireless algorithmic results , with work on fading models like sinr slowly catching up with the better studied graph - based models .", "one approach for capturing more realism in sinr model is to move beyond euclidean metrics @xcite , even to general ones @xcite .", "one can view relative signal decrease as implicitly defining a quasi - distance metric @xcite .", "link scheduling problems can be formulated on edge - weighted interference graphs @xcite that properly generalize both graph - based and sinr models , linked by a graph - theoretic parameter . distributed dissemination problems , however , necessarily require metric restrictions , such as doubling or _ fading _", "metrics @xcite , and limits on communication abilities in order to capture both types of models .", "the local broadcast and global broadcast problems have been extensively studied in both graph - based radio network models  @xcite and the sinr model  @xcite . for local broadcast ,", "the best results known in the radio network model are both @xmath6 with and without knowing ( an upper bound on ) @xmath3  @xcite . in the sinr model , with knowledge of @xmath3 , the local broadcast can be accomplished in the same time bound as in the radio network model @xcite .", "if @xmath3 is not known , the best result is @xmath7  @xcite , which is improved to @xmath2 with free acknowledgments @xcite .", "this can be further improved to @xmath8 in the spontaneous case , when @xmath3 is known  @xcite .", "the time complexity of non - spontaneous broadcasting in the radio network model is @xmath9  @xcite without collision detection . with collision detection , this lower bound", "was recently broken in  @xcite , where a solution of @xmath10 was given .", "broadcasting has also been treated in the sinr model under a variety of assumptions .", "some are stronger than ours ( location information @xcite , power control @xcite ) , while others relax the assumption about the connectivity property , incurring necessarily much higher complexity @xcite . results in our setting , but without carrier sensing , include time complexity of @xmath11 @xcite ( see also @xcite ) , where @xmath12 denotes the maximum ratio between distances of stations connected in the communication graph ; and @xmath13 @xcite . in the spontaneous setting , where the nodes can build an overlay structure along which", "the message is then propagated , scheideler et al .", "@xcite used carrier sense to give a dominator algorithms , which can be applied to solve broadcast in @xmath14 rounds .", "yu et al .", "@xcite solved the problem in the same time bound using power control , while the algorithm in  @xcite that requires neither power control nor carrier sense runs in time @xmath15 .", "these problems have also been treated in dynamic networks . in", "the _ unstructured _ model @xcite , where nodes may wake up asynchronously ( modeling the node insertion ) , the local broadcast problem is well studied , even in the sinr setting  @xcite , but this model does not consider node deletion . in the _ dual graph", "_ model  @xcite ( originally due to @xcite ) , both the local broadcast  @xcite and global broadcast  @xcite problems are studied . but", "this model involves only edge behavior and not node changes ( churn ) .", "hence , the impact of dynamicity on wireless information dissemination is still largely unexplored .", "considering wireless interference , there are two classes of wireless network models : graph - based and physical models .", "basically , the graph - based models define a local and binary type of interference , while the physical models consider fading effect of signal on wireless channels .", "there are many widely used graph - based models , such as the unit disc graph ( udg ) model  @xcite ( unit ball graph ( ubg ) model  @xcite ) , the quasi unit disc graph ( qudg ) model  @xcite , the protocol model  @xcite , the bounded - independence graph ( big ) model  @xcite and the @xmath0-hop variants of the above models  @xcite .", "physical models , also known as sinr models  @xcite , capture the missing practical issues in wireless interference , such as fading and cumulativeness of wireless interference . most of current works in the sinr model focus on networks embedded in euclidean space , while many of the results hold also for doubling or `` bounded growth '' metrics @xcite .", "those doubling metrics constrain growth at every ( or arbitrarily small ) granularity , while ours only bounds regions proportional to the transmission range , in particular capturing bounded independence graphs ( big ) .", "all above models are included by our proposed communication model as shown in sec .", "[ sec : primitiveproofs ] . for", "more on wireless models , please refer to  @xcite .", "more detailed related work is introduced in sec .", "[ drw ] .    * our contributions*.", "we have obtained generalized and improved algorithms for two of the most fundamental dissemination problems , in some cases improving the best results known in static settings . beyond these specific results , we identify the following technical contributions :    _ unified model of wireless networks_. the model proposed appears to be the first that allows for the development of pan - model distributed dissemination algorithms .", "this hopefully prompts further studies crossing the artificial boundary between graph- and fading - based models .", "_ dynamic networks under interference_. this appears to be the first work to address dynamic networks in the presence of comprehensive interference .", "_ uniform algorithms_. our algorithms in the static spontaneous setting appear to be the first in fading models that work independent of instance parameters ( number of nodes , max .  degree ) .    _", "primitives for carrier - sense .", "_ we introduce several primitives or _ capabilities _ that can be implemented using environmental sensing , and propose to study the power of such primitives .    _ stabilization mechanisms .", "_ we identify contention adaptation as a fundamental ability in wireless networks , that appears to be of crucial value to implement other distributed tasks .", "* roadmap*.   the formal model and basic definitions , including the definitions of communication model and carrier sensing primitives , are given in sec .", "[ sec : model ] .", "[ sec : balancingcontention ] contains the core technical part of the paper that includes the basic contention balance routine and its analysis .", "the main results concerning local and global broadcast problems are presented in sections  [ sec : localbroadcast ] and  [ sec : broadcast ] , respectively . due to space constraints", ", most proofs are relegated to appendices ."], ["we consider a dynamic network of point - size wireless devices ( nodes ) .", "nodes can transmit messages in time slots / rounds that are sufficiently long to allow a transmission of a single message .", "no global clock or synchronization of rounds is required , but the clocks of different nodes run at a similar rate , i.e. , the length of a round differs between nodes at most by a factor of 2 .", "nodes may arrive and leave the network at any time .", "unless specified otherwise , the nodes are assumed to work _ non - spontaneously _ : they can initially be in sleep state and join the execution of an algorithm only after receiving a message .", "we say a node is _ alive _ at some point in time if it is present in the network .", "we assume the total number of nodes in the network is polynomially bounded by a number @xmath16 in each round .", "we use @xmath17 to denote the set of alive nodes at any fixed point in time and also use @xmath16 to denote the current number of nodes , i.e. @xmath18 .", "we assume all nodes use the same transmission power @xmath19 for communication in all rounds .    _", "metrics_.   the _ signal strength _  or _ interference _ , depending on context  of transmitting node @xmath20 on a node @xmath1 is @xmath21 , where @xmath22 is the _ path loss _ from @xmath20 to @xmath1 .", "the _ metricity _ of a space @xmath23 is the smallest number @xmath24 such that for every triplet @xmath25 ,", "@xmath26 @xcite .", "we define @xmath27 if @xmath20 and @xmath1 are different nodes and @xmath28 when @xmath29 .", "note that @xmath30 is a _ quasi - metric _ , as all metric axioms except symmetry hold . in the rest of the paper", ", we assume that in each round the metricity of the network is bounded by a fixed constant @xmath24 and will work with values @xmath31 instead of @xmath32 .", "we assume the quasi - metric @xmath30 has _ bounded independence _ , defined below , roughly stating that there can not be many nodes each causing high interference to a _ fixed _", "node , while having low mutual interferences .", "first , some notations . the _ ball _ with radius @xmath33 centered at @xmath20", "is defined as @xmath34 .", "the _ in - ball _ with radius @xmath33 centered at @xmath20 is defined as @xmath35 ; clearly , @xmath36 .", "a set @xmath37 is a @xmath33-_packing _ for set @xmath38 if balls of radius @xmath33 centered at nodes in @xmath39 are contained in @xmath38 and are disjoint .", "@xmath39 is a @xmath40-_cover _ for @xmath38 if the union of balls of radius @xmath40 centered at nodes in @xmath39 contains @xmath38 .", "note that any maximal @xmath33-packing is a @xmath41-cover , and thus one can bound sizes of covers by packings .", "we say that @xmath30 has @xmath42-_bounded independence _ , for given @xmath43 and @xmath44 , if for every @xmath45 and every in - ball @xmath46 of radius @xmath47 , the size of a maximum cardinality @xmath48-packing of @xmath46 is at most @xmath49 , where @xmath50 is a constant , possibly depending on @xmath51 .", "for instance , the euclidean plane is @xmath52-bounded independent , for every @xmath33 . _", "neighborhoods , communication graph and dissemination problems_.   let @xmath53 denote the maximum transmission distance possible when no other node transmits . as the latter event is arguably very rare ,", "we define the _ communication radius _ @xmath54 as a slightly smaller distance , where @xmath55 is a _ precision _ parameter", ". we will drop the parameter @xmath55 whenever it is fixed and clear from the context .", "fix a round @xmath56 .", "the _ neighborhood _ of a node is @xmath57 , describing who @xmath20 can communicate with directly .", "the basic operation of interest is when @xmath20 broadcasts a message to its neighbors @xmath58 .", "the _ communication graph _ is a directed graph @xmath59 , where @xmath60 if and only if @xmath61 .", "thus , the sequence @xmath62 defines a _ dynamic graph_. the _ vicinity _ of @xmath20 refers to a larger region , @xmath63 , for a parameter @xmath64 .", "the data dissemination problems that we consider are defined below .", "we say that a node @xmath20 _ mass - delivers _ in round @xmath56 if it transmits and all its neighbors ( @xmath58 ) receive the message .    in the _ local broadcast _ problem , given a node @xmath20 , it is required to minimize the time from the beginning of the algorithm until node @xmath20 mass - delivers at least once , assuming it stays alive during that time .    in the _", "( global ) broadcast _ problem , given a distinguished _ source _ node that initially holds a message , the goal is to minimize the time needed to deliver the message to every node in the network through multihop transmissions .", "_ one hop communication_.   when is a transmission successfully received ?", "suppose a node @xmath20 transmits in round @xmath56 , and let @xmath39 be the set of concurrently transmitting nodes .", "let @xmath65 and @xmath66 be parameters that depend on the precision @xmath55 .", "( , success on a clear channel )", "if no other node in @xmath67 transmits _ and _ the total interference at node @xmath20 is at most @xmath68 ( i.e. @xmath69 and @xmath70 ) , then the transmission of node @xmath20 is successfully received by _ all _ its neighbors ( @xmath58 ) .", "otherwise , the reception is under adversarial control .    _", "randomized algorithms_.   we mainly consider randomized algorithms of the following form : in each round @xmath56 , node @xmath1 makes a transmission with probability @xmath71 , independent of other nodes transmissions in that round .", "an important notion for the analysis of such algorithms is _ local contention _ , the sum of the transmission probabilities in a close region .", "the contention in the _ close neighborhood _ of a node @xmath1 in round @xmath56 is @xmath72 , where the radius @xmath73 allows all pair of nodes in @xmath74 to potentially communicate .", "also , let @xmath75 denote the contention in the larger _ vicinity _ of @xmath1 in round @xmath56 ( @xmath76 will be fixed later ) .", "we will also use the notation @xmath77 to denote the interference at @xmath1 from nodes outside its vicinity ( in @xmath78 ) in round @xmath56 .", "the expected value of @xmath77 is @xmath79 .    _", "sensing primitives_.   we assume the nodes have abilities to sense activity on the channel .", "namely , we assume the nodes are able to detect high and low contention in their vicinity , detect ( under some conditions ) whether their transmission in a given round succeeded and detect a single very near transmission . in the following ,", "we formalize these notions in three primitives : , and .", "we show in sec .", "[ sec : primitiveproofs ] how _ all _ these primitives can be implemented with _", "basic _ physical carrier sensing and possibly also with other means .", "( ) . contention can be probabilistically deduced from measured level of radio activity .", "we want to relax this ability and will use the following variant , where the outcome of is one of the two values : busy or idle channel . formally ,", "for each node @xmath1 and round @xmath56 :    if contention among close neighbors is high ( @xmath80 ) then they _ all _ detect busy channel in round @xmath56 with probability at least @xmath81 , for given @xmath82 , where @xmath83 is a constant ,    if the contention in the vicinity of @xmath1 is low ( @xmath84 ) _ and _ the interference on @xmath1 from outside its vicinity is above a threshold ( @xmath85 ) then @xmath1 detects idle channel in round @xmath56 with probability at least @xmath86 , for given @xmath87 , where @xmath88 and @xmath89 are constants .", "if a node @xmath20 has the = ( @xmath55 ) primitive ( depending on the precision parameter @xmath55 ) then : if @xmath20 transmits in round @xmath56 , the interference at @xmath20 is bounded by @xmath90 and the transmission is received by all nodes in @xmath91 , then the outcome of is 1 , where @xmath90 is a parameter .", "if the transmission is not received by a node @xmath92 then the outcome is @xmath93 . otherwise , the outcome is @xmath93 or @xmath94 , adversarially .", "( ) . with =", "( @xmath55 ) primitive , a node is able to detect if a transmitter is very close , assuming that it receives the transmitted message .", "the outcome of is @xmath94 for node @xmath1 in round @xmath56 if @xmath1 _ receives _ a transmission from a node @xmath20 , @xmath95 .", "otherwise , the outcome of is @xmath93 .", "this can also be made approximate .    _", "dynamicity_.   we consider a dynamic network where the topology may change adversarially in each round due to node churn ( node arrivals / departures ) and edge changes .", "we assume that arriving nodes start running the algorithms from an initial configuration , so we do not limit the rate of churn . with edge changes , existing nodes that were not neighbors before , may become neighbors ( e.g. due to mobility ) .", "the new neighbors may cause too much interference in a too short time , so the edge changes should be limited .", "we assume the amount of edge changes is bounded for each node @xmath1 , as follows . consider a time interval @xmath96 of length @xmath97 .", "we require that the number of new neighbors of @xmath1 during @xmath96 ( not counting churn ) is bounded by @xmath98 , where @xmath99 denotes the number of rounds in @xmath96 and @xmath100 is a constant , to be fixed later .", "we further assume the fraction of rounds in @xmath96 when there are more than @xmath101 new neighbors of @xmath1 is bounded by @xmath102 for every @xmath103 , where @xmath104 .", "note that there is no restriction on distance changes inside the neighborhood of @xmath1 ( e.g. it is fine for node @xmath1 if its neighbors move , as far as they remain neighbors ) .", "note that the edge changes may affect the underlying metric , but we require that the upper bounds on metricity and independence are maintained .    _ requirements and assumptions_.   for the convenience of the reader , we gather together all of our assumptions and requirements in a single place .", "communication is assumed to succeed in a clear channel ( ) . we assume constant metricity @xmath105 and that @xmath30 has @xmath42-bounded independence with @xmath106 .", "for the local broadcast problem , we assume that @xmath107 , and for the broadcast problem that @xmath108 . as is standard in fading models , the communication radius @xmath109 is necessarily an @xmath110-fraction of the maximum transmission distance in a clear channel .", "besides the knowledge required by the primitives that are needed for a particular algorithm , the nodes are assumed to know the precision parameter @xmath55 .", "a polynomial estimate on the number of nodes , @xmath16 , is needed in dynamic and non - spontaneous algorithms , but not in the static spontaneous problems .", "knowledge of approximations of model parameters are needed to implement primitives , including @xmath111 , @xmath105 , @xmath53 , @xmath112 , and @xmath68 .", "knowledge of the maximum degree @xmath3 is not needed .", "synchronous operation is only assumed in the broadcast algorithm .", "aspects not defined or constrained are assumed to be under ( adaptive ) adversarial control , including when transmissions that fail are successful , or when nodes appear or disappear from the network .", "the extent of increases in edge strengths over a period is restricted , as detailed above , while decreases are not and neither are node changes .", "the dissemination problems are only expected to function with the set of nodes that are sufficiently stable , as detailed in the respective section ."], ["in order to keep the contention in the network balanced , we propose a basic procedure called try&adjust , which will be the main building block in our algorithms . the idea is to let each node adapt its transmission probability to the contention detected using the assumed primitive .", "the parameter @xmath113 describes the passiveness of the newly arriving nodes .    _", "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ try&adjust(@xmath114 ) : each node @xmath1 maintains transmission probability @xmath115 in each round @xmath56 , initialized as @xmath116 when @xmath1 enters the network . in round @xmath117 , @xmath1 does : + 1 . transmit with probability @xmath71 , and + 2 .", "set @xmath118 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the aim for controlling contention is , of course , to ensure that transmissions made have a fair chance of being successful , which means they sufficiently overpower the interference experienced at intended receiver from all other transmissions made in that round", ". we account for this interference in two ways : the _ contention _ captures the expected interference from the nodes neighbors , while the _ interference _ integrates also the interference from nodes further away .", "we will measure the contention in the _ vicinity _ of each node @xmath1 , i.e. in @xmath119 , where @xmath76 is a large enough constant .", "we specify a threshold @xmath120 ( recall @xmath121 from definition ) for measuring contention : if @xmath122 then round @xmath56 is a _ high contention round _ for node @xmath1 and is a _ low contention round _ , otherwise .", "we further specify a threshold @xmath123 for interference : if @xmath124 then round @xmath56 is a _ high interference round _ for @xmath1 and is _ low interference round _ , otherwise .", "these thresholds are chosen so as to ensure that in a low - contention / interference round , node @xmath1 will be likely to succeed if it transmits . however , requiring all or most rounds to be low contention for all nodes will lead to high delays", "instead , it turns out that most rounds will have _ bounded contention _ and low interference , which allows for good progress ; we say node @xmath1 experiences _ bounded contention _ in round @xmath56 if @xmath125 , where @xmath126 is a large enough constant , to be specified later .", "we say that a round @xmath56 is _ good _ for node @xmath1 if @xmath56 is both bounded contention and low interference round ( in which case , some node in @xmath1 s vicinity has a good chance of successfully transmitting ) .", "we analyze the properties of try&adjust using the notion of _ a phase _ , the shortest time in which at least @xmath127 rounds occur for all nodes , where @xmath128 is sufficiently large ( given in prop .", "[ pr : pgoodroundl ] ) .", "we use @xmath129 to denote a general phase and also the set of rounds in that phase .", "@xmath130 denotes the number of rounds in a phase @xmath129 ( i.e. @xmath131 ) .", "the fundamental property of try&adjust ( prop .", "[ pr : pgoodroundl ] ) is that , for each node and each phase , most of the rounds in the phase are good for that node .", "this property is then used to show that :    if most of the good rounds in a phase have low contention , then node @xmath1 detects idle channel in most of those rounds .    otherwise , during a constant fraction of the rounds , a node in the vicinity of @xmath1 mass - delivers .    by choosing the parameters carefully ,", "i.e.  requiring _ low enough _ contention , we can make sure that during a phase with mostly low contention , the node detects idle channel in _ most of the rounds _ ( more than half ) in a phase , thus leading to an increase of transmission probability by the end of the phase , which , after sufficiently many phases ensures message delivery , w.h.p . on the other hand , during a phase with mostly high contention , there will be many nodes in the vicinity of @xmath1 that successfully transmit , leading to lower contention .", "these ideas are applied in thms .", "[ thm : localbroadcast ] and  [ thm : broadcast ] . the core idea behind this analysis", "is based on  @xcite .", "[ pr : pgoodroundl ] let @xmath132 .", "if constants @xmath133 and @xmath134 are large enough then for each node @xmath1 and phase @xmath129 , with probability @xmath135 , a @xmath136-fraction of the rounds in @xmath129 are good", ".    the proof is rather technical and is deferred to sec .  [ sec : contentiondetails ] but the intuition is as follows .", "the contention in each neighborhood must be bounded most of the time , because when it becomes large , it has a high chance of being decreased due to busy channel .", "moreover , we show that in expectation , the contention in _ all _ local neighborhoods is bounded , which is then combined with a geometric argument to show that the expected interference at each node is low most of the time .", "we derive from the fundamental property two useful propositions .", "the first says that if contention is high , then nodes in the vicinity deliver the message .", "[ pr : pdecrease ] assume that constants @xmath137 are large enough . for each node @xmath1 and phase @xmath129 , if at least @xmath138-fraction of the rounds of @xmath129 are of high contention , then @xmath139 nodes in @xmath140 mass - deliver , with probability @xmath135 .    to this end", ", we first show that if a round is good for node @xmath20 and a node in its vicinity transmits , then it mass - delivers with constant probability , utilizing both metric assumptions and the properties of good rounds .", "we then argue that since most rounds are good ( by prop .", "[ pr : pgoodroundl ] ) and most rounds have by assumption sufficient contention , many rounds will be both good and with sufficient contention , and in each of those , a node in the vicinity of @xmath1 is likely to transmit and succeed .", "when contention is low in a lot of rounds of a phase , the node will detect idle channel by the primitive in many rounds .", "this will actually happen during many good rounds , which have the low local contention and low external interference to allow for this detection .", "[ pr : idle ] assume that @xmath137 are large enough .", "for each node @xmath1 and phase @xmath129 , if at least @xmath141-fraction of the rounds of @xmath129 are low contention rounds , then with probability @xmath135 , in at least @xmath142-fraction of the rounds of @xmath129 , @xmath1 will detect idle channel and have low contention and low interference ."], ["we propose an algorithm for asynchronous local broadcast in dynamic networks .", "the algorithm is an extension of the try&adjust procedure , where the nodes try to balance the contention in the network and stop transmitting as soon as they deliver their message .", "we assume the nodes are powered with and primitives .", "note that the passiveness parameter is set to @xmath143 , which means that the transmission probability of nodes does not get below @xmath144 .", "localbcast : each node @xmath1 executes try&adjust(1 ) with the following additional step : if @xmath1 transmits and detects , it stops ( i.e. @xmath145 for @xmath146 ) .    we will estimate the performance of the algorithm using the notion of _ dynamic degree _ , defined as follows . given a parameter @xmath147", ", we denote @xmath148 for node @xmath1 and rounds @xmath149 with @xmath150 , where @xmath151 denotes the in - ball @xmath119 in round @xmath33 .    below", "we prove that if there are not too many node insertions in the neighborhood of a node @xmath1 , then @xmath1 mass - delivers ( delivers to all its neighbors ) in time comparable to its dynamic degree with @xmath76 a constant .", "the main tools for proving the bound are props .", "[ pr : pdecrease ] and  [ pr : idle ] .", "first we argue that if there is a phase of mostly low contention , then node @xmath1 will deliver its message , w.h.p .", "then we show that if the insertions are not too intensive then the contention around @xmath1 will decrease and a phase with mostly low contention will happen .", "[ thm : localbroadcast ] there is a constant @xmath147 , such that a node @xmath1 performing localbcast asynchronously in a time interval @xmath152 $ ] with @xmath153 mass - delivers , w.h.p .", ", provided that @xmath154 .", "note that the assumption @xmath154 is needed only for making the claim w.h.p .", ": it can be relaxed to higher degree polynomials by only increasing constant factors .", "let us fix constants @xmath155 so that props .", "[ pr : pdecrease ] and  [ pr : idle ] with @xmath156 .", "we partition @xmath96 into phases ( for node @xmath1 ) and classify them into two types : ( type a ) phases @xmath129 where at least @xmath138-fraction of rounds are high contention rounds ( i.e. @xmath157 ) , and ( type b ) phases @xmath129 where at least @xmath141-fraction of rounds are low contention rounds ( i.e. @xmath158 ) .", "node @xmath1 mass - delivers in a type b phase , w.h.p .", "let @xmath159 be the low contention and interference rounds during phase @xmath129 where @xmath1 detects idle channel . by prop .", "[ pr : idle ] @xmath160 > 1-o(n^{-3})$ ] .", "assume for now that the latter happens .", "for each @xmath161 we have @xmath162 .", "let us call this operation doubling .", "the value of @xmath71 at the beginning of the phase is at least @xmath144 , so @xmath163 doubling operations are sufficient to raise it to @xmath164 .", "the probability can be further halved during the phase at most @xmath165 times .", "thus , we may assume we have at most @xmath166 halving and at least @xmath167 doubling operations applied to an initial value @xmath164 .", "if @xmath168 , then the total number of halving operations is less than @xmath169 .", "it follows that @xmath1 has @xmath170 in at least @xmath171 low contention / interference rounds . by lemma  [ le : broadgood ] , in each such round , @xmath1 mass - delivers with probability at least @xmath172 ; hence , if @xmath128 is large enough , @xmath1 mass - delivers in @xmath129 , w.h.p .", "it remains to argue that there will be a type b phase during time interval @xmath96 .", "consider a type a phase @xmath129 .", "[ pr : pdecrease ] implies that with probability @xmath135 , @xmath173 nodes in @xmath119 deliver their message and stop during phase @xmath129 .", "thus , with probability @xmath174 , there are at most @xmath175 type a phases with @xmath176 a constant , as there are at most @xmath177 nodes in @xmath140 during the time interval @xmath96 ( also recall that @xmath178 ) .", "we conclude that if @xmath96 consists of at least @xmath179 phases , it will contain a type b phase and @xmath1 will deliver its message w.h.p .", "* implications for static networks*.   in static networks , the parameter @xmath177 is at most @xmath180 if @xmath76 is constant , where @xmath181 is the maximum size of a neighborhood in the network .", "thus , we obtain the following optimal result ( up to constant factors ) for static networks , as @xmath3 and @xmath163 are lower bounds even when running in the spontaneous mode  @xcite .", "when running localbcast in a static asynchronous network , each node @xmath1 completes local broadcast in @xmath182 rounds , w.h.p .    * remark . * in the special case when the nodes can start executing the algorithm simultaneously , i.e. in the _ spontaneous mode _ , the nodes need not know an upper bound on the size of the network . indeed ,", "each node may start running try&adjust with initial probability set to an arbitrary value and with no lower limit .", "the first phase will be spent for stabilization and can be ignored , while the argument for the rest of the phases is nearly identical to the one in thm .", "[ thm : localbroadcast ] ."], ["for the broadcast problem , we assume nodes communicate in synchronized rounds of equal length .", "each round consists of two slots .", "the idea is to use the first slot of each round for disseminating the message with try&adjust and the second slot for notifying nodes which have no uninformed neighbors .", "the latter is accomplished by using higher precision primitives , namely ( @xmath183 ) and ( @xmath183 ) , when executing try&adjust .", "this helps to detect a transmission that is successfully received by all nodes in @xmath184 of a node @xmath1 . upon detecting such a transmission , node", "@xmath1 resends the message in the second slot , in order to inform nodes @xmath20 with @xmath185 that their neighborhood ( @xmath186 ) has been covered .", "a node @xmath20 can detect that @xmath185 using .", "the algorithm is presented below .", "we assume the passiveness parameter @xmath114 of try&adjust is large enough , to be defined later .", "note that the algorithm works for the non - spontaneous mode , as nodes act only after receiving the message .", "bcast(@xmath114 ) : initially , only a source node @xmath187 has the message .", "a node @xmath1 , upon receiving a message , starts executing try&adjust(@xmath114 ) in the first slot of rounds .", "in addition , in each round @xmath56 , + 1 . if @xmath1 detects in the first slot , it retransmits in the second slot and restarts try&adjust(@xmath114 ) , + 2", ". if @xmath1 receives a message in the first slot and detects in the second slot , it restarts try&adjust(@xmath114 ) .    in order to evaluate the progress of the algorithm ,", "we use a notion of a _ dynamic distance _ , as defined below .", "let @xmath188 be a parameter .", "a sequence @xmath189 is called a _ stable @xmath187-@xmath1 path _ if there is a sequence @xmath190 of time intervals with @xmath191 $ ] , such that @xmath192 , @xmath193 and nodes @xmath194 and @xmath195 are both alive and @xmath196 during @xmath197 .", "the _ time - length _ of a stable @xmath187-@xmath1 path is @xmath198 .", "the _ stable @xmath187-@xmath1 distance _", "@xmath199 is defined as the minimum time - length of a stable @xmath187-@xmath1 path .", "note that a stable path need not be connected at any fixed point in time .", "moreover , most of the nodes might be missing at any given point in time .", "the core idea behind the analysis of the following theorem is similar to the case of local broadcast : we show that as soon as a neighbor @xmath20 of a node @xmath1 has the message and @xmath20 and @xmath1 keep being neighbors for @xmath200 rounds , @xmath1 will receive a transmission of @xmath20 during those rounds .", "[ thm : broadcast ] assume the edge change rate @xmath100 is sufficiently small .", "there are constants @xmath201 , such that when running bcast(@xmath114 ) in the synchronous mode , each node @xmath1 receives the message in @xmath202 rounds w.h.p .", "let us fix constants @xmath155 so that props .", "[ pr : pdecrease ] and  [ pr : idle ] hold with interference threshold @xmath203 , where @xmath204 and @xmath205 are the parameters corresponding to precision @xmath183 .", "we set the passiveness parameter of try&adjust to @xmath206 and the stable distance parameter @xmath207 .", "consider a node @xmath20 that receives the message in round @xmath56 .", "let @xmath1 be such that @xmath208 in the time interval @xmath209 $ ] ( and both stay alive during @xmath96 ) .", "the theorem follows by an induction and union bound from the claim below .", "node @xmath1 gets the message of @xmath20 during @xmath96 , with probability @xmath210 .", "we prove the claim by contradiction , and assume that @xmath1 can not get the message during @xmath96 .", "let us split @xmath96 into phases ( for @xmath20 ) and , similar to the proof of thm .", "[ thm : localbroadcast ] , classify the phases @xmath129 into types : ( type a ) at least @xmath138-th of rounds in @xmath129 are high contention rounds for @xmath20  @xmath211 , and ( type b ) at least @xmath141-th of rounds in @xmath129 are low contention rounds for @xmath20  @xmath212 .", "we show that with probability @xmath135 , all phases in @xmath96 are of type b. consider a type a phase @xmath129 . we know from prop .", "[ pr : pdecrease ] that , with probability @xmath135 , there is a set @xmath39 of @xmath139 nodes in @xmath140 that deliver their messages and restart try&adjust during @xmath129 .", "we split @xmath39 into three subsets , @xmath213  those inserted by churn during @xmath129 , @xmath214  those inserted by edge change during @xmath129 and @xmath215  the rest .", "consider @xmath214 first .", "we know that in each local neighborhood , there are at most @xmath216 nodes arriving due to edge changes .", "note that @xmath217 can be covered with @xmath218 local neighborhoods ; hence , the total number of nodes arriving in @xmath217 due to edge changes is @xmath219 . since @xmath220 , setting @xmath221 a small enough constant gives @xmath222 .", "now consider @xmath215 . by the primitive , for each node @xmath223 , all nodes in @xmath224 also restart try&adjust , setting their transmission probability to @xmath225 . by the setting of @xmath226 ,", "the probability that a node restarting try&adjust in phase @xmath129 transmits again in @xmath129 is at most @xmath227 .", "thus , with probability at least @xmath135 , the nodes in @xmath214 constitute an @xmath228-packing of @xmath140 and are at most @xmath229 . as for @xmath213 ,", "the probability that a newly arriving node transmits during phase @xmath129 is at most @xmath227 , so @xmath230 w.h.p .", "these bounds lead to a contradiction  @xmath231 , if constant @xmath128 is large enough .", "thus , each phase @xmath129 of @xmath96 is of type b with probability @xmath135 .", "it remains to recall that @xmath96 contains constant number of phases .", "now assume that @xmath129 is of type b. in a similar way as in theorem  [ thm : localbroadcast ] , we can apply prop .", "[ pr : idle ] and show that if @xmath232 at the beginning of @xmath129 , then by the end of @xmath129 , the effect on @xmath71 is equivalent to applying at least @xmath233 of @xmath234 operations on @xmath235 .", "since @xmath236 at the beginning of @xmath96 is at least @xmath225 , it will take at most 11 phases to increase the probability to the value @xmath164 .", "then , a single phase will suffice for @xmath20 to deliver its message ( including to node @xmath1 ) , with probability at least @xmath210 .", "* implications for static networks*.   when the network is static , theorem  [ thm : broadcast ] can be reformulated in terms of hop - distance @xmath237 in the communication graph , which is defined as the length of the shortest directed @xmath187-@xmath1 path in @xmath238 : we have that @xmath239 for any node @xmath1 .", "note also that in this setting nodes that succeeded transmitting or detected need not continue the algorithm , so they stop transmitting . in this case , setting the passiveness parameter to @xmath143 suffices .", "we call this variant of the algorithm bcast@xmath240 .    when running bcast@xmath240 in synchronous non - spontaneous mode in a static network with source @xmath187 , each node @xmath1 receives the message in @xmath241 rounds w.h.p .", "when the communication graph is strongly connected , the broadcast from any source node is completed in @xmath242 rounds , where @xmath243 is the diameter of the communication graph .    in the spontaneous mode ,", "the bound above can be further improved to @xmath244 ; see appendix [ sec : spontaneous ] .", "this is based on finding a constant - density dominating set in @xmath200 time @xcite and simultaneously propagating along the dominators in @xmath244 time .", "we can therefore extend the approach based on @xcite to uniform algorithms in bounded - independence metrics .", "these results are close to best possible .", "we show below that in order to obtain bounds of that magnitude , it is necessary to have the primitive . to this end , we extend the lower bound construction of  ( * ? ? ?", "* thm .  7 ) for `` compact sinr '' to our setting .", "this construction leverages the property of our model that there can be arbitrarily many nodes that are mutually close to each other .", "namely , the bounded - independence metric is strictly more relaxed than the standard euclidean metrics .", "indeed , there is a @xmath245-round broadcast algorithm for the sinr model that does not need or other carrier sensing primitives @xcite .", "what the lower bound then illustrates is that to obtain such results , one must depend on opportune traits of the sinr model that we have tried to avoid and are not necessary for problems like local broadcast .", "thus we can observe concrete tradeoffs depending on model assumptions .", "[ thm : brlowerbound ] for every ( possibly randomized ) broadcast algorithm @xmath246 that uses neither node coordinates nor primitive , there is a @xmath247-bounded - independence metric space where @xmath246 needs @xmath248 rounds to do broadcast in a @xmath249-broadcastable network , even if the nodes have and primitives and operate spontaneously .", "10    n.  alon , a.  bar - noy , n.  linial , and d.  peleg . .", "in _ stoc _ , pages 274285 , 1989 .", "n.  alon , a.  bar - noy , n.  linial , and d.  peleg .", "a lower bound for radio broadcast . , 43(2):290298 , 1991 .", "r.  bar - yehuda , o.  goldreich , and a.  itai . on the time - complexity of broadcast in radio networks : an exponential gap between determinism and randomization . in _", "podc87 _ , 1987 .", "l.  barenboim and d.  peleg .", "nearly optimal local broadcasting in the sinr model with feedback . in _ sirocco _ , pages 164178 , 2015 .", "m.  h. bodlaender and m.  m. halldrsson . beyond geometry : towards fully realistic wireless models . in _ podc _ , pages 347356 , 2014 .", "m.  h. bodlaender , m.  m. halldrsson , and p.  mitra . .", "in _ podc _ , pages 355364 .", "acm , 2013 .", "i.  chlamtac and s.  kutten . on broadcasting in radio networks ", "problem analysis and protocol design .", ", 33(12):12401246 , 1985 .", "b.  s. chlebus , d.  r. kowalski , and s.  vaya . distributed communication in bare - bones wireless networks . , 2015 .", "b.  n. clark , c.  j. colbourn , and d.  s. johnson .", "unit disk graphs .", ", 86(1):165177 , 1990 .", "a.  e. clementi , a.  monti , and r.  silvestri .", "round robin is optimal for fault - tolerant broadcasting on wireless networks . , 64(1):8996 , 2004 .", "a.  czumaj and w.  rytter . broadcasting algorithms in radio networks with unknown topology . in _", "focs03 _ , 2003 .", "s.  daum , s.  gilbert , f.  kuhn , and c.  newport . .", "in _ disc _ , pages 358372 , 2013 .", "b.  derbel and e.  talbi . .", "in _ icdcn10 _ , pages 155166 , 2010 .", "s.  dolev , s.  gilbert , m.  khabbazian , and c.  newport . .", "in _ disc _ , pages 252267 , 2011 .", "a.  fanghnel , t.  kesselheim , and b.  vcking . .", ", 412(24):26572667 , 2011 .", "m.  ghaffari , s.  gilbert , c.  newport , and h.  tan . optimal broadcast in shared spectrum radio networks . in _ opodis12 _ , 2012 .", "m.  ghaffari , b.  haeupler , and m.  khabbazian . randomized broadcast in radio networks with collision detection . in _ podc13 _ , 2013 .", "m.  ghaffari , b.  haeupler , n.  lynch , and c.  newport .", "bounds on contention management in radio networks . in _ disc _ , 2012 .", "m.  ghaffari , e.  kantor , n.  lynch , and c.  newport .", "multi - message broadcast with abstract mac layers and unreliable links . in _", "podc14 _ , 2014 .", "m.  ghaffari , n.  lynch , and c.  newport . the cost of radio network broadcast for different models of unreliable links . in", "podc13 _ , 2013 .", "o.  goussevskaia , t.  moscibroda , and r.  wattenhofer . . in _ dialm - pomc 08", "_ , pages 3544 , 2008 .    p.  gupta and p.  kumar .", ", 46(2):388404 , 2000 .", "m.  m. halldrsson . .", ", 9(1):7:17:20 , dec .", "m.  m. halldrsson , s.  holzer , and n.  a. lynch . a local broadcast layer for the sinr network model . in _", "podc _ , pages 129138 , 2015 .", "m.  m. halldrsson , s.  holzer , p.  mitra , and r.  wattenhofer . .", "in _ soda13 _ , pages 15951606 , 2013 .", "m.  m. halldrsson and p.  mitra . .", "in _ icalp _ , pages 625636 , 2011 .", "m.  m. halldrsson and p.  mitra . .", "in _ podc _ , 2012 .", "m.  m. halldrsson and p.  mitra . .", "fomc _ , 2012 .", "m.  m. halldrsson , y.  wang , and d.  yu .", "leveraging multiple channels in ad hoc networks . in _ podc15 _ , 2015 .", "m.  hoefer , t.  kesselheim , and b.  vcking .", "approximation algorithms for secondary spectrum auctions . in _ proceedings of the twenty - third annual acm symposium on parallelism in algorithms and architectures _ , spaa 11 , pages 177186 , new york , ny , usa , 2011 .", "r.  impagliazzo and v.  kabanets", ". constructive proofs of concentration bounds . in _", "approx - random _ , pages 617631 , 2010 .", "t.  jurdzinski , d.  kowalski , m.  rozanski , and g.  stachowiak .", "distributed randomized broadcasting in wireless networks under the sinr model . in _ disc _ ,", "pages 373387 , 2013 .", "t.  jurdzinski , d.  kowalski , m.  rozanski , and g.  stachowiak . on the impact of geometry on ad hoc communication in wireless networks . in _ podc 14 _ , 2014 .", "t.  jurdzinski , d.  r. kowalski , and g.  stachowiak . .", "in _ fct13 _ , pages 195209 , 2013 .", "t.  jurdzinski , d.  r. kowalski , and g.  stachowiak .", "distributed deterministic broadcasting in wireless networks of weak devices . in _", "icalp13 _ , 2013 .", "t.  kesselheim and b.  vcking . .", "in _ disc _ , pages 163178 , 2010 .    d.  kowalski and a.  pelc .", "broadcasting in undirected ad hoc radio networks . in _", "podc03 _ , 2003 .", "f.  kuhn , n.  lynch , and c.  newport .", "brief announcement : hardness of broadcasting in wireless networks with unreliable communication . in _ disc _ , 2009 .", "f.  kuhn , n.  lynch , c.  newport , r.  oshman , and a.  richa . .", "in _ podc _ , pages 336345 .", "acm , 2010 .", "f.  kuhn , t.  moscibroda , and r.  wattenhofer . .", "in _ mobicom _ , pages 260274 .", "acm , 2004 .", "f.  kuhn and r.  oshman .", "dynamic networks : models and algorithms . , 42(1):8296 , 2011 .", "f.  kuhn , r.  wattenhofer , and a.  zollinger .", "ad hoc networks beyond unit disk graphs .", ", 14:715729 , 2008 .", "e.  kushilevitz and y.  mansour . .", ", 27(3):702712 , 1998 .    c.  lenzen and r.  wattenhofer .", "distributed algorithms for sensor networks . , 370(1958 ) , 2012 .", "t.  moscibroda and r.  wattenhofer . .", ", 21(4):271284 , 2008 .    c.  newport", "radio network lower bounds made easy . in _", "disc14 _ , 2014 .", "c.  scheideler , a.  richa , and p.  santi . . in _", "mobihoc 08 _ ,", "pages 91100 , 2008 .", "s.  schmid and r.  wattenhofer .", "algorithmic models for sensor networks . in _ wpdrts06 _ , 2006 .", "j.  schneider and r.  wattenhofer . .", "in _ podc _ , pages 210219 , 2009 .    j.  schneider and r.  wattenhofer . in _ disc _ , pages 133147 .", "springer - verlag , 2010 .", "d.  yu , q .- s .", "hua , y.  wang , and f.  lau . .", "dcoss 12 _ , pages 132139 , 2012 .", "d.  yu , q .- s .", "hua , y.  wang , h.  tan , and f.  lau . .", "in _ sirocco12 _ , pages 111122 , 2012 .", "d.  yu , q .- s .", "hua , y.  wang , j.  yu , and f.  lau . . in _ infocom13 _ , pages 24272435 , 2013 .    d.  yu , y.  wang , q .- s .", "hua , and f.  c.  m. lau . .", "in _ distributed computing in sensor systems and workshops ( dcoss ) , 2011 international conference on _ , pages 18 , 2011 .", "d.  yu , y.  wang , y.  yan , j.  yu , and f.  lau . speedup of information exchange using multiple channels in wireless ad hoc networks . in _", "infocom15 _ , 2015 ."], ["_ wireless models : _ considering wireless interference , there are two classes of wireless network models : graph - based and physical models .", "basically , the graph - based models define a local and binary type of interference , while the physical models consider fading effect of signal on wireless channels .", "the most classical graph - based model is the radio network model  @xcite . in this model", ", the network is modeled using a communication graph , where each pair of nodes that can communicate with each other is connected by an edge .", "it defines the interference just from direct neighbors , and a transmission can succeed if and only if there is only one neighbor of the receiver transmitting .", "there are many widely used variants of the classical radio network models , including : 1 ) the @xmath0-hop model where the interference comes from @xmath0-hop neighbors  @xcite ; 2 ) unit disc graph ( udg ) model  @xcite which defines the neighborhood using a unit disc ; 3 ) quasi unit disc graph ( qudg ) model  @xcite which just defines all pairs of nodes with distance at most @xmath76 for some given @xmath250 $ ] are adjacent , and leave the ` grey ' area in @xmath251 $ ] being determined by an adversary ; 4 ) protocol model  @xcite , where each node has a transmission range and an interference range , and a successful transmission occurs if a node falls into the transmission range of a transmitter and outside the interference ranges of all other transmitter ; 5 ) bounded - independence graph ( big ) model  @xcite , which defines abstractly and requires that the size of the maximal independent set in the @xmath33-hop neighborhood of each node is bounded by a polynomial function with @xmath33 . though the graph - based interference models miss certain crucial aspects of actual wireless networks , the simple definition of these models can help derive novel insights into distributed solutions to wireless problems", ".    physical models , also known as sinr models  @xcite , capture the fading and cumulative features of receptions in actual wireless environments .", "the default assumption is that interference fades with a polynomial of the distance , and transmission succeeds only if the received signal strength is sufficiently larger than the total interference plus noise .", "recently , the sinr model has attracted great attentions in the distributed community  @xcite .", "most of these works focus on networks embedded in euclidean space , while many of the results hold also for doubling or `` bounded growth '' metrics @xcite .", "those doubling metrics constrain growth at every ( or arbitrarily small ) granularity , while ours only bounds regions proportional to the transmission range , in particular capturing bounded independence graphs ( big ) . for more on wireless models", ", please refer to  @xcite .", "_ local broadcast _ : in the radio network model , probably the first local broadcast result was a randomized algorithm of alon et al .", "@xcite in a synchronous model , running in @xmath6 rounds .", "derbel and talbi  @xcite later generalized their algorithm to work without knowledge of @xmath3 and their proposed algorithm can accomplish local broadcast in @xmath7 rounds .", "the decay strategy also yields an @xmath6 time algorithm for local broadcast without knowledge of @xmath3  @xcite .", "goussevskaia et al .", "@xcite gave the first results for local broadcast in the sinr model , running in time @xmath6 and @xmath252 with and without knowledge of @xmath3 , respectively .", "the latter was improved in @xcite and further improved , independently , to @xmath7 time @xcite . with free acknowledgements ,", "this was improved to @xmath253 @xcite .", "when additionally @xmath3 is known , this was further improved recently to @xmath8 in the spontaneous setting @xcite .", "the speedup of multiple channels on local broadcast was considered in  @xcite    _ broadcast _ : the complexity of broadcasting is well understood in graph - based models . in the radio network model , bar - yehuda et al .", "@xcite presented the decay protocol which can accomplish non - spontaneous broadcast in @xmath254 rounds , where @xmath46 is the diameter .", "this result was improved to @xmath9 independently by czumaj and rytter  @xcite , and kowalski and pelc  @xcite .", "these algorithms can be viewed as clever optimizations of the decay protocol and match the lower bound  @xcite . with collision detection ,", "this lower bound was recently broken in  @xcite , where a solution of @xmath10 was given .", "broadcast in multi - channel radio networks was considered in  @xcite . for the udg model ,", "an @xmath255 time algorithm was given in  @xcite in the spontaneous setting .", "broadcasting has been treated in the sinr model under a variety of assumptions .", "some are stronger than ours ( location information @xcite , power control @xcite ) .", "some relax the assumption about the connectivity property , but incur necessarily super - linear complexity @xcite .", "results in our setting , but without carrier sensing , include time complexity of @xmath256 @xcite ( see also @xcite ) , where @xmath12 denotes the maximum ratio between distances of stations connected in the communication graph ; and @xmath13 @xcite . in the spontaneous setting , where the nodes can build an overlay structure along which the message is then propagated , scheideler et al .", "@xcite solved the problem in @xmath257 rounds using physical carrier sensing .", "yu et al .", "@xcite solved the problem in @xmath258 rounds using power control , allowing stations to decide the strength of a transmitted signal in each step . and", "the algorithm in  @xcite then runs in time @xmath254 .", "_ distributed models of temporal variability _ : dynamic networks have been studied extensively in recent years ( see @xcite for a survey ) , but generally not in the presence of interference . the _ dual graph _", "model  @xcite ( originally due to @xcite ) was designed to capture inherent _ unreliability _ in wireless networks , much of which can be due to dynamicity .", "the main focus of that work is on extending the radio network model in general graphs .", "importantly , the dual graph model does not distinguish between interference and communication edges ; only that the unreliable edges can transmit both interference and the usual communication , but their availability is under adversarial control .", "thus , there is no way to capture interference from further away nodes .", "most problems become extremely difficult against a powerful adversary , and to get good result , one must assume a much weaker one  @xcite .", "this model only involves edge behavior and not node changes ( churn ) .", "both the local broadcast  @xcite and global broadcast  @xcite problems are studied in the dual graph model .    a dynamic model that considers node insertion is the _ unstructured _ model  @xcite , which admits arbitrary wake - up mode and asynchronous communication .", "this model was first proposed in the unit - disc setting , and then extended to bounded independence graphs ( big )  @xcite and sinr  @xcite .", "it has been widely used in the solution of a variety of distributed wireless problems  @xcite , including local broadcast  @xcite , but there are no known global broadcast results .", "the model neither consider node deletion nor edge changes .", "hence , the impact of dynamicity on global communication is largely unexplored ."], ["[ [ modeling - communication ] ] modeling communication + + + + + + + + + + + + + + + + + + + + + +    our communication model captures most known algorithmic wireless models , as it is demonstrated below on the example of sinr , disk - graph based and protocol models .", "note that we assume below the distance @xmath259 to be symmetric , but the results hold also for `` almost symmetric '' functions , i.e. when there is a constant @xmath260 such that @xmath261 for all @xmath262 .", "_ sinr model .", "_ consider a network in a metric space . in the sinr model of communication , if @xmath39 is the set of simultaneously transmitting nodes in the network , a node @xmath1 receives the transmission of node @xmath20 if and only if @xmath263 where constants @xmath113 and @xmath264 denote the minimum sinr threshold and the ambient noise , respectively .", "note that @xmath265 in this setting .", "we can implement with parameters @xmath266 and @xmath267 , as shown in the proposition below .", "[ pr : scc ] if the interference at a node @xmath1 is less than @xmath68 in round @xmath56 , then @xmath1 will deliver its message if it transmits .", "note that if the interference at node @xmath1 is not more than @xmath68 then there is no node @xmath268 transmitting in round @xmath56 , as otherwise the interference at @xmath1 would be at least @xmath269 ; hence , only nodes in @xmath270 can transmit .", "consider an arbitrary node @xmath271 .", "for each node @xmath272 , @xmath273 .", "then , the interference at @xmath274 is at most : @xmath275 which implies that node @xmath274 receives @xmath1 s transmission : @xmath276 .", "_ the udg and ubg models . _", "these models are described by geometric graphs : a node @xmath20 receives a message from another node @xmath1 if and only if @xmath1 is the only transmitting neighbor of @xmath20 .    in the unit disk graph ( udg ) and unit ball graph ( ubg ) models", "the nodes are located in a metric space and two nodes are connected by an edge if and only if their distance is at most @xmath277 .", "the functionality of can be modeled as follows : the transmission of a node @xmath1 is received by all its neighbors if there is no other node at distance less than @xmath278 from @xmath1 transmitting simultaneously , i.e. we can set the parameters to @xmath279 and @xmath280 .    _ the quasi - udg model . _", "the quasi - udg model is an extension of the udg model : a ) if @xmath281 then @xmath20 and @xmath1 are connected by an edge , b ) if @xmath282 then they are disconnected , c ) otherwise , @xmath20 and @xmath1 may be connected or not . in this case", "may be implemented by setting @xmath279 and @xmath283 , with the adversary constrained to follow the specific static situation captured by the qudg .    _ the protocol model .", "_ in the protocol model , the nodes are in a metric space and there are two radii : @xmath277  the communication radius , and @xmath284  the interference radius .", "a node @xmath1 receives the transmission of a node @xmath20 if and only if : 1 ) @xmath1 is in the communication range of node @xmath20 , i.e. @xmath281 , and 2 ) there is no transmitting node @xmath274 such that @xmath1 is in the interference range of @xmath274 : for each transmitting node @xmath285 , @xmath286 .", "may be implemented here by setting @xmath279 and @xmath283 .", "_ the big model . _ in the bounded independence graph ( big ) model , for a parameter @xmath287 , we are given a graph on the nodes with the property that for every node @xmath1 and every @xmath288 , the maximum independent set in the @xmath0-neighborhood of @xmath1 is @xmath289 .", "the shortest - path distance metric on the graph is now naturally a @xmath290-bounded independence metric . to fit in our model ,", "the growth parameter @xmath287 must be less than @xmath105 .", "_ @xmath0-hop variants .", "_ these graph models can be naturally generalized to a model on interference , where nodes of distance at most @xmath0 cause interference , for some @xmath291 .", "we capture this by extending @xmath112 as needed .", "[ [ implementing - primitives - with - physical - carrier - sensing ] ] implementing primitives with physical carrier sensing + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    one way of implementing the primitives mentioned in this paper is to use physical carrier sensing , i.e. we assume the nodes have technology to detect if the interference ( plus noise ) is higher than a given threshold .", "we show below how to implement , and primitives using carrier sensing .", "* primitive .", "* the primitive can be implemented using a carrier sensing threshold @xmath292 and setting the parameter @xmath293 ; busy channel is detected if and only if the interference is at least @xmath96 .", "we will need the following technical fact .", "[ ieq ] for every @xmath294 $ ] , @xmath295 , it holds that @xmath296    if @xmath297 in round @xmath56 then all nodes in @xmath298 detect busy channel with probability at least @xmath299 . in particular", ", we can take @xmath300 if @xmath301 .    by the setting of @xmath96 and the definition of @xmath74 ,", "if two nodes in @xmath74 transmit in round @xmath56 , then all nodes in @xmath74 will detect busy channel .", "hence , the probability of all nodes in @xmath74 detecting busy channel is at least the probability of more than one node transmitting in round @xmath56 .", "the probability of no node transmitting is @xmath302 by lemma  [ ieq ] .", "the probability of exactly one node transmitting is : @xmath303 where we used the assumption that @xmath304 and lemma  [ ieq ] .", "thus , the probability of detecting busy channel is at least @xmath305 .    for every @xmath147 ,", "if @xmath306 and @xmath307 then @xmath1 detects idle channel with probability at least @xmath308 .    by the setting of the threshold @xmath96 , if there is no node transmitting in @xmath119 then node @xmath1 will detect idle channel .", "thus , the probability that @xmath1 detects idle channel is at least @xmath309 using lemma  [ ieq ] .", "* primitive . * in order to detect successful transmission , we can use interference threshold @xmath310 and set @xmath311 , where @xmath312 and @xmath313 are the parameters of .", "if a node @xmath1 senses that the interference is no higher than @xmath96 , it knows that : 1 .", "there is no node in @xmath314 transmitting , as otherwise the interference would be at least @xmath315 , 2 .", "the interference is at most @xmath68 ; thus , it knows that its transmission has been received by all neighbors in @xmath316 by .", "* primitive . *", "if a node @xmath1 receives a message from a node @xmath20 then @xmath1 can separate the signal from the interference and measure the received signal strength . as the nodes use uniform power assignment", ", node @xmath1 knows that @xmath95 if the received signal is stronger than @xmath317 .", "[ [ implementing - primitives - by - other - means ] ] implementing primitives by other means + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the primitives can frequently be implemented in other ways , often with the logarithmic blowup that explains the differences with the best carrier - sense - free results .    *", "* in an asynchronous system , it may be impossible to implement by other means than carrier sense . in a synchronized system , however , we can be achieved with logarithmic or polylogarithmic factor overhead .", "consider a given round .", "for each probability @xmath318 , repeat @xmath319 times : the senders in the original round transmit with probability @xmath320 .", "using concentration bound with @xmath50 sufficiently large , one can infer the contention within an small approximation , with high probability .", "such a strategy has been applied , e.g. , in @xcite .    *", "primitive . *", "a simple strategy is to work with only probabilistic guarantees of a transmission being received by all neighbors .", "then , simply repeat the protocol until this has been achieved @xmath319 times , which gives an guarantee , w.h.p .", "this approach underlies , e.g. , the local broadcast algorithms without carrier sense @xcite .", "* primitive . *", "this primitive , which is essential for dominator - based strategies for broadcast , can be implemented using power control : by lowering the power on all units appropriately , one can ensure that nodes further away ( by a small constant factor ) will not be able to hear the message due to the ambient noise term , see e.g.  @xcite .", "alternatively , one can assume that distances can be determined in other ways , such as by gps @xcite ."], ["recall that we need to prove the following .", "we assume that at the beginning of the _ first _ phase under consideration , the contention in the whole network is bounded by a constant .", "this holds for all algorithms in this paper , as the initial probability of nodes is always at most @xmath321 .    *", "* proposition [ pr : pgoodroundl ] . * * let @xmath132 . if constants @xmath133 and @xmath134 are large enough then for each node @xmath1 and phase @xmath129 , with probability @xmath135 , a @xmath136-fraction of the rounds in @xmath129 are good .", "recall that in a good round , there should be both bounded contention and low interference .", "the proof is split into two parts , each handling one of these properties .", "[ pr : pgoodroundl ] follows by simply combining those two parts .", "we will need the following concentration bounds .", "@xcite[le : chernoff ] consider a collection of binary random variables @xmath322 , and let @xmath323 .", "if there are probabilities @xmath324 with @xmath325\\leq \\prod_{i\\in s}p_i$ ] for every set @xmath326 , then it holds for @xmath327 and @xmath328 that @xmath329\\leq\\left(\\frac{e^{\\delta}}{(1+\\delta)^{1+\\delta}}\\right)^\\mu\\leq e^{-\\frac{\\delta^2\\mu}{2(1+\\delta/3)}}.\\ ] ] if , on the other side , there are probabilities @xmath324 with @xmath325\\geq \\prod_{i\\in s}p_i$ ] for every set @xmath326 , then it holds for @xmath327 and @xmath330 that @xmath331\\leq\\left(\\frac{e^{-\\delta}}{(1-\\delta)^{1-\\delta}}\\right)^\\mu\\leq e^{-\\delta^2\\mu/2}.\\ ] ]      first , we show that for each fixed node @xmath20 , the contention in the _ local _ neighborhood @xmath332 is bounded in most of the rounds of a phase . to this end , we show that in each round , the contention is either already low or will be halved with significant probability , then apply a concentration bound to show the claim .", "recall the constant @xmath333 from the definition of .", "[ le : phi ] let @xmath129 be a time interval and assume that @xmath334 at the beginning of @xmath129 .", "then for every @xmath335 , @xmath336 happens at most @xmath337 times during @xmath129 , with probability @xmath338 , where @xmath339 and @xmath340 is the edge change parameter .    by the assumptions on edge changes", ", we have that during phase @xmath129 , the fraction of rounds where more than @xmath341 nodes become a neighbor of @xmath20 because of edge changes is @xmath102 .", "let @xmath159 denote the remaining set of rounds .", "we have @xmath342 . in each of those rounds ,", "the contribution of edge changes in @xmath343 is clearly at most @xmath344 .", "next we bound the number of rounds in @xmath159 , where @xmath345 .", "consider such a round @xmath161 .", "let @xmath346 . by the definition of , all nodes in @xmath332 detect busy channel in round @xmath56 and halve their transmission probabilities with probability at least @xmath347 .", "thus , @xmath348\\ge 1-\\mu'$ ] , where the additive @xmath164 accounts for the sum of probabilities of the nodes that just join the ball @xmath332 due to node churn ( recall that each of them has an initial probability at most @xmath349 ) and @xmath344 is an upper bound on the contention due to edge changes ( by the definition of @xmath159 ) . for each round @xmath161 ,", "let us define a binary random variable @xmath350 as follows : @xmath351 if @xmath352 or @xmath353 and @xmath354 ( recall that @xmath355 ) and @xmath356 otherwise . by the discussion above , we have : @xmath357&=pr[p_{t}(u)\\leq\\phi]+pr[p_{t}(u)>\\phi\\wedge p_{t+1}(u)\\leq\\frac{5}{6}p_t(u)]\\\\ & = pr[p_{t}(u)\\leq\\phi]+pr[p_{t}(u)>\\phi]\\cdot pr[p_{t+1}(u)\\leq\\frac{5}{6}p_t(u)|p_{t}(u)>\\phi]\\\\ & \\geq pr[p_{t}(u)\\leq\\phi]+pr[p_{t}(u)>\\phi]\\cdot ( 1-\\mu')\\\\ & \\geq 1-\\mu ' . \\end{aligned}\\ ] ] this implies that for each round @xmath358 and each subset @xmath359 of earlier rounds ( @xmath360 for @xmath361 ) , @xmath362\\geq 1-\\mu'$ ] and @xmath363\\leq \\mu'$ ] .", "thus , for each subset @xmath364 we get @xmath365\\le \\mu'^{|s| } , $ ] and if we denote @xmath366 , then @xmath367\\le \\mu ' |h'|=\\mu |h|$ ] , where we denote @xmath368 .", "thus , we can apply chernoff bound ( lemma  [ le : chernoff ] ) with @xmath369 to bound @xmath370 with high probability : @xmath371\\le 2^{-\\mu|h|}.\\ ] ] using this bound , we obtain a bound on the number of rounds @xmath56 with @xmath372 .", "let @xmath373 denote this number .", "consider a maximal interval @xmath374\\subseteq h'$ ] such that @xmath345 for all @xmath375 .", "for each round @xmath375 , @xmath376 if @xmath351 and @xmath377 otherwise . by maximality of @xmath378 , if @xmath379 _ is not _ the first round of phase @xmath129 , then we have @xmath380 .", "then a simple calculation shows that @xmath356 for at least @xmath381 part of the rounds of @xmath378 , i.e. @xmath382 . on the other hand , if there is a unique maximal interval @xmath378 starting at the first round of phase @xmath129 and such that @xmath383 , then it must hold that @xmath384 , where @xmath385 . combining these observations", ", we get that @xmath386 .    by combining this bound with  ( [ eq : xupperl ] )", ", we have : @xmath387 \\le pr[x\\ge \\delta\\mu|h| ] < 2^{-\\mu|h|}.\\ ] ] thus , with probability @xmath388 , in at most @xmath389 fraction of rounds in @xmath129 there can be @xmath345 .", "recall that for node @xmath1 we need to show that @xmath390 for most rounds in a phase .", "we prove this by taking a _ constant size _ @xmath73-cover of @xmath119 and applying lemma  [ le : phi ] to all nodes in the cover _", "[ pr : lowc ] for every @xmath132 and @xmath147 , there are constants @xmath391 and @xmath392 , such that for every node @xmath1 and phase @xmath129 , with probability @xmath135 , at least @xmath136-fraction of rounds in @xmath129 are bounded contention rounds , i.e. @xmath393", ".    it will be sufficient to choose @xmath128 and @xmath394 such that @xmath395 and @xmath396 . by the definition of @xmath287 and", "bounded independence , there is a @xmath73-cover @xmath39 of @xmath119 of size @xmath397 .", "set @xmath398 .", "let us fix a node @xmath399 .", "if @xmath129 is the first phase or @xmath400 then we set @xmath401 .", "otherwise , consider the phase @xmath159 of @xmath127 rounds preceding @xmath129 . by lemma  [ le : phi ] , if we set @xmath402 then , with probability @xmath403 , there is a round in @xmath159 where @xmath404 .", "let @xmath405 be such a round and set @xmath406 $ ] , where @xmath407 is the last round of @xmath129 .", "clearly , @xmath408 .", "now we can apply lemma  [ le : phi ] for node @xmath20 with @xmath101 as above and conclude that with probability at least @xmath403 , there are at most @xmath409 rounds in @xmath378 ( note that @xmath410 ) where @xmath345 , so there are at most @xmath411 rounds in @xmath129 where @xmath345 .", "now we can apply the same argument for all nodes in @xmath412 simultaneously and conclude that with probability at least @xmath135 , there are at most @xmath413 rounds in @xmath129 where @xmath345 for _ every _", "@xmath399 , if we take @xmath395 to be large enough . in the remaining @xmath414 rounds we have @xmath415 .", "we extract the following result from the proof of lemma  [ le : phi ] , to use it later .", "the proof is similar to the proof of prop .", "[ pr : lowc ] .", "[ co : pexpectation ] for every @xmath335 and each node @xmath1 , the expected number of rounds in each phase @xmath129 where @xmath416 , is @xmath411 .", "let @xmath378 be the shortest time interval containing @xmath129 such that @xmath417 at the beginning of @xmath378 .", "let @xmath418 denote the event that @xmath408 .", "as in the proof of prop .", "[ pr : lowc ] , we have @xmath419 > 1-n^{-3}$ ] .", "let @xmath370 denote the number of rounds @xmath375 with @xmath80 .", "we know from the proof of lemma  [ le : phi ] that @xmath420 = o(\\phi^{-k})\\cdot|\\hat h| = o(\\phi^{-k})\\cdot |h|$ ] and @xmath367=e[x|\\mathcal{e}]\\cdot pr[\\mathcal{e } ] + e[x|\\bar{\\mathcal{e}}]\\cdot pr[\\bar{\\mathcal{e}}]= o(\\phi^{-k})\\cdot |h|$ ] .", "this completes the proof as @xmath421 .", "next we show that if @xmath76 is appropriately chosen then for each node @xmath1 , @xmath422 happens most of the time during each phase w.h.p .", "in order to show this , first we split the set of nodes in @xmath423 into local neighborhoods .", "we show that if in a given round the contention in each local neighborhood is bounded by an appropriate threshold , then the expected interference is small . for each local neighborhood", ", the expected number of rounds when the contention is higher than its threshold is bounded using corollary  [ co : pexpectation ] . combining these results into one we get a bound on the expected number of rounds when @xmath424 is small .", "it then remains to apply a concentration bound to conclude the proof .", "[ pr : pinterferenceboundl ] for every @xmath132 and @xmath425 , there are constants @xmath426 and @xmath427 , such that for each node @xmath1 and phase @xmath129 , with probability @xmath135 , at least @xmath136-fraction of rounds in @xmath129 are low interference rounds , i.e. @xmath428 .", "let @xmath429 denote the in - ball of radius @xmath430 centered at node @xmath1 for @xmath431 .", "let @xmath432 be an @xmath73-cover of @xmath429 of size @xmath433 for a constant @xmath434 , which exists by bounded independence .", "let @xmath435 for @xmath436 , where @xmath437 is a large enough constant to be defined later .", "[ le : pexpectal ] if @xmath438 holds for all @xmath439 and @xmath440 and @xmath441 then @xmath442 .", "we have , by the definition of expected interference ( explanations below ) , @xmath443 where the first inequality follows from the definition of sets @xmath444 and the assumption of the claim , the second one follows by @xmath435 and @xmath445 , the third one is a rearrangement of the sum , the fourth one follows by the fact that @xmath446 for every @xmath447 and @xmath448 ( here , @xmath449 ) , and the last one follows because @xmath450 .", "thus , if @xmath441 then the claim follows .", "let us put the nodes in an arbitrary order and let @xmath451 denote the number of rounds @xmath452 s.t .", "@xmath453 , where @xmath454 is the @xmath455-th node in @xmath456 . by corollary  [ co : pexpectation ] , @xmath457= o(\\phi_i^{-k})\\cdot|h| $ ] holds for each @xmath458 .", "let @xmath459 denote the number of rounds in @xmath129 when @xmath460 . by claim  [ le : pexpectal ] , @xmath461 $ ] can be bounded as follows , @xmath462\\leq\\sum_{i}\\sum_{j}e[z_{i , j } ] \\leq \\sum_i|s_i\\setminus s_{i-1}|\\cdot o(\\phi_i^{-k})\\cdot |h| \\leq \\frac{\\sigma}{3}|h|,\\ ] ] where the last inequality holds if @xmath463 for a large enough constant @xmath464 ; we have @xmath465 and @xmath466 with @xmath467 ( by definition ) , so we have @xmath468 .", "each @xmath451 can be considered as the sum of binary random variables similar to that in the proof of lemma  [ le : phi ] .", "thus , @xmath459 can also be seen as the sum of binary random variables .", "moreover , the binary random variables satisfy the conditions of the chernoff bound in lemma  [ le : chernoff ] .", "thus , by taking @xmath469 and @xmath470 , we have : @xmath471\\leq 2^{-\\sigma|h|/3}\\le n^{-3}. $ ]"], ["[ le : broadgood ] let @xmath472 , @xmath473 and @xmath474 , where @xmath475 are the parameters of .", "let @xmath56 be a round such that for a node @xmath20 , @xmath476 and @xmath477 .", "+ if a node in @xmath332 transmits in round @xmath56 then it mass - delivers , with probability at least @xmath478 .", "let @xmath1 be the transmitting node , which could possibly be @xmath20 itself .", "the probability that no other node in @xmath479 transmits is @xmath480 where we used lemma  [ ieq ] with the assumption that @xmath481 and the bounded contention assumption .    on the other hand", ", we have that @xmath482\\ge 9/10 $ ] .", "indeed , @xmath483\\ge 9/10 $ ] follows by the bounded interference assumption and markov inequality , so it suffices to show that @xmath484 , which we have because for each node @xmath485 , @xmath486 , implying that @xmath487 thus , we have that with probability @xmath478 , @xmath1 transmits , no other node in @xmath217 transmits and the interference from @xmath488 at @xmath1 is at most @xmath68 , so @xmath1 delivers its message , by .", "* proposition [ pr : pdecrease ] * _ assume that constants @xmath137 are large enough . for each node @xmath1 and phase @xmath129 ,", "if at least @xmath138-fraction of the rounds of @xmath129 are of high contention , then @xmath139 nodes in @xmath140 mass - deliver , with probability @xmath135 .", "_    let @xmath489 .", "let us choose constants @xmath490 such that prop .", "[ pr : pgoodroundl ] holds for @xmath491 and @xmath492 and such that @xmath493 is large enough , as stated below , where @xmath475 are the parameters of .", "we will use the fact that in prop .", "[ pr : pgoodroundl ] , @xmath394 has the following dependency on @xmath494 : @xmath495 , where @xmath0 is the edge change parameter ( see the proof of prop .", "[ pr : lowc ] ) .", "we set @xmath496 .", "namely , if we choose @xmath137 large enough ( satisfying the relation above ) , then we have that with probability @xmath135 , there are at least @xmath497 rounds @xmath358 such that @xmath498 and @xmath499 .", "let @xmath56 be such a round .    for each node @xmath500", ", it holds that @xmath501 and @xmath502 .", "let us fix a node @xmath500 .", "first note that @xmath503 , implying that @xmath504 .", "it remains to bound the expected interference at @xmath20 by nodes in @xmath488 .", "this interference can be split into two parts : 1 .", "the interference by nodes in @xmath505 and 2 .", "the interference by nodes in @xmath506 . the first part can be bounded by @xmath507 using a computation as in the proof of lemma  [ le : broadgood ] .", "the second part can be bounded as follows : the contention in the area @xmath506 is at most @xmath394 and the distance of those nodes to node @xmath20 is at least @xmath76 , so the expected interference by nodes in @xmath506 is at most @xmath508 , and is less than @xmath509 if @xmath76 is large enough , as @xmath510 .", "here we used the assumption that @xmath511 .", "recall that there are at least @xmath512 high contention rounds for node @xmath1 , i.e. rounds where @xmath122 .", "let @xmath56 be such a round . by the bounded independence property", ", there is a @xmath513-cover @xmath39 of @xmath119 of constant size . by the pigeonhole principle", ", there is a node @xmath399 such that @xmath514 .    by summarizing the above said ,", "we get that there are at least @xmath497 rounds @xmath56 s.t . for all nodes", "@xmath500 , @xmath515 and @xmath516", ". on the other hand , there are at least @xmath512 rounds where there is a node in @xmath119 that has @xmath517 .", "thus , there are at least @xmath518 rounds @xmath56 with the following property : there is a node @xmath500 such that @xmath519 and @xmath516 .", "fix such round @xmath56 and node @xmath20 . by lemma", "[ le : broadgood ] , there is a node @xmath520 that transmits ( the probability of this is at least @xmath521 ) and delivers its message with probability @xmath522 .", "thus , the expected number of nodes in @xmath119 that deliver their message during @xmath129 is @xmath523 .", "the proof now follows by applying a chernoff bound with the assumption that the constant @xmath128 is large enough : @xmath524 ."], ["* proposition  [ pr : idle ] * _ assume that @xmath137 are large enough . for each node @xmath1 and phase @xmath129 ,", "if at least @xmath141-fraction of the rounds of @xmath129 are low contention rounds , then with probability @xmath135 , in at least @xmath142-fraction of the rounds of @xmath129 , @xmath1 will detect idle channel and have low contention and low interference .", "_    let @xmath525 .", "assume that @xmath137 large enough , such that prop .", "[ pr : pgoodroundl ] holds for @xmath526 and @xmath527 , where @xmath528 is the parameter of .", "let @xmath418 denote the event that there are at least @xmath529 good rounds for @xmath1 in phase @xmath129 . by prop .", "[ pr : pgoodroundl ] , we have @xmath419 = 1-o(n^{-3})\\ge 19/20 $ ] , if @xmath128 is large enough . given @xmath418 , there are at least @xmath530 rounds @xmath56 that are both good and low contention for @xmath1 .", "let @xmath159 denote the set of such rounds and let @xmath350 be a binary random variable with value 1 if and only if @xmath1 detects idle channel in round @xmath161 .", "let @xmath531 .", "for each round @xmath161 , since @xmath56 is a good round and @xmath532 , it holds with probability at least @xmath141 that the interference at node @xmath1 from nodes in @xmath119 is at most @xmath528 . given this and the fact that in low contention rounds we have @xmath533", ", @xmath1 will detect idle channel with probability at least @xmath534 in round @xmath56 .", "thus , in each round @xmath161 , irrespective of prior rounds , we have @xmath535\\ge 81/100 $ ] .", "this implies : @xmath420\\ge ( 9 ^ 2/10 ^ 2)|h'|\\ge      \\frac{9 ^ 2}{10 ^ 2}\\cdot \\frac{9 ^ 2}{10 ^ 2}|h|$ ] and @xmath536 \\ge ( 19/20)\\cdot\\frac{9 ^ 4}{10 ^ 4}|h| > 0.62|h|.\\ ] ] just as in the proof of lemma  [ le : phi ] , we can now apply chernoff bound to obtain that @xmath537 < n^{-3}$ ] if @xmath128 is large enough .", "this completes the proof by recalling that all rounds of @xmath370 are low contention and low interference rounds for node @xmath1 ."], ["* theorem [ thm : broadcast ] * _ assume the edge change rate @xmath100 is sufficiently small .", "there are constants @xmath201 , such that when running bcast(@xmath114 ) in the synchronous mode , each node @xmath1 receives the message in @xmath202 rounds w.h.p .", "_    let us fix constants @xmath155 so that props .", "[ pr : pdecrease ] and  [ pr : idle ] hold with interference threshold @xmath203 , where @xmath204 and @xmath205 are the parameters corresponding to precision @xmath183 .", "we set the passiveness parameter of try&adjust to @xmath206 and the stable distance parameter @xmath207 .", "consider a node @xmath20 that receives the message in round @xmath56 .", "let @xmath1 be such that @xmath208 in the time interval @xmath209 $ ] ( and both stay alive during @xmath96 ) .", "the theorem follows by an induction and union bound from the claim below .", "node @xmath1 gets the message of @xmath20 during @xmath96 , with probability @xmath210 .", "we prove the claim by contradiction , and assume that @xmath1 can not get the message during @xmath96 .", "let us split @xmath96 into phases ( for @xmath20 ) and , similar to the proof of thm .", "[ thm : localbroadcast ] , classify the phases @xmath129 into types : ( type a ) at least @xmath138-th of rounds in @xmath129 are high contention rounds for @xmath20  @xmath211 , and ( type b ) at least @xmath141-th of rounds in @xmath129 are low contention rounds for @xmath20  @xmath212 .", "we show that with probability @xmath135 , all phases in @xmath96 are of type b. consider a type a phase @xmath129 . we know from prop .  [ pr : pdecrease ] that , with probability @xmath135", ", there is a set @xmath39 of @xmath139 nodes in @xmath140 that deliver their messages and restart try&adjust during @xmath129 .", "we split @xmath39 into three subsets , @xmath213  those inserted by churn during @xmath129 , @xmath214  those inserted by edge change during @xmath129 and @xmath215  the rest .", "consider @xmath214 first .", "we know that in each local neighborhood , there are at most @xmath216 nodes arriving due to edge changes .", "note that @xmath217 can be covered with @xmath218 local neighborhoods ; hence , the total number of nodes arriving in @xmath217 due to edge changes is @xmath219 . since @xmath220 , setting @xmath221 a small enough constant gives @xmath222 .", "now consider @xmath215 . by the primitive , for each node @xmath223 , all nodes in @xmath224 also restart try&adjust , setting their transmission probability to @xmath225 . by the setting of @xmath226 ,", "the probability that a node restarting try&adjust in phase @xmath129 transmits again in @xmath129 is at most @xmath227 .", "thus , with probability at least @xmath135 , the nodes in @xmath214 constitute an @xmath228-packing of @xmath140 and are at most @xmath229 . as for @xmath213 ,", "the probability that a newly arriving node transmits during phase @xmath129 is at most @xmath227 , so @xmath230 w.h.p .", "these bounds lead to a contradiction  @xmath231 , if constant @xmath128 is large enough .", "thus , each phase @xmath129 of @xmath96 is of type b with probability @xmath135 . it remains to recall that @xmath96 contains constant number of phases .", "now assume that @xmath129 is of type b. in a similar way as in theorem  [ thm : localbroadcast ] , we can apply prop .", "[ pr : idle ] and show that if @xmath232 at the beginning of @xmath129 , then by the end of @xmath129 , the effect on @xmath71 is equivalent to applying at least @xmath233 of @xmath234 operations on @xmath235 . since @xmath236 at the beginning of @xmath96 is at least @xmath225 , it will take at most 11 phases to increase the probability to the value @xmath164 .", "then , a single phase will suffice for @xmath20 to deliver its message ( including to node @xmath1 ) , with probability at least @xmath210 ."], ["the basic observation is that if all the nodes start running bcast@xmath240 simultaneously , a _ constant density dominating set _ can be computed in @xmath200 rounds ( cf .", "@xcite ) , where an @xmath33-dominating set of density @xmath538 is a set @xmath39 of nodes such that for each node @xmath20 , @xmath539 .", "the dominator algorithm is as follows : all nodes run bcast@xmath240 simultaneously , and 1 .", "if a node @xmath1 stops by then it is a _", "dominator _ ,", "if a node @xmath20 stops by detecting of node @xmath1 then it is dominated by @xmath1 .", "having formed a constant density dominating set , it remains to disseminate the message using only dominators . in order for the set of dominators to derive the connectivity properties of the original graph", ", we need that @xmath30 forms a @xmath540-bounded independence _", "metric _ space .    in the static spontaneous setting", ", there is a uniform algorithm that performs broadcast in @xmath541 rounds , w.h.p .", "[ thm : static - spont - bcast ]    the algorithm for spontaneous broadcast consists of two stages : 1 . compute a constant density @xmath542-dominating set @xmath543 , 2 . transmit the message using only the nodes in @xmath543 .    [ [ dominating - set . ] ] dominating set .", "+ + + + + + + + + + + + + + +    the dominating set is constructed by running bcast@xmath240 in the spontaneous mode , i.e. all nodes start running the algorithm simultaneously .", "let @xmath543 be the set of nodes that stop the algorithm by .", "since the dominated nodes use @xmath544 as a stopping condition , @xmath543 is a @xmath542-dominating set .", "moreover , @xmath543 is a @xmath545-packing , which implies that each in - ball @xmath46 of radius @xmath546 contains at most @xmath547 dominators .", "in particular each node @xmath20 is dominated by at most constant number of dominators .", "the proof the algorithm terminates after at most @xmath200 rounds is almost identical to the proof of theorem  [ thm : broadcast ] .", "[ [ broadcast . ] ] broadcast .", "+ + + + + + + + + +    recall that we assume that the communication graph is connected and has diameter @xmath243 .", "also , since @xmath548 is a metric , the graph is undirected .", "the broadcast part is as follows : in the first round , the source node transmits the message to its neighbors ; each dominator @xmath20 , upon receiving the message , transmits it in each round with probability @xmath232 until detecting ( @xmath183 ) , where @xmath235 is a small enough constant .", "note that as soon as all dominators successfully transmit at least once , all nodes will get the message .", "moreover , if the constant @xmath235 is small enough , then the two algorithms can be run simultaneously : the key point is that the number of dominators in each @xmath549-neighborhood is bounded by a constant ( not depending on @xmath235 ) .", "this ensures that the nodes _ need not know @xmath16 _ , in order to coordinate the two algorithms .", "it remains to show that the message will get to all dominators in @xmath244 rounds using the broadcast algorithm .", "consider a graph @xmath129 defined over the dominating set @xmath543 , where for every pair of nodes @xmath550 , @xmath551 form an edge if @xmath552 .", "the diameter of @xmath129 is at most @xmath243 .", "note that for every pair of nodes @xmath553 with @xmath554 , the corresponding dominators @xmath555 and @xmath556 are adjacent in @xmath129 . indeed , by the definition of the dominating set ,", "we have @xmath557 let @xmath558 be two arbitrary nodes in @xmath543 and let @xmath19 be the path of length at most @xmath243 in @xmath238 connecting nodes @xmath555 and @xmath556 . by the observation above ,", "if we replace each node @xmath559 with its dominator @xmath560 , we obtain a path @xmath561 of length @xmath46 in @xmath129 , connecting @xmath555 and @xmath556 .", "the completes the proof of the claim .    for every transmitting node @xmath562 ,", "the probability that @xmath274 delivers its message to all its neighbors in @xmath129 is @xmath563 in each round @xmath56 , if constant @xmath235 is sufficiently small .", "let @xmath39 denote the set of neighbors of node @xmath274 in @xmath129 , i.e. the nodes of @xmath543 that are at distance at most @xmath564 from @xmath274 .", "it suffices to show that all nodes in @xmath39 receive the message in a fixed round @xmath56 where @xmath274 transmits with @xmath563 probability .", "this event holds if the interference at @xmath274 is no more than @xmath68  event @xmath565 , and no other node transmits in @xmath566 ", "event @xmath567 , in round @xmath56 .", "by the properties of the dominating set , the contention in a ball of radius @xmath568 is at most @xmath569 .", "similarly to the proof of prop .", "[ pr : pinterferenceboundl ] , it can be shown that the expected interference by nodes in @xmath570 is @xmath571 .", "thus , by markov inequality , @xmath572 if @xmath235 is small enough", ". on the other hand , since the contention in @xmath566 is @xmath571 , @xmath573 follows by lemma  [ ieq ] .", "thus , @xmath574 .", "given that each informed node in @xmath543 has probability at least @xmath575 of successfully broadcasting the message , the rest of the proof essentially follows along the lines of the proof of  ( * ? ? ?", "* lemma 6 ) .", "we present a sketch of the proof for completeness of the argument .", "let @xmath576 be the set of nodes in @xmath543 that have been informed by round @xmath56 , where @xmath577 contains only the source node .", "let us fix a node @xmath578 and let @xmath579 be the distance ( in graph @xmath129 ) from @xmath1 to the nearest node in @xmath576 .", "note that @xmath580 .", "the difference @xmath581 is the progress made in round @xmath56 and is a bernoulli random variable with @xmath582\\ge \\eta$ ] for all @xmath56 when @xmath583 and @xmath584 otherwise .", "let @xmath585 be a random variable that has the same distribution as @xmath586 when @xmath583 and is an i.i.d .", "bernoulli random variable with mean @xmath587 otherwise .", "let @xmath588 .", "note that node @xmath1 has been informed by round @xmath56 iff @xmath589 .", "thus , we need to bound the probability @xmath590 .", "let @xmath591 .", "it is easy to show that the sequence @xmath592 is a submartingale .", "moreover , for a round @xmath593 and constant @xmath594 , @xmath595 implies that @xmath596 .", "since the sequence @xmath592 is a submartingale , we can apply azuma - hoeffding bound to show that @xmath597 .", "now the theorem follows by union bound over all nodes , by choosing the constant @xmath260 large enough ."], ["* theorem  [ thm : brlowerbound ] * _ for every ( possibly randomized ) broadcast algorithm @xmath246 that uses neither node coordinates nor primitive , there is a @xmath247-bounded - independence metric space where @xmath246 needs @xmath248 rounds to do broadcast in a @xmath249-broadcastable network , even if the nodes have and primitives and operate spontaneously . _", "assume that @xmath598 .", "we present the construction using a distance function @xmath548 : it is then straightforward to construct the corresponding path - loss matrix .", "denote @xmath599 .", "first , assume the nodes operate non - spontaneously , i.e. a non - source node may need to receive a message in order to start participating in a protocol .", "recall that @xmath600 .", "consider @xmath16 points @xmath601 , such that for every @xmath602 , it holds that @xmath603 , @xmath604 , @xmath605 and @xmath606 , where @xmath607 ( see the diagram in fig .", "[ fig : nonspon ] ) .", "clearly , this set of points forms a @xmath608-bounded independence metric space .", "we place @xmath16 wireless nodes at distinct points @xmath601 uniformly at random .", "let @xmath195 be the node at @xmath609 .", "we assume , further , that communication only happens according to ; in particular , if the interference at a node is more than @xmath68 then _ none _ of its neighbors receives its transmission .", "note that broadcast in this network can be completed in 2 steps , starting at any point .", "note also that @xmath610 can not be directly reached from nodes @xmath195 with @xmath611 . moreover ,", "if at least @xmath612 nodes @xmath195 with @xmath611 transmit simultaneously , _ no node _ receives a message , including the potential communication between @xmath613 and @xmath610 .", "this follows by observing that @xmath614 : the signal power at the neighboring nodes must be more than the interference threshold , otherwise a node could receive two signals of the same power .", "assume @xmath195 with some @xmath611 is the source .", "at the first round of the algorithm , nodes @xmath615 receive the message .", "let @xmath616 denote the event that @xmath613 transmits and no more than @xmath617 nodes @xmath195 with @xmath611 transmit in round @xmath56 . by the observations above , @xmath610 will not receive the message until @xmath616 happens .", "moreover , during the subsequent steps of the algorithm , until @xmath616 happens for the first time , all nodes at @xmath615 will have the same history and will be symmetric with respect to and primitives .    it remains to bound the expected time until @xmath616 happens for the first time .", "we can assume w.l.o.g .", "that in each round there are no more than @xmath187 nodes transmitting . as discussed above ,", "if @xmath610 is not informed in such a round , then the nodes that transmitted in the given round can learn at best that none of them is at @xmath613 and stop transmitting thereafter .", "thus , we can assume that after each unsuccessful round there are at most @xmath618 nodes that stop transmitting and the probability @xmath619 of success at round @xmath620 ( if no success occurred before round @xmath33 ) is at most @xmath621 where the numerator counts the number subsets of an @xmath622-element set that contain a fixed element ( the node @xmath613 ) and no more than @xmath187 elements in total , and the denominator counts all subsets containing no more than @xmath187 elements .", "a further straightforward calculation shows that the expected number of rounds until the first success is @xmath248 ( see e.g.  ( * ? ? ?"]]}
{"article_id": "1311.0269", "article_text": ["the computing requirements for high energy physics ( hep ) projects like the large hadron collider ( lhc )  @xcite at the european laboratory for particle physics ( cern ) in geneva , switzerland are larger than can be met with resources deployed in a single computing center .", "this has led to the construction of a global distributed computing system known as the worldwide lhc computing grid ( wlcg )  @xcite , which brings together resources from nearly 160 computer centers in 35 countries .", "computing at this scale has been used , for example , by the cms  @xcite and atlas  @xcite experiments for the discovery of the higgs boson  @xcite . to achieve this and other results the cms experiment , for example , typically used during 2012 a processing capacity between 80,000 and 100,000 x86 - 64 cores from the wlcg .", "further discoveries are possible in the next decade as the lhc moves to its design energy and increases the machine luminosity .", "however , increases in dataset sizes by 2 - 3 orders of magnitude ( and commensurate processing capacity ) will eventually be required to realize the full potential of this scientific instrument .", "the scale and longevity of the lhc computing require continual r&d into new technologies which may be relevant in the coming years .", "in this paper we report on our investigations into one such technology , low power arm processors , for scientific computing .", "the construction of the wlcg was greatly facilitated by the convergence around the year 2000 on commodity x86 hardware and the standardized use of linux as the operating system for scientific computing clusters .", "even if multiple generations of x86 hardware ( and hardware from both intel and amd ) are provided in the various computer centers , this was a far simpler situation than the typical mix of proprietary unix operating systems and processors .    until around 2005", ", a combination of increased instruction level parallelism and ( in particular ) processor clock frequency increases insured that performance gains expected from moore s law would be seen by single sequential applications running on a single processor . the combination of linux , commodity x86 processors and moore s law gains for sequential applications made for a simple software environment .    since around 2005", ", however , processors have hit scaling limits , largely driven by overall power consumption  @xcite .", "the first large change in commercial processor products as a result of these limits was the introduction of `` multicore '' cpus , with more than one functional processor on a chip .", "at the same time clock frequencies ceased to increase with each processor generation and indeed were often reduced relative to the peak .", "the result of this was that one could no longer expect that single , sequential applications would run faster on newer processors .", "however in the first approximation , the individual cores in the multicore cpus appeared more or less like the single standalone processors used previously .", "most large scientific applications ( hpc / parallel or high throughput ) run in any case on clusters and the additional cores are often simply scheduled as if they were additional nodes in the cluster .", "this allows overall throughput to continue to scale even if that of a single application does not .", "it has several disadvantages , though , in that a number of things that would have been roughly constant over subsequent purchasing generations in a given cluster ( with a more or less fixed number of rack slots , say ) now grow with each generation of machines in the computer center .", "this includes the total memory required in each box , the number of open files and/or database connections , increasing number of independent ( and incoherent ) i / o streams , the number of jobs handled by batch schedulers , etc .", "the specifics vary from application to application , but potential difficulties in continually scaling these system parameters puts some pressure on applications to make code changes in response , for example by introducing thread - level parallelism where it did not previously exist .", "there is moreover a more general expectation that the limit of power consumption on future moore s law scaling will lead to more profound changes going forward . in particular ,", "the power hungry x86 - 64 `` large '' cores of today will likely be replaced by simpler and less power hungry `` small '' cores with a greater emphasis on aggregate throughput performance per watt , rather than just raw performance .", "this has rekindled interest in solutions that would lead back to a more heterogeneous computing environment .", "a strong contender for this evolving low power ( high performance / watt ) server market is the arm processor  @xcite due its nearly complete dominance in the low power mobile market for smartphones and tablets , which has also seen dramatic growth since around 2005 .", "the size of the mobile market , and its traditional focus on low power , has led to interest in using these processors also in a server environment .", "as such arm - based server products such as boston viridis  @xcite are starting to appear .", "the arm processor has a long history  @xcite dating back to acorn computers and the early days of personal computers .", "it is a risc processor and the current generation ( armv7 ) , used in most high - end mobile devices and the new server products , is a 32bit processor .", "we are interested in the `` a '' series of general purpose `` application '' processors .", "( arm also produces `` r '' and `` m '' series designs for use in real - time and embedded microcontroller environments , respectively . )", "a 64bit version of the arm processor ( armv8 ) has also been designed and is expected to appear in server products from fall 2013 or early 2014 .", "intel has also announced the development of products ( silvermont ) aimed at a low power market .", "however , much like the current mobile market , it is nt positioned to dominate the low power server market as it has dominated the commodity processor market in the past .", "as the arm processors are general purpose and run linux , only a standard port of the cms software is required , similar to what was done , for example , to port the cms software from 32bit ( ia32 ) to 64bit ( x86 - 64 ) .", "such a port is reasonably straightforward relative to the changes required to use other high performance per watt solutions ( e.g. gpgpu s , which require actual software rewrites ) , thus the effort required for these initial investigations was also relatively modest .", "for the tests described in this paper we used a low - cost development board , the odroid - u2  @xcite .", "the processor on the board is an exynos 4412 prime , a system - on - chip ( soc ) produced by samsung for use in mobile devices .", "it is a quad - core cortex a9 armv7 processor operating at 1.7ghz with 2 gb of lp - ddr2 memory .", "the processor also contains an arm mali-400 quad - core gpu accelerator , although that was not used for the work described in this paper .", "the board has emmc and microsd slots , two usb 2.0 ports and 10/100mbps ethernet with an rj-45 port .", "power is provided a 5v dc power adaptor .", "the cost of the board alone was $ 89 and with the relevant accessories ( cables , a cooling fan , a 64 gb emmc storage module , etc . )", "the total cost was $ 233 .", "this extremely modest cost permitted us to do meaningful initial investigations without investing in a full - fledged server .", "all build tests were done using a 500 gb @xmath0 ata disk connected via usb .", "runtime tests were done with output written to the emmc storage .    for the linux operating system on the board we used fedora 18 arm remix with kernel version 3.0.75 ( provided by hardkernel , the vendor for the odroid - u2 board ) due to its similarities to scientific linux cern ( slc ) .", "it is fully hard float capable and uses the floating point unit on the soc .", "the kernel was reconfigured to enable swap devices / files , which is required for cmssw compilation .", "a 4 gb swap file was used in our build environment .    in order to compare results from the arm board we also used two typical x86 - 64 servers currently deployed at cern .", "the first is a dual quad - core intel xeon l5520 @xmath1 2.27 ghz ( nehalem ) with 24 gb of memory .", "the second is dual hexa - core intel xeon e5 - 2630 @xmath1 2.00ghz ( sandy bridge ) with 64 gb of memory .", "both machines were equipped with a large local disk for output and used software installed on an afs filesystem at cern .", "these machines were purchased about three years apart and very roughly represent the range of x86 - 64 hardware being operated at the time of our arm tests .", "the software written by the cms collaboration itself ( cmssw ) consists of approximately 3.6 m source lines of code ( sloc ) , as measured by the sloccount tool  @xcite .", "the entire software stack includes also 125 `` external '' packages , including hep software packages like root  @xcite ( @xmath21.7 m sloc ) , geant4  @xcite ( @xmath21.2 m sloc ) and many general open source packages : gcc , boost , qt , python , etc .      for the compilation and linking of this large set of software we considered three options :", "compilation directly on the odroid - u2 board itself , cross compilation for the odroid - u2 board from an x86 - 64 host and compilation for the odroid - u2 board from an emulation environment such as qemu  @xcite running on an x86 - 64 host    our experience with arm emulation with qemu prior to purchasing the odroid - u2 led us to believe that it was not yet quite mature enough to provide a stable build environment sufficient for our needs .", "even though it was clear that the small odroid - u2 development board is much less powerful than most standard x86 - 64 servers , we decided to attempt compilation of the full stack directly on the arm board .", "this was motivated by the idea that should we eventually adopt arm as a standard production architecture , we would probably aim for direct compilation on arm servers .      during the port to armv7 we had to resolve a number of issues in the software and", "build recipes :    * oracle : * we did not have oracle libraries for the armv7 architecture . by construction , however , no standard grid - capable cms applications depend on oracle . thus this affected only a small number of cmssw packages used primarily for writing calibrations into the oracle database at cern .", "cms applications which read calibrations do not interact directly with oracle , but instead access the calibrations via the frontier web service  @xcite , with no direct dependency on oracle .", "* configuration : * there were a number of minor configuration issues , for example :    * the -m32 and -m64 options do not work .", "( on x86 - 64 cms had made a complete transition to 64bit a couple of years prior to this work . ) * in a number of places there were x86-based assumptions leading to attempts to configure for x86 - 64 sse and avx    * memory use : * compilation of some translation units ( primarily generated root dictionaries ) exhausted the virtual memory address space . here", "the solution was simply to refactor the dictionaries .", "* root cintex : * a patch was needed for the cintex trampoline in root to support the arm architecture .", "this was submitted to the root developers .", "* signedness : * x86 - 64 and arm treat the signedness of char / bit - fields differently , intel is signed and arm is unsigned by default . this was dealt with on arm by imposing the use of the compiler options -fsigned - char and -fsigned - bitfields , along with a few small code modifications to fix non - portable code .", "* dictionary generation and i / o : * there were several bugs in the root i / o infrastructure , as well as non cross - platform types ( that crept in after cms transitioned to 64bit on x86 - 64 and stopped regularly producing 32bit builds ) , that at time of these tests prevented us from properly reading and writing root files .      with changes to the build recipes resulting from fixing these issues we were able to build all of the standard cmssw externals and 99% of the cmssw code .", "the cmssw code which did not build was the small subset requiring oracle .", "this demonstrates the advantages of relying primarily on open source software and , when closed source software can not be avoided , carefully restricting the code which can depend on the closed source libraries .    after making the changes described above", ", we achieved the following results for the total build times directly on the odroid - u2 board :    * @xmath24h - compilation of a `` bootstrap '' kit consisting of the gcc compiler ( version 4.8.0 ) and a small set of packages ( rpm , apt , zlib , ncurses , nspr , sqlite , etc . ) that we use for packaging and distributing the results of our builds * @xmath212h - compilation of all of the 125 external packages not included in the `` bootstrap '' kit * @xmath225.5h - compilation of the cmssw code itself as well as a set of generated root dictionaries    these are quite reasonable results .", "taking into account that the externals do not change frequently , these results are already very close to consistent with an eventual `` nightly '' integration build where we compile the very latest versions of all of the cmssw code , but reuse pre - existing builds of the externals .      for a run time test and benchmark we used an actual cms application from the build described above rather than a synthetic benchmark .", "this application performs a monte carlo simulation of 8 tev lhc minimum bias events using pythia8  @xcite ( event generation ) followed by simulation with geant4  @xcite . due to the problem mentioned above with generation of the dictionaries used for data input / output ,", "output was turned off .", "the data output however has little effect on the total cpu time ( and thus these benchmarks ) as the cpu cost is heavily dominated by the geant4 simulation .", "the application itself is single - threaded ( sequential ) and thus ran on a single core at a time . to simplify testing we ran only a single job at a time on each machine with the aim of measuring the single core performance .", "multiple tests were performed of sufficient length to estimate properly an average per event time and dedicated tests were run to subtract off job startup times .", "a proper validation of the application output was complicated by the lack of an output file , but checks done by enabling printout indicated consistency between arm and x86 - 64 .", "the results for performance ( events simulated per minute per core ) are shown in table  [ tab : results ] .", "[ tab : results ]    .results of run time tests [ cols=\"<,^,^,^,^ \" , ]     in order to calculate values for the performance per watt , it would be a bit misleading to compare the total power used by real ( x86 - 64 ) servers with a small development board . to get a better estimate , which more directly compares the processors themselves", ", we used the `` thermal design power '' ( tdp ) numbers for the processors themselves . here", "tdp numbers for the two intel xeon x86 - 64 processors were taken from their website  @xcite .", "for the odroid - u2 we were not able to find specific tdp numbers , but based on our own measurements we have estimated the tdp - equivalent fully loaded power at about 4w . using these values we have calculated an equivalent performance ( in events per minute ) per watt in the last column of table  [ tab : results ] . a clear advantage in terms of performance per watt is seen for the arm processor .", "we have done a port of the entire cms software stack , including 3.6 m sloc of cms - written code in cmssw and 125 external support packages , to the armv7-based odroid - u2 development board .", "we chose to build directly on the development board itself and measured build times consistent with operating a `` nightly '' build of the cms software .", "we report performance and performance per ( tdp ) watt numbers both for the armv7 board and for two typical x86 - 64 servers at cern . on the basis of these results", "we conclude that arm - based low power servers , if they succeed in the market , show great potential for use with typical hep high throughput computing applications .", "this work was partially supported by the national science foundation , under cooperative agreement phy-1120138 , and by the u.s .", "department of energy .", "9    evans l and bryant p 2008 lhc machine _ jinst _ * 3 * s08001 bird i 2011 computing for the large hadron collider _ annual review of nuclear and particle science _ * 61 * 99 - 118 chatrchyan s et al ( cms collaboration ) 2008 the cms experiment at the cern lhc _ jinst _ * 3 * s08004 aad g et al ( atlas collaboration ) 2008 the atlas experiment at the cern large hadron collider _ jinst _ * 3 * s08003 chatrchyan s et al ( cms collaboration ) 2012 observation of a new boson at a mass of 125 gev with the cms experiment at the lhc _ phys.lett . _", "* b716 * 30 - 61 aad g et al ( atlas collaboration ) 2012 observation of a new particle in the search for the standard model higgs boson with the atlas detector at the lhc", "_ phys.lett . _", "* b716 * 1 - 29 fuller s h and millet l i ( editors ) 2011 _ the future of computing performance : game over or next level ? _ the national academies press .", "puechased via the hardkernel website : http://www.hardkernel.com source code and documentation available at http://www.dwheeler.com/sloccount/ http://root.cern.ch agostinelli s et al 2003 geant4 - a simulation toolkit _ nuclear instruments and methods in physics research _", "* a 506 * 250 - 303 http://ark.intel.com/products/40201/ and http://ark.intel.com/products/64586/ http://www.qemu.org blumenfeld b , dykstra d , lueking l and wicklund e 2008 cms conditions data access using frontier _ j. phys .", ". ser . _ * 119 * 072007 sjostrand t , mrenna s and skands p 2008 a brief introduction to pythia 8.1 _ comput .", "_ * 178 * 852 - 867"], "abstract_text": ["<S> power efficiency is becoming an ever more important metric for both high performance and high throughput computing . </S>", "<S> over the course of next decade it is expected that flops / watt will be a major driver for the evolution of computer architecture . </S>", "<S> servers with large numbers of arm processors , already ubiquitous in mobile computing , are a promising alternative to traditional x86 - 64 computing . </S>", "<S> we present the results of our initial investigations into the use of arm processors for scientific computing applications . </S>", "<S> in particular we report the results from our work with a current generation armv7 development board to explore arm - specific issues regarding the software development environment , operating system , performance benchmarks and issues for porting high energy physics software . </S>"], "labels": null, "section_names": ["introduction", "processor architectures and power efficiency", "arm processors", "test setup", "experimental results", "conclusions", "acknowledgment", "references"], "sections": [["the computing requirements for high energy physics ( hep ) projects like the large hadron collider ( lhc )  @xcite at the european laboratory for particle physics ( cern ) in geneva , switzerland are larger than can be met with resources deployed in a single computing center .", "this has led to the construction of a global distributed computing system known as the worldwide lhc computing grid ( wlcg )  @xcite , which brings together resources from nearly 160 computer centers in 35 countries .", "computing at this scale has been used , for example , by the cms  @xcite and atlas  @xcite experiments for the discovery of the higgs boson  @xcite . to achieve this and other results the cms experiment , for example , typically used during 2012 a processing capacity between 80,000 and 100,000 x86 - 64 cores from the wlcg .", "further discoveries are possible in the next decade as the lhc moves to its design energy and increases the machine luminosity .", "however , increases in dataset sizes by 2 - 3 orders of magnitude ( and commensurate processing capacity ) will eventually be required to realize the full potential of this scientific instrument .", "the scale and longevity of the lhc computing require continual r&d into new technologies which may be relevant in the coming years .", "in this paper we report on our investigations into one such technology , low power arm processors , for scientific computing ."], ["the construction of the wlcg was greatly facilitated by the convergence around the year 2000 on commodity x86 hardware and the standardized use of linux as the operating system for scientific computing clusters .", "even if multiple generations of x86 hardware ( and hardware from both intel and amd ) are provided in the various computer centers , this was a far simpler situation than the typical mix of proprietary unix operating systems and processors .    until around 2005", ", a combination of increased instruction level parallelism and ( in particular ) processor clock frequency increases insured that performance gains expected from moore s law would be seen by single sequential applications running on a single processor . the combination of linux , commodity x86 processors and moore s law gains for sequential applications made for a simple software environment .    since around 2005", ", however , processors have hit scaling limits , largely driven by overall power consumption  @xcite .", "the first large change in commercial processor products as a result of these limits was the introduction of `` multicore '' cpus , with more than one functional processor on a chip .", "at the same time clock frequencies ceased to increase with each processor generation and indeed were often reduced relative to the peak .", "the result of this was that one could no longer expect that single , sequential applications would run faster on newer processors .", "however in the first approximation , the individual cores in the multicore cpus appeared more or less like the single standalone processors used previously .", "most large scientific applications ( hpc / parallel or high throughput ) run in any case on clusters and the additional cores are often simply scheduled as if they were additional nodes in the cluster .", "this allows overall throughput to continue to scale even if that of a single application does not .", "it has several disadvantages , though , in that a number of things that would have been roughly constant over subsequent purchasing generations in a given cluster ( with a more or less fixed number of rack slots , say ) now grow with each generation of machines in the computer center .", "this includes the total memory required in each box , the number of open files and/or database connections , increasing number of independent ( and incoherent ) i / o streams , the number of jobs handled by batch schedulers , etc .", "the specifics vary from application to application , but potential difficulties in continually scaling these system parameters puts some pressure on applications to make code changes in response , for example by introducing thread - level parallelism where it did not previously exist .", "there is moreover a more general expectation that the limit of power consumption on future moore s law scaling will lead to more profound changes going forward . in particular ,", "the power hungry x86 - 64 `` large '' cores of today will likely be replaced by simpler and less power hungry `` small '' cores with a greater emphasis on aggregate throughput performance per watt , rather than just raw performance .", "this has rekindled interest in solutions that would lead back to a more heterogeneous computing environment ."], ["a strong contender for this evolving low power ( high performance / watt ) server market is the arm processor  @xcite due its nearly complete dominance in the low power mobile market for smartphones and tablets , which has also seen dramatic growth since around 2005 .", "the size of the mobile market , and its traditional focus on low power , has led to interest in using these processors also in a server environment .", "as such arm - based server products such as boston viridis  @xcite are starting to appear .", "the arm processor has a long history  @xcite dating back to acorn computers and the early days of personal computers .", "it is a risc processor and the current generation ( armv7 ) , used in most high - end mobile devices and the new server products , is a 32bit processor .", "we are interested in the `` a '' series of general purpose `` application '' processors .", "( arm also produces `` r '' and `` m '' series designs for use in real - time and embedded microcontroller environments , respectively . )", "a 64bit version of the arm processor ( armv8 ) has also been designed and is expected to appear in server products from fall 2013 or early 2014 .", "intel has also announced the development of products ( silvermont ) aimed at a low power market .", "however , much like the current mobile market , it is nt positioned to dominate the low power server market as it has dominated the commodity processor market in the past .", "as the arm processors are general purpose and run linux , only a standard port of the cms software is required , similar to what was done , for example , to port the cms software from 32bit ( ia32 ) to 64bit ( x86 - 64 ) .", "such a port is reasonably straightforward relative to the changes required to use other high performance per watt solutions ( e.g. gpgpu s , which require actual software rewrites ) , thus the effort required for these initial investigations was also relatively modest ."], ["for the tests described in this paper we used a low - cost development board , the odroid - u2  @xcite .", "the processor on the board is an exynos 4412 prime , a system - on - chip ( soc ) produced by samsung for use in mobile devices .", "it is a quad - core cortex a9 armv7 processor operating at 1.7ghz with 2 gb of lp - ddr2 memory .", "the processor also contains an arm mali-400 quad - core gpu accelerator , although that was not used for the work described in this paper .", "the board has emmc and microsd slots , two usb 2.0 ports and 10/100mbps ethernet with an rj-45 port .", "power is provided a 5v dc power adaptor .", "the cost of the board alone was $ 89 and with the relevant accessories ( cables , a cooling fan , a 64 gb emmc storage module , etc . )", "the total cost was $ 233 .", "this extremely modest cost permitted us to do meaningful initial investigations without investing in a full - fledged server .", "all build tests were done using a 500 gb @xmath0 ata disk connected via usb .", "runtime tests were done with output written to the emmc storage .    for the linux operating system on the board we used fedora 18 arm remix with kernel version 3.0.75 ( provided by hardkernel , the vendor for the odroid - u2 board ) due to its similarities to scientific linux cern ( slc ) .", "it is fully hard float capable and uses the floating point unit on the soc .", "the kernel was reconfigured to enable swap devices / files , which is required for cmssw compilation .", "a 4 gb swap file was used in our build environment .    in order to compare results from the arm board we also used two typical x86 - 64 servers currently deployed at cern .", "the first is a dual quad - core intel xeon l5520 @xmath1 2.27 ghz ( nehalem ) with 24 gb of memory .", "the second is dual hexa - core intel xeon e5 - 2630 @xmath1 2.00ghz ( sandy bridge ) with 64 gb of memory .", "both machines were equipped with a large local disk for output and used software installed on an afs filesystem at cern .", "these machines were purchased about three years apart and very roughly represent the range of x86 - 64 hardware being operated at the time of our arm tests ."], ["the software written by the cms collaboration itself ( cmssw ) consists of approximately 3.6 m source lines of code ( sloc ) , as measured by the sloccount tool  @xcite .", "the entire software stack includes also 125 `` external '' packages , including hep software packages like root  @xcite ( @xmath21.7 m sloc ) , geant4  @xcite ( @xmath21.2 m sloc ) and many general open source packages : gcc , boost , qt , python , etc .      for the compilation and linking of this large set of software we considered three options :", "compilation directly on the odroid - u2 board itself , cross compilation for the odroid - u2 board from an x86 - 64 host and compilation for the odroid - u2 board from an emulation environment such as qemu  @xcite running on an x86 - 64 host    our experience with arm emulation with qemu prior to purchasing the odroid - u2 led us to believe that it was not yet quite mature enough to provide a stable build environment sufficient for our needs .", "even though it was clear that the small odroid - u2 development board is much less powerful than most standard x86 - 64 servers , we decided to attempt compilation of the full stack directly on the arm board .", "this was motivated by the idea that should we eventually adopt arm as a standard production architecture , we would probably aim for direct compilation on arm servers .      during the port to armv7 we had to resolve a number of issues in the software and", "build recipes :    * oracle : * we did not have oracle libraries for the armv7 architecture . by construction , however , no standard grid - capable cms applications depend on oracle . thus this affected only a small number of cmssw packages used primarily for writing calibrations into the oracle database at cern .", "cms applications which read calibrations do not interact directly with oracle , but instead access the calibrations via the frontier web service  @xcite , with no direct dependency on oracle .", "* configuration : * there were a number of minor configuration issues , for example :    * the -m32 and -m64 options do not work .", "( on x86 - 64 cms had made a complete transition to 64bit a couple of years prior to this work . ) * in a number of places there were x86-based assumptions leading to attempts to configure for x86 - 64 sse and avx    * memory use : * compilation of some translation units ( primarily generated root dictionaries ) exhausted the virtual memory address space . here", "the solution was simply to refactor the dictionaries .", "* root cintex : * a patch was needed for the cintex trampoline in root to support the arm architecture .", "this was submitted to the root developers .", "* signedness : * x86 - 64 and arm treat the signedness of char / bit - fields differently , intel is signed and arm is unsigned by default . this was dealt with on arm by imposing the use of the compiler options -fsigned - char and -fsigned - bitfields , along with a few small code modifications to fix non - portable code .", "* dictionary generation and i / o : * there were several bugs in the root i / o infrastructure , as well as non cross - platform types ( that crept in after cms transitioned to 64bit on x86 - 64 and stopped regularly producing 32bit builds ) , that at time of these tests prevented us from properly reading and writing root files .      with changes to the build recipes resulting from fixing these issues we were able to build all of the standard cmssw externals and 99% of the cmssw code .", "the cmssw code which did not build was the small subset requiring oracle .", "this demonstrates the advantages of relying primarily on open source software and , when closed source software can not be avoided , carefully restricting the code which can depend on the closed source libraries .    after making the changes described above", ", we achieved the following results for the total build times directly on the odroid - u2 board :    * @xmath24h - compilation of a `` bootstrap '' kit consisting of the gcc compiler ( version 4.8.0 ) and a small set of packages ( rpm , apt , zlib , ncurses , nspr , sqlite , etc . ) that we use for packaging and distributing the results of our builds * @xmath212h - compilation of all of the 125 external packages not included in the `` bootstrap '' kit * @xmath225.5h - compilation of the cmssw code itself as well as a set of generated root dictionaries    these are quite reasonable results .", "taking into account that the externals do not change frequently , these results are already very close to consistent with an eventual `` nightly '' integration build where we compile the very latest versions of all of the cmssw code , but reuse pre - existing builds of the externals .      for a run time test and benchmark we used an actual cms application from the build described above rather than a synthetic benchmark .", "this application performs a monte carlo simulation of 8 tev lhc minimum bias events using pythia8  @xcite ( event generation ) followed by simulation with geant4  @xcite . due to the problem mentioned above with generation of the dictionaries used for data input / output ,", "output was turned off .", "the data output however has little effect on the total cpu time ( and thus these benchmarks ) as the cpu cost is heavily dominated by the geant4 simulation .", "the application itself is single - threaded ( sequential ) and thus ran on a single core at a time . to simplify testing we ran only a single job at a time on each machine with the aim of measuring the single core performance .", "multiple tests were performed of sufficient length to estimate properly an average per event time and dedicated tests were run to subtract off job startup times .", "a proper validation of the application output was complicated by the lack of an output file , but checks done by enabling printout indicated consistency between arm and x86 - 64 .", "the results for performance ( events simulated per minute per core ) are shown in table  [ tab : results ] .", "[ tab : results ]    .results of run time tests [ cols=\"<,^,^,^,^ \" , ]     in order to calculate values for the performance per watt , it would be a bit misleading to compare the total power used by real ( x86 - 64 ) servers with a small development board . to get a better estimate , which more directly compares the processors themselves", ", we used the `` thermal design power '' ( tdp ) numbers for the processors themselves . here", "tdp numbers for the two intel xeon x86 - 64 processors were taken from their website  @xcite .", "for the odroid - u2 we were not able to find specific tdp numbers , but based on our own measurements we have estimated the tdp - equivalent fully loaded power at about 4w . using these values we have calculated an equivalent performance ( in events per minute ) per watt in the last column of table  [ tab : results ] . a clear advantage in terms of performance per watt is seen for the arm processor ."], ["we have done a port of the entire cms software stack , including 3.6 m sloc of cms - written code in cmssw and 125 external support packages , to the armv7-based odroid - u2 development board .", "we chose to build directly on the development board itself and measured build times consistent with operating a `` nightly '' build of the cms software .", "we report performance and performance per ( tdp ) watt numbers both for the armv7 board and for two typical x86 - 64 servers at cern . on the basis of these results", "we conclude that arm - based low power servers , if they succeed in the market , show great potential for use with typical hep high throughput computing applications ."], ["this work was partially supported by the national science foundation , under cooperative agreement phy-1120138 , and by the u.s .", "department of energy ."], ["9    evans l and bryant p 2008 lhc machine _ jinst _ * 3 * s08001 bird i 2011 computing for the large hadron collider _ annual review of nuclear and particle science _ * 61 * 99 - 118 chatrchyan s et al ( cms collaboration ) 2008 the cms experiment at the cern lhc _ jinst _ * 3 * s08004 aad g et al ( atlas collaboration ) 2008 the atlas experiment at the cern large hadron collider _ jinst _ * 3 * s08003 chatrchyan s et al ( cms collaboration ) 2012 observation of a new boson at a mass of 125 gev with the cms experiment at the lhc _ phys.lett . _", "* b716 * 30 - 61 aad g et al ( atlas collaboration ) 2012 observation of a new particle in the search for the standard model higgs boson with the atlas detector at the lhc", "_ phys.lett . _", "* b716 * 1 - 29 fuller s h and millet l i ( editors ) 2011 _ the future of computing performance : game over or next level ? _ the national academies press .", "puechased via the hardkernel website : http://www.hardkernel.com source code and documentation available at http://www.dwheeler.com/sloccount/ http://root.cern.ch agostinelli s et al 2003 geant4 - a simulation toolkit _ nuclear instruments and methods in physics research _", "* a 506 * 250 - 303 http://ark.intel.com/products/40201/ and http://ark.intel.com/products/64586/ http://www.qemu.org blumenfeld b , dykstra d , lueking l and wicklund e 2008 cms conditions data access using frontier _ j. phys .", ". ser . _ * 119 * 072007 sjostrand t , mrenna s and skands p 2008 a brief introduction to pythia 8.1 _ comput .", "_ * 178 * 852 - 867"]]}
